2024-01-07 13:38:15 INFO     INITIALIZE GRID SEARCHER: 4 configs to try
2024-01-07 13:38:15 INFO     ## 1st RUN: Configuration 0/4 ##
2024-01-07 13:38:15 INFO     initialize model trainer
2024-01-07 13:38:15 INFO     initialize checkpoint at small_finetuned_modified_ckpt/model_pzcecq
2024-01-07 13:38:15 INFO     hyperparameters
2024-01-07 13:38:15 INFO     	 * dataset_path: StellarMilk/newsqa_modified
2024-01-07 13:38:15 INFO     	 * dataset_name: default
2024-01-07 13:38:15 INFO     	 * input_types: ['paragraph']
2024-01-07 13:38:15 INFO     	 * output_types: ['questions_answers']
2024-01-07 13:38:15 INFO     	 * prefix_types: ['qag']
2024-01-07 13:38:15 INFO     	 * model: lmqg/t5-small-squad-qag
2024-01-07 13:38:15 INFO     	 * max_length: 512
2024-01-07 13:38:15 INFO     	 * max_length_output: 512
2024-01-07 13:38:15 INFO     	 * epoch: 10
2024-01-07 13:38:15 INFO     	 * batch: 2
2024-01-07 13:38:15 INFO     	 * lr: 1e-05
2024-01-07 13:38:15 INFO     	 * fp16: False
2024-01-07 13:38:15 INFO     	 * random_seed: 1
2024-01-07 13:38:15 INFO     	 * gradient_accumulation_steps: 4
2024-01-07 13:38:15 INFO     	 * label_smoothing: 0.15
2024-01-07 13:38:15 INFO     initialize checkpoint with lmqg/t5-small-squad-qag
2024-01-07 13:38:35 INFO     use spaCy answer extraction model: positionrank
2024-01-07 13:39:42 INFO     Model `lmqg/t5-small-squad-qag`
2024-01-07 13:39:42 INFO     	 * Num of GPU in use: 1
2024-01-07 13:39:42 INFO     	 * Prefix: True
2024-01-07 13:39:42 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 13:39:42 INFO     dataset preprocessing
/home2/g.torresgamez/.local/lib/python3.10/site-packages/datasets/load.py:2088: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
Downloading readme:   0%|          | 0.00/488 [00:00<?, ?B/s]Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 488/488 [00:00<00:00, 5.67MB/s]
Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]
Downloading data:   0%|          | 0.00/23.6M [00:00<?, ?B/s][A
Downloading data:  18%|â–ˆâ–Š        | 4.19M/23.6M [00:00<00:03, 5.38MB/s][A
Downloading data:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12.6M/23.6M [00:01<00:01, 8.80MB/s][A
Downloading data:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21.0M/23.6M [00:02<00:00, 11.2MB/s][ADownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23.6M/23.6M [00:02<00:00, 11.3MB/s]
Downloading data files:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:04,  2.09s/it]
Downloading data:   0%|          | 0.00/1.40M [00:00<?, ?B/s][A
Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.40M/1.40M [00:00<00:00, 2.72MB/s][ADownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.40M/1.40M [00:00<00:00, 2.70MB/s]
Downloading data files:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:02<00:01,  1.17s/it]
Downloading data:   0%|          | 0.00/1.44M [00:00<?, ?B/s][A
Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.44M/1.44M [00:00<00:00, 3.43MB/s][ADownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.44M/1.44M [00:00<00:00, 3.40MB/s]
Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:03<00:00,  1.20it/s]Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:03<00:00,  1.02s/it]
Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 159.87it/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 10000 examples [00:00, 21362.54 examples/s]Generating train split: 10250 examples [00:00, 19387.09 examples/s]
Generating validation split: 0 examples [00:00, ? examples/s]Generating validation split: 569 examples [00:00, 31181.77 examples/s]
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 570 examples [00:00, 31419.66 examples/s]
2024-01-07 13:39:49 INFO     encode all the data       : 10250
  0%|          | 0/10250 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors
  0%|          | 27/10250 [00:00<00:38, 263.15it/s]  1%|          | 54/10250 [00:00<00:50, 201.36it/s]  1%|          | 87/10250 [00:00<00:40, 250.25it/s]  1%|          | 121/10250 [00:00<00:36, 279.43it/s]  2%|â–         | 155/10250 [00:00<00:33, 298.99it/s]  2%|â–         | 195/10250 [00:00<00:30, 330.59it/s]  2%|â–         | 234/10250 [00:00<00:28, 347.61it/s]  3%|â–Ž         | 270/10250 [00:00<00:28, 345.39it/s]  3%|â–Ž         | 310/10250 [00:00<00:27, 361.11it/s]  3%|â–Ž         | 347/10250 [00:01<00:27, 360.06it/s]  4%|â–Ž         | 384/10250 [00:01<00:27, 357.43it/s]  4%|â–         | 420/10250 [00:01<00:27, 351.71it/s]  4%|â–         | 459/10250 [00:01<00:27, 360.28it/s]  5%|â–         | 496/10250 [00:01<00:27, 361.08it/s]  5%|â–Œ         | 533/10250 [00:01<00:27, 353.94it/s]  6%|â–Œ         | 570/10250 [00:01<00:27, 357.48it/s]  6%|â–Œ         | 606/10250 [00:01<00:26, 357.21it/s]  6%|â–‹         | 642/10250 [00:01<00:27, 349.11it/s]  7%|â–‹         | 682/10250 [00:02<00:26, 363.42it/s]  7%|â–‹         | 721/10250 [00:02<00:25, 369.68it/s]  7%|â–‹         | 759/10250 [00:02<00:26, 362.76it/s]  8%|â–Š         | 801/10250 [00:02<00:24, 378.64it/s]  8%|â–Š         | 839/10250 [00:02<00:25, 372.07it/s]  9%|â–Š         | 877/10250 [00:02<00:25, 363.32it/s]  9%|â–‰         | 915/10250 [00:02<00:25, 367.91it/s]  9%|â–‰         | 954/10250 [00:02<00:24, 371.95it/s] 10%|â–‰         | 992/10250 [00:02<00:25, 364.80it/s] 10%|â–ˆ         | 1029/10250 [00:02<00:26, 349.48it/s] 10%|â–ˆ         | 1072/10250 [00:03<00:24, 368.73it/s] 11%|â–ˆ         | 1110/10250 [00:03<00:25, 356.34it/s] 11%|â–ˆ         | 1146/10250 [00:03<00:25, 352.75it/s] 12%|â–ˆâ–        | 1184/10250 [00:03<00:25, 359.89it/s] 12%|â–ˆâ–        | 1221/10250 [00:03<00:25, 355.54it/s] 12%|â–ˆâ–        | 1257/10250 [00:03<00:25, 353.12it/s] 13%|â–ˆâ–Ž        | 1297/10250 [00:03<00:24, 365.97it/s] 13%|â–ˆâ–Ž        | 1334/10250 [00:03<00:24, 362.01it/s] 13%|â–ˆâ–Ž        | 1371/10250 [00:03<00:25, 354.84it/s] 14%|â–ˆâ–        | 1410/10250 [00:04<00:24, 363.65it/s] 14%|â–ˆâ–        | 1451/10250 [00:04<00:23, 377.00it/s] 15%|â–ˆâ–        | 1490/10250 [00:04<00:23, 377.78it/s] 15%|â–ˆâ–        | 1528/10250 [00:04<00:23, 367.52it/s] 15%|â–ˆâ–Œ        | 1570/10250 [00:04<00:22, 378.53it/s] 16%|â–ˆâ–Œ        | 1608/10250 [00:04<00:23, 363.89it/s] 16%|â–ˆâ–Œ        | 1645/10250 [00:04<00:24, 350.80it/s] 16%|â–ˆâ–‹        | 1683/10250 [00:04<00:23, 358.05it/s] 17%|â–ˆâ–‹        | 1722/10250 [00:04<00:23, 365.19it/s] 17%|â–ˆâ–‹        | 1759/10250 [00:04<00:23, 358.26it/s] 18%|â–ˆâ–Š        | 1799/10250 [00:05<00:22, 370.20it/s] 18%|â–ˆâ–Š        | 1837/10250 [00:05<00:22, 366.17it/s] 18%|â–ˆâ–Š        | 1874/10250 [00:05<00:23, 363.14it/s] 19%|â–ˆâ–Š        | 1911/10250 [00:05<00:22, 364.51it/s] 19%|â–ˆâ–‰        | 1956/10250 [00:05<00:21, 386.52it/s] 19%|â–ˆâ–‰        | 1995/10250 [00:05<00:21, 379.15it/s] 20%|â–ˆâ–‰        | 2033/10250 [00:05<00:22, 368.46it/s] 20%|â–ˆâ–ˆ        | 2071/10250 [00:05<00:22, 369.53it/s] 21%|â–ˆâ–ˆ        | 2109/10250 [00:05<00:22, 364.24it/s] 21%|â–ˆâ–ˆ        | 2146/10250 [00:06<00:22, 357.10it/s] 21%|â–ˆâ–ˆâ–       | 2187/10250 [00:06<00:21, 371.73it/s] 22%|â–ˆâ–ˆâ–       | 2225/10250 [00:06<00:22, 363.66it/s] 22%|â–ˆâ–ˆâ–       | 2262/10250 [00:06<00:22, 360.89it/s] 22%|â–ˆâ–ˆâ–       | 2299/10250 [00:06<00:21, 362.68it/s] 23%|â–ˆâ–ˆâ–Ž       | 2336/10250 [00:06<00:22, 346.36it/s] 23%|â–ˆâ–ˆâ–Ž       | 2371/10250 [00:06<00:22, 345.02it/s] 24%|â–ˆâ–ˆâ–Ž       | 2412/10250 [00:06<00:21, 362.96it/s] 24%|â–ˆâ–ˆâ–       | 2451/10250 [00:06<00:21, 370.78it/s] 24%|â–ˆâ–ˆâ–       | 2489/10250 [00:06<00:21, 366.90it/s] 25%|â–ˆâ–ˆâ–       | 2528/10250 [00:07<00:20, 372.20it/s] 25%|â–ˆâ–ˆâ–Œ       | 2566/10250 [00:07<00:21, 359.86it/s] 25%|â–ˆâ–ˆâ–Œ       | 2603/10250 [00:07<00:21, 360.11it/s] 26%|â–ˆâ–ˆâ–Œ       | 2640/10250 [00:07<00:21, 362.28it/s] 26%|â–ˆâ–ˆâ–Œ       | 2680/10250 [00:07<00:20, 372.78it/s] 27%|â–ˆâ–ˆâ–‹       | 2718/10250 [00:07<00:20, 367.62it/s] 27%|â–ˆâ–ˆâ–‹       | 2755/10250 [00:07<00:20, 357.19it/s] 27%|â–ˆâ–ˆâ–‹       | 2792/10250 [00:07<00:20, 358.23it/s] 28%|â–ˆâ–ˆâ–Š       | 2829/10250 [00:07<00:20, 359.03it/s] 28%|â–ˆâ–ˆâ–Š       | 2865/10250 [00:08<00:21, 351.24it/s] 28%|â–ˆâ–ˆâ–Š       | 2905/10250 [00:08<00:20, 364.93it/s] 29%|â–ˆâ–ˆâ–Š       | 2942/10250 [00:08<00:19, 366.08it/s] 29%|â–ˆâ–ˆâ–‰       | 2979/10250 [00:08<00:20, 355.90it/s] 29%|â–ˆâ–ˆâ–‰       | 3019/10250 [00:08<00:19, 368.46it/s] 30%|â–ˆâ–ˆâ–‰       | 3056/10250 [00:08<00:20, 357.37it/s] 30%|â–ˆâ–ˆâ–ˆ       | 3092/10250 [00:08<00:20, 355.32it/s] 31%|â–ˆâ–ˆâ–ˆ       | 3128/10250 [00:08<00:20, 353.79it/s] 31%|â–ˆâ–ˆâ–ˆ       | 3168/10250 [00:08<00:19, 364.73it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 3206/10250 [00:08<00:19, 366.39it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 3243/10250 [00:09<00:19, 361.52it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 3280/10250 [00:09<00:19, 363.85it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 3317/10250 [00:09<00:19, 362.22it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3354/10250 [00:09<00:19, 354.96it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3394/10250 [00:09<00:18, 367.03it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 3435/10250 [00:09<00:18, 377.14it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 3473/10250 [00:09<00:18, 363.49it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 3512/10250 [00:09<00:18, 368.82it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 3549/10250 [00:09<00:18, 358.96it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 3585/10250 [00:10<00:18, 352.24it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 3622/10250 [00:10<00:18, 355.57it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 3662/10250 [00:10<00:17, 368.25it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 3701/10250 [00:10<00:17, 374.29it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 3739/10250 [00:10<00:17, 371.50it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 3777/10250 [00:10<00:17, 364.68it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 3814/10250 [00:10<00:17, 363.63it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3851/10250 [00:10<00:18, 354.76it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3892/10250 [00:10<00:17, 369.53it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3930/10250 [00:10<00:17, 362.81it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 3967/10250 [00:11<00:18, 347.47it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 4003/10250 [00:11<00:17, 349.35it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 4039/10250 [00:11<00:17, 348.24it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 4074/10250 [00:11<00:17, 346.74it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4116/10250 [00:11<00:16, 367.51it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4153/10250 [00:11<00:16, 367.24it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4190/10250 [00:11<00:16, 358.08it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4229/10250 [00:11<00:16, 363.76it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4266/10250 [00:11<00:17, 349.21it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4302/10250 [00:12<00:17, 346.41it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4343/10250 [00:12<00:16, 364.08it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 4381/10250 [00:12<00:16, 366.12it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 4418/10250 [00:12<00:16, 362.33it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 4457/10250 [00:12<00:16, 360.02it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4494/10250 [00:12<00:16, 346.04it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4532/10250 [00:12<00:16, 352.46it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4568/10250 [00:12<00:16, 354.36it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4608/10250 [00:12<00:15, 366.66it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 4645/10250 [00:12<00:15, 366.00it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 4682/10250 [00:13<00:15, 361.61it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 4719/10250 [00:13<00:15, 362.26it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 4756/10250 [00:13<00:15, 354.29it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 4792/10250 [00:13<00:15, 351.02it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 4837/10250 [00:13<00:14, 379.58it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 4876/10250 [00:13<00:14, 374.01it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 4914/10250 [00:13<00:14, 364.67it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 4954/10250 [00:13<00:14, 373.12it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 4992/10250 [00:13<00:14, 359.52it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 5029/10250 [00:14<00:14, 353.99it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 5069/10250 [00:14<00:14, 366.44it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 5106/10250 [00:14<00:14, 364.94it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5143/10250 [00:14<00:14, 359.82it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5181/10250 [00:14<00:13, 365.53it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5218/10250 [00:14<00:14, 351.09it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5257/10250 [00:14<00:13, 361.83it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5294/10250 [00:14<00:13, 362.79it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5337/10250 [00:14<00:12, 380.30it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5377/10250 [00:14<00:12, 383.78it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 5416/10250 [00:15<00:13, 369.29it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 5454/10250 [00:15<00:12, 369.69it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 5493/10250 [00:15<00:12, 371.71it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5535/10250 [00:15<00:12, 384.06it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5574/10250 [00:15<00:12, 383.12it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5613/10250 [00:15<00:12, 383.99it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5654/10250 [00:15<00:11, 391.28it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5694/10250 [00:15<00:11, 381.23it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5733/10250 [00:15<00:12, 371.82it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 5771/10250 [00:15<00:12, 369.81it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 5809/10250 [00:16<00:12, 358.49it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 5847/10250 [00:16<00:12, 363.87it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 5889/10250 [00:16<00:11, 378.17it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 5927/10250 [00:16<00:11, 370.88it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 5965/10250 [00:16<00:11, 360.44it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 6002/10250 [00:16<00:11, 355.69it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 6039/10250 [00:16<00:11, 358.42it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 6075/10250 [00:16<00:11, 356.73it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 6116/10250 [00:16<00:11, 370.49it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6154/10250 [00:17<00:11, 371.16it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6192/10250 [00:17<00:11, 362.38it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6234/10250 [00:17<00:10, 377.99it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6272/10250 [00:17<00:11, 354.62it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6308/10250 [00:17<00:11, 345.42it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6348/10250 [00:17<00:10, 359.05it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6389/10250 [00:17<00:10, 370.78it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 6427/10250 [00:17<00:10, 365.87it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 6465/10250 [00:17<00:10, 368.86it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 6503/10250 [00:18<00:10, 356.63it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6539/10250 [00:18<00:10, 349.92it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6577/10250 [00:18<00:10, 356.43it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6619/10250 [00:18<00:09, 372.67it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6657/10250 [00:18<00:09, 368.15it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 6694/10250 [00:18<00:09, 359.77it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 6731/10250 [00:18<00:09, 354.97it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 6769/10250 [00:18<00:09, 359.63it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6806/10250 [00:18<00:09, 352.04it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6845/10250 [00:18<00:09, 360.33it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6882/10250 [00:19<00:09, 356.53it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6918/10250 [00:19<00:09, 345.26it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 6957/10250 [00:19<00:09, 357.33it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 6993/10250 [00:19<00:09, 346.25it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 7028/10250 [00:19<00:09, 342.96it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 7069/10250 [00:19<00:08, 360.67it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 7108/10250 [00:19<00:08, 366.81it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 7145/10250 [00:19<00:08, 359.64it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7182/10250 [00:19<00:08, 360.98it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7219/10250 [00:20<00:08, 353.63it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7255/10250 [00:20<00:08, 353.74it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7294/10250 [00:20<00:08, 362.40it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7337/10250 [00:20<00:07, 380.92it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7376/10250 [00:20<00:07, 378.15it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7414/10250 [00:20<00:07, 371.59it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 7452/10250 [00:20<00:07, 370.00it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 7490/10250 [00:20<00:07, 358.65it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 7526/10250 [00:20<00:07, 357.50it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7567/10250 [00:20<00:07, 371.89it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7607/10250 [00:21<00:06, 379.81it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7647/10250 [00:21<00:06, 382.98it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7686/10250 [00:21<00:06, 377.97it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 7724/10250 [00:21<00:06, 365.31it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 7761/10250 [00:21<00:06, 358.21it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 7798/10250 [00:21<00:06, 360.16it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 7840/10250 [00:21<00:06, 374.53it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 7878/10250 [00:21<00:06, 365.61it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 7915/10250 [00:21<00:06, 358.24it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7951/10250 [00:22<00:06, 358.60it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7987/10250 [00:22<00:06, 355.78it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 8023/10250 [00:22<00:06, 354.42it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 8063/10250 [00:22<00:05, 365.99it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 8104/10250 [00:22<00:05, 377.88it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 8142/10250 [00:22<00:05, 368.62it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 8179/10250 [00:22<00:05, 360.11it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8216/10250 [00:22<00:05, 351.11it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8252/10250 [00:22<00:05, 347.97it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8287/10250 [00:22<00:05, 347.33it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8329/10250 [00:23<00:05, 366.51it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8368/10250 [00:23<00:05, 373.15it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8406/10250 [00:23<00:04, 369.36it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8448/10250 [00:23<00:04, 384.08it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 8487/10250 [00:23<00:04, 379.85it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 8526/10250 [00:23<00:04, 372.48it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 8564/10250 [00:23<00:04, 372.31it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8607/10250 [00:23<00:04, 386.68it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8646/10250 [00:23<00:04, 379.56it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8685/10250 [00:23<00:04, 373.89it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 8723/10250 [00:24<00:04, 373.57it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 8761/10250 [00:24<00:04, 352.38it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 8797/10250 [00:24<00:04, 352.98it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 8838/10250 [00:24<00:03, 367.63it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8881/10250 [00:24<00:03, 381.88it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8920/10250 [00:24<00:03, 375.66it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8960/10250 [00:24<00:03, 380.53it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 8999/10250 [00:24<00:03, 367.87it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 9036/10250 [00:24<00:03, 364.60it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 9075/10250 [00:25<00:03, 369.81it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 9116/10250 [00:25<00:03, 377.29it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 9154/10250 [00:25<00:02, 375.12it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 9193/10250 [00:25<00:02, 378.67it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9231/10250 [00:25<00:02, 371.82it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9269/10250 [00:25<00:02, 359.00it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9308/10250 [00:25<00:02, 365.74it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9352/10250 [00:25<00:02, 382.85it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9391/10250 [00:25<00:02, 373.34it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9429/10250 [00:26<00:02, 366.88it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9466/10250 [00:26<00:02, 361.25it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 9503/10250 [00:26<00:02, 360.06it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 9540/10250 [00:26<00:01, 357.92it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 9579/10250 [00:26<00:01, 366.26it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9620/10250 [00:26<00:01, 376.73it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9658/10250 [00:26<00:01, 361.68it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9696/10250 [00:26<00:01, 364.51it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9733/10250 [00:26<00:01, 350.66it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9769/10250 [00:26<00:01, 347.54it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9810/10250 [00:27<00:01, 363.31it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9850/10250 [00:27<00:01, 371.33it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9888/10250 [00:27<00:01, 361.48it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9928/10250 [00:27<00:00, 372.29it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9966/10250 [00:27<00:00, 339.69it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 10001/10250 [00:27<00:00, 340.89it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 10042/10250 [00:27<00:00, 359.32it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 10081/10250 [00:27<00:00, 367.16it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 10119/10250 [00:27<00:00, 363.62it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 10156/10250 [00:28<00:00, 358.66it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 10194/10250 [00:28<00:00, 364.17it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 10231/10250 [00:28<00:00, 353.41it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10250/10250 [00:28<00:00, 362.16it/s]
2024-01-07 13:40:19 INFO     after remove the overflow : 2473
2024-01-07 13:40:19 INFO     preprocessed feature is saved at /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa_modified/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2024-01-07 13:40:19 INFO     start model training
2024-01-07 13:41:32 INFO     	 * (global step 50: loss: 0.37435510754585266, lr: 1e-05
2024-01-07 13:41:47 INFO     	 * (global step 100: loss: 0.3700089380145073, lr: 1e-05
2024-01-07 13:42:02 INFO     	 * (global step 150: loss: 0.3026483729481697, lr: 1e-05
2024-01-07 13:42:17 INFO     	 * (global step 200: loss: 0.3886946439743042, lr: 1e-05
2024-01-07 13:42:32 INFO     	 * (global step 250: loss: 0.45254699885845184, lr: 1e-05
2024-01-07 13:42:48 INFO     	 * (global step 300: loss: 0.32738081738352776, lr: 1e-05
2024-01-07 13:42:50 INFO     [epoch 0/10] average loss: 0.393, lr: 1e-05
2024-01-07 13:42:50 INFO     saving model related files
2024-01-07 13:42:50 INFO     saving model
2024-01-07 13:42:51 INFO     saving tokenizer
2024-01-07 13:42:51 INFO     saving optimizer
2024-01-07 13:42:52 INFO     remove old optimizer files
2024-01-07 13:43:05 INFO     	 * (global step 350: loss: 0.2505012471228838, lr: 1e-05
2024-01-07 13:43:20 INFO     	 * (global step 400: loss: 0.4167075529694557, lr: 1e-05
2024-01-07 13:43:36 INFO     	 * (global step 450: loss: 0.2920372597873211, lr: 1e-05
2024-01-07 13:43:51 INFO     	 * (global step 500: loss: 0.41816471144557, lr: 1e-05
2024-01-07 13:44:06 INFO     	 * (global step 550: loss: 0.3448331840336323, lr: 1e-05
2024-01-07 13:44:22 INFO     	 * (global step 600: loss: 0.4260798394680023, lr: 1e-05
2024-01-07 13:44:27 INFO     [epoch 1/10] average loss: 0.366, lr: 1e-05
2024-01-07 13:44:27 INFO     saving model related files
2024-01-07 13:44:27 INFO     saving model
2024-01-07 13:44:28 INFO     saving tokenizer
2024-01-07 13:44:28 INFO     saving optimizer
2024-01-07 13:44:28 INFO     remove old optimizer files
2024-01-07 13:44:38 INFO     	 * (global step 650: loss: 0.47418757528066635, lr: 1e-05
2024-01-07 13:44:54 INFO     	 * (global step 700: loss: 0.34526636078953743, lr: 1e-05
2024-01-07 13:45:09 INFO     	 * (global step 750: loss: 0.36618079245090485, lr: 1e-05
2024-01-07 13:45:24 INFO     	 * (global step 800: loss: 0.3319547176361084, lr: 1e-05
2024-01-07 13:45:40 INFO     	 * (global step 850: loss: 0.2733786962926388, lr: 1e-05
2024-01-07 13:45:55 INFO     	 * (global step 900: loss: 0.44445621967315674, lr: 1e-05
2024-01-07 13:46:04 INFO     [epoch 2/10] average loss: 0.357, lr: 1e-05
2024-01-07 13:46:04 INFO     saving model related files
2024-01-07 13:46:04 INFO     saving model
2024-01-07 13:46:04 INFO     saving tokenizer
2024-01-07 13:46:04 INFO     saving optimizer
2024-01-07 13:46:05 INFO     remove old optimizer files
2024-01-07 13:46:12 INFO     	 * (global step 950: loss: 0.4724271707236767, lr: 1e-05
2024-01-07 13:46:27 INFO     	 * (global step 1000: loss: 0.3115299753844738, lr: 1e-05
2024-01-07 13:46:43 INFO     	 * (global step 1050: loss: 0.4053511843085289, lr: 1e-05
2024-01-07 13:46:58 INFO     	 * (global step 1100: loss: 0.3939792588353157, lr: 1e-05
2024-01-07 13:47:14 INFO     	 * (global step 1150: loss: 0.29928940534591675, lr: 1e-05
2024-01-07 13:47:29 INFO     	 * (global step 1200: loss: 0.24861827865242958, lr: 1e-05
2024-01-07 13:47:40 INFO     [epoch 3/10] average loss: 0.352, lr: 1e-05
2024-01-07 13:47:40 INFO     saving model related files
2024-01-07 13:47:40 INFO     saving model
2024-01-07 13:47:41 INFO     saving tokenizer
2024-01-07 13:47:41 INFO     saving optimizer
2024-01-07 13:47:42 INFO     remove old optimizer files
2024-01-07 13:47:46 INFO     	 * (global step 1250: loss: 0.3932754658162594, lr: 1e-05
2024-01-07 13:48:01 INFO     	 * (global step 1300: loss: 0.3834603428840637, lr: 1e-05
2024-01-07 13:48:17 INFO     	 * (global step 1350: loss: 0.42098841071128845, lr: 1e-05
2024-01-07 13:48:32 INFO     	 * (global step 1400: loss: 0.2791079506278038, lr: 1e-05
2024-01-07 13:48:47 INFO     	 * (global step 1450: loss: 0.3959065452218056, lr: 1e-05
2024-01-07 13:49:03 INFO     	 * (global step 1500: loss: 0.2671380341053009, lr: 1e-05
2024-01-07 13:49:16 INFO     [epoch 4/10] average loss: 0.348, lr: 1e-05
2024-01-07 13:49:16 INFO     saving model related files
2024-01-07 13:49:16 INFO     saving model
2024-01-07 13:49:17 INFO     saving tokenizer
2024-01-07 13:49:17 INFO     saving optimizer
2024-01-07 13:49:18 INFO     remove old optimizer files
2024-01-07 13:49:18 INFO     complete training: model ckpt was saved at small_finetuned_modified_ckpt/model_pzcecq
2024-01-07 13:49:18 INFO     ## 1st RUN: Configuration 1/4 ##
2024-01-07 13:49:18 INFO     initialize model trainer
2024-01-07 13:49:18 INFO     initialize checkpoint at small_finetuned_modified_ckpt/model_eszyci
2024-01-07 13:49:18 INFO     hyperparameters
2024-01-07 13:49:18 INFO     	 * dataset_path: StellarMilk/newsqa_modified
2024-01-07 13:49:18 INFO     	 * dataset_name: default
2024-01-07 13:49:18 INFO     	 * input_types: ['paragraph']
2024-01-07 13:49:18 INFO     	 * output_types: ['questions_answers']
2024-01-07 13:49:18 INFO     	 * prefix_types: ['qag']
2024-01-07 13:49:18 INFO     	 * model: lmqg/t5-small-squad-qag
2024-01-07 13:49:18 INFO     	 * max_length: 512
2024-01-07 13:49:18 INFO     	 * max_length_output: 512
2024-01-07 13:49:18 INFO     	 * epoch: 10
2024-01-07 13:49:18 INFO     	 * batch: 2
2024-01-07 13:49:18 INFO     	 * lr: 1e-05
2024-01-07 13:49:18 INFO     	 * fp16: False
2024-01-07 13:49:18 INFO     	 * random_seed: 1
2024-01-07 13:49:18 INFO     	 * gradient_accumulation_steps: 2
2024-01-07 13:49:18 INFO     	 * label_smoothing: 0.15
2024-01-07 13:49:18 INFO     initialize checkpoint with lmqg/t5-small-squad-qag
2024-01-07 13:49:19 INFO     use spaCy answer extraction model: positionrank
2024-01-07 13:49:20 INFO     Model `lmqg/t5-small-squad-qag`
2024-01-07 13:49:20 INFO     	 * Num of GPU in use: 1
2024-01-07 13:49:20 INFO     	 * Prefix: True
2024-01-07 13:49:20 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 13:49:20 INFO     dataset preprocessing
2024-01-07 13:49:21 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa_modified/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2024-01-07 13:49:24 INFO     start model training
2024-01-07 13:49:32 INFO     	 * (global step 50: loss: 0.5317152440547943, lr: 1e-05
2024-01-07 13:49:39 INFO     	 * (global step 100: loss: 0.5074271708726883, lr: 1e-05
2024-01-07 13:49:47 INFO     	 * (global step 150: loss: 0.33326922357082367, lr: 1e-05
2024-01-07 13:49:56 INFO     	 * (global step 200: loss: 0.31616584956645966, lr: 1e-05
2024-01-07 13:50:04 INFO     	 * (global step 250: loss: 0.3068310171365738, lr: 1e-05
2024-01-07 13:50:12 INFO     	 * (global step 300: loss: 0.3415028601884842, lr: 1e-05
2024-01-07 13:50:19 INFO     	 * (global step 350: loss: 0.25870198756456375, lr: 1e-05
2024-01-07 13:50:27 INFO     	 * (global step 400: loss: 0.4840714931488037, lr: 1e-05
2024-01-07 13:50:35 INFO     	 * (global step 450: loss: 0.3634491413831711, lr: 1e-05
2024-01-07 13:50:43 INFO     	 * (global step 500: loss: 0.5439731925725937, lr: 1e-05
2024-01-07 13:50:51 INFO     	 * (global step 550: loss: 0.3408122658729553, lr: 1e-05
2024-01-07 13:50:59 INFO     	 * (global step 600: loss: 0.30176926404237747, lr: 1e-05
2024-01-07 13:51:02 INFO     [epoch 0/10] average loss: 0.386, lr: 1e-05
2024-01-07 13:51:02 INFO     saving model related files
2024-01-07 13:51:02 INFO     saving model
2024-01-07 13:51:02 INFO     saving tokenizer
2024-01-07 13:51:02 INFO     saving optimizer
2024-01-07 13:51:03 INFO     remove old optimizer files
2024-01-07 13:51:08 INFO     	 * (global step 650: loss: 0.4358596056699753, lr: 1e-05
2024-01-07 13:51:16 INFO     	 * (global step 700: loss: 0.1251055784523487, lr: 1e-05
2024-01-07 13:51:24 INFO     	 * (global step 750: loss: 0.3308354541659355, lr: 1e-05
2024-01-07 13:51:32 INFO     	 * (global step 800: loss: 0.47775352001190186, lr: 1e-05
2024-01-07 13:51:40 INFO     	 * (global step 850: loss: 0.33778080344200134, lr: 1e-05
2024-01-07 13:51:48 INFO     	 * (global step 900: loss: 0.17537076771259308, lr: 1e-05
2024-01-07 13:51:56 INFO     	 * (global step 950: loss: 0.31662751734256744, lr: 1e-05
2024-01-07 13:52:04 INFO     	 * (global step 1000: loss: 0.5596107393503189, lr: 1e-05
2024-01-07 13:52:12 INFO     	 * (global step 1050: loss: 0.4008745849132538, lr: 1e-05
2024-01-07 13:52:20 INFO     	 * (global step 1100: loss: 0.33461809158325195, lr: 1e-05
2024-01-07 13:52:27 INFO     	 * (global step 1150: loss: 0.23547687381505966, lr: 1e-05
2024-01-07 13:52:35 INFO     	 * (global step 1200: loss: 0.2893764227628708, lr: 1e-05
2024-01-07 13:52:41 INFO     [epoch 1/10] average loss: 0.36, lr: 1e-05
2024-01-07 13:52:41 INFO     saving model related files
2024-01-07 13:52:41 INFO     saving model
2024-01-07 13:52:41 INFO     saving tokenizer
2024-01-07 13:52:41 INFO     saving optimizer
2024-01-07 13:52:42 INFO     remove old optimizer files
2024-01-07 13:52:45 INFO     	 * (global step 1250: loss: 0.33980873227119446, lr: 1e-05
2024-01-07 13:52:52 INFO     	 * (global step 1300: loss: 0.4835594743490219, lr: 1e-05
2024-01-07 13:53:00 INFO     	 * (global step 1350: loss: 0.36238668859004974, lr: 1e-05
2024-01-07 13:53:08 INFO     	 * (global step 1400: loss: 0.27674550563097, lr: 1e-05
2024-01-07 13:53:16 INFO     	 * (global step 1450: loss: 0.26655205339193344, lr: 1e-05
2024-01-07 13:53:24 INFO     	 * (global step 1500: loss: 0.3109831064939499, lr: 1e-05
2024-01-07 13:53:32 INFO     	 * (global step 1550: loss: 0.3820246309041977, lr: 1e-05
2024-01-07 13:53:40 INFO     	 * (global step 1600: loss: 0.3476361781358719, lr: 1e-05
2024-01-07 13:53:48 INFO     	 * (global step 1650: loss: 0.37382739782333374, lr: 1e-05
2024-01-07 13:53:56 INFO     	 * (global step 1700: loss: 0.27384698390960693, lr: 1e-05
2024-01-07 13:54:03 INFO     	 * (global step 1750: loss: 0.2286393791437149, lr: 1e-05
2024-01-07 13:54:11 INFO     	 * (global step 1800: loss: 0.42108070850372314, lr: 1e-05
2024-01-07 13:54:19 INFO     	 * (global step 1850: loss: 0.34187695384025574, lr: 1e-05
2024-01-07 13:54:20 INFO     [epoch 2/10] average loss: 0.351, lr: 1e-05
2024-01-07 13:54:20 INFO     saving model related files
2024-01-07 13:54:20 INFO     saving model
2024-01-07 13:54:20 INFO     saving tokenizer
2024-01-07 13:54:20 INFO     saving optimizer
2024-01-07 13:54:21 INFO     remove old optimizer files
2024-01-07 13:54:29 INFO     	 * (global step 1900: loss: 0.308396652340889, lr: 1e-05
2024-01-07 13:54:37 INFO     	 * (global step 1950: loss: 0.3355336934328079, lr: 1e-05
2024-01-07 13:54:45 INFO     	 * (global step 2000: loss: 0.2985702157020569, lr: 1e-05
2024-01-07 13:54:53 INFO     	 * (global step 2050: loss: 0.5785903334617615, lr: 1e-05
2024-01-07 13:55:01 INFO     	 * (global step 2100: loss: 0.5103716850280762, lr: 1e-05
2024-01-07 13:55:09 INFO     	 * (global step 2150: loss: 0.29401756823062897, lr: 1e-05
2024-01-07 13:55:17 INFO     	 * (global step 2200: loss: 0.38157641887664795, lr: 1e-05
2024-01-07 13:55:25 INFO     	 * (global step 2250: loss: 0.368318110704422, lr: 1e-05
2024-01-07 13:55:33 INFO     	 * (global step 2300: loss: 0.36193306744098663, lr: 1e-05
2024-01-07 13:55:40 INFO     	 * (global step 2350: loss: 0.3526819944381714, lr: 1e-05
2024-01-07 13:55:48 INFO     	 * (global step 2400: loss: 0.24912289530038834, lr: 1e-05
2024-01-07 13:55:56 INFO     	 * (global step 2450: loss: 0.3673015236854553, lr: 1e-05
2024-01-07 13:56:00 INFO     [epoch 3/10] average loss: 0.346, lr: 1e-05
2024-01-07 13:56:00 INFO     saving model related files
2024-01-07 13:56:00 INFO     saving model
2024-01-07 13:56:00 INFO     saving tokenizer
2024-01-07 13:56:00 INFO     saving optimizer
2024-01-07 13:56:01 INFO     remove old optimizer files
2024-01-07 13:56:06 INFO     	 * (global step 2500: loss: 0.3338671028614044, lr: 1e-05
2024-01-07 13:56:13 INFO     	 * (global step 2550: loss: 0.3443359434604645, lr: 1e-05
2024-01-07 13:56:21 INFO     	 * (global step 2600: loss: 0.3049229681491852, lr: 1e-05
2024-01-07 13:56:29 INFO     	 * (global step 2650: loss: 0.3118137940764427, lr: 1e-05
2024-01-07 13:56:37 INFO     	 * (global step 2700: loss: 0.44621551036834717, lr: 1e-05
2024-01-07 13:56:45 INFO     	 * (global step 2750: loss: 0.2576860040426254, lr: 1e-05
2024-01-07 13:56:53 INFO     	 * (global step 2800: loss: 0.27334319055080414, lr: 1e-05
2024-01-07 13:57:01 INFO     	 * (global step 2850: loss: 0.36536554992198944, lr: 1e-05
2024-01-07 13:57:09 INFO     	 * (global step 2900: loss: 0.31190092861652374, lr: 1e-05
2024-01-07 13:57:17 INFO     	 * (global step 2950: loss: 0.2590034231543541, lr: 1e-05
2024-01-07 13:57:25 INFO     	 * (global step 3000: loss: 0.23702020198106766, lr: 1e-05
2024-01-07 13:57:32 INFO     	 * (global step 3050: loss: 0.34309121966362, lr: 1e-05
2024-01-07 13:57:39 INFO     [epoch 4/10] average loss: 0.341, lr: 1e-05
2024-01-07 13:57:39 INFO     saving model related files
2024-01-07 13:57:39 INFO     saving model
2024-01-07 13:57:39 INFO     saving tokenizer
2024-01-07 13:57:39 INFO     saving optimizer
2024-01-07 13:57:40 INFO     remove old optimizer files
2024-01-07 13:57:40 INFO     complete training: model ckpt was saved at small_finetuned_modified_ckpt/model_eszyci
2024-01-07 13:57:40 INFO     ## 1st RUN: Configuration 2/4 ##
2024-01-07 13:57:40 INFO     initialize model trainer
2024-01-07 13:57:40 INFO     initialize checkpoint at small_finetuned_modified_ckpt/model_dpyopu
2024-01-07 13:57:40 INFO     hyperparameters
2024-01-07 13:57:40 INFO     	 * dataset_path: StellarMilk/newsqa_modified
2024-01-07 13:57:40 INFO     	 * dataset_name: default
2024-01-07 13:57:40 INFO     	 * input_types: ['paragraph']
2024-01-07 13:57:40 INFO     	 * output_types: ['questions_answers']
2024-01-07 13:57:40 INFO     	 * prefix_types: ['qag']
2024-01-07 13:57:40 INFO     	 * model: lmqg/t5-small-squad-qag
2024-01-07 13:57:40 INFO     	 * max_length: 512
2024-01-07 13:57:40 INFO     	 * max_length_output: 512
2024-01-07 13:57:40 INFO     	 * epoch: 10
2024-01-07 13:57:40 INFO     	 * batch: 2
2024-01-07 13:57:40 INFO     	 * lr: 1e-05
2024-01-07 13:57:40 INFO     	 * fp16: False
2024-01-07 13:57:40 INFO     	 * random_seed: 1
2024-01-07 13:57:40 INFO     	 * gradient_accumulation_steps: 4
2024-01-07 13:57:40 INFO     	 * label_smoothing: 0.0
2024-01-07 13:57:40 INFO     initialize checkpoint with lmqg/t5-small-squad-qag
2024-01-07 13:57:41 INFO     use spaCy answer extraction model: positionrank
2024-01-07 13:57:42 INFO     Model `lmqg/t5-small-squad-qag`
2024-01-07 13:57:42 INFO     	 * Num of GPU in use: 1
2024-01-07 13:57:42 INFO     	 * Prefix: True
2024-01-07 13:57:42 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 13:57:42 INFO     dataset preprocessing
2024-01-07 13:57:43 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa_modified/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2024-01-07 13:57:43 INFO     start model training
2024-01-07 13:57:58 INFO     	 * (global step 50: loss: 0.37435510754585266, lr: 1e-05
2024-01-07 13:58:14 INFO     	 * (global step 100: loss: 0.3700089380145073, lr: 1e-05
2024-01-07 13:58:29 INFO     	 * (global step 150: loss: 0.3026483729481697, lr: 1e-05
2024-01-07 13:58:44 INFO     	 * (global step 200: loss: 0.3886946439743042, lr: 1e-05
2024-01-07 13:59:00 INFO     	 * (global step 250: loss: 0.45254699885845184, lr: 1e-05
2024-01-07 13:59:15 INFO     	 * (global step 300: loss: 0.32738081738352776, lr: 1e-05
2024-01-07 13:59:18 INFO     [epoch 0/10] average loss: 0.393, lr: 1e-05
2024-01-07 13:59:18 INFO     saving model related files
2024-01-07 13:59:18 INFO     saving model
2024-01-07 13:59:19 INFO     saving tokenizer
2024-01-07 13:59:19 INFO     saving optimizer
2024-01-07 13:59:19 INFO     remove old optimizer files
2024-01-07 13:59:32 INFO     	 * (global step 350: loss: 0.2505012471228838, lr: 1e-05
2024-01-07 13:59:47 INFO     	 * (global step 400: loss: 0.4167075529694557, lr: 1e-05
2024-01-07 14:00:03 INFO     	 * (global step 450: loss: 0.2920372597873211, lr: 1e-05
2024-01-07 14:00:18 INFO     	 * (global step 500: loss: 0.41816471144557, lr: 1e-05
2024-01-07 14:00:33 INFO     	 * (global step 550: loss: 0.3448331840336323, lr: 1e-05
2024-01-07 14:00:49 INFO     	 * (global step 600: loss: 0.4260798394680023, lr: 1e-05
2024-01-07 14:00:54 INFO     [epoch 1/10] average loss: 0.366, lr: 1e-05
2024-01-07 14:00:54 INFO     saving model related files
2024-01-07 14:00:54 INFO     saving model
2024-01-07 14:00:55 INFO     saving tokenizer
2024-01-07 14:00:55 INFO     saving optimizer
2024-01-07 14:00:56 INFO     remove old optimizer files
2024-01-07 14:01:06 INFO     	 * (global step 650: loss: 0.47418757528066635, lr: 1e-05
2024-01-07 14:01:21 INFO     	 * (global step 700: loss: 0.34526636078953743, lr: 1e-05
2024-01-07 14:01:37 INFO     	 * (global step 750: loss: 0.36618079245090485, lr: 1e-05
2024-01-07 14:01:57 INFO     	 * (global step 800: loss: 0.3319547176361084, lr: 1e-05
2024-01-07 14:02:13 INFO     	 * (global step 850: loss: 0.2733786962926388, lr: 1e-05
2024-01-07 14:02:28 INFO     	 * (global step 900: loss: 0.44445621967315674, lr: 1e-05
2024-01-07 14:02:36 INFO     [epoch 2/10] average loss: 0.357, lr: 1e-05
2024-01-07 14:02:36 INFO     saving model related files
2024-01-07 14:02:36 INFO     saving model
2024-01-07 14:02:37 INFO     saving tokenizer
2024-01-07 14:02:37 INFO     saving optimizer
2024-01-07 14:02:38 INFO     remove old optimizer files
2024-01-07 14:02:45 INFO     	 * (global step 950: loss: 0.4724271707236767, lr: 1e-05
2024-01-07 14:03:00 INFO     	 * (global step 1000: loss: 0.3115299753844738, lr: 1e-05
2024-01-07 14:03:16 INFO     	 * (global step 1050: loss: 0.4053511843085289, lr: 1e-05
2024-01-07 14:03:31 INFO     	 * (global step 1100: loss: 0.3939792588353157, lr: 1e-05
2024-01-07 14:03:46 INFO     	 * (global step 1150: loss: 0.29928940534591675, lr: 1e-05
2024-01-07 14:04:02 INFO     	 * (global step 1200: loss: 0.24861827865242958, lr: 1e-05
2024-01-07 14:04:13 INFO     [epoch 3/10] average loss: 0.352, lr: 1e-05
2024-01-07 14:04:13 INFO     saving model related files
2024-01-07 14:04:13 INFO     saving model
2024-01-07 14:04:13 INFO     saving tokenizer
2024-01-07 14:04:13 INFO     saving optimizer
2024-01-07 14:04:14 INFO     remove old optimizer files
2024-01-07 14:04:19 INFO     	 * (global step 1250: loss: 0.3932754658162594, lr: 1e-05
2024-01-07 14:04:34 INFO     	 * (global step 1300: loss: 0.3834603428840637, lr: 1e-05
2024-01-07 14:04:50 INFO     	 * (global step 1350: loss: 0.42098841071128845, lr: 1e-05
2024-01-07 14:05:05 INFO     	 * (global step 1400: loss: 0.2791079506278038, lr: 1e-05
2024-01-07 14:05:20 INFO     	 * (global step 1450: loss: 0.3959065452218056, lr: 1e-05
2024-01-07 14:05:36 INFO     	 * (global step 1500: loss: 0.2671380341053009, lr: 1e-05
2024-01-07 14:05:50 INFO     [epoch 4/10] average loss: 0.348, lr: 1e-05
2024-01-07 14:05:50 INFO     saving model related files
2024-01-07 14:05:50 INFO     saving model
2024-01-07 14:05:51 INFO     saving tokenizer
2024-01-07 14:05:51 INFO     saving optimizer
2024-01-07 14:05:52 INFO     remove old optimizer files
2024-01-07 14:05:53 INFO     complete training: model ckpt was saved at small_finetuned_modified_ckpt/model_dpyopu
2024-01-07 14:05:53 INFO     ## 1st RUN: Configuration 3/4 ##
2024-01-07 14:05:53 INFO     initialize model trainer
2024-01-07 14:05:53 INFO     initialize checkpoint at small_finetuned_modified_ckpt/model_mzgdpa
2024-01-07 14:05:53 INFO     hyperparameters
2024-01-07 14:05:53 INFO     	 * dataset_path: StellarMilk/newsqa_modified
2024-01-07 14:05:53 INFO     	 * dataset_name: default
2024-01-07 14:05:53 INFO     	 * input_types: ['paragraph']
2024-01-07 14:05:53 INFO     	 * output_types: ['questions_answers']
2024-01-07 14:05:53 INFO     	 * prefix_types: ['qag']
2024-01-07 14:05:53 INFO     	 * model: lmqg/t5-small-squad-qag
2024-01-07 14:05:53 INFO     	 * max_length: 512
2024-01-07 14:05:53 INFO     	 * max_length_output: 512
2024-01-07 14:05:53 INFO     	 * epoch: 10
2024-01-07 14:05:53 INFO     	 * batch: 2
2024-01-07 14:05:53 INFO     	 * lr: 1e-05
2024-01-07 14:05:53 INFO     	 * fp16: False
2024-01-07 14:05:53 INFO     	 * random_seed: 1
2024-01-07 14:05:53 INFO     	 * gradient_accumulation_steps: 2
2024-01-07 14:05:53 INFO     	 * label_smoothing: 0.0
2024-01-07 14:05:53 INFO     initialize checkpoint with lmqg/t5-small-squad-qag
2024-01-07 14:05:54 INFO     use spaCy answer extraction model: positionrank
2024-01-07 14:05:55 INFO     Model `lmqg/t5-small-squad-qag`
2024-01-07 14:05:55 INFO     	 * Num of GPU in use: 1
2024-01-07 14:05:55 INFO     	 * Prefix: True
2024-01-07 14:05:55 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 14:05:55 INFO     dataset preprocessing
2024-01-07 14:05:56 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa_modified/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2024-01-07 14:05:57 INFO     start model training
2024-01-07 14:06:05 INFO     	 * (global step 50: loss: 0.5317152440547943, lr: 1e-05
2024-01-07 14:06:13 INFO     	 * (global step 100: loss: 0.5074271708726883, lr: 1e-05
2024-01-07 14:06:21 INFO     	 * (global step 150: loss: 0.33326922357082367, lr: 1e-05
2024-01-07 14:06:29 INFO     	 * (global step 200: loss: 0.31616584956645966, lr: 1e-05
2024-01-07 14:06:37 INFO     	 * (global step 250: loss: 0.3068310171365738, lr: 1e-05
2024-01-07 14:06:46 INFO     	 * (global step 300: loss: 0.3415028601884842, lr: 1e-05
2024-01-07 14:06:54 INFO     	 * (global step 350: loss: 0.25870198756456375, lr: 1e-05
2024-01-07 14:07:01 INFO     	 * (global step 400: loss: 0.4840714931488037, lr: 1e-05
2024-01-07 14:07:09 INFO     	 * (global step 450: loss: 0.3634491413831711, lr: 1e-05
2024-01-07 14:07:17 INFO     	 * (global step 500: loss: 0.5439731925725937, lr: 1e-05
2024-01-07 14:07:25 INFO     	 * (global step 550: loss: 0.3408122658729553, lr: 1e-05
2024-01-07 14:07:33 INFO     	 * (global step 600: loss: 0.30176926404237747, lr: 1e-05
2024-01-07 14:07:36 INFO     [epoch 0/10] average loss: 0.386, lr: 1e-05
2024-01-07 14:07:36 INFO     saving model related files
2024-01-07 14:07:36 INFO     saving model
2024-01-07 14:07:36 INFO     saving tokenizer
2024-01-07 14:07:36 INFO     saving optimizer
2024-01-07 14:07:37 INFO     remove old optimizer files
2024-01-07 14:07:42 INFO     	 * (global step 650: loss: 0.4358596056699753, lr: 1e-05
2024-01-07 14:07:50 INFO     	 * (global step 700: loss: 0.1251055784523487, lr: 1e-05
2024-01-07 14:07:58 INFO     	 * (global step 750: loss: 0.3308354541659355, lr: 1e-05
2024-01-07 14:08:06 INFO     	 * (global step 800: loss: 0.47775352001190186, lr: 1e-05
2024-01-07 14:08:14 INFO     	 * (global step 850: loss: 0.33778080344200134, lr: 1e-05
2024-01-07 14:08:22 INFO     	 * (global step 900: loss: 0.17537076771259308, lr: 1e-05
2024-01-07 14:08:30 INFO     	 * (global step 950: loss: 0.31662751734256744, lr: 1e-05
2024-01-07 14:08:38 INFO     	 * (global step 1000: loss: 0.5596107393503189, lr: 1e-05
2024-01-07 14:08:46 INFO     	 * (global step 1050: loss: 0.4008745849132538, lr: 1e-05
2024-01-07 14:08:54 INFO     	 * (global step 1100: loss: 0.33461809158325195, lr: 1e-05
2024-01-07 14:09:02 INFO     	 * (global step 1150: loss: 0.23547687381505966, lr: 1e-05
2024-01-07 14:09:10 INFO     	 * (global step 1200: loss: 0.2893764227628708, lr: 1e-05
2024-01-07 14:09:15 INFO     [epoch 1/10] average loss: 0.36, lr: 1e-05
2024-01-07 14:09:15 INFO     saving model related files
2024-01-07 14:09:15 INFO     saving model
2024-01-07 14:09:16 INFO     saving tokenizer
2024-01-07 14:09:16 INFO     saving optimizer
2024-01-07 14:09:17 INFO     remove old optimizer files
2024-01-07 14:09:19 INFO     	 * (global step 1250: loss: 0.33980873227119446, lr: 1e-05
2024-01-07 14:09:27 INFO     	 * (global step 1300: loss: 0.4835594743490219, lr: 1e-05
2024-01-07 14:09:35 INFO     	 * (global step 1350: loss: 0.36238668859004974, lr: 1e-05
2024-01-07 14:09:43 INFO     	 * (global step 1400: loss: 0.27674550563097, lr: 1e-05
2024-01-07 14:09:51 INFO     	 * (global step 1450: loss: 0.26655205339193344, lr: 1e-05
2024-01-07 14:09:59 INFO     	 * (global step 1500: loss: 0.3109831064939499, lr: 1e-05
2024-01-07 14:10:07 INFO     	 * (global step 1550: loss: 0.3820246309041977, lr: 1e-05
2024-01-07 14:10:15 INFO     	 * (global step 1600: loss: 0.3476361781358719, lr: 1e-05
2024-01-07 14:10:23 INFO     	 * (global step 1650: loss: 0.37382739782333374, lr: 1e-05
2024-01-07 14:10:31 INFO     	 * (global step 1700: loss: 0.27384698390960693, lr: 1e-05
2024-01-07 14:10:39 INFO     	 * (global step 1750: loss: 0.2286393791437149, lr: 1e-05
2024-01-07 14:10:46 INFO     	 * (global step 1800: loss: 0.42108070850372314, lr: 1e-05
2024-01-07 14:10:54 INFO     	 * (global step 1850: loss: 0.34187695384025574, lr: 1e-05
2024-01-07 14:10:55 INFO     [epoch 2/10] average loss: 0.351, lr: 1e-05
2024-01-07 14:10:55 INFO     saving model related files
2024-01-07 14:10:55 INFO     saving model
2024-01-07 14:10:56 INFO     saving tokenizer
2024-01-07 14:10:56 INFO     saving optimizer
2024-01-07 14:10:56 INFO     remove old optimizer files
2024-01-07 14:11:04 INFO     	 * (global step 1900: loss: 0.308396652340889, lr: 1e-05
2024-01-07 14:11:12 INFO     	 * (global step 1950: loss: 0.3355336934328079, lr: 1e-05
2024-01-07 14:11:20 INFO     	 * (global step 2000: loss: 0.2985702157020569, lr: 1e-05
2024-01-07 14:11:28 INFO     	 * (global step 2050: loss: 0.5785903334617615, lr: 1e-05
2024-01-07 14:11:36 INFO     	 * (global step 2100: loss: 0.5103716850280762, lr: 1e-05
2024-01-07 14:11:44 INFO     	 * (global step 2150: loss: 0.29401756823062897, lr: 1e-05
2024-01-07 14:11:51 INFO     	 * (global step 2200: loss: 0.38157641887664795, lr: 1e-05
2024-01-07 14:11:59 INFO     	 * (global step 2250: loss: 0.368318110704422, lr: 1e-05
2024-01-07 14:12:07 INFO     	 * (global step 2300: loss: 0.36193306744098663, lr: 1e-05
2024-01-07 14:12:15 INFO     	 * (global step 2350: loss: 0.3526819944381714, lr: 1e-05
2024-01-07 14:12:23 INFO     	 * (global step 2400: loss: 0.24912289530038834, lr: 1e-05
2024-01-07 14:12:31 INFO     	 * (global step 2450: loss: 0.3673015236854553, lr: 1e-05
2024-01-07 14:12:35 INFO     [epoch 3/10] average loss: 0.346, lr: 1e-05
2024-01-07 14:12:35 INFO     saving model related files
2024-01-07 14:12:35 INFO     saving model
2024-01-07 14:12:35 INFO     saving tokenizer
2024-01-07 14:12:35 INFO     saving optimizer
2024-01-07 14:12:36 INFO     remove old optimizer files
2024-01-07 14:12:41 INFO     	 * (global step 2500: loss: 0.3338671028614044, lr: 1e-05
2024-01-07 14:12:49 INFO     	 * (global step 2550: loss: 0.3443359434604645, lr: 1e-05
2024-01-07 14:12:57 INFO     	 * (global step 2600: loss: 0.3049229681491852, lr: 1e-05
2024-01-07 14:13:04 INFO     	 * (global step 2650: loss: 0.3118137940764427, lr: 1e-05
2024-01-07 14:13:12 INFO     	 * (global step 2700: loss: 0.44621551036834717, lr: 1e-05
2024-01-07 14:13:20 INFO     	 * (global step 2750: loss: 0.2576860040426254, lr: 1e-05
2024-01-07 14:13:28 INFO     	 * (global step 2800: loss: 0.27334319055080414, lr: 1e-05
2024-01-07 14:13:36 INFO     	 * (global step 2850: loss: 0.36536554992198944, lr: 1e-05
2024-01-07 14:13:44 INFO     	 * (global step 2900: loss: 0.31190092861652374, lr: 1e-05
2024-01-07 14:13:52 INFO     	 * (global step 2950: loss: 0.2590034231543541, lr: 1e-05
2024-01-07 14:14:00 INFO     	 * (global step 3000: loss: 0.23702020198106766, lr: 1e-05
2024-01-07 14:14:08 INFO     	 * (global step 3050: loss: 0.34309121966362, lr: 1e-05
2024-01-07 14:14:14 INFO     [epoch 4/10] average loss: 0.341, lr: 1e-05
2024-01-07 14:14:14 INFO     saving model related files
2024-01-07 14:14:14 INFO     saving model
2024-01-07 14:14:15 INFO     saving tokenizer
2024-01-07 14:14:15 INFO     saving optimizer
2024-01-07 14:14:16 INFO     remove old optimizer files
2024-01-07 14:14:16 INFO     complete training: model ckpt was saved at small_finetuned_modified_ckpt/model_mzgdpa
2024-01-07 14:14:16 INFO     ## 1st RUN (EVAL): Configuration 0/4 ##
2024-01-07 14:14:33 INFO     use spaCy answer extraction model: positionrank
2024-01-07 14:14:33 INFO     Model `small_finetuned_modified_ckpt/model_pzcecq/epoch_5`
2024-01-07 14:14:33 INFO     	 * Num of GPU in use: 1
2024-01-07 14:14:33 INFO     	 * Prefix: True
2024-01-07 14:14:33 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 14:14:34 INFO     encode all the data       : 570
  0%|          | 0/570 [00:00<?, ?it/s] 12%|â–ˆâ–        | 66/570 [00:00<00:00, 652.45it/s] 26%|â–ˆâ–ˆâ–‹       | 151/570 [00:00<00:00, 764.14it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 232/570 [00:00<00:00, 781.80it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 312/570 [00:00<00:00, 784.23it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 391/570 [00:00<00:00, 769.50it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 468/570 [00:00<00:00, 768.04it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 545/570 [00:00<00:00, 768.41it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 570/570 [00:00<00:00, 765.63it/s]
2024-01-07 14:14:35 INFO     after remove the overflow : 570
2024-01-07 14:14:35 INFO     preprocessed feature is saved at /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 14:19:27 INFO     encode all the data       : 569
  0%|          | 0/569 [00:00<?, ?it/s] 12%|â–ˆâ–        | 70/569 [00:00<00:00, 692.84it/s] 27%|â–ˆâ–ˆâ–‹       | 151/569 [00:00<00:00, 761.17it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 238/569 [00:00<00:00, 809.87it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 320/569 [00:00<00:00, 811.22it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 402/569 [00:00<00:00, 802.61it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 484/569 [00:00<00:00, 807.93it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 565/569 [00:00<00:00, 805.74it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 569/569 [00:00<00:00, 799.12it/s]
2024-01-07 14:19:28 INFO     after remove the overflow : 569
2024-01-07 14:19:28 INFO     preprocessed feature is saved at /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 14:24:31 INFO     	Bleu_1: 0.16239135043459635
2024-01-07 14:24:31 INFO     	Bleu_2: 0.08612227859263
2024-01-07 14:24:31 INFO     	Bleu_3: 0.04303268674467106
2024-01-07 14:24:31 INFO     	Bleu_4: 0.026717586741645736
2024-01-07 14:24:32 INFO     	Bleu_1: 0.15301374541108007
2024-01-07 14:24:32 INFO     	Bleu_2: 0.08296942999323542
2024-01-07 14:24:32 INFO     	Bleu_3: 0.0444760573226046
2024-01-07 14:24:32 INFO     	Bleu_4: 0.028665268266756524
2024-01-07 14:24:32 INFO     ## 1st RUN (EVAL): Configuration 1/4 ##
2024-01-07 14:24:51 INFO     use spaCy answer extraction model: positionrank
2024-01-07 14:24:51 INFO     Model `small_finetuned_modified_ckpt/model_eszyci/epoch_5`
2024-01-07 14:24:51 INFO     	 * Num of GPU in use: 1
2024-01-07 14:24:51 INFO     	 * Prefix: True
2024-01-07 14:24:51 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 14:24:52 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 14:29:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 14:34:27 INFO     	Bleu_1: 0.1480506067647803
2024-01-07 14:34:27 INFO     	Bleu_2: 0.0790912204026735
2024-01-07 14:34:27 INFO     	Bleu_3: 0.04008174448946867
2024-01-07 14:34:27 INFO     	Bleu_4: 0.02513953768725251
2024-01-07 14:34:28 INFO     	Bleu_1: 0.14126524523997405
2024-01-07 14:34:28 INFO     	Bleu_2: 0.07684573864597237
2024-01-07 14:34:28 INFO     	Bleu_3: 0.04142463185087003
2024-01-07 14:34:28 INFO     	Bleu_4: 0.027026200391869905
2024-01-07 14:34:28 INFO     ## 1st RUN (EVAL): Configuration 2/4 ##
2024-01-07 14:34:49 INFO     use spaCy answer extraction model: positionrank
2024-01-07 14:34:50 INFO     Model `small_finetuned_modified_ckpt/model_dpyopu/epoch_5`
2024-01-07 14:34:50 INFO     	 * Num of GPU in use: 1
2024-01-07 14:34:50 INFO     	 * Prefix: True
2024-01-07 14:34:50 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 14:34:51 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 14:39:41 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 14:44:27 INFO     	Bleu_1: 0.16239135043459635
2024-01-07 14:44:27 INFO     	Bleu_2: 0.08612227859263
2024-01-07 14:44:27 INFO     	Bleu_3: 0.04303268674467106
2024-01-07 14:44:27 INFO     	Bleu_4: 0.026717586741645736
2024-01-07 14:44:27 INFO     	Bleu_1: 0.15301374541108007
2024-01-07 14:44:27 INFO     	Bleu_2: 0.08296942999323542
2024-01-07 14:44:27 INFO     	Bleu_3: 0.0444760573226046
2024-01-07 14:44:27 INFO     	Bleu_4: 0.028665268266756524
2024-01-07 14:44:27 INFO     ## 1st RUN (EVAL): Configuration 3/4 ##
2024-01-07 14:44:44 INFO     use spaCy answer extraction model: positionrank
2024-01-07 14:44:44 INFO     Model `small_finetuned_modified_ckpt/model_mzgdpa/epoch_5`
2024-01-07 14:44:44 INFO     	 * Num of GPU in use: 1
2024-01-07 14:44:44 INFO     	 * Prefix: True
2024-01-07 14:44:44 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 14:44:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 14:49:40 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 14:54:54 INFO     	Bleu_1: 0.1480506067647803
2024-01-07 14:54:54 INFO     	Bleu_2: 0.0790912204026735
2024-01-07 14:54:54 INFO     	Bleu_3: 0.04008174448946867
2024-01-07 14:54:54 INFO     	Bleu_4: 0.02513953768725251
2024-01-07 14:54:55 INFO     	Bleu_1: 0.14126524523997405
2024-01-07 14:54:55 INFO     	Bleu_2: 0.07684573864597237
2024-01-07 14:54:55 INFO     	Bleu_3: 0.04142463185087003
2024-01-07 14:54:55 INFO     	Bleu_4: 0.027026200391869905
2024-01-07 14:54:55 INFO     1st RUN RESULTS (validation/Bleu_4)
2024-01-07 14:54:55 INFO     	 * rank: 0 | metric: 0.027 | model: small_finetuned_modified_ckpt/model_pzcecq/epoch_5 |
2024-01-07 14:54:55 INFO     	 * rank: 1 | metric: 0.027 | model: small_finetuned_modified_ckpt/model_dpyopu/epoch_5 |
2024-01-07 14:54:55 INFO     	 * rank: 2 | metric: 0.025 | model: small_finetuned_modified_ckpt/model_eszyci/epoch_5 |
2024-01-07 14:54:55 INFO     	 * rank: 3 | metric: 0.025 | model: small_finetuned_modified_ckpt/model_mzgdpa/epoch_5 |
2024-01-07 14:54:55 INFO     ## 2nd RUN: Configuration 0/4: validation/Bleu_4 = 0.026717586741645736
2024-01-07 14:54:55 INFO     initialize model trainer
2024-01-07 14:54:55 INFO     load config from existing checkpoint at small_finetuned_modified_ckpt/model_pzcecq
2024-01-07 14:54:55 INFO     hyperparameters
2024-01-07 14:54:55 INFO     	 * dataset_path: StellarMilk/newsqa_modified
2024-01-07 14:54:55 INFO     	 * dataset_name: default
2024-01-07 14:54:55 INFO     	 * input_types: ['paragraph']
2024-01-07 14:54:55 INFO     	 * output_types: ['questions_answers']
2024-01-07 14:54:55 INFO     	 * prefix_types: ['qag']
2024-01-07 14:54:55 INFO     	 * model: lmqg/t5-small-squad-qag
2024-01-07 14:54:55 INFO     	 * max_length: 512
2024-01-07 14:54:55 INFO     	 * max_length_output: 512
2024-01-07 14:54:55 INFO     	 * epoch: 10
2024-01-07 14:54:55 INFO     	 * batch: 2
2024-01-07 14:54:55 INFO     	 * lr: 1e-05
2024-01-07 14:54:55 INFO     	 * fp16: False
2024-01-07 14:54:55 INFO     	 * random_seed: 1
2024-01-07 14:54:55 INFO     	 * gradient_accumulation_steps: 4
2024-01-07 14:54:55 INFO     	 * label_smoothing: 0.15
2024-01-07 14:54:55 INFO     load checkpoint from small_finetuned_modified_ckpt/model_pzcecq/epoch_5
2024-01-07 14:54:55 INFO     use spaCy answer extraction model: positionrank
2024-01-07 14:54:56 INFO     Model `small_finetuned_modified_ckpt/model_pzcecq/epoch_5`
2024-01-07 14:54:56 INFO     	 * Num of GPU in use: 1
2024-01-07 14:54:56 INFO     	 * Prefix: True
2024-01-07 14:54:56 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 14:54:56 INFO     load optimizer from small_finetuned_modified_ckpt/model_pzcecq/optimizers/optimizer.5.pt
2024-01-07 14:54:56 INFO     optimizer is loading on cuda
2024-01-07 14:55:25 INFO     dataset preprocessing
2024-01-07 14:55:27 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa_modified/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2024-01-07 14:55:27 INFO     start model training
2024-01-07 14:55:43 INFO     	 * (global step 50: loss: 3.17464280128479, lr: 1e-05
2024-01-07 14:55:59 INFO     	 * (global step 100: loss: 2.9399224519729614, lr: 1e-05
2024-01-07 14:56:14 INFO     	 * (global step 150: loss: 2.839768171310425, lr: 1e-05
2024-01-07 14:56:30 INFO     	 * (global step 200: loss: 2.803089201450348, lr: 1e-05
2024-01-07 14:56:46 INFO     	 * (global step 250: loss: 2.7958696484565735, lr: 1e-05
2024-01-07 14:57:02 INFO     	 * (global step 300: loss: 2.6782291531562805, lr: 1e-05
2024-01-07 14:57:05 INFO     [epoch 5/10] average loss: 3.04, lr: 1e-05
2024-01-07 14:57:05 INFO     saving model related files
2024-01-07 14:57:05 INFO     saving model
2024-01-07 14:57:05 INFO     saving tokenizer
2024-01-07 14:57:06 INFO     saving optimizer
2024-01-07 14:57:07 INFO     remove old optimizer files
2024-01-07 14:57:20 INFO     	 * (global step 350: loss: 2.592772662639618, lr: 1e-05
2024-01-07 14:57:36 INFO     	 * (global step 400: loss: 2.7057631611824036, lr: 1e-05
2024-01-07 14:57:52 INFO     	 * (global step 450: loss: 2.5824493169784546, lr: 1e-05
2024-01-07 14:58:08 INFO     	 * (global step 500: loss: 2.679174304008484, lr: 1e-05
2024-01-07 14:58:24 INFO     	 * (global step 550: loss: 2.601011633872986, lr: 1e-05
2024-01-07 14:58:40 INFO     	 * (global step 600: loss: 2.6552801728248596, lr: 1e-05
2024-01-07 14:58:46 INFO     [epoch 6/10] average loss: 2.651, lr: 1e-05
2024-01-07 14:58:46 INFO     saving model related files
2024-01-07 14:58:46 INFO     saving model
2024-01-07 14:58:47 INFO     saving tokenizer
2024-01-07 14:58:47 INFO     saving optimizer
2024-01-07 14:58:48 INFO     remove old optimizer files
2024-01-07 14:58:58 INFO     	 * (global step 650: loss: 2.701190173625946, lr: 1e-05
2024-01-07 14:59:14 INFO     	 * (global step 700: loss: 2.5640650391578674, lr: 1e-05
2024-01-07 14:59:30 INFO     	 * (global step 750: loss: 2.602820575237274, lr: 1e-05
2024-01-07 14:59:46 INFO     	 * (global step 800: loss: 2.5632484555244446, lr: 1e-05
2024-01-07 15:00:02 INFO     	 * (global step 850: loss: 2.4805845618247986, lr: 1e-05
2024-01-07 15:00:18 INFO     	 * (global step 900: loss: 2.640108048915863, lr: 1e-05
2024-01-07 15:00:27 INFO     [epoch 7/10] average loss: 2.582, lr: 1e-05
2024-01-07 15:00:27 INFO     saving model related files
2024-01-07 15:00:27 INFO     saving model
2024-01-07 15:00:27 INFO     saving tokenizer
2024-01-07 15:00:27 INFO     saving optimizer
2024-01-07 15:00:28 INFO     remove old optimizer files
2024-01-07 15:00:36 INFO     	 * (global step 950: loss: 2.6645941138267517, lr: 1e-05
2024-01-07 15:00:52 INFO     	 * (global step 1000: loss: 2.495231866836548, lr: 1e-05
2024-01-07 15:01:08 INFO     	 * (global step 1050: loss: 2.579211175441742, lr: 1e-05
2024-01-07 15:01:24 INFO     	 * (global step 1100: loss: 2.582064211368561, lr: 1e-05
2024-01-07 15:01:40 INFO     	 * (global step 1150: loss: 2.4830378890037537, lr: 1e-05
2024-01-07 15:01:56 INFO     	 * (global step 1200: loss: 2.455976665019989, lr: 1e-05
2024-01-07 15:02:07 INFO     [epoch 8/10] average loss: 2.545, lr: 1e-05
2024-01-07 15:02:07 INFO     saving model related files
2024-01-07 15:02:07 INFO     saving model
2024-01-07 15:02:08 INFO     saving tokenizer
2024-01-07 15:02:08 INFO     saving optimizer
2024-01-07 15:02:09 INFO     remove old optimizer files
2024-01-07 15:02:13 INFO     	 * (global step 1250: loss: 2.565724790096283, lr: 1e-05
2024-01-07 15:02:29 INFO     	 * (global step 1300: loss: 2.564147412776947, lr: 1e-05
2024-01-07 15:02:45 INFO     	 * (global step 1350: loss: 2.5977689623832703, lr: 1e-05
2024-01-07 15:03:01 INFO     	 * (global step 1400: loss: 2.441361129283905, lr: 1e-05
2024-01-07 15:03:17 INFO     	 * (global step 1450: loss: 2.560541570186615, lr: 1e-05
2024-01-07 15:03:33 INFO     	 * (global step 1500: loss: 2.452678620815277, lr: 1e-05
2024-01-07 15:03:48 INFO     [epoch 9/10] average loss: 2.52, lr: 1e-05
2024-01-07 15:03:48 INFO     saving model related files
2024-01-07 15:03:48 INFO     saving model
2024-01-07 15:03:48 INFO     saving tokenizer
2024-01-07 15:03:48 INFO     saving optimizer
2024-01-07 15:03:49 INFO     remove old optimizer files
2024-01-07 15:03:49 INFO     complete training: model ckpt was saved at small_finetuned_modified_ckpt/model_pzcecq
2024-01-07 15:03:49 INFO     ## 2nd RUN: Configuration 1/4: validation/Bleu_4 = 0.026717586741645736
2024-01-07 15:03:49 INFO     initialize model trainer
2024-01-07 15:03:49 INFO     load config from existing checkpoint at small_finetuned_modified_ckpt/model_dpyopu
2024-01-07 15:03:49 INFO     hyperparameters
2024-01-07 15:03:49 INFO     	 * dataset_path: StellarMilk/newsqa_modified
2024-01-07 15:03:49 INFO     	 * dataset_name: default
2024-01-07 15:03:49 INFO     	 * input_types: ['paragraph']
2024-01-07 15:03:49 INFO     	 * output_types: ['questions_answers']
2024-01-07 15:03:49 INFO     	 * prefix_types: ['qag']
2024-01-07 15:03:49 INFO     	 * model: lmqg/t5-small-squad-qag
2024-01-07 15:03:49 INFO     	 * max_length: 512
2024-01-07 15:03:49 INFO     	 * max_length_output: 512
2024-01-07 15:03:49 INFO     	 * epoch: 10
2024-01-07 15:03:49 INFO     	 * batch: 2
2024-01-07 15:03:49 INFO     	 * lr: 1e-05
2024-01-07 15:03:49 INFO     	 * fp16: False
2024-01-07 15:03:49 INFO     	 * random_seed: 1
2024-01-07 15:03:49 INFO     	 * gradient_accumulation_steps: 4
2024-01-07 15:03:49 INFO     	 * label_smoothing: 0.0
2024-01-07 15:03:49 INFO     load checkpoint from small_finetuned_modified_ckpt/model_dpyopu/epoch_5
2024-01-07 15:03:50 INFO     use spaCy answer extraction model: positionrank
2024-01-07 15:03:50 INFO     Model `small_finetuned_modified_ckpt/model_dpyopu/epoch_5`
2024-01-07 15:03:50 INFO     	 * Num of GPU in use: 1
2024-01-07 15:03:50 INFO     	 * Prefix: True
2024-01-07 15:03:50 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 15:03:50 INFO     load optimizer from small_finetuned_modified_ckpt/model_dpyopu/optimizers/optimizer.5.pt
2024-01-07 15:03:50 INFO     optimizer is loading on cuda
2024-01-07 15:04:22 INFO     dataset preprocessing
2024-01-07 15:04:23 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa_modified/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2024-01-07 15:04:24 INFO     start model training
2024-01-07 15:04:39 INFO     	 * (global step 50: loss: 0.3205409534275532, lr: 1e-05
2024-01-07 15:04:55 INFO     	 * (global step 100: loss: 0.3276893310248852, lr: 1e-05
2024-01-07 15:05:10 INFO     	 * (global step 150: loss: 0.27259208634495735, lr: 1e-05
2024-01-07 15:05:26 INFO     	 * (global step 200: loss: 0.3412935771048069, lr: 1e-05
2024-01-07 15:05:41 INFO     	 * (global step 250: loss: 0.4120093658566475, lr: 1e-05
2024-01-07 15:05:57 INFO     	 * (global step 300: loss: 0.30197974294424057, lr: 1e-05
2024-01-07 15:06:00 INFO     [epoch 5/10] average loss: 0.342, lr: 1e-05
2024-01-07 15:06:00 INFO     saving model related files
2024-01-07 15:06:00 INFO     saving model
2024-01-07 15:06:00 INFO     saving tokenizer
2024-01-07 15:06:00 INFO     saving optimizer
2024-01-07 15:06:01 INFO     remove old optimizer files
2024-01-07 15:06:14 INFO     	 * (global step 350: loss: 0.23082357086241245, lr: 1e-05
2024-01-07 15:06:30 INFO     	 * (global step 400: loss: 0.38833756744861603, lr: 1e-05
2024-01-07 15:06:45 INFO     	 * (global step 450: loss: 0.2695715483278036, lr: 1e-05
2024-01-07 15:07:06 INFO     	 * (global step 500: loss: 0.3858475275337696, lr: 1e-05
2024-01-07 15:07:21 INFO     	 * (global step 550: loss: 0.3219282552599907, lr: 1e-05
2024-01-07 15:07:37 INFO     	 * (global step 600: loss: 0.3958299681544304, lr: 1e-05
2024-01-07 15:07:43 INFO     [epoch 6/10] average loss: 0.338, lr: 1e-05
2024-01-07 15:07:43 INFO     saving model related files
2024-01-07 15:07:43 INFO     saving model
2024-01-07 15:07:43 INFO     saving tokenizer
2024-01-07 15:07:43 INFO     saving optimizer
2024-01-07 15:07:44 INFO     remove old optimizer files
2024-01-07 15:07:54 INFO     	 * (global step 650: loss: 0.44018854945898056, lr: 1e-05
2024-01-07 15:08:10 INFO     	 * (global step 700: loss: 0.32723215594887733, lr: 1e-05
2024-01-07 15:08:25 INFO     	 * (global step 750: loss: 0.34050989896059036, lr: 1e-05
2024-01-07 15:08:41 INFO     	 * (global step 800: loss: 0.31170399114489555, lr: 1e-05
2024-01-07 15:08:57 INFO     	 * (global step 850: loss: 0.2550313025712967, lr: 1e-05
2024-01-07 15:09:12 INFO     	 * (global step 900: loss: 0.41591423749923706, lr: 1e-05
2024-01-07 15:09:21 INFO     [epoch 7/10] average loss: 0.335, lr: 1e-05
2024-01-07 15:09:21 INFO     saving model related files
2024-01-07 15:09:21 INFO     saving model
2024-01-07 15:09:21 INFO     saving tokenizer
2024-01-07 15:09:21 INFO     saving optimizer
2024-01-07 15:09:22 INFO     remove old optimizer files
2024-01-07 15:09:29 INFO     	 * (global step 950: loss: 0.4433816783130169, lr: 1e-05
2024-01-07 15:09:45 INFO     	 * (global step 1000: loss: 0.2972869873046875, lr: 1e-05
2024-01-07 15:10:00 INFO     	 * (global step 1050: loss: 0.38876669108867645, lr: 1e-05
2024-01-07 15:10:16 INFO     	 * (global step 1100: loss: 0.3751447722315788, lr: 1e-05
2024-01-07 15:10:33 INFO     	 * (global step 1150: loss: 0.28262729942798615, lr: 1e-05
2024-01-07 15:10:49 INFO     	 * (global step 1200: loss: 0.23678765818476677, lr: 1e-05
2024-01-07 15:11:00 INFO     [epoch 8/10] average loss: 0.334, lr: 1e-05
2024-01-07 15:11:00 INFO     saving model related files
2024-01-07 15:11:00 INFO     saving model
2024-01-07 15:11:01 INFO     saving tokenizer
2024-01-07 15:11:01 INFO     saving optimizer
2024-01-07 15:11:02 INFO     remove old optimizer files
2024-01-07 15:11:07 INFO     	 * (global step 1250: loss: 0.3727181442081928, lr: 1e-05
2024-01-07 15:11:22 INFO     	 * (global step 1300: loss: 0.36549925804138184, lr: 1e-05
2024-01-07 15:11:38 INFO     	 * (global step 1350: loss: 0.3948224112391472, lr: 1e-05
2024-01-07 15:11:54 INFO     	 * (global step 1400: loss: 0.2676287814974785, lr: 1e-05
2024-01-07 15:12:10 INFO     	 * (global step 1450: loss: 0.3760189302265644, lr: 1e-05
2024-01-07 15:12:27 INFO     	 * (global step 1500: loss: 0.25596947595477104, lr: 1e-05
2024-01-07 15:12:41 INFO     [epoch 9/10] average loss: 0.331, lr: 1e-05
2024-01-07 15:12:41 INFO     saving model related files
2024-01-07 15:12:41 INFO     saving model
2024-01-07 15:12:41 INFO     saving tokenizer
2024-01-07 15:12:42 INFO     saving optimizer
2024-01-07 15:12:43 INFO     remove old optimizer files
2024-01-07 15:12:43 INFO     complete training: model ckpt was saved at small_finetuned_modified_ckpt/model_dpyopu
2024-01-07 15:12:43 INFO     ## 2nd RUN: Configuration 2/4: validation/Bleu_4 = 0.02513953768725251
2024-01-07 15:12:43 INFO     initialize model trainer
2024-01-07 15:12:43 INFO     load config from existing checkpoint at small_finetuned_modified_ckpt/model_eszyci
2024-01-07 15:12:43 INFO     hyperparameters
2024-01-07 15:12:43 INFO     	 * dataset_path: StellarMilk/newsqa_modified
2024-01-07 15:12:43 INFO     	 * dataset_name: default
2024-01-07 15:12:43 INFO     	 * input_types: ['paragraph']
2024-01-07 15:12:43 INFO     	 * output_types: ['questions_answers']
2024-01-07 15:12:43 INFO     	 * prefix_types: ['qag']
2024-01-07 15:12:43 INFO     	 * model: lmqg/t5-small-squad-qag
2024-01-07 15:12:43 INFO     	 * max_length: 512
2024-01-07 15:12:43 INFO     	 * max_length_output: 512
2024-01-07 15:12:43 INFO     	 * epoch: 10
2024-01-07 15:12:43 INFO     	 * batch: 2
2024-01-07 15:12:43 INFO     	 * lr: 1e-05
2024-01-07 15:12:43 INFO     	 * fp16: False
2024-01-07 15:12:43 INFO     	 * random_seed: 1
2024-01-07 15:12:43 INFO     	 * gradient_accumulation_steps: 2
2024-01-07 15:12:43 INFO     	 * label_smoothing: 0.15
2024-01-07 15:12:43 INFO     load checkpoint from small_finetuned_modified_ckpt/model_eszyci/epoch_5
2024-01-07 15:12:45 INFO     use spaCy answer extraction model: positionrank
2024-01-07 15:12:46 INFO     Model `small_finetuned_modified_ckpt/model_eszyci/epoch_5`
2024-01-07 15:12:46 INFO     	 * Num of GPU in use: 1
2024-01-07 15:12:46 INFO     	 * Prefix: True
2024-01-07 15:12:46 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 15:12:46 INFO     load optimizer from small_finetuned_modified_ckpt/model_eszyci/optimizers/optimizer.5.pt
2024-01-07 15:12:46 INFO     optimizer is loading on cuda
2024-01-07 15:13:19 INFO     dataset preprocessing
2024-01-07 15:13:20 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa_modified/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2024-01-07 15:13:20 INFO     start model training
2024-01-07 15:13:28 INFO     	 * (global step 50: loss: 3.2537829875946045, lr: 1e-05
2024-01-07 15:13:36 INFO     	 * (global step 100: loss: 3.0172306299209595, lr: 1e-05
2024-01-07 15:13:45 INFO     	 * (global step 150: loss: 2.8254024982452393, lr: 1e-05
2024-01-07 15:13:53 INFO     	 * (global step 200: loss: 2.760247588157654, lr: 1e-05
2024-01-07 15:14:01 INFO     	 * (global step 250: loss: 2.713607907295227, lr: 1e-05
2024-01-07 15:14:09 INFO     	 * (global step 300: loss: 2.7355895042419434, lr: 1e-05
2024-01-07 15:14:18 INFO     	 * (global step 350: loss: 2.6029804944992065, lr: 1e-05
2024-01-07 15:14:26 INFO     	 * (global step 400: loss: 2.7791234254837036, lr: 1e-05
2024-01-07 15:14:34 INFO     	 * (global step 450: loss: 2.656345844268799, lr: 1e-05
2024-01-07 15:14:43 INFO     	 * (global step 500: loss: 2.783870220184326, lr: 1e-05
2024-01-07 15:14:51 INFO     	 * (global step 550: loss: 2.605305790901184, lr: 1e-05
2024-01-07 15:14:59 INFO     	 * (global step 600: loss: 2.5566989183425903, lr: 1e-05
2024-01-07 15:15:02 INFO     [epoch 5/10] average loss: 2.858, lr: 1e-05
2024-01-07 15:15:02 INFO     saving model related files
2024-01-07 15:15:02 INFO     saving model
2024-01-07 15:15:03 INFO     saving tokenizer
2024-01-07 15:15:03 INFO     saving optimizer
2024-01-07 15:15:04 INFO     remove old optimizer files
2024-01-07 15:15:09 INFO     	 * (global step 650: loss: 2.6826231479644775, lr: 1e-05
2024-01-07 15:15:18 INFO     	 * (global step 700: loss: 2.390062689781189, lr: 1e-05
2024-01-07 15:15:26 INFO     	 * (global step 750: loss: 2.5667078495025635, lr: 1e-05
2024-01-07 15:15:34 INFO     	 * (global step 800: loss: 2.696627616882324, lr: 1e-05
2024-01-07 15:15:43 INFO     	 * (global step 850: loss: 2.543051600456238, lr: 1e-05
2024-01-07 15:15:51 INFO     	 * (global step 900: loss: 2.4194798469543457, lr: 1e-05
2024-01-07 15:16:00 INFO     	 * (global step 950: loss: 2.5291777849197388, lr: 1e-05
2024-01-07 15:16:08 INFO     	 * (global step 1000: loss: 2.7398791313171387, lr: 1e-05
2024-01-07 15:16:16 INFO     	 * (global step 1050: loss: 2.601726770401001, lr: 1e-05
2024-01-07 15:16:25 INFO     	 * (global step 1100: loss: 2.5207302570343018, lr: 1e-05
2024-01-07 15:16:33 INFO     	 * (global step 1150: loss: 2.4468482732772827, lr: 1e-05
2024-01-07 15:16:46 INFO     	 * (global step 1200: loss: 2.4767520427703857, lr: 1e-05
2024-01-07 15:16:52 INFO     [epoch 6/10] average loss: 2.572, lr: 1e-05
2024-01-07 15:16:52 INFO     saving model related files
2024-01-07 15:16:52 INFO     saving model
2024-01-07 15:16:52 INFO     saving tokenizer
2024-01-07 15:16:53 INFO     saving optimizer
2024-01-07 15:16:53 INFO     remove old optimizer files
2024-01-07 15:16:56 INFO     	 * (global step 1250: loss: 2.5220141410827637, lr: 1e-05
2024-01-07 15:17:04 INFO     	 * (global step 1300: loss: 2.6617066860198975, lr: 1e-05
2024-01-07 15:17:13 INFO     	 * (global step 1350: loss: 2.534255623817444, lr: 1e-05
2024-01-07 15:17:21 INFO     	 * (global step 1400: loss: 2.4516890048980713, lr: 1e-05
2024-01-07 15:17:29 INFO     	 * (global step 1450: loss: 2.4562554359436035, lr: 1e-05
2024-01-07 15:17:38 INFO     	 * (global step 1500: loss: 2.4703266620635986, lr: 1e-05
2024-01-07 15:17:46 INFO     	 * (global step 1550: loss: 2.5631370544433594, lr: 1e-05
2024-01-07 15:17:54 INFO     	 * (global step 1600: loss: 2.5172799825668335, lr: 1e-05
2024-01-07 15:18:03 INFO     	 * (global step 1650: loss: 2.522609233856201, lr: 1e-05
2024-01-07 15:18:11 INFO     	 * (global step 1700: loss: 2.4401010274887085, lr: 1e-05
2024-01-07 15:18:19 INFO     	 * (global step 1750: loss: 2.397502064704895, lr: 1e-05
2024-01-07 15:18:28 INFO     	 * (global step 1800: loss: 2.5754562616348267, lr: 1e-05
2024-01-07 15:18:36 INFO     	 * (global step 1850: loss: 2.497308373451233, lr: 1e-05
2024-01-07 15:18:37 INFO     [epoch 7/10] average loss: 2.519, lr: 1e-05
2024-01-07 15:18:37 INFO     saving model related files
2024-01-07 15:18:37 INFO     saving model
2024-01-07 15:18:37 INFO     saving tokenizer
2024-01-07 15:18:37 INFO     saving optimizer
2024-01-07 15:18:38 INFO     remove old optimizer files
2024-01-07 15:18:46 INFO     	 * (global step 1900: loss: 2.4659870862960815, lr: 1e-05
2024-01-07 15:18:54 INFO     	 * (global step 1950: loss: 2.5053406953811646, lr: 1e-05
2024-01-07 15:19:03 INFO     	 * (global step 2000: loss: 2.4372833967208862, lr: 1e-05
2024-01-07 15:19:11 INFO     	 * (global step 2050: loss: 2.7025530338287354, lr: 1e-05
2024-01-07 15:19:20 INFO     	 * (global step 2100: loss: 2.621797204017639, lr: 1e-05
2024-01-07 15:19:28 INFO     	 * (global step 2150: loss: 2.4368966817855835, lr: 1e-05
2024-01-07 15:19:36 INFO     	 * (global step 2200: loss: 2.528983950614929, lr: 1e-05
2024-01-07 15:19:45 INFO     	 * (global step 2250: loss: 2.506245732307434, lr: 1e-05
2024-01-07 15:19:53 INFO     	 * (global step 2300: loss: 2.496954083442688, lr: 1e-05
2024-01-07 15:20:02 INFO     	 * (global step 2350: loss: 2.4917123317718506, lr: 1e-05
2024-01-07 15:20:10 INFO     	 * (global step 2400: loss: 2.4085450172424316, lr: 1e-05
2024-01-07 15:20:19 INFO     	 * (global step 2450: loss: 2.4985413551330566, lr: 1e-05
2024-01-07 15:20:23 INFO     [epoch 8/10] average loss: 2.489, lr: 1e-05
2024-01-07 15:20:23 INFO     saving model related files
2024-01-07 15:20:23 INFO     saving model
2024-01-07 15:20:23 INFO     saving tokenizer
2024-01-07 15:20:23 INFO     saving optimizer
2024-01-07 15:20:25 INFO     remove old optimizer files
2024-01-07 15:20:30 INFO     	 * (global step 2500: loss: 2.462600350379944, lr: 1e-05
2024-01-07 15:20:39 INFO     	 * (global step 2550: loss: 2.4719988107681274, lr: 1e-05
2024-01-07 15:20:47 INFO     	 * (global step 2600: loss: 2.445806622505188, lr: 1e-05
2024-01-07 15:20:56 INFO     	 * (global step 2650: loss: 2.434630036354065, lr: 1e-05
2024-01-07 15:21:05 INFO     	 * (global step 2700: loss: 2.565191864967346, lr: 1e-05
2024-01-07 15:21:13 INFO     	 * (global step 2750: loss: 2.392338514328003, lr: 1e-05
2024-01-07 15:21:22 INFO     	 * (global step 2800: loss: 2.390728712081909, lr: 1e-05
2024-01-07 15:21:30 INFO     	 * (global step 2850: loss: 2.4811079502105713, lr: 1e-05
2024-01-07 15:21:39 INFO     	 * (global step 2900: loss: 2.4244542121887207, lr: 1e-05
2024-01-07 15:21:47 INFO     	 * (global step 2950: loss: 2.38859760761261, lr: 1e-05
2024-01-07 15:21:55 INFO     	 * (global step 3000: loss: 2.3763961791992188, lr: 1e-05
2024-01-07 15:22:04 INFO     	 * (global step 3050: loss: 2.4607152938842773, lr: 1e-05
2024-01-07 15:22:10 INFO     [epoch 9/10] average loss: 2.467, lr: 1e-05
2024-01-07 15:22:10 INFO     saving model related files
2024-01-07 15:22:10 INFO     saving model
2024-01-07 15:22:11 INFO     saving tokenizer
2024-01-07 15:22:11 INFO     saving optimizer
2024-01-07 15:22:11 INFO     remove old optimizer files
2024-01-07 15:22:12 INFO     complete training: model ckpt was saved at small_finetuned_modified_ckpt/model_eszyci
2024-01-07 15:22:12 INFO     ## 2nd RUN: Configuration 3/4: validation/Bleu_4 = 0.02513953768725251
2024-01-07 15:22:12 INFO     initialize model trainer
2024-01-07 15:22:12 INFO     load config from existing checkpoint at small_finetuned_modified_ckpt/model_mzgdpa
2024-01-07 15:22:12 INFO     hyperparameters
2024-01-07 15:22:12 INFO     	 * dataset_path: StellarMilk/newsqa_modified
2024-01-07 15:22:12 INFO     	 * dataset_name: default
2024-01-07 15:22:12 INFO     	 * input_types: ['paragraph']
2024-01-07 15:22:12 INFO     	 * output_types: ['questions_answers']
2024-01-07 15:22:12 INFO     	 * prefix_types: ['qag']
2024-01-07 15:22:12 INFO     	 * model: lmqg/t5-small-squad-qag
2024-01-07 15:22:12 INFO     	 * max_length: 512
2024-01-07 15:22:12 INFO     	 * max_length_output: 512
2024-01-07 15:22:12 INFO     	 * epoch: 10
2024-01-07 15:22:12 INFO     	 * batch: 2
2024-01-07 15:22:12 INFO     	 * lr: 1e-05
2024-01-07 15:22:12 INFO     	 * fp16: False
2024-01-07 15:22:12 INFO     	 * random_seed: 1
2024-01-07 15:22:12 INFO     	 * gradient_accumulation_steps: 2
2024-01-07 15:22:12 INFO     	 * label_smoothing: 0.0
2024-01-07 15:22:12 INFO     load checkpoint from small_finetuned_modified_ckpt/model_mzgdpa/epoch_5
2024-01-07 15:22:12 INFO     use spaCy answer extraction model: positionrank
2024-01-07 15:22:13 INFO     Model `small_finetuned_modified_ckpt/model_mzgdpa/epoch_5`
2024-01-07 15:22:13 INFO     	 * Num of GPU in use: 1
2024-01-07 15:22:13 INFO     	 * Prefix: True
2024-01-07 15:22:13 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 15:22:13 INFO     load optimizer from small_finetuned_modified_ckpt/model_mzgdpa/optimizers/optimizer.5.pt
2024-01-07 15:22:13 INFO     optimizer is loading on cuda
2024-01-07 15:22:37 INFO     dataset preprocessing
2024-01-07 15:22:38 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa_modified/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2024-01-07 15:22:39 INFO     start model training
2024-01-07 15:22:46 INFO     	 * (global step 50: loss: 0.4447929859161377, lr: 1e-05
2024-01-07 15:22:54 INFO     	 * (global step 100: loss: 0.43541058897972107, lr: 1e-05
2024-01-07 15:23:03 INFO     	 * (global step 150: loss: 0.2785387262701988, lr: 1e-05
2024-01-07 15:23:11 INFO     	 * (global step 200: loss: 0.2791803330183029, lr: 1e-05
2024-01-07 15:23:19 INFO     	 * (global step 250: loss: 0.27497394382953644, lr: 1e-05
2024-01-07 15:23:27 INFO     	 * (global step 300: loss: 0.30468323826789856, lr: 1e-05
2024-01-07 15:23:35 INFO     	 * (global step 350: loss: 0.23795195668935776, lr: 1e-05
2024-01-07 15:23:43 INFO     	 * (global step 400: loss: 0.4347539246082306, lr: 1e-05
2024-01-07 15:23:51 INFO     	 * (global step 450: loss: 0.33173951506614685, lr: 1e-05
2024-01-07 15:23:59 INFO     	 * (global step 500: loss: 0.4920995235443115, lr: 1e-05
2024-01-07 15:24:07 INFO     	 * (global step 550: loss: 0.3049423396587372, lr: 1e-05
2024-01-07 15:24:15 INFO     	 * (global step 600: loss: 0.2789667770266533, lr: 1e-05
2024-01-07 15:24:18 INFO     [epoch 5/10] average loss: 0.334, lr: 1e-05
2024-01-07 15:24:18 INFO     saving model related files
2024-01-07 15:24:18 INFO     saving model
2024-01-07 15:24:18 INFO     saving tokenizer
2024-01-07 15:24:19 INFO     saving optimizer
2024-01-07 15:24:19 INFO     remove old optimizer files
2024-01-07 15:24:25 INFO     	 * (global step 650: loss: 0.3935094028711319, lr: 1e-05
2024-01-07 15:24:33 INFO     	 * (global step 700: loss: 0.11185391619801521, lr: 1e-05
2024-01-07 15:24:41 INFO     	 * (global step 750: loss: 0.3027367815375328, lr: 1e-05
2024-01-07 15:24:49 INFO     	 * (global step 800: loss: 0.43912364542484283, lr: 1e-05
2024-01-07 15:24:57 INFO     	 * (global step 850: loss: 0.3146324008703232, lr: 1e-05
2024-01-07 15:25:05 INFO     	 * (global step 900: loss: 0.16239764913916588, lr: 1e-05
2024-01-07 15:25:14 INFO     	 * (global step 950: loss: 0.29384201020002365, lr: 1e-05
2024-01-07 15:25:22 INFO     	 * (global step 1000: loss: 0.50975701212883, lr: 1e-05
2024-01-07 15:25:30 INFO     	 * (global step 1050: loss: 0.3788820952177048, lr: 1e-05
2024-01-07 15:25:38 INFO     	 * (global step 1100: loss: 0.3117540627717972, lr: 1e-05
2024-01-07 15:25:46 INFO     	 * (global step 1150: loss: 0.22328020632266998, lr: 1e-05
2024-01-07 15:25:54 INFO     	 * (global step 1200: loss: 0.27100038528442383, lr: 1e-05
2024-01-07 15:26:00 INFO     [epoch 6/10] average loss: 0.33, lr: 1e-05
2024-01-07 15:26:00 INFO     saving model related files
2024-01-07 15:26:00 INFO     saving model
2024-01-07 15:26:00 INFO     saving tokenizer
2024-01-07 15:26:00 INFO     saving optimizer
2024-01-07 15:26:01 INFO     remove old optimizer files
2024-01-07 15:26:03 INFO     	 * (global step 1250: loss: 0.3139035776257515, lr: 1e-05
2024-01-07 15:26:12 INFO     	 * (global step 1300: loss: 0.44676539301872253, lr: 1e-05
2024-01-07 15:26:20 INFO     	 * (global step 1350: loss: 0.33998507261276245, lr: 1e-05
2024-01-07 15:26:28 INFO     	 * (global step 1400: loss: 0.25536955893039703, lr: 1e-05
2024-01-07 15:26:36 INFO     	 * (global step 1450: loss: 0.24278582632541656, lr: 1e-05
2024-01-07 15:26:44 INFO     	 * (global step 1500: loss: 0.28660765290260315, lr: 1e-05
2024-01-07 15:26:52 INFO     	 * (global step 1550: loss: 0.3606821298599243, lr: 1e-05
2024-01-07 15:27:00 INFO     	 * (global step 1600: loss: 0.3253641724586487, lr: 1e-05
2024-01-07 15:27:08 INFO     	 * (global step 1650: loss: 0.3524002581834793, lr: 1e-05
2024-01-07 15:27:17 INFO     	 * (global step 1700: loss: 0.252114936709404, lr: 1e-05
2024-01-07 15:27:25 INFO     	 * (global step 1750: loss: 0.20908858627080917, lr: 1e-05
2024-01-07 15:27:33 INFO     	 * (global step 1800: loss: 0.3875613957643509, lr: 1e-05
2024-01-07 15:27:41 INFO     	 * (global step 1850: loss: 0.32104240357875824, lr: 1e-05
2024-01-07 15:27:41 INFO     [epoch 7/10] average loss: 0.327, lr: 1e-05
2024-01-07 15:27:41 INFO     saving model related files
2024-01-07 15:27:41 INFO     saving model
2024-01-07 15:27:42 INFO     saving tokenizer
2024-01-07 15:27:42 INFO     saving optimizer
2024-01-07 15:27:43 INFO     remove old optimizer files
2024-01-07 15:27:50 INFO     	 * (global step 1900: loss: 0.28777551651000977, lr: 1e-05
2024-01-07 15:27:58 INFO     	 * (global step 1950: loss: 0.3116496801376343, lr: 1e-05
2024-01-07 15:28:07 INFO     	 * (global step 2000: loss: 0.2825738713145256, lr: 1e-05
2024-01-07 15:28:15 INFO     	 * (global step 2050: loss: 0.5376895070075989, lr: 1e-05
2024-01-07 15:28:23 INFO     	 * (global step 2100: loss: 0.4860439747571945, lr: 1e-05
2024-01-07 15:28:31 INFO     	 * (global step 2150: loss: 0.2815755605697632, lr: 1e-05
2024-01-07 15:28:39 INFO     	 * (global step 2200: loss: 0.3531354069709778, lr: 1e-05
2024-01-07 15:28:47 INFO     	 * (global step 2250: loss: 0.34006717801094055, lr: 1e-05
2024-01-07 15:28:55 INFO     	 * (global step 2300: loss: 0.3353649526834488, lr: 1e-05
2024-01-07 15:29:03 INFO     	 * (global step 2350: loss: 0.33097386360168457, lr: 1e-05
2024-01-07 15:29:11 INFO     	 * (global step 2400: loss: 0.23448225110769272, lr: 1e-05
2024-01-07 15:29:19 INFO     	 * (global step 2450: loss: 0.3441483676433563, lr: 1e-05
2024-01-07 15:29:23 INFO     [epoch 8/10] average loss: 0.325, lr: 1e-05
2024-01-07 15:29:23 INFO     saving model related files
2024-01-07 15:29:23 INFO     saving model
2024-01-07 15:29:23 INFO     saving tokenizer
2024-01-07 15:29:23 INFO     saving optimizer
2024-01-07 15:29:24 INFO     remove old optimizer files
2024-01-07 15:29:29 INFO     	 * (global step 2500: loss: 0.31503976136446, lr: 1e-05
2024-01-07 15:29:37 INFO     	 * (global step 2550: loss: 0.32928894460201263, lr: 1e-05
2024-01-07 15:29:45 INFO     	 * (global step 2600: loss: 0.2890942394733429, lr: 1e-05
2024-01-07 15:29:53 INFO     	 * (global step 2650: loss: 0.30051007866859436, lr: 1e-05
2024-01-07 15:30:01 INFO     	 * (global step 2700: loss: 0.4103441536426544, lr: 1e-05
2024-01-07 15:30:09 INFO     	 * (global step 2750: loss: 0.2471761703491211, lr: 1e-05
2024-01-07 15:30:17 INFO     	 * (global step 2800: loss: 0.2617530971765518, lr: 1e-05
2024-01-07 15:30:25 INFO     	 * (global step 2850: loss: 0.34484510123729706, lr: 1e-05
2024-01-07 15:30:34 INFO     	 * (global step 2900: loss: 0.2967992424964905, lr: 1e-05
2024-01-07 15:30:42 INFO     	 * (global step 2950: loss: 0.24377980083227158, lr: 1e-05
2024-01-07 15:30:50 INFO     	 * (global step 3000: loss: 0.22654979676008224, lr: 1e-05
2024-01-07 15:30:58 INFO     	 * (global step 3050: loss: 0.32416220754384995, lr: 1e-05
2024-01-07 15:31:04 INFO     [epoch 9/10] average loss: 0.322, lr: 1e-05
2024-01-07 15:31:04 INFO     saving model related files
2024-01-07 15:31:04 INFO     saving model
2024-01-07 15:31:05 INFO     saving tokenizer
2024-01-07 15:31:05 INFO     saving optimizer
2024-01-07 15:31:05 INFO     remove old optimizer files
2024-01-07 15:31:06 INFO     complete training: model ckpt was saved at small_finetuned_modified_ckpt/model_mzgdpa
2024-01-07 15:31:06 INFO     ## 2nd RUN (EVAL): Configuration 0/4 ##
2024-01-07 15:31:24 INFO     use spaCy answer extraction model: positionrank
2024-01-07 15:31:25 INFO     Model `small_finetuned_modified_ckpt/model_pzcecq/epoch_1`
2024-01-07 15:31:25 INFO     	 * Num of GPU in use: 1
2024-01-07 15:31:25 INFO     	 * Prefix: True
2024-01-07 15:31:25 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 15:31:26 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 15:34:07 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 15:36:39 INFO     	Bleu_1: 0.23991313489942884
2024-01-07 15:36:39 INFO     	Bleu_2: 0.13240233798702825
2024-01-07 15:36:39 INFO     	Bleu_3: 0.07322500573569289
2024-01-07 15:36:39 INFO     	Bleu_4: 0.047803634928764695
2024-01-07 15:36:40 INFO     	Bleu_1: 0.23449745749911502
2024-01-07 15:36:40 INFO     	Bleu_2: 0.12932127191155304
2024-01-07 15:36:40 INFO     	Bleu_3: 0.07175210571706507
2024-01-07 15:36:40 INFO     	Bleu_4: 0.04663414698465953
2024-01-07 15:36:57 INFO     use spaCy answer extraction model: positionrank
2024-01-07 15:36:57 INFO     Model `small_finetuned_modified_ckpt/model_pzcecq/epoch_10`
2024-01-07 15:36:57 INFO     	 * Num of GPU in use: 1
2024-01-07 15:36:57 INFO     	 * Prefix: True
2024-01-07 15:36:57 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 15:36:58 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 15:42:15 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 15:47:35 INFO     	Bleu_1: 0.11329121802452143
2024-01-07 15:47:35 INFO     	Bleu_2: 0.06073805477860831
2024-01-07 15:47:35 INFO     	Bleu_3: 0.030324604466891208
2024-01-07 15:47:35 INFO     	Bleu_4: 0.018622524239511707
2024-01-07 15:47:35 INFO     	Bleu_1: 0.10837498388875287
2024-01-07 15:47:35 INFO     	Bleu_2: 0.05871953048017548
2024-01-07 15:47:35 INFO     	Bleu_3: 0.030670319809400486
2024-01-07 15:47:35 INFO     	Bleu_4: 0.01939814457308781
2024-01-07 15:47:51 INFO     use spaCy answer extraction model: positionrank
2024-01-07 15:47:51 INFO     Model `small_finetuned_modified_ckpt/model_pzcecq/epoch_2`
2024-01-07 15:47:51 INFO     	 * Num of GPU in use: 1
2024-01-07 15:47:51 INFO     	 * Prefix: True
2024-01-07 15:47:51 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 15:47:52 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 15:52:10 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 15:56:09 INFO     	Bleu_1: 0.2023991114402043
2024-01-07 15:56:09 INFO     	Bleu_2: 0.11013976114206207
2024-01-07 15:56:09 INFO     	Bleu_3: 0.058238516492820365
2024-01-07 15:56:09 INFO     	Bleu_4: 0.03721317900060897
2024-01-07 15:56:09 INFO     	Bleu_1: 0.19054459069375315
2024-01-07 15:56:09 INFO     	Bleu_2: 0.10355907927490644
2024-01-07 15:56:09 INFO     	Bleu_3: 0.054539398041289135
2024-01-07 15:56:09 INFO     	Bleu_4: 0.0340536020175938
2024-01-07 15:56:26 INFO     use spaCy answer extraction model: positionrank
2024-01-07 15:56:27 INFO     Model `small_finetuned_modified_ckpt/model_pzcecq/epoch_3`
2024-01-07 15:56:27 INFO     	 * Num of GPU in use: 1
2024-01-07 15:56:27 INFO     	 * Prefix: True
2024-01-07 15:56:27 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 15:56:28 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 16:00:54 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 16:04:51 INFO     	Bleu_1: 0.19138674292543156
2024-01-07 16:04:51 INFO     	Bleu_2: 0.10366910494794022
2024-01-07 16:04:51 INFO     	Bleu_3: 0.05445562921466695
2024-01-07 16:04:51 INFO     	Bleu_4: 0.034927056341631006
2024-01-07 16:04:51 INFO     	Bleu_1: 0.18617277992277742
2024-01-07 16:04:51 INFO     	Bleu_2: 0.10118781905257782
2024-01-07 16:04:51 INFO     	Bleu_3: 0.05454252950411726
2024-01-07 16:04:51 INFO     	Bleu_4: 0.034908231261302124
2024-01-07 16:05:06 INFO     use spaCy answer extraction model: positionrank
2024-01-07 16:05:06 INFO     Model `small_finetuned_modified_ckpt/model_pzcecq/epoch_4`
2024-01-07 16:05:06 INFO     	 * Num of GPU in use: 1
2024-01-07 16:05:06 INFO     	 * Prefix: True
2024-01-07 16:05:06 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 16:05:07 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 16:09:39 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 16:14:02 INFO     	Bleu_1: 0.16774201417263085
2024-01-07 16:14:02 INFO     	Bleu_2: 0.09016415526518434
2024-01-07 16:14:02 INFO     	Bleu_3: 0.04657821423920196
2024-01-07 16:14:02 INFO     	Bleu_4: 0.02954074361871371
2024-01-07 16:14:02 INFO     	Bleu_1: 0.16442808883110058
2024-01-07 16:14:02 INFO     	Bleu_2: 0.08914869001876234
2024-01-07 16:14:02 INFO     	Bleu_3: 0.047634459707548024
2024-01-07 16:14:02 INFO     	Bleu_4: 0.030556402875482565
2024-01-07 16:14:26 INFO     use spaCy answer extraction model: positionrank
2024-01-07 16:14:26 INFO     Model `small_finetuned_modified_ckpt/model_pzcecq/epoch_6`
2024-01-07 16:14:26 INFO     	 * Num of GPU in use: 1
2024-01-07 16:14:26 INFO     	 * Prefix: True
2024-01-07 16:14:26 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 16:14:28 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 16:16:29 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 16:18:23 INFO     	Bleu_1: 0.22846661679013427
2024-01-07 16:18:23 INFO     	Bleu_2: 0.12189415663132994
2024-01-07 16:18:23 INFO     	Bleu_3: 0.06376047962038522
2024-01-07 16:18:23 INFO     	Bleu_4: 0.040360942729443586
2024-01-07 16:18:23 INFO     	Bleu_1: 0.2338655627833583
2024-01-07 16:18:23 INFO     	Bleu_2: 0.12557152702657845
2024-01-07 16:18:23 INFO     	Bleu_3: 0.06721463916024357
2024-01-07 16:18:23 INFO     	Bleu_4: 0.042746618786461174
2024-01-07 16:18:41 INFO     use spaCy answer extraction model: positionrank
2024-01-07 16:18:42 INFO     Model `small_finetuned_modified_ckpt/model_pzcecq/epoch_7`
2024-01-07 16:18:42 INFO     	 * Num of GPU in use: 1
2024-01-07 16:18:42 INFO     	 * Prefix: True
2024-01-07 16:18:42 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 16:18:43 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 16:23:26 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 16:27:49 INFO     	Bleu_1: 0.1771360072750296
2024-01-07 16:27:49 INFO     	Bleu_2: 0.09474362186942942
2024-01-07 16:27:49 INFO     	Bleu_3: 0.04911627594588745
2024-01-07 16:27:49 INFO     	Bleu_4: 0.03089602375663115
2024-01-07 16:27:49 INFO     	Bleu_1: 0.17290193462307973
2024-01-07 16:27:49 INFO     	Bleu_2: 0.09237703773876409
2024-01-07 16:27:49 INFO     	Bleu_3: 0.048845489175702514
2024-01-07 16:27:49 INFO     	Bleu_4: 0.03072403635506369
2024-01-07 16:28:05 INFO     use spaCy answer extraction model: positionrank
2024-01-07 16:28:05 INFO     Model `small_finetuned_modified_ckpt/model_pzcecq/epoch_8`
2024-01-07 16:28:05 INFO     	 * Num of GPU in use: 1
2024-01-07 16:28:05 INFO     	 * Prefix: True
2024-01-07 16:28:05 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 16:28:06 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 16:33:15 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 16:38:38 INFO     	Bleu_1: 0.14006335496093647
2024-01-07 16:38:38 INFO     	Bleu_2: 0.07492365897278887
2024-01-07 16:38:38 INFO     	Bleu_3: 0.038258006262762274
2024-01-07 16:38:38 INFO     	Bleu_4: 0.02392012565969385
2024-01-07 16:38:38 INFO     	Bleu_1: 0.13888498947157185
2024-01-07 16:38:38 INFO     	Bleu_2: 0.07531594119428933
2024-01-07 16:38:38 INFO     	Bleu_3: 0.04006768499429433
2024-01-07 16:38:38 INFO     	Bleu_4: 0.025407412865859413
2024-01-07 16:38:55 INFO     use spaCy answer extraction model: positionrank
2024-01-07 16:38:55 INFO     Model `small_finetuned_modified_ckpt/model_pzcecq/epoch_9`
2024-01-07 16:38:55 INFO     	 * Num of GPU in use: 1
2024-01-07 16:38:55 INFO     	 * Prefix: True
2024-01-07 16:38:55 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 16:38:56 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 16:44:11 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 16:49:43 INFO     	Bleu_1: 0.12317233732935934
2024-01-07 16:49:43 INFO     	Bleu_2: 0.066318408593598
2024-01-07 16:49:43 INFO     	Bleu_3: 0.03375939506099596
2024-01-07 16:49:43 INFO     	Bleu_4: 0.021028034986917018
2024-01-07 16:49:44 INFO     	Bleu_1: 0.11946106328128703
2024-01-07 16:49:44 INFO     	Bleu_2: 0.06459065483587598
2024-01-07 16:49:44 INFO     	Bleu_3: 0.033748328594747246
2024-01-07 16:49:44 INFO     	Bleu_4: 0.021263884214379605
2024-01-07 16:49:44 INFO     ## 2nd RUN (EVAL): Configuration 1/4 ##
2024-01-07 16:50:01 INFO     use spaCy answer extraction model: positionrank
2024-01-07 16:50:02 INFO     Model `small_finetuned_modified_ckpt/model_dpyopu/epoch_1`
2024-01-07 16:50:02 INFO     	 * Num of GPU in use: 1
2024-01-07 16:50:02 INFO     	 * Prefix: True
2024-01-07 16:50:02 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 16:50:03 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 16:52:44 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 16:55:16 INFO     	Bleu_1: 0.23991313489942884
2024-01-07 16:55:16 INFO     	Bleu_2: 0.13240233798702825
2024-01-07 16:55:16 INFO     	Bleu_3: 0.07322500573569289
2024-01-07 16:55:16 INFO     	Bleu_4: 0.047803634928764695
2024-01-07 16:55:17 INFO     	Bleu_1: 0.23449745749911502
2024-01-07 16:55:17 INFO     	Bleu_2: 0.12932127191155304
2024-01-07 16:55:17 INFO     	Bleu_3: 0.07175210571706507
2024-01-07 16:55:17 INFO     	Bleu_4: 0.04663414698465953
2024-01-07 16:55:36 INFO     use spaCy answer extraction model: positionrank
2024-01-07 16:55:36 INFO     Model `small_finetuned_modified_ckpt/model_dpyopu/epoch_10`
2024-01-07 16:55:36 INFO     	 * Num of GPU in use: 1
2024-01-07 16:55:36 INFO     	 * Prefix: True
2024-01-07 16:55:36 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 16:55:37 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 17:00:42 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 17:06:09 INFO     	Bleu_1: 0.1338691117976956
2024-01-07 17:06:09 INFO     	Bleu_2: 0.0715844538539443
2024-01-07 17:06:09 INFO     	Bleu_3: 0.03623346151332077
2024-01-07 17:06:09 INFO     	Bleu_4: 0.0226762992930446
2024-01-07 17:06:10 INFO     	Bleu_1: 0.12524893966285758
2024-01-07 17:06:10 INFO     	Bleu_2: 0.06732968263828301
2024-01-07 17:06:10 INFO     	Bleu_3: 0.03526528965627461
2024-01-07 17:06:10 INFO     	Bleu_4: 0.02254620843534825
2024-01-07 17:06:31 INFO     use spaCy answer extraction model: positionrank
2024-01-07 17:06:31 INFO     Model `small_finetuned_modified_ckpt/model_dpyopu/epoch_2`
2024-01-07 17:06:31 INFO     	 * Num of GPU in use: 1
2024-01-07 17:06:31 INFO     	 * Prefix: True
2024-01-07 17:06:31 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 17:06:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 17:10:35 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 17:14:36 INFO     	Bleu_1: 0.2023991114402043
2024-01-07 17:14:36 INFO     	Bleu_2: 0.11013976114206207
2024-01-07 17:14:36 INFO     	Bleu_3: 0.058238516492820365
2024-01-07 17:14:36 INFO     	Bleu_4: 0.03721317900060897
2024-01-07 17:14:36 INFO     	Bleu_1: 0.19054459069375315
2024-01-07 17:14:36 INFO     	Bleu_2: 0.10355907927490644
2024-01-07 17:14:36 INFO     	Bleu_3: 0.054539398041289135
2024-01-07 17:14:36 INFO     	Bleu_4: 0.0340536020175938
2024-01-07 17:14:52 INFO     use spaCy answer extraction model: positionrank
2024-01-07 17:14:53 INFO     Model `small_finetuned_modified_ckpt/model_dpyopu/epoch_3`
2024-01-07 17:14:53 INFO     	 * Num of GPU in use: 1
2024-01-07 17:14:53 INFO     	 * Prefix: True
2024-01-07 17:14:53 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 17:14:54 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 17:19:04 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 17:23:13 INFO     	Bleu_1: 0.19138674292543156
2024-01-07 17:23:13 INFO     	Bleu_2: 0.10366910494794022
2024-01-07 17:23:13 INFO     	Bleu_3: 0.05445562921466695
2024-01-07 17:23:13 INFO     	Bleu_4: 0.034927056341631006
2024-01-07 17:23:13 INFO     	Bleu_1: 0.18617277992277742
2024-01-07 17:23:13 INFO     	Bleu_2: 0.10118781905257782
2024-01-07 17:23:13 INFO     	Bleu_3: 0.05454252950411726
2024-01-07 17:23:13 INFO     	Bleu_4: 0.034908231261302124
2024-01-07 17:23:34 INFO     use spaCy answer extraction model: positionrank
2024-01-07 17:23:34 INFO     Model `small_finetuned_modified_ckpt/model_dpyopu/epoch_4`
2024-01-07 17:23:34 INFO     	 * Num of GPU in use: 1
2024-01-07 17:23:34 INFO     	 * Prefix: True
2024-01-07 17:23:34 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 17:23:36 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 17:28:11 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 17:32:51 INFO     	Bleu_1: 0.16774201417263085
2024-01-07 17:32:51 INFO     	Bleu_2: 0.09016415526518434
2024-01-07 17:32:51 INFO     	Bleu_3: 0.04657821423920196
2024-01-07 17:32:51 INFO     	Bleu_4: 0.02954074361871371
2024-01-07 17:32:51 INFO     	Bleu_1: 0.16442808883110058
2024-01-07 17:32:51 INFO     	Bleu_2: 0.08914869001876234
2024-01-07 17:32:51 INFO     	Bleu_3: 0.047634459707548024
2024-01-07 17:32:51 INFO     	Bleu_4: 0.030556402875482565
2024-01-07 17:33:12 INFO     use spaCy answer extraction model: positionrank
2024-01-07 17:33:12 INFO     Model `small_finetuned_modified_ckpt/model_dpyopu/epoch_6`
2024-01-07 17:33:12 INFO     	 * Num of GPU in use: 1
2024-01-07 17:33:12 INFO     	 * Prefix: True
2024-01-07 17:33:12 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 17:33:14 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 17:38:11 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 17:42:57 INFO     	Bleu_1: 0.15176743840744783
2024-01-07 17:42:57 INFO     	Bleu_2: 0.08127369684732857
2024-01-07 17:42:57 INFO     	Bleu_3: 0.04152834532451742
2024-01-07 17:42:57 INFO     	Bleu_4: 0.026079672106481582
2024-01-07 17:42:57 INFO     	Bleu_1: 0.1390974620813923
2024-01-07 17:42:57 INFO     	Bleu_2: 0.07547529240520906
2024-01-07 17:42:57 INFO     	Bleu_3: 0.040465135169130105
2024-01-07 17:42:57 INFO     	Bleu_4: 0.026274111816343766
2024-01-07 17:43:16 INFO     use spaCy answer extraction model: positionrank
2024-01-07 17:43:16 INFO     Model `small_finetuned_modified_ckpt/model_dpyopu/epoch_7`
2024-01-07 17:43:16 INFO     	 * Num of GPU in use: 1
2024-01-07 17:43:16 INFO     	 * Prefix: True
2024-01-07 17:43:16 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 17:43:17 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 17:48:29 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 17:53:46 INFO     	Bleu_1: 0.13580620602991905
2024-01-07 17:53:46 INFO     	Bleu_2: 0.07307966349718945
2024-01-07 17:53:46 INFO     	Bleu_3: 0.037496771239739456
2024-01-07 17:53:46 INFO     	Bleu_4: 0.02361457931779822
2024-01-07 17:53:47 INFO     	Bleu_1: 0.12893144548713786
2024-01-07 17:53:47 INFO     	Bleu_2: 0.07007057226596485
2024-01-07 17:53:47 INFO     	Bleu_3: 0.03756132375475709
2024-01-07 17:53:47 INFO     	Bleu_4: 0.024367428644340664
2024-01-07 17:54:07 INFO     use spaCy answer extraction model: positionrank
2024-01-07 17:54:07 INFO     Model `small_finetuned_modified_ckpt/model_dpyopu/epoch_8`
2024-01-07 17:54:07 INFO     	 * Num of GPU in use: 1
2024-01-07 17:54:07 INFO     	 * Prefix: True
2024-01-07 17:54:07 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 17:54:08 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 17:58:55 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 18:03:32 INFO     	Bleu_1: 0.14936073106940626
2024-01-07 18:03:32 INFO     	Bleu_2: 0.08005065693519002
2024-01-07 18:03:32 INFO     	Bleu_3: 0.04124066009537637
2024-01-07 18:03:32 INFO     	Bleu_4: 0.026072516049854347
2024-01-07 18:03:32 INFO     	Bleu_1: 0.13944859375149288
2024-01-07 18:03:32 INFO     	Bleu_2: 0.07519592193555212
2024-01-07 18:03:32 INFO     	Bleu_3: 0.039992391101004165
2024-01-07 18:03:32 INFO     	Bleu_4: 0.02576819584206995
2024-01-07 18:03:53 INFO     use spaCy answer extraction model: positionrank
2024-01-07 18:03:53 INFO     Model `small_finetuned_modified_ckpt/model_dpyopu/epoch_9`
2024-01-07 18:03:53 INFO     	 * Num of GPU in use: 1
2024-01-07 18:03:53 INFO     	 * Prefix: True
2024-01-07 18:03:53 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 18:03:54 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 18:08:58 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 18:13:52 INFO     	Bleu_1: 0.13670911700835384
2024-01-07 18:13:52 INFO     	Bleu_2: 0.07372334583049459
2024-01-07 18:13:52 INFO     	Bleu_3: 0.038200983456278134
2024-01-07 18:13:52 INFO     	Bleu_4: 0.024244866990642162
2024-01-07 18:13:53 INFO     	Bleu_1: 0.12706956462492228
2024-01-07 18:13:53 INFO     	Bleu_2: 0.06852596578678806
2024-01-07 18:13:53 INFO     	Bleu_3: 0.035746013684563704
2024-01-07 18:13:53 INFO     	Bleu_4: 0.022635346877145354
2024-01-07 18:13:53 INFO     ## 2nd RUN (EVAL): Configuration 2/4 ##
2024-01-07 18:14:15 INFO     use spaCy answer extraction model: positionrank
2024-01-07 18:14:15 INFO     Model `small_finetuned_modified_ckpt/model_eszyci/epoch_1`
2024-01-07 18:14:15 INFO     	 * Num of GPU in use: 1
2024-01-07 18:14:15 INFO     	 * Prefix: True
2024-01-07 18:14:15 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 18:14:16 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 18:18:07 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 18:21:17 INFO     	Bleu_1: 0.22474488550363386
2024-01-07 18:21:17 INFO     	Bleu_2: 0.12364643634463128
2024-01-07 18:21:17 INFO     	Bleu_3: 0.06768287232916305
2024-01-07 18:21:17 INFO     	Bleu_4: 0.0440133702814989
2024-01-07 18:21:18 INFO     	Bleu_1: 0.21511095947768846
2024-01-07 18:21:18 INFO     	Bleu_2: 0.11822726217379781
2024-01-07 18:21:18 INFO     	Bleu_3: 0.06464069515655077
2024-01-07 18:21:18 INFO     	Bleu_4: 0.041688912916527605
2024-01-07 18:21:37 INFO     use spaCy answer extraction model: positionrank
2024-01-07 18:21:38 INFO     Model `small_finetuned_modified_ckpt/model_eszyci/epoch_10`
2024-01-07 18:21:38 INFO     	 * Num of GPU in use: 1
2024-01-07 18:21:38 INFO     	 * Prefix: True
2024-01-07 18:21:38 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 18:21:39 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 18:27:01 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 18:32:26 INFO     	Bleu_1: 0.10152603351656368
2024-01-07 18:32:26 INFO     	Bleu_2: 0.05426035512230693
2024-01-07 18:32:26 INFO     	Bleu_3: 0.027071383686314045
2024-01-07 18:32:26 INFO     	Bleu_4: 0.01664525562828387
2024-01-07 18:32:27 INFO     	Bleu_1: 0.09613847383631498
2024-01-07 18:32:27 INFO     	Bleu_2: 0.05187941665642668
2024-01-07 18:32:27 INFO     	Bleu_3: 0.02652696221639971
2024-01-07 18:32:27 INFO     	Bleu_4: 0.01646779483204919
2024-01-07 18:32:44 INFO     use spaCy answer extraction model: positionrank
2024-01-07 18:32:45 INFO     Model `small_finetuned_modified_ckpt/model_eszyci/epoch_2`
2024-01-07 18:32:45 INFO     	 * Num of GPU in use: 1
2024-01-07 18:32:45 INFO     	 * Prefix: True
2024-01-07 18:32:45 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 18:32:46 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 18:37:23 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 18:41:41 INFO     	Bleu_1: 0.17355754203913015
2024-01-07 18:41:41 INFO     	Bleu_2: 0.09421882862255222
2024-01-07 18:41:41 INFO     	Bleu_3: 0.04952645281081036
2024-01-07 18:41:41 INFO     	Bleu_4: 0.03169175770562416
2024-01-07 18:41:41 INFO     	Bleu_1: 0.17383578885022366
2024-01-07 18:41:41 INFO     	Bleu_2: 0.09435143418972734
2024-01-07 18:41:41 INFO     	Bleu_3: 0.05058811494427377
2024-01-07 18:41:41 INFO     	Bleu_4: 0.03234855619983433
2024-01-07 18:42:03 INFO     use spaCy answer extraction model: positionrank
2024-01-07 18:42:03 INFO     Model `small_finetuned_modified_ckpt/model_eszyci/epoch_3`
2024-01-07 18:42:03 INFO     	 * Num of GPU in use: 1
2024-01-07 18:42:03 INFO     	 * Prefix: True
2024-01-07 18:42:03 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 18:42:04 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 18:46:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 18:51:11 INFO     	Bleu_1: 0.17019523643081627
2024-01-07 18:51:11 INFO     	Bleu_2: 0.09187707081618036
2024-01-07 18:51:11 INFO     	Bleu_3: 0.04792878901191193
2024-01-07 18:51:11 INFO     	Bleu_4: 0.030546739682887836
2024-01-07 18:51:11 INFO     	Bleu_1: 0.1660655351249391
2024-01-07 18:51:11 INFO     	Bleu_2: 0.08955719079530002
2024-01-07 18:51:11 INFO     	Bleu_3: 0.0474798727752711
2024-01-07 18:51:11 INFO     	Bleu_4: 0.030407781411118138
2024-01-07 18:51:31 INFO     use spaCy answer extraction model: positionrank
2024-01-07 18:51:31 INFO     Model `small_finetuned_modified_ckpt/model_eszyci/epoch_4`
2024-01-07 18:51:31 INFO     	 * Num of GPU in use: 1
2024-01-07 18:51:31 INFO     	 * Prefix: True
2024-01-07 18:51:31 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 18:51:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 18:56:31 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 19:01:38 INFO     	Bleu_1: 0.14611099858213342
2024-01-07 19:01:38 INFO     	Bleu_2: 0.07840159045003771
2024-01-07 19:01:38 INFO     	Bleu_3: 0.03994451757435584
2024-01-07 19:01:38 INFO     	Bleu_4: 0.025055370732675147
2024-01-07 19:01:38 INFO     	Bleu_1: 0.1398860123647591
2024-01-07 19:01:38 INFO     	Bleu_2: 0.07569649490475948
2024-01-07 19:01:38 INFO     	Bleu_3: 0.03988869454605294
2024-01-07 19:01:38 INFO     	Bleu_4: 0.02538389807966685
2024-01-07 19:02:03 INFO     use spaCy answer extraction model: positionrank
2024-01-07 19:02:04 INFO     Model `small_finetuned_modified_ckpt/model_eszyci/epoch_6`
2024-01-07 19:02:04 INFO     	 * Num of GPU in use: 1
2024-01-07 19:02:04 INFO     	 * Prefix: True
2024-01-07 19:02:04 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 19:02:05 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 19:06:05 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 19:10:10 INFO     	Bleu_1: 0.185956314547283
2024-01-07 19:10:10 INFO     	Bleu_2: 0.10008963001565349
2024-01-07 19:10:10 INFO     	Bleu_3: 0.05313312244836998
2024-01-07 19:10:10 INFO     	Bleu_4: 0.03414022060543363
2024-01-07 19:10:10 INFO     	Bleu_1: 0.18933451127933787
2024-01-07 19:10:10 INFO     	Bleu_2: 0.1020253165395761
2024-01-07 19:10:10 INFO     	Bleu_3: 0.054801708694864455
2024-01-07 19:10:10 INFO     	Bleu_4: 0.035237689501408216
2024-01-07 19:10:29 INFO     use spaCy answer extraction model: positionrank
2024-01-07 19:10:29 INFO     Model `small_finetuned_modified_ckpt/model_eszyci/epoch_7`
2024-01-07 19:10:29 INFO     	 * Num of GPU in use: 1
2024-01-07 19:10:29 INFO     	 * Prefix: True
2024-01-07 19:10:29 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 19:10:30 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 19:15:49 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 19:21:25 INFO     	Bleu_1: 0.12070574264144439
2024-01-07 19:21:25 INFO     	Bleu_2: 0.06509865026373093
2024-01-07 19:21:25 INFO     	Bleu_3: 0.03331843605977507
2024-01-07 19:21:25 INFO     	Bleu_4: 0.020931827029669375
2024-01-07 19:21:26 INFO     	Bleu_1: 0.11879300955492748
2024-01-07 19:21:26 INFO     	Bleu_2: 0.06430600306440694
2024-01-07 19:21:26 INFO     	Bleu_3: 0.03372903090589114
2024-01-07 19:21:26 INFO     	Bleu_4: 0.021283873730138094
2024-01-07 19:21:44 INFO     use spaCy answer extraction model: positionrank
2024-01-07 19:21:44 INFO     Model `small_finetuned_modified_ckpt/model_eszyci/epoch_8`
2024-01-07 19:21:44 INFO     	 * Num of GPU in use: 1
2024-01-07 19:21:44 INFO     	 * Prefix: True
2024-01-07 19:21:44 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 19:21:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 19:27:23 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 19:32:39 INFO     	Bleu_1: 0.11274404304381158
2024-01-07 19:32:39 INFO     	Bleu_2: 0.060613256747564216
2024-01-07 19:32:39 INFO     	Bleu_3: 0.030853945971077303
2024-01-07 19:32:39 INFO     	Bleu_4: 0.019231935569644712
2024-01-07 19:32:40 INFO     	Bleu_1: 0.10591431999435313
2024-01-07 19:32:40 INFO     	Bleu_2: 0.05696733457380408
2024-01-07 19:32:40 INFO     	Bleu_3: 0.02935024221575057
2024-01-07 19:32:40 INFO     	Bleu_4: 0.018351948460211195
2024-01-07 19:33:00 INFO     use spaCy answer extraction model: positionrank
2024-01-07 19:33:00 INFO     Model `small_finetuned_modified_ckpt/model_eszyci/epoch_9`
2024-01-07 19:33:00 INFO     	 * Num of GPU in use: 1
2024-01-07 19:33:00 INFO     	 * Prefix: True
2024-01-07 19:33:00 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 19:33:01 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 19:38:52 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 19:44:09 INFO     	Bleu_1: 0.10442487647334789
2024-01-07 19:44:09 INFO     	Bleu_2: 0.05592458329995654
2024-01-07 19:44:09 INFO     	Bleu_3: 0.028017505476003277
2024-01-07 19:44:09 INFO     	Bleu_4: 0.017223612334770832
2024-01-07 19:44:10 INFO     	Bleu_1: 0.10131160314265242
2024-01-07 19:44:10 INFO     	Bleu_2: 0.054496677375712455
2024-01-07 19:44:10 INFO     	Bleu_3: 0.02775886474330337
2024-01-07 19:44:10 INFO     	Bleu_4: 0.017191900775119135
2024-01-07 19:44:10 INFO     ## 2nd RUN (EVAL): Configuration 3/4 ##
2024-01-07 19:44:31 INFO     use spaCy answer extraction model: positionrank
2024-01-07 19:44:31 INFO     Model `small_finetuned_modified_ckpt/model_mzgdpa/epoch_1`
2024-01-07 19:44:31 INFO     	 * Num of GPU in use: 1
2024-01-07 19:44:31 INFO     	 * Prefix: True
2024-01-07 19:44:31 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 19:44:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 19:47:57 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 19:51:08 INFO     	Bleu_1: 0.22474488550363386
2024-01-07 19:51:08 INFO     	Bleu_2: 0.12364643634463128
2024-01-07 19:51:08 INFO     	Bleu_3: 0.06768287232916305
2024-01-07 19:51:08 INFO     	Bleu_4: 0.0440133702814989
2024-01-07 19:51:08 INFO     	Bleu_1: 0.21511095947768846
2024-01-07 19:51:08 INFO     	Bleu_2: 0.11822726217379781
2024-01-07 19:51:08 INFO     	Bleu_3: 0.06464069515655077
2024-01-07 19:51:08 INFO     	Bleu_4: 0.041688912916527605
2024-01-07 19:51:28 INFO     use spaCy answer extraction model: positionrank
2024-01-07 19:51:28 INFO     Model `small_finetuned_modified_ckpt/model_mzgdpa/epoch_10`
2024-01-07 19:51:28 INFO     	 * Num of GPU in use: 1
2024-01-07 19:51:28 INFO     	 * Prefix: True
2024-01-07 19:51:28 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 19:51:29 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 19:56:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 20:01:31 INFO     	Bleu_1: 0.12886217173132167
2024-01-07 20:01:31 INFO     	Bleu_2: 0.06902851792289784
2024-01-07 20:01:31 INFO     	Bleu_3: 0.03508294957328582
2024-01-07 20:01:31 INFO     	Bleu_4: 0.022115401155880936
2024-01-07 20:01:32 INFO     	Bleu_1: 0.11892034683158685
2024-01-07 20:01:32 INFO     	Bleu_2: 0.06482496964605047
2024-01-07 20:01:32 INFO     	Bleu_3: 0.03490516644116198
2024-01-07 20:01:32 INFO     	Bleu_4: 0.022628642696432492
2024-01-07 20:01:50 INFO     use spaCy answer extraction model: positionrank
2024-01-07 20:01:50 INFO     Model `small_finetuned_modified_ckpt/model_mzgdpa/epoch_2`
2024-01-07 20:01:50 INFO     	 * Num of GPU in use: 1
2024-01-07 20:01:50 INFO     	 * Prefix: True
2024-01-07 20:01:50 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 20:01:51 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 20:06:24 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 20:10:41 INFO     	Bleu_1: 0.17355754203913015
2024-01-07 20:10:41 INFO     	Bleu_2: 0.09421882862255222
2024-01-07 20:10:41 INFO     	Bleu_3: 0.04952645281081036
2024-01-07 20:10:41 INFO     	Bleu_4: 0.03169175770562416
2024-01-07 20:10:42 INFO     	Bleu_1: 0.17383578885022366
2024-01-07 20:10:42 INFO     	Bleu_2: 0.09435143418972734
2024-01-07 20:10:42 INFO     	Bleu_3: 0.05058811494427377
2024-01-07 20:10:42 INFO     	Bleu_4: 0.03234855619983433
2024-01-07 20:11:00 INFO     use spaCy answer extraction model: positionrank
2024-01-07 20:11:00 INFO     Model `small_finetuned_modified_ckpt/model_mzgdpa/epoch_3`
2024-01-07 20:11:00 INFO     	 * Num of GPU in use: 1
2024-01-07 20:11:00 INFO     	 * Prefix: True
2024-01-07 20:11:00 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 20:11:02 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 20:15:24 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 20:19:40 INFO     	Bleu_1: 0.17019523643081627
2024-01-07 20:19:40 INFO     	Bleu_2: 0.09187707081618036
2024-01-07 20:19:40 INFO     	Bleu_3: 0.04792878901191193
2024-01-07 20:19:40 INFO     	Bleu_4: 0.030546739682887836
2024-01-07 20:19:40 INFO     	Bleu_1: 0.1660655351249391
2024-01-07 20:19:40 INFO     	Bleu_2: 0.08955719079530002
2024-01-07 20:19:40 INFO     	Bleu_3: 0.0474798727752711
2024-01-07 20:19:40 INFO     	Bleu_4: 0.030407781411118138
2024-01-07 20:19:58 INFO     use spaCy answer extraction model: positionrank
2024-01-07 20:19:58 INFO     Model `small_finetuned_modified_ckpt/model_mzgdpa/epoch_4`
2024-01-07 20:19:58 INFO     	 * Num of GPU in use: 1
2024-01-07 20:19:58 INFO     	 * Prefix: True
2024-01-07 20:19:58 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 20:19:59 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 20:24:58 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 20:30:25 INFO     	Bleu_1: 0.14611099858213342
2024-01-07 20:30:25 INFO     	Bleu_2: 0.07840159045003771
2024-01-07 20:30:25 INFO     	Bleu_3: 0.03994451757435584
2024-01-07 20:30:25 INFO     	Bleu_4: 0.025055370732675147
2024-01-07 20:30:26 INFO     	Bleu_1: 0.1398860123647591
2024-01-07 20:30:26 INFO     	Bleu_2: 0.07569649490475948
2024-01-07 20:30:26 INFO     	Bleu_3: 0.03988869454605294
2024-01-07 20:30:26 INFO     	Bleu_4: 0.02538389807966685
2024-01-07 20:30:47 INFO     use spaCy answer extraction model: positionrank
2024-01-07 20:30:47 INFO     Model `small_finetuned_modified_ckpt/model_mzgdpa/epoch_6`
2024-01-07 20:30:47 INFO     	 * Num of GPU in use: 1
2024-01-07 20:30:47 INFO     	 * Prefix: True
2024-01-07 20:30:47 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 20:30:49 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 20:36:02 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 20:41:12 INFO     	Bleu_1: 0.13518435886461405
2024-01-07 20:41:12 INFO     	Bleu_2: 0.0727416711666275
2024-01-07 20:41:12 INFO     	Bleu_3: 0.03746452394058488
2024-01-07 20:41:12 INFO     	Bleu_4: 0.023657790469219397
2024-01-07 20:41:12 INFO     	Bleu_1: 0.12661086531276916
2024-01-07 20:41:12 INFO     	Bleu_2: 0.06859931482419133
2024-01-07 20:41:12 INFO     	Bleu_3: 0.03648951353793955
2024-01-07 20:41:12 INFO     	Bleu_4: 0.023626805199037832
2024-01-07 20:41:35 INFO     use spaCy answer extraction model: positionrank
2024-01-07 20:41:35 INFO     Model `small_finetuned_modified_ckpt/model_mzgdpa/epoch_7`
2024-01-07 20:41:35 INFO     	 * Num of GPU in use: 1
2024-01-07 20:41:35 INFO     	 * Prefix: True
2024-01-07 20:41:35 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 20:41:37 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 20:46:52 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 20:52:12 INFO     	Bleu_1: 0.1205743681076215
2024-01-07 20:52:12 INFO     	Bleu_2: 0.06467155597449963
2024-01-07 20:52:12 INFO     	Bleu_3: 0.03283570565336687
2024-01-07 20:52:12 INFO     	Bleu_4: 0.020607525214120362
2024-01-07 20:52:12 INFO     	Bleu_1: 0.11206257413997546
2024-01-07 20:52:12 INFO     	Bleu_2: 0.060707146859470734
2024-01-07 20:52:12 INFO     	Bleu_3: 0.03207038389572476
2024-01-07 20:52:12 INFO     	Bleu_4: 0.020642035124278008
2024-01-07 20:52:28 INFO     use spaCy answer extraction model: positionrank
2024-01-07 20:52:28 INFO     Model `small_finetuned_modified_ckpt/model_mzgdpa/epoch_8`
2024-01-07 20:52:28 INFO     	 * Num of GPU in use: 1
2024-01-07 20:52:28 INFO     	 * Prefix: True
2024-01-07 20:52:28 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 20:52:29 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 20:57:48 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 21:02:43 INFO     	Bleu_1: 0.13454508900829645
2024-01-07 21:02:43 INFO     	Bleu_2: 0.0723141469751679
2024-01-07 21:02:43 INFO     	Bleu_3: 0.037141364526291104
2024-01-07 21:02:43 INFO     	Bleu_4: 0.023507262350675334
2024-01-07 21:02:44 INFO     	Bleu_1: 0.1283800903396024
2024-01-07 21:02:44 INFO     	Bleu_2: 0.06971899820486577
2024-01-07 21:02:44 INFO     	Bleu_3: 0.03719884749313352
2024-01-07 21:02:44 INFO     	Bleu_4: 0.024068149448217613
2024-01-07 21:03:05 INFO     use spaCy answer extraction model: positionrank
2024-01-07 21:03:06 INFO     Model `small_finetuned_modified_ckpt/model_mzgdpa/epoch_9`
2024-01-07 21:03:06 INFO     	 * Num of GPU in use: 1
2024-01-07 21:03:06 INFO     	 * Prefix: True
2024-01-07 21:03:06 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 21:03:07 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2024-01-07 21:08:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqa_modifiedlmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2024-01-07 21:13:47 INFO     	Bleu_1: 0.1207881323217751
2024-01-07 21:13:47 INFO     	Bleu_2: 0.06469756315509718
2024-01-07 21:13:47 INFO     	Bleu_3: 0.03299985823183003
2024-01-07 21:13:47 INFO     	Bleu_4: 0.020908794093896173
2024-01-07 21:13:48 INFO     	Bleu_1: 0.1144470355345927
2024-01-07 21:13:48 INFO     	Bleu_2: 0.06197777767387859
2024-01-07 21:13:48 INFO     	Bleu_3: 0.03242980694140014
2024-01-07 21:13:48 INFO     	Bleu_4: 0.020554427643107616
2024-01-07 21:13:48 INFO     2nd RUN RESULTS: 
[('small_finetuned_modified_ckpt/model_pzcecq/epoch_1', 0.047803634928764695), ('small_finetuned_modified_ckpt/model_dpyopu/epoch_1', 0.047803634928764695), ('small_finetuned_modified_ckpt/model_eszyci/epoch_1', 0.0440133702814989), ('small_finetuned_modified_ckpt/model_mzgdpa/epoch_1', 0.0440133702814989), ('small_finetuned_modified_ckpt/model_pzcecq/epoch_6', 0.040360942729443586), ('small_finetuned_modified_ckpt/model_pzcecq/epoch_2', 0.03721317900060897), ('small_finetuned_modified_ckpt/model_dpyopu/epoch_2', 0.03721317900060897), ('small_finetuned_modified_ckpt/model_pzcecq/epoch_3', 0.034927056341631006), ('small_finetuned_modified_ckpt/model_dpyopu/epoch_3', 0.034927056341631006), ('small_finetuned_modified_ckpt/model_eszyci/epoch_6', 0.03414022060543363), ('small_finetuned_modified_ckpt/model_eszyci/epoch_2', 0.03169175770562416), ('small_finetuned_modified_ckpt/model_mzgdpa/epoch_2', 0.03169175770562416), ('small_finetuned_modified_ckpt/model_pzcecq/epoch_7', 0.03089602375663115), ('small_finetuned_modified_ckpt/model_eszyci/epoch_3', 0.030546739682887836), ('small_finetuned_modified_ckpt/model_mzgdpa/epoch_3', 0.030546739682887836), ('small_finetuned_modified_ckpt/model_pzcecq/epoch_4', 0.02954074361871371), ('small_finetuned_modified_ckpt/model_dpyopu/epoch_4', 0.02954074361871371), ('small_finetuned_modified_ckpt/model_pzcecq/epoch_5', 0.026717586741645736), ('small_finetuned_modified_ckpt/model_dpyopu/epoch_5', 0.026717586741645736), ('small_finetuned_modified_ckpt/model_dpyopu/epoch_6', 0.026079672106481582), ('small_finetuned_modified_ckpt/model_dpyopu/epoch_8', 0.026072516049854347), ('small_finetuned_modified_ckpt/model_eszyci/epoch_5', 0.02513953768725251), ('small_finetuned_modified_ckpt/model_mzgdpa/epoch_5', 0.02513953768725251), ('small_finetuned_modified_ckpt/model_eszyci/epoch_4', 0.025055370732675147), ('small_finetuned_modified_ckpt/model_mzgdpa/epoch_4', 0.025055370732675147), ('small_finetuned_modified_ckpt/model_dpyopu/epoch_9', 0.024244866990642162), ('small_finetuned_modified_ckpt/model_pzcecq/epoch_8', 0.02392012565969385), ('small_finetuned_modified_ckpt/model_mzgdpa/epoch_6', 0.023657790469219397), ('small_finetuned_modified_ckpt/model_dpyopu/epoch_7', 0.02361457931779822), ('small_finetuned_modified_ckpt/model_mzgdpa/epoch_8', 0.023507262350675334), ('small_finetuned_modified_ckpt/model_dpyopu/epoch_10', 0.0226762992930446), ('small_finetuned_modified_ckpt/model_mzgdpa/epoch_10', 0.022115401155880936), ('small_finetuned_modified_ckpt/model_pzcecq/epoch_9', 0.021028034986917018), ('small_finetuned_modified_ckpt/model_eszyci/epoch_7', 0.020931827029669375), ('small_finetuned_modified_ckpt/model_mzgdpa/epoch_9', 0.020908794093896173), ('small_finetuned_modified_ckpt/model_mzgdpa/epoch_7', 0.020607525214120362), ('small_finetuned_modified_ckpt/model_eszyci/epoch_8', 0.019231935569644712), ('small_finetuned_modified_ckpt/model_pzcecq/epoch_10', 0.018622524239511707), ('small_finetuned_modified_ckpt/model_eszyci/epoch_9', 0.017223612334770832), ('small_finetuned_modified_ckpt/model_eszyci/epoch_10', 0.01664525562828387)]
2024-01-07 21:13:48 INFO     	 * rank: 0 | metric: 0.048 | model: small_finetuned_modified_ckpt/model_pzcecq/epoch_1 |
2024-01-07 21:13:48 INFO     	 * rank: 1 | metric: 0.048 | model: small_finetuned_modified_ckpt/model_dpyopu/epoch_1 |
2024-01-07 21:13:48 INFO     	 * rank: 2 | metric: 0.044 | model: small_finetuned_modified_ckpt/model_eszyci/epoch_1 |
2024-01-07 21:13:48 INFO     	 * rank: 3 | metric: 0.044 | model: small_finetuned_modified_ckpt/model_mzgdpa/epoch_1 |
2024-01-07 21:13:48 INFO     	 * rank: 4 | metric: 0.04 | model: small_finetuned_modified_ckpt/model_pzcecq/epoch_6 |
2024-01-07 21:13:48 INFO     	 * rank: 5 | metric: 0.037 | model: small_finetuned_modified_ckpt/model_pzcecq/epoch_2 |
2024-01-07 21:13:48 INFO     	 * rank: 6 | metric: 0.037 | model: small_finetuned_modified_ckpt/model_dpyopu/epoch_2 |
2024-01-07 21:13:48 INFO     	 * rank: 7 | metric: 0.035 | model: small_finetuned_modified_ckpt/model_pzcecq/epoch_3 |
2024-01-07 21:13:48 INFO     	 * rank: 8 | metric: 0.035 | model: small_finetuned_modified_ckpt/model_dpyopu/epoch_3 |
2024-01-07 21:13:48 INFO     	 * rank: 9 | metric: 0.034 | model: small_finetuned_modified_ckpt/model_eszyci/epoch_6 |
2024-01-07 21:13:48 INFO     	 * rank: 10 | metric: 0.032 | model: small_finetuned_modified_ckpt/model_eszyci/epoch_2 |
2024-01-07 21:13:48 INFO     	 * rank: 11 | metric: 0.032 | model: small_finetuned_modified_ckpt/model_mzgdpa/epoch_2 |
2024-01-07 21:13:48 INFO     	 * rank: 12 | metric: 0.031 | model: small_finetuned_modified_ckpt/model_pzcecq/epoch_7 |
2024-01-07 21:13:48 INFO     	 * rank: 13 | metric: 0.031 | model: small_finetuned_modified_ckpt/model_eszyci/epoch_3 |
2024-01-07 21:13:48 INFO     	 * rank: 14 | metric: 0.031 | model: small_finetuned_modified_ckpt/model_mzgdpa/epoch_3 |
2024-01-07 21:13:48 INFO     	 * rank: 15 | metric: 0.03 | model: small_finetuned_modified_ckpt/model_pzcecq/epoch_4 |
2024-01-07 21:13:48 INFO     	 * rank: 16 | metric: 0.03 | model: small_finetuned_modified_ckpt/model_dpyopu/epoch_4 |
2024-01-07 21:13:48 INFO     	 * rank: 17 | metric: 0.027 | model: small_finetuned_modified_ckpt/model_pzcecq/epoch_5 |
2024-01-07 21:13:48 INFO     	 * rank: 18 | metric: 0.027 | model: small_finetuned_modified_ckpt/model_dpyopu/epoch_5 |
2024-01-07 21:13:48 INFO     	 * rank: 19 | metric: 0.026 | model: small_finetuned_modified_ckpt/model_dpyopu/epoch_6 |
2024-01-07 21:13:48 INFO     	 * rank: 20 | metric: 0.026 | model: small_finetuned_modified_ckpt/model_dpyopu/epoch_8 |
2024-01-07 21:13:48 INFO     	 * rank: 21 | metric: 0.025 | model: small_finetuned_modified_ckpt/model_eszyci/epoch_5 |
2024-01-07 21:13:48 INFO     	 * rank: 22 | metric: 0.025 | model: small_finetuned_modified_ckpt/model_mzgdpa/epoch_5 |
2024-01-07 21:13:48 INFO     	 * rank: 23 | metric: 0.025 | model: small_finetuned_modified_ckpt/model_eszyci/epoch_4 |
2024-01-07 21:13:48 INFO     	 * rank: 24 | metric: 0.025 | model: small_finetuned_modified_ckpt/model_mzgdpa/epoch_4 |
2024-01-07 21:13:48 INFO     	 * rank: 25 | metric: 0.024 | model: small_finetuned_modified_ckpt/model_dpyopu/epoch_9 |
2024-01-07 21:13:48 INFO     	 * rank: 26 | metric: 0.024 | model: small_finetuned_modified_ckpt/model_pzcecq/epoch_8 |
2024-01-07 21:13:48 INFO     	 * rank: 27 | metric: 0.024 | model: small_finetuned_modified_ckpt/model_mzgdpa/epoch_6 |
2024-01-07 21:13:48 INFO     	 * rank: 28 | metric: 0.024 | model: small_finetuned_modified_ckpt/model_dpyopu/epoch_7 |
2024-01-07 21:13:48 INFO     	 * rank: 29 | metric: 0.024 | model: small_finetuned_modified_ckpt/model_mzgdpa/epoch_8 |
2024-01-07 21:13:48 INFO     	 * rank: 30 | metric: 0.023 | model: small_finetuned_modified_ckpt/model_dpyopu/epoch_10 |
2024-01-07 21:13:48 INFO     	 * rank: 31 | metric: 0.022 | model: small_finetuned_modified_ckpt/model_mzgdpa/epoch_10 |
2024-01-07 21:13:48 INFO     	 * rank: 32 | metric: 0.021 | model: small_finetuned_modified_ckpt/model_pzcecq/epoch_9 |
2024-01-07 21:13:48 INFO     	 * rank: 33 | metric: 0.021 | model: small_finetuned_modified_ckpt/model_eszyci/epoch_7 |
2024-01-07 21:13:48 INFO     	 * rank: 34 | metric: 0.021 | model: small_finetuned_modified_ckpt/model_mzgdpa/epoch_9 |
2024-01-07 21:13:48 INFO     	 * rank: 35 | metric: 0.021 | model: small_finetuned_modified_ckpt/model_mzgdpa/epoch_7 |
2024-01-07 21:13:48 INFO     	 * rank: 36 | metric: 0.019 | model: small_finetuned_modified_ckpt/model_eszyci/epoch_8 |
2024-01-07 21:13:48 INFO     	 * rank: 37 | metric: 0.019 | model: small_finetuned_modified_ckpt/model_pzcecq/epoch_10 |
2024-01-07 21:13:48 INFO     	 * rank: 38 | metric: 0.017 | model: small_finetuned_modified_ckpt/model_eszyci/epoch_9 |
2024-01-07 21:13:48 INFO     	 * rank: 39 | metric: 0.017 | model: small_finetuned_modified_ckpt/model_eszyci/epoch_10 |
2024-01-07 21:13:48 INFO     creating small_finetuned_modified_ckpt/best_model
2024-01-07 21:13:48 INFO     copying small_finetuned_modified_ckpt/model_pzcecq/epoch_1/config.json -> small_finetuned_modified_ckpt/best_model
2024-01-07 21:13:48 INFO     copying small_finetuned_modified_ckpt/model_pzcecq/epoch_1/generation_config.json -> small_finetuned_modified_ckpt/best_model
2024-01-07 21:13:48 INFO     copying small_finetuned_modified_ckpt/model_pzcecq/epoch_1/pytorch_model.bin -> small_finetuned_modified_ckpt/best_model
2024-01-07 21:13:48 INFO     copying small_finetuned_modified_ckpt/model_pzcecq/epoch_1/tokenizer_config.json -> small_finetuned_modified_ckpt/best_model
2024-01-07 21:13:48 INFO     copying small_finetuned_modified_ckpt/model_pzcecq/epoch_1/special_tokens_map.json -> small_finetuned_modified_ckpt/best_model
2024-01-07 21:13:48 INFO     copying small_finetuned_modified_ckpt/model_pzcecq/epoch_1/added_tokens.json -> small_finetuned_modified_ckpt/best_model
2024-01-07 21:13:48 INFO     copying small_finetuned_modified_ckpt/model_pzcecq/epoch_1/spiece.model -> small_finetuned_modified_ckpt/best_model
2024-01-07 21:13:49 INFO     copying small_finetuned_modified_ckpt/model_pzcecq/epoch_1/tokenizer.json -> small_finetuned_modified_ckpt/best_model
2024-01-07 21:13:49 INFO     copying small_finetuned_modified_ckpt/model_pzcecq/epoch_1/trainer_config.json -> small_finetuned_modified_ckpt/best_model
2024-01-07 21:13:49 INFO     creating small_finetuned_modified_ckpt/best_model/eval
2024-01-07 21:13:49 INFO     copying small_finetuned_modified_ckpt/model_pzcecq/epoch_1/eval/samples.test.hyp.paragraph.questions_answers.StellarMilk_newsqa_modified.default.txt -> small_finetuned_modified_ckpt/best_model/eval
2024-01-07 21:13:49 INFO     copying small_finetuned_modified_ckpt/model_pzcecq/epoch_1/eval/samples.validation.hyp.paragraph.questions_answers.StellarMilk_newsqa_modified.default.txt -> small_finetuned_modified_ckpt/best_model/eval
2024-01-07 21:13:49 INFO     copying small_finetuned_modified_ckpt/model_pzcecq/epoch_1/eval/metric.first.answer.paragraph.questions_answers.StellarMilk_newsqa_modified.default.json -> small_finetuned_modified_ckpt/best_model/eval
