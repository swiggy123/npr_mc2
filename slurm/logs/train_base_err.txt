2023-12-16 17:53:35 INFO     INITIALIZE GRID SEARCHER: 12 configs to try
2023-12-16 17:53:35 INFO     ## 1st RUN: Configuration 0/12 ##
2023-12-16 17:53:35 INFO     initialize model trainer
2023-12-16 17:53:35 INFO     initialize checkpoint at base_trained_ckpt/model_rillvb
2023-12-16 17:53:35 INFO     hyperparameters
2023-12-16 17:53:35 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-16 17:53:35 INFO     	 * dataset_name: default
2023-12-16 17:53:35 INFO     	 * input_types: ['paragraph']
2023-12-16 17:53:35 INFO     	 * output_types: ['questions_answers']
2023-12-16 17:53:35 INFO     	 * prefix_types: ['qag']
2023-12-16 17:53:35 INFO     	 * model: t5-base
2023-12-16 17:53:35 INFO     	 * max_length: 512
2023-12-16 17:53:35 INFO     	 * max_length_output: 512
2023-12-16 17:53:35 INFO     	 * epoch: 15
2023-12-16 17:53:35 INFO     	 * batch: 2
2023-12-16 17:53:35 INFO     	 * lr: 0.0001
2023-12-16 17:53:35 INFO     	 * fp16: False
2023-12-16 17:53:35 INFO     	 * random_seed: 1
2023-12-16 17:53:35 INFO     	 * gradient_accumulation_steps: 4
2023-12-16 17:53:35 INFO     	 * label_smoothing: 0.15
2023-12-16 17:53:35 INFO     initialize checkpoint with t5-base
/home2/g.torresgamez/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.
For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.
- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.
- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.
- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.
  warnings.warn(
2023-12-16 17:53:38 INFO     use spaCy answer extraction model: positionrank
2023-12-16 17:53:40 INFO     Model `t5-base`
2023-12-16 17:53:40 INFO     	 * Num of GPU in use: 1
2023-12-16 17:53:40 INFO     	 * Prefix: True
2023-12-16 17:53:40 INFO     	 * Language: en (ignore at the training phase)
2023-12-16 17:53:40 INFO     dataset preprocessing
/home2/g.torresgamez/.local/lib/python3.10/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=False' instead.
  warnings.warn(
2023-12-16 17:53:42 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-base.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-16 17:53:42 INFO     start model training
2023-12-16 17:54:28 INFO     	 * (global step 50: loss: 0.6667779386043549, lr: 0.0001
2023-12-16 17:55:14 INFO     	 * (global step 100: loss: 0.3620779812335968, lr: 0.0001
2023-12-16 17:56:01 INFO     	 * (global step 150: loss: 0.6301687881350517, lr: 0.0001
2023-12-16 17:56:48 INFO     	 * (global step 200: loss: 0.45351649820804596, lr: 0.0001
2023-12-16 17:57:34 INFO     	 * (global step 250: loss: 0.392860047519207, lr: 0.0001
2023-12-16 17:58:21 INFO     	 * (global step 300: loss: 0.4983123764395714, lr: 0.0001
2023-12-16 17:58:31 INFO     [epoch 0/15] average loss: 0.677, lr: 0.0001
2023-12-16 17:58:31 INFO     saving model related files
2023-12-16 17:58:31 INFO     saving model
2023-12-16 17:58:33 INFO     saving tokenizer
2023-12-16 17:58:33 INFO     saving optimizer
2023-12-16 17:58:36 INFO     remove old optimizer files
2023-12-16 17:59:13 INFO     	 * (global step 350: loss: 0.36926744133234024, lr: 0.0001
2023-12-16 18:00:00 INFO     	 * (global step 400: loss: 0.39811941981315613, lr: 0.0001
2023-12-16 18:00:47 INFO     	 * (global step 450: loss: 0.39766258746385574, lr: 0.0001
2023-12-16 18:01:34 INFO     	 * (global step 500: loss: 0.28730830550193787, lr: 0.0001
2023-12-16 18:02:20 INFO     	 * (global step 550: loss: 0.3246122673153877, lr: 0.0001
2023-12-16 18:03:07 INFO     	 * (global step 600: loss: 0.31850384920835495, lr: 0.0001
2023-12-16 18:03:26 INFO     [epoch 1/15] average loss: 0.368, lr: 0.0001
2023-12-16 18:03:26 INFO     saving model related files
2023-12-16 18:03:26 INFO     saving model
2023-12-16 18:03:28 INFO     saving tokenizer
2023-12-16 18:03:28 INFO     saving optimizer
2023-12-16 18:03:32 INFO     remove old optimizer files
2023-12-16 18:04:00 INFO     	 * (global step 650: loss: 0.27254194021224976, lr: 0.0001
2023-12-16 18:04:47 INFO     	 * (global step 700: loss: 0.29764799773693085, lr: 0.0001
2023-12-16 18:05:33 INFO     	 * (global step 750: loss: 0.29989705979824066, lr: 0.0001
2023-12-16 18:06:20 INFO     	 * (global step 800: loss: 0.29200412333011627, lr: 0.0001
2023-12-16 18:07:07 INFO     	 * (global step 850: loss: 0.32812967151403427, lr: 0.0001
2023-12-16 18:07:54 INFO     	 * (global step 900: loss: 0.42493222653865814, lr: 0.0001
2023-12-16 18:08:22 INFO     [epoch 2/15] average loss: 0.335, lr: 0.0001
2023-12-16 18:08:22 INFO     saving model related files
2023-12-16 18:08:22 INFO     saving model
2023-12-16 18:08:24 INFO     saving tokenizer
2023-12-16 18:08:24 INFO     saving optimizer
2023-12-16 18:08:27 INFO     remove old optimizer files
2023-12-16 18:08:46 INFO     	 * (global step 950: loss: 0.31920865178108215, lr: 0.0001
2023-12-16 18:09:33 INFO     	 * (global step 1000: loss: 0.3452780209481716, lr: 0.0001
2023-12-16 18:10:19 INFO     	 * (global step 1050: loss: 0.338620588183403, lr: 0.0001
2023-12-16 18:11:06 INFO     	 * (global step 1100: loss: 0.33324792236089706, lr: 0.0001
2023-12-16 18:11:53 INFO     	 * (global step 1150: loss: 0.34516941010951996, lr: 0.0001
2023-12-16 18:12:40 INFO     	 * (global step 1200: loss: 0.2507483549416065, lr: 0.0001
2023-12-16 18:13:18 INFO     [epoch 3/15] average loss: 0.313, lr: 0.0001
2023-12-16 18:13:18 INFO     saving model related files
2023-12-16 18:13:18 INFO     saving model
2023-12-16 18:13:19 INFO     saving tokenizer
2023-12-16 18:13:19 INFO     saving optimizer
2023-12-16 18:13:23 INFO     remove old optimizer files
2023-12-16 18:13:33 INFO     	 * (global step 1250: loss: 0.2763531841337681, lr: 0.0001
2023-12-16 18:14:19 INFO     	 * (global step 1300: loss: 0.3029669225215912, lr: 0.0001
2023-12-16 18:15:06 INFO     	 * (global step 1350: loss: 0.3350086957216263, lr: 0.0001
2023-12-16 18:15:53 INFO     	 * (global step 1400: loss: 0.3301493301987648, lr: 0.0001
2023-12-16 18:16:39 INFO     	 * (global step 1450: loss: 0.3242044486105442, lr: 0.0001
2023-12-16 18:17:26 INFO     	 * (global step 1500: loss: 0.20335986465215683, lr: 0.0001
2023-12-16 18:18:13 INFO     	 * (global step 1550: loss: 0.3620617836713791, lr: 0.0001
2023-12-16 18:18:14 INFO     [epoch 4/15] average loss: 0.294, lr: 0.0001
2023-12-16 18:18:14 INFO     saving model related files
2023-12-16 18:18:14 INFO     saving model
2023-12-16 18:18:16 INFO     saving tokenizer
2023-12-16 18:18:16 INFO     saving optimizer
2023-12-16 18:18:19 INFO     remove old optimizer files
2023-12-16 18:19:06 INFO     	 * (global step 1600: loss: 0.3196807987987995, lr: 0.0001
2023-12-16 18:19:53 INFO     	 * (global step 1650: loss: 0.28186779096722603, lr: 0.0001
2023-12-16 18:20:40 INFO     	 * (global step 1700: loss: 0.3467637784779072, lr: 0.0001
2023-12-16 18:21:27 INFO     	 * (global step 1750: loss: 0.2620423212647438, lr: 0.0001
2023-12-16 18:22:14 INFO     	 * (global step 1800: loss: 0.3457694910466671, lr: 0.0001
2023-12-16 18:23:01 INFO     	 * (global step 1850: loss: 0.4129613824188709, lr: 0.0001
2023-12-16 18:23:12 INFO     [epoch 5/15] average loss: 0.279, lr: 0.0001
2023-12-16 18:23:12 INFO     saving model related files
2023-12-16 18:23:12 INFO     saving model
2023-12-16 18:23:13 INFO     saving tokenizer
2023-12-16 18:23:13 INFO     saving optimizer
2023-12-16 18:23:17 INFO     remove old optimizer files
2023-12-16 18:23:55 INFO     	 * (global step 1900: loss: 0.2549196369946003, lr: 0.0001
2023-12-16 18:24:42 INFO     	 * (global step 1950: loss: 0.2221701443195343, lr: 0.0001
2023-12-16 18:25:29 INFO     	 * (global step 2000: loss: 0.3558594733476639, lr: 0.0001
2023-12-16 18:26:16 INFO     	 * (global step 2050: loss: 0.2072228640317917, lr: 0.0001
2023-12-16 18:27:03 INFO     	 * (global step 2100: loss: 0.29715898260474205, lr: 0.0001
2023-12-16 18:27:50 INFO     	 * (global step 2150: loss: 0.27227428928017616, lr: 0.0001
2023-12-16 18:28:09 INFO     [epoch 6/15] average loss: 0.265, lr: 0.0001
2023-12-16 18:28:09 INFO     saving model related files
2023-12-16 18:28:09 INFO     saving model
2023-12-16 18:28:11 INFO     saving tokenizer
2023-12-16 18:28:11 INFO     saving optimizer
2023-12-16 18:28:14 INFO     remove old optimizer files
2023-12-16 18:28:43 INFO     	 * (global step 2200: loss: 0.19317150861024857, lr: 0.0001
2023-12-16 18:29:30 INFO     	 * (global step 2250: loss: 0.21794893592596054, lr: 0.0001
2023-12-16 18:30:17 INFO     	 * (global step 2300: loss: 0.22538327053189278, lr: 0.0001
2023-12-16 18:31:04 INFO     	 * (global step 2350: loss: 0.24400119110941887, lr: 0.0001
2023-12-16 18:31:51 INFO     	 * (global step 2400: loss: 0.20067688822746277, lr: 0.0001
2023-12-16 18:32:38 INFO     	 * (global step 2450: loss: 0.26073218509554863, lr: 0.0001
2023-12-16 18:33:07 INFO     [epoch 7/15] average loss: 0.252, lr: 0.0001
2023-12-16 18:33:07 INFO     saving model related files
2023-12-16 18:33:07 INFO     saving model
2023-12-16 18:33:09 INFO     saving tokenizer
2023-12-16 18:33:09 INFO     saving optimizer
2023-12-16 18:33:12 INFO     remove old optimizer files
2023-12-16 18:33:31 INFO     	 * (global step 2500: loss: 0.23213711380958557, lr: 0.0001
2023-12-16 18:34:18 INFO     	 * (global step 2550: loss: 0.2097875513136387, lr: 0.0001
2023-12-16 18:35:05 INFO     	 * (global step 2600: loss: 0.24367118254303932, lr: 0.0001
2023-12-16 18:35:52 INFO     	 * (global step 2650: loss: 0.2752571105957031, lr: 0.0001
2023-12-16 18:36:39 INFO     	 * (global step 2700: loss: 0.2078171968460083, lr: 0.0001
2023-12-16 18:37:26 INFO     	 * (global step 2750: loss: 0.27077286690473557, lr: 0.0001
2023-12-16 18:38:05 INFO     [epoch 8/15] average loss: 0.24, lr: 0.0001
2023-12-16 18:38:05 INFO     saving model related files
2023-12-16 18:38:05 INFO     saving model
2023-12-16 18:38:06 INFO     saving tokenizer
2023-12-16 18:38:06 INFO     saving optimizer
2023-12-16 18:38:10 INFO     remove old optimizer files
2023-12-16 18:38:19 INFO     	 * (global step 2800: loss: 0.17123450711369514, lr: 0.0001
2023-12-16 18:39:06 INFO     	 * (global step 2850: loss: 0.19707165658473969, lr: 0.0001
2023-12-16 18:39:53 INFO     	 * (global step 2900: loss: 0.18398868665099144, lr: 0.0001
2023-12-16 18:40:40 INFO     	 * (global step 2950: loss: 0.24617380276322365, lr: 0.0001
2023-12-16 18:41:27 INFO     	 * (global step 3000: loss: 0.19246955960988998, lr: 0.0001
2023-12-16 18:42:14 INFO     	 * (global step 3050: loss: 0.21906980499625206, lr: 0.0001
2023-12-16 18:43:01 INFO     	 * (global step 3100: loss: 0.23676327243447304, lr: 0.0001
2023-12-16 18:43:01 INFO     [epoch 9/15] average loss: 0.229, lr: 0.0001
2023-12-16 18:43:01 INFO     saving model related files
2023-12-16 18:43:01 INFO     saving model
2023-12-16 18:43:03 INFO     saving tokenizer
2023-12-16 18:43:03 INFO     saving optimizer
2023-12-16 18:43:07 INFO     remove old optimizer files
2023-12-16 18:43:07 INFO     complete training: model ckpt was saved at base_trained_ckpt/model_rillvb
2023-12-16 18:43:07 INFO     ## 1st RUN: Configuration 1/12 ##
2023-12-16 18:43:07 INFO     initialize model trainer
2023-12-16 18:43:07 INFO     initialize checkpoint at base_trained_ckpt/model_eszyci
2023-12-16 18:43:07 INFO     hyperparameters
2023-12-16 18:43:07 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-16 18:43:07 INFO     	 * dataset_name: default
2023-12-16 18:43:07 INFO     	 * input_types: ['paragraph']
2023-12-16 18:43:07 INFO     	 * output_types: ['questions_answers']
2023-12-16 18:43:07 INFO     	 * prefix_types: ['qag']
2023-12-16 18:43:07 INFO     	 * model: t5-base
2023-12-16 18:43:07 INFO     	 * max_length: 512
2023-12-16 18:43:07 INFO     	 * max_length_output: 512
2023-12-16 18:43:07 INFO     	 * epoch: 15
2023-12-16 18:43:07 INFO     	 * batch: 2
2023-12-16 18:43:07 INFO     	 * lr: 0.0001
2023-12-16 18:43:07 INFO     	 * fp16: False
2023-12-16 18:43:07 INFO     	 * random_seed: 1
2023-12-16 18:43:07 INFO     	 * gradient_accumulation_steps: 2
2023-12-16 18:43:07 INFO     	 * label_smoothing: 0.15
2023-12-16 18:43:07 INFO     initialize checkpoint with t5-base
/home2/g.torresgamez/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.
For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.
- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.
- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.
- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.
  warnings.warn(
2023-12-16 18:43:10 INFO     use spaCy answer extraction model: positionrank
2023-12-16 18:43:10 INFO     Model `t5-base`
2023-12-16 18:43:10 INFO     	 * Num of GPU in use: 1
2023-12-16 18:43:10 INFO     	 * Prefix: True
2023-12-16 18:43:10 INFO     	 * Language: en (ignore at the training phase)
2023-12-16 18:43:10 INFO     dataset preprocessing
2023-12-16 18:43:11 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-base.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-16 18:43:12 INFO     start model training
2023-12-16 18:43:36 INFO     	 * (global step 50: loss: 0.49418579041957855, lr: 0.0001
2023-12-16 18:44:00 INFO     	 * (global step 100: loss: 0.5203526616096497, lr: 0.0001
2023-12-16 18:44:24 INFO     	 * (global step 150: loss: 0.4631658345460892, lr: 0.0001
2023-12-16 18:44:48 INFO     	 * (global step 200: loss: 0.3211568593978882, lr: 0.0001
2023-12-16 18:45:13 INFO     	 * (global step 250: loss: 0.3257419914007187, lr: 0.0001
2023-12-16 18:45:37 INFO     	 * (global step 300: loss: 0.6540381610393524, lr: 0.0001
2023-12-16 18:46:01 INFO     	 * (global step 350: loss: 0.3902387171983719, lr: 0.0001
2023-12-16 18:46:25 INFO     	 * (global step 400: loss: 0.32217617332935333, lr: 0.0001
2023-12-16 18:46:50 INFO     	 * (global step 450: loss: 0.36867697536945343, lr: 0.0001
2023-12-16 18:47:14 INFO     	 * (global step 500: loss: 0.3087269067764282, lr: 0.0001
2023-12-16 18:47:38 INFO     	 * (global step 550: loss: 0.756430983543396, lr: 0.0001
2023-12-16 18:48:03 INFO     	 * (global step 600: loss: 0.599729061126709, lr: 0.0001
2023-12-16 18:48:13 INFO     [epoch 0/15] average loss: 0.548, lr: 0.0001
2023-12-16 18:48:13 INFO     saving model related files
2023-12-16 18:48:13 INFO     saving model
2023-12-16 18:48:15 INFO     saving tokenizer
2023-12-16 18:48:15 INFO     saving optimizer
2023-12-16 18:48:18 INFO     remove old optimizer files
2023-12-16 18:48:32 INFO     	 * (global step 650: loss: 0.4929016977548599, lr: 0.0001
2023-12-16 18:48:56 INFO     	 * (global step 700: loss: 0.35122162103652954, lr: 0.0001
2023-12-16 18:49:20 INFO     	 * (global step 750: loss: 0.3768063634634018, lr: 0.0001
2023-12-16 18:49:45 INFO     	 * (global step 800: loss: 0.4411477744579315, lr: 0.0001
2023-12-16 18:50:09 INFO     	 * (global step 850: loss: 0.34015868604183197, lr: 0.0001
2023-12-16 18:50:33 INFO     	 * (global step 900: loss: 0.3403104990720749, lr: 0.0001
2023-12-16 18:50:58 INFO     	 * (global step 950: loss: 0.42871369421482086, lr: 0.0001
2023-12-16 18:51:22 INFO     	 * (global step 1000: loss: 0.3327028304338455, lr: 0.0001
2023-12-16 18:51:46 INFO     	 * (global step 1050: loss: 0.35794295370578766, lr: 0.0001
2023-12-16 18:52:10 INFO     	 * (global step 1100: loss: 0.32378941774368286, lr: 0.0001
2023-12-16 18:52:35 INFO     	 * (global step 1150: loss: 0.3940761238336563, lr: 0.0001
2023-12-16 18:52:59 INFO     	 * (global step 1200: loss: 0.31409725546836853, lr: 0.0001
2023-12-16 18:53:20 INFO     [epoch 1/15] average loss: 0.343, lr: 0.0001
2023-12-16 18:53:20 INFO     saving model related files
2023-12-16 18:53:20 INFO     saving model
2023-12-16 18:53:22 INFO     saving tokenizer
2023-12-16 18:53:22 INFO     saving optimizer
2023-12-16 18:53:25 INFO     remove old optimizer files
2023-12-16 18:53:29 INFO     	 * (global step 1250: loss: 0.25309765338897705, lr: 0.0001
2023-12-16 18:53:53 INFO     	 * (global step 1300: loss: 0.25012849271297455, lr: 0.0001
2023-12-16 18:54:18 INFO     	 * (global step 1350: loss: 0.2862309664487839, lr: 0.0001
2023-12-16 18:54:42 INFO     	 * (global step 1400: loss: 0.4213569164276123, lr: 0.0001
2023-12-16 18:55:06 INFO     	 * (global step 1450: loss: 0.37407152354717255, lr: 0.0001
2023-12-16 18:55:31 INFO     	 * (global step 1500: loss: 0.27324433624744415, lr: 0.0001
2023-12-16 18:55:55 INFO     	 * (global step 1550: loss: 0.2589186951518059, lr: 0.0001
2023-12-16 18:56:20 INFO     	 * (global step 1600: loss: 0.5157589614391327, lr: 0.0001
2023-12-16 18:56:44 INFO     	 * (global step 1650: loss: 0.2834719270467758, lr: 0.0001
2023-12-16 18:57:09 INFO     	 * (global step 1700: loss: 0.33662088215351105, lr: 0.0001
2023-12-16 18:57:33 INFO     	 * (global step 1750: loss: 0.26195765286684036, lr: 0.0001
2023-12-16 18:57:57 INFO     	 * (global step 1800: loss: 0.21240635961294174, lr: 0.0001
2023-12-16 18:58:22 INFO     	 * (global step 1850: loss: 0.3028103709220886, lr: 0.0001
2023-12-16 18:58:28 INFO     [epoch 2/15] average loss: 0.311, lr: 0.0001
2023-12-16 18:58:28 INFO     saving model related files
2023-12-16 18:58:28 INFO     saving model
2023-12-16 18:58:30 INFO     saving tokenizer
2023-12-16 18:58:30 INFO     saving optimizer
2023-12-16 18:58:33 INFO     remove old optimizer files
2023-12-16 18:58:51 INFO     	 * (global step 1900: loss: 0.2677692472934723, lr: 0.0001
2023-12-16 18:59:16 INFO     	 * (global step 1950: loss: 0.30841729044914246, lr: 0.0001
2023-12-16 18:59:40 INFO     	 * (global step 2000: loss: 0.23540370166301727, lr: 0.0001
2023-12-16 19:00:04 INFO     	 * (global step 2050: loss: 0.296060673892498, lr: 0.0001
2023-12-16 19:00:28 INFO     	 * (global step 2100: loss: 0.19964437186717987, lr: 0.0001
2023-12-16 19:00:53 INFO     	 * (global step 2150: loss: 0.2661914676427841, lr: 0.0001
2023-12-16 19:01:17 INFO     	 * (global step 2200: loss: 0.29411663115024567, lr: 0.0001
2023-12-16 19:01:42 INFO     	 * (global step 2250: loss: 0.24919825047254562, lr: 0.0001
2023-12-16 19:02:06 INFO     	 * (global step 2300: loss: 0.22244399040937424, lr: 0.0001
2023-12-16 19:02:30 INFO     	 * (global step 2350: loss: 0.2795008271932602, lr: 0.0001
2023-12-16 19:02:55 INFO     	 * (global step 2400: loss: 0.3154192864894867, lr: 0.0001
2023-12-16 19:03:19 INFO     	 * (global step 2450: loss: 0.3527766913175583, lr: 0.0001
2023-12-16 19:03:36 INFO     [epoch 3/15] average loss: 0.287, lr: 0.0001
2023-12-16 19:03:36 INFO     saving model related files
2023-12-16 19:03:36 INFO     saving model
2023-12-16 19:03:38 INFO     saving tokenizer
2023-12-16 19:03:38 INFO     saving optimizer
2023-12-16 19:03:42 INFO     remove old optimizer files
2023-12-16 19:03:50 INFO     	 * (global step 2500: loss: 0.2615421265363693, lr: 0.0001
2023-12-16 19:04:14 INFO     	 * (global step 2550: loss: 0.24940432608127594, lr: 0.0001
2023-12-16 19:04:38 INFO     	 * (global step 2600: loss: 0.3015890121459961, lr: 0.0001
2023-12-16 19:05:03 INFO     	 * (global step 2650: loss: 0.1576932892203331, lr: 0.0001
2023-12-16 19:05:27 INFO     	 * (global step 2700: loss: 0.23415205627679825, lr: 0.0001
2023-12-16 19:05:51 INFO     	 * (global step 2750: loss: 0.24991074576973915, lr: 0.0001
2023-12-16 19:06:16 INFO     	 * (global step 2800: loss: 0.22527548670768738, lr: 0.0001
2023-12-16 19:06:40 INFO     	 * (global step 2850: loss: 0.24551092088222504, lr: 0.0001
2023-12-16 19:07:05 INFO     	 * (global step 2900: loss: 0.13563117012381554, lr: 0.0001
2023-12-16 19:07:29 INFO     	 * (global step 2950: loss: 0.2781895101070404, lr: 0.0001
2023-12-16 19:07:53 INFO     	 * (global step 3000: loss: 0.28965824097394943, lr: 0.0001
2023-12-16 19:08:18 INFO     	 * (global step 3050: loss: 0.22606680542230606, lr: 0.0001
2023-12-16 19:08:42 INFO     	 * (global step 3100: loss: 0.24867833405733109, lr: 0.0001
2023-12-16 19:08:45 INFO     [epoch 4/15] average loss: 0.267, lr: 0.0001
2023-12-16 19:08:45 INFO     saving model related files
2023-12-16 19:08:45 INFO     saving model
2023-12-16 19:08:47 INFO     saving tokenizer
2023-12-16 19:08:47 INFO     saving optimizer
2023-12-16 19:08:50 INFO     remove old optimizer files
2023-12-16 19:09:12 INFO     	 * (global step 3150: loss: 0.18731175363063812, lr: 0.0001
2023-12-16 19:09:36 INFO     	 * (global step 3200: loss: 0.23560383915901184, lr: 0.0001
2023-12-16 19:10:01 INFO     	 * (global step 3250: loss: 0.29365983605384827, lr: 0.0001
2023-12-16 19:10:25 INFO     	 * (global step 3300: loss: 0.20363309234380722, lr: 0.0001
2023-12-16 19:10:49 INFO     	 * (global step 3350: loss: 0.31668469309806824, lr: 0.0001
2023-12-16 19:11:14 INFO     	 * (global step 3400: loss: 0.24753794074058533, lr: 0.0001
2023-12-16 19:11:38 INFO     	 * (global step 3450: loss: 0.2485618218779564, lr: 0.0001
2023-12-16 19:12:02 INFO     	 * (global step 3500: loss: 0.2761819362640381, lr: 0.0001
2023-12-16 19:12:27 INFO     	 * (global step 3550: loss: 0.29911942780017853, lr: 0.0001
2023-12-16 19:12:51 INFO     	 * (global step 3600: loss: 0.19538744539022446, lr: 0.0001
2023-12-16 19:13:15 INFO     	 * (global step 3650: loss: 0.35315240919589996, lr: 0.0001
2023-12-16 19:13:40 INFO     	 * (global step 3700: loss: 0.1845780685544014, lr: 0.0001
2023-12-16 19:13:53 INFO     [epoch 5/15] average loss: 0.249, lr: 0.0001
2023-12-16 19:13:53 INFO     saving model related files
2023-12-16 19:13:53 INFO     saving model
2023-12-16 19:13:54 INFO     saving tokenizer
2023-12-16 19:13:54 INFO     saving optimizer
2023-12-16 19:13:57 INFO     remove old optimizer files
2023-12-16 19:14:09 INFO     	 * (global step 3750: loss: 0.27562594413757324, lr: 0.0001
2023-12-16 19:14:33 INFO     	 * (global step 3800: loss: 0.2783967852592468, lr: 0.0001
2023-12-16 19:14:58 INFO     	 * (global step 3850: loss: 0.24765698611736298, lr: 0.0001
2023-12-16 19:15:22 INFO     	 * (global step 3900: loss: 0.281341090798378, lr: 0.0001
2023-12-16 19:15:46 INFO     	 * (global step 3950: loss: 0.22525851428508759, lr: 0.0001
2023-12-16 19:16:10 INFO     	 * (global step 4000: loss: 0.19879963248968124, lr: 0.0001
2023-12-16 19:16:35 INFO     	 * (global step 4050: loss: 0.2658332288265228, lr: 0.0001
2023-12-16 19:16:59 INFO     	 * (global step 4100: loss: 0.29403918981552124, lr: 0.0001
2023-12-16 19:17:23 INFO     	 * (global step 4150: loss: 0.1973194032907486, lr: 0.0001
2023-12-16 19:17:48 INFO     	 * (global step 4200: loss: 0.18010998144745827, lr: 0.0001
2023-12-16 19:18:12 INFO     	 * (global step 4250: loss: 0.1989646479487419, lr: 0.0001
2023-12-16 19:18:36 INFO     	 * (global step 4300: loss: 0.1847168579697609, lr: 0.0001
2023-12-16 19:18:59 INFO     [epoch 6/15] average loss: 0.232, lr: 0.0001
2023-12-16 19:18:59 INFO     saving model related files
2023-12-16 19:18:59 INFO     saving model
2023-12-16 19:19:01 INFO     saving tokenizer
2023-12-16 19:19:01 INFO     saving optimizer
2023-12-16 19:19:04 INFO     remove old optimizer files
2023-12-16 19:19:06 INFO     	 * (global step 4350: loss: 0.18821974098682404, lr: 0.0001
2023-12-16 19:19:30 INFO     	 * (global step 4400: loss: 0.2169208601117134, lr: 0.0001
2023-12-16 19:19:54 INFO     	 * (global step 4450: loss: 0.1989087238907814, lr: 0.0001
2023-12-16 19:20:19 INFO     	 * (global step 4500: loss: 0.2106582596898079, lr: 0.0001
2023-12-16 19:20:43 INFO     	 * (global step 4550: loss: 0.2133587971329689, lr: 0.0001
2023-12-16 19:21:07 INFO     	 * (global step 4600: loss: 0.24668803066015244, lr: 0.0001
2023-12-16 19:21:31 INFO     	 * (global step 4650: loss: 0.18238750100135803, lr: 0.0001
2023-12-16 19:21:56 INFO     	 * (global step 4700: loss: 0.2098023071885109, lr: 0.0001
2023-12-16 19:22:20 INFO     	 * (global step 4750: loss: 0.1715858355164528, lr: 0.0001
2023-12-16 19:22:44 INFO     	 * (global step 4800: loss: 0.12553144991397858, lr: 0.0001
2023-12-16 19:23:09 INFO     	 * (global step 4850: loss: 0.36634694039821625, lr: 0.0001
2023-12-16 19:23:33 INFO     	 * (global step 4900: loss: 0.21525400131940842, lr: 0.0001
2023-12-16 19:23:57 INFO     	 * (global step 4950: loss: 0.16520032659173012, lr: 0.0001
2023-12-16 19:24:06 INFO     [epoch 7/15] average loss: 0.218, lr: 0.0001
2023-12-16 19:24:06 INFO     saving model related files
2023-12-16 19:24:06 INFO     saving model
2023-12-16 19:24:08 INFO     saving tokenizer
2023-12-16 19:24:08 INFO     saving optimizer
2023-12-16 19:24:11 INFO     remove old optimizer files
2023-12-16 19:24:27 INFO     	 * (global step 5000: loss: 0.2622179388999939, lr: 0.0001
2023-12-16 19:24:51 INFO     	 * (global step 5050: loss: 0.1769808605313301, lr: 0.0001
2023-12-16 19:25:15 INFO     	 * (global step 5100: loss: 0.14724934473633766, lr: 0.0001
2023-12-16 19:25:39 INFO     	 * (global step 5150: loss: 0.14218832552433014, lr: 0.0001
2023-12-16 19:26:04 INFO     	 * (global step 5200: loss: 0.18075869046151638, lr: 0.0001
2023-12-16 19:26:28 INFO     	 * (global step 5250: loss: 0.2308674305677414, lr: 0.0001
2023-12-16 19:26:52 INFO     	 * (global step 5300: loss: 0.19653374701738358, lr: 0.0001
2023-12-16 19:27:16 INFO     	 * (global step 5350: loss: 0.150162935256958, lr: 0.0001
2023-12-16 19:27:41 INFO     	 * (global step 5400: loss: 0.23056413978338242, lr: 0.0001
2023-12-16 19:28:05 INFO     	 * (global step 5450: loss: 0.17484163492918015, lr: 0.0001
2023-12-16 19:28:29 INFO     	 * (global step 5500: loss: 0.13719802349805832, lr: 0.0001
2023-12-16 19:28:53 INFO     	 * (global step 5550: loss: 0.19636163860559464, lr: 0.0001
2023-12-16 19:29:13 INFO     [epoch 8/15] average loss: 0.204, lr: 0.0001
2023-12-16 19:29:13 INFO     saving model related files
2023-12-16 19:29:13 INFO     saving model
2023-12-16 19:29:14 INFO     saving tokenizer
2023-12-16 19:29:14 INFO     saving optimizer
2023-12-16 19:29:18 INFO     remove old optimizer files
2023-12-16 19:29:24 INFO     	 * (global step 5600: loss: 0.25256312638521194, lr: 0.0001
2023-12-16 19:29:48 INFO     	 * (global step 5650: loss: 0.19099213182926178, lr: 0.0001
2023-12-16 19:30:12 INFO     	 * (global step 5700: loss: 0.20025350898504257, lr: 0.0001
2023-12-16 19:30:36 INFO     	 * (global step 5750: loss: 0.16809700429439545, lr: 0.0001
2023-12-16 19:31:01 INFO     	 * (global step 5800: loss: 0.17965321987867355, lr: 0.0001
2023-12-16 19:31:25 INFO     	 * (global step 5850: loss: 0.1833951622247696, lr: 0.0001
2023-12-16 19:31:49 INFO     	 * (global step 5900: loss: 0.1619410142302513, lr: 0.0001
2023-12-16 19:32:14 INFO     	 * (global step 5950: loss: 0.12452447786927223, lr: 0.0001
2023-12-16 19:32:38 INFO     	 * (global step 6000: loss: 0.27730078995227814, lr: 0.0001
2023-12-16 19:33:02 INFO     	 * (global step 6050: loss: 0.22075244039297104, lr: 0.0001
2023-12-16 19:33:27 INFO     	 * (global step 6100: loss: 0.1945212110877037, lr: 0.0001
2023-12-16 19:33:51 INFO     	 * (global step 6150: loss: 0.19313987344503403, lr: 0.0001
2023-12-16 19:34:15 INFO     	 * (global step 6200: loss: 0.1721375435590744, lr: 0.0001
2023-12-16 19:34:20 INFO     [epoch 9/15] average loss: 0.191, lr: 0.0001
2023-12-16 19:34:20 INFO     saving model related files
2023-12-16 19:34:20 INFO     saving model
2023-12-16 19:34:22 INFO     saving tokenizer
2023-12-16 19:34:22 INFO     saving optimizer
2023-12-16 19:34:26 INFO     remove old optimizer files
2023-12-16 19:34:26 INFO     complete training: model ckpt was saved at base_trained_ckpt/model_eszyci
2023-12-16 19:34:26 INFO     ## 1st RUN: Configuration 2/12 ##
2023-12-16 19:34:26 INFO     initialize model trainer
2023-12-16 19:34:26 INFO     initialize checkpoint at base_trained_ckpt/model_dpyopu
2023-12-16 19:34:26 INFO     hyperparameters
2023-12-16 19:34:26 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-16 19:34:26 INFO     	 * dataset_name: default
2023-12-16 19:34:26 INFO     	 * input_types: ['paragraph']
2023-12-16 19:34:26 INFO     	 * output_types: ['questions_answers']
2023-12-16 19:34:26 INFO     	 * prefix_types: ['qag']
2023-12-16 19:34:26 INFO     	 * model: t5-base
2023-12-16 19:34:26 INFO     	 * max_length: 512
2023-12-16 19:34:26 INFO     	 * max_length_output: 512
2023-12-16 19:34:26 INFO     	 * epoch: 15
2023-12-16 19:34:26 INFO     	 * batch: 2
2023-12-16 19:34:26 INFO     	 * lr: 0.0001
2023-12-16 19:34:26 INFO     	 * fp16: False
2023-12-16 19:34:26 INFO     	 * random_seed: 1
2023-12-16 19:34:26 INFO     	 * gradient_accumulation_steps: 4
2023-12-16 19:34:26 INFO     	 * label_smoothing: 0.0
2023-12-16 19:34:26 INFO     initialize checkpoint with t5-base
2023-12-16 19:34:29 INFO     use spaCy answer extraction model: positionrank
2023-12-16 19:34:29 INFO     Model `t5-base`
2023-12-16 19:34:29 INFO     	 * Num of GPU in use: 1
2023-12-16 19:34:29 INFO     	 * Prefix: True
2023-12-16 19:34:29 INFO     	 * Language: en (ignore at the training phase)
2023-12-16 19:34:29 INFO     dataset preprocessing
2023-12-16 19:34:30 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-base.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-16 19:34:31 INFO     start model training
2023-12-16 19:35:17 INFO     	 * (global step 50: loss: 0.6667779386043549, lr: 0.0001
2023-12-16 19:36:04 INFO     	 * (global step 100: loss: 0.3620779812335968, lr: 0.0001
2023-12-16 19:36:51 INFO     	 * (global step 150: loss: 0.6301687881350517, lr: 0.0001
2023-12-16 19:37:38 INFO     	 * (global step 200: loss: 0.45351649820804596, lr: 0.0001
2023-12-16 19:38:25 INFO     	 * (global step 250: loss: 0.392860047519207, lr: 0.0001
2023-12-16 19:39:12 INFO     	 * (global step 300: loss: 0.4983123764395714, lr: 0.0001
2023-12-16 19:39:22 INFO     [epoch 0/15] average loss: 0.677, lr: 0.0001
2023-12-16 19:39:22 INFO     saving model related files
2023-12-16 19:39:22 INFO     saving model
2023-12-16 19:39:24 INFO     saving tokenizer
2023-12-16 19:39:24 INFO     saving optimizer
2023-12-16 19:39:28 INFO     remove old optimizer files
2023-12-16 19:40:05 INFO     	 * (global step 350: loss: 0.36926744133234024, lr: 0.0001
2023-12-16 19:40:52 INFO     	 * (global step 400: loss: 0.39811941981315613, lr: 0.0001
2023-12-16 19:41:39 INFO     	 * (global step 450: loss: 0.39766258746385574, lr: 0.0001
2023-12-16 19:42:26 INFO     	 * (global step 500: loss: 0.28730830550193787, lr: 0.0001
2023-12-16 19:43:13 INFO     	 * (global step 550: loss: 0.3246122673153877, lr: 0.0001
2023-12-16 19:44:00 INFO     	 * (global step 600: loss: 0.31850384920835495, lr: 0.0001
2023-12-16 19:44:20 INFO     [epoch 1/15] average loss: 0.368, lr: 0.0001
2023-12-16 19:44:20 INFO     saving model related files
2023-12-16 19:44:20 INFO     saving model
2023-12-16 19:44:21 INFO     saving tokenizer
2023-12-16 19:44:21 INFO     saving optimizer
2023-12-16 19:44:25 INFO     remove old optimizer files
2023-12-16 19:44:54 INFO     	 * (global step 650: loss: 0.27254194021224976, lr: 0.0001
2023-12-16 19:45:41 INFO     	 * (global step 700: loss: 0.29764799773693085, lr: 0.0001
2023-12-16 19:46:28 INFO     	 * (global step 750: loss: 0.29989705979824066, lr: 0.0001
2023-12-16 19:47:15 INFO     	 * (global step 800: loss: 0.29200412333011627, lr: 0.0001
2023-12-16 19:48:02 INFO     	 * (global step 850: loss: 0.32812967151403427, lr: 0.0001
2023-12-16 19:48:49 INFO     	 * (global step 900: loss: 0.42493222653865814, lr: 0.0001
2023-12-16 19:49:18 INFO     [epoch 2/15] average loss: 0.335, lr: 0.0001
2023-12-16 19:49:18 INFO     saving model related files
2023-12-16 19:49:18 INFO     saving model
2023-12-16 19:49:20 INFO     saving tokenizer
2023-12-16 19:49:20 INFO     saving optimizer
2023-12-16 19:49:24 INFO     remove old optimizer files
2023-12-16 19:49:43 INFO     	 * (global step 950: loss: 0.31920865178108215, lr: 0.0001
2023-12-16 19:50:29 INFO     	 * (global step 1000: loss: 0.3452780209481716, lr: 0.0001
2023-12-16 19:51:16 INFO     	 * (global step 1050: loss: 0.338620588183403, lr: 0.0001
2023-12-16 19:52:03 INFO     	 * (global step 1100: loss: 0.33324792236089706, lr: 0.0001
2023-12-16 19:52:50 INFO     	 * (global step 1150: loss: 0.34516941010951996, lr: 0.0001
2023-12-16 19:53:37 INFO     	 * (global step 1200: loss: 0.2507483549416065, lr: 0.0001
2023-12-16 19:54:15 INFO     [epoch 3/15] average loss: 0.313, lr: 0.0001
2023-12-16 19:54:15 INFO     saving model related files
2023-12-16 19:54:15 INFO     saving model
2023-12-16 19:54:17 INFO     saving tokenizer
2023-12-16 19:54:17 INFO     saving optimizer
2023-12-16 19:54:21 INFO     remove old optimizer files
2023-12-16 19:54:30 INFO     	 * (global step 1250: loss: 0.2763531841337681, lr: 0.0001
2023-12-16 19:55:17 INFO     	 * (global step 1300: loss: 0.3029669225215912, lr: 0.0001
2023-12-16 19:56:04 INFO     	 * (global step 1350: loss: 0.3350086957216263, lr: 0.0001
2023-12-16 19:56:51 INFO     	 * (global step 1400: loss: 0.3301493301987648, lr: 0.0001
2023-12-16 19:57:38 INFO     	 * (global step 1450: loss: 0.3242044486105442, lr: 0.0001
2023-12-16 19:58:25 INFO     	 * (global step 1500: loss: 0.20335986465215683, lr: 0.0001
2023-12-16 19:59:11 INFO     	 * (global step 1550: loss: 0.3620617836713791, lr: 0.0001
2023-12-16 19:59:12 INFO     [epoch 4/15] average loss: 0.294, lr: 0.0001
2023-12-16 19:59:12 INFO     saving model related files
2023-12-16 19:59:12 INFO     saving model
2023-12-16 19:59:14 INFO     saving tokenizer
2023-12-16 19:59:14 INFO     saving optimizer
2023-12-16 19:59:17 INFO     remove old optimizer files
2023-12-16 20:00:04 INFO     	 * (global step 1600: loss: 0.3196807987987995, lr: 0.0001
2023-12-16 20:00:50 INFO     	 * (global step 1650: loss: 0.28186779096722603, lr: 0.0001
2023-12-16 20:01:37 INFO     	 * (global step 1700: loss: 0.3467637784779072, lr: 0.0001
2023-12-16 20:02:24 INFO     	 * (global step 1750: loss: 0.2620423212647438, lr: 0.0001
2023-12-16 20:03:11 INFO     	 * (global step 1800: loss: 0.3457694910466671, lr: 0.0001
2023-12-16 20:03:58 INFO     	 * (global step 1850: loss: 0.4129613824188709, lr: 0.0001
2023-12-16 20:04:08 INFO     [epoch 5/15] average loss: 0.279, lr: 0.0001
2023-12-16 20:04:08 INFO     saving model related files
2023-12-16 20:04:08 INFO     saving model
2023-12-16 20:04:10 INFO     saving tokenizer
2023-12-16 20:04:10 INFO     saving optimizer
2023-12-16 20:04:14 INFO     remove old optimizer files
2023-12-16 20:04:51 INFO     	 * (global step 1900: loss: 0.2549196369946003, lr: 0.0001
2023-12-16 20:05:38 INFO     	 * (global step 1950: loss: 0.2221701443195343, lr: 0.0001
2023-12-16 20:06:25 INFO     	 * (global step 2000: loss: 0.3558594733476639, lr: 0.0001
2023-12-16 20:07:12 INFO     	 * (global step 2050: loss: 0.2072228640317917, lr: 0.0001
2023-12-16 20:07:59 INFO     	 * (global step 2100: loss: 0.29715898260474205, lr: 0.0001
2023-12-16 20:08:46 INFO     	 * (global step 2150: loss: 0.27227428928017616, lr: 0.0001
2023-12-16 20:09:06 INFO     [epoch 6/15] average loss: 0.265, lr: 0.0001
2023-12-16 20:09:06 INFO     saving model related files
2023-12-16 20:09:06 INFO     saving model
2023-12-16 20:09:07 INFO     saving tokenizer
2023-12-16 20:09:07 INFO     saving optimizer
2023-12-16 20:09:12 INFO     remove old optimizer files
2023-12-16 20:09:40 INFO     	 * (global step 2200: loss: 0.19317150861024857, lr: 0.0001
2023-12-16 20:10:27 INFO     	 * (global step 2250: loss: 0.21794893592596054, lr: 0.0001
2023-12-16 20:11:14 INFO     	 * (global step 2300: loss: 0.22538327053189278, lr: 0.0001
2023-12-16 20:12:01 INFO     	 * (global step 2350: loss: 0.24400119110941887, lr: 0.0001
2023-12-16 20:12:48 INFO     	 * (global step 2400: loss: 0.20067688822746277, lr: 0.0001
2023-12-16 20:13:35 INFO     	 * (global step 2450: loss: 0.26073218509554863, lr: 0.0001
2023-12-16 20:14:04 INFO     [epoch 7/15] average loss: 0.252, lr: 0.0001
2023-12-16 20:14:04 INFO     saving model related files
2023-12-16 20:14:04 INFO     saving model
2023-12-16 20:14:06 INFO     saving tokenizer
2023-12-16 20:14:06 INFO     saving optimizer
2023-12-16 20:14:09 INFO     remove old optimizer files
2023-12-16 20:14:28 INFO     	 * (global step 2500: loss: 0.23213711380958557, lr: 0.0001
2023-12-16 20:15:16 INFO     	 * (global step 2550: loss: 0.2097875513136387, lr: 0.0001
2023-12-16 20:16:03 INFO     	 * (global step 2600: loss: 0.24367118254303932, lr: 0.0001
2023-12-16 20:16:50 INFO     	 * (global step 2650: loss: 0.2752571105957031, lr: 0.0001
2023-12-16 20:17:37 INFO     	 * (global step 2700: loss: 0.2078171968460083, lr: 0.0001
2023-12-16 20:18:24 INFO     	 * (global step 2750: loss: 0.27077286690473557, lr: 0.0001
2023-12-16 20:19:02 INFO     [epoch 8/15] average loss: 0.24, lr: 0.0001
2023-12-16 20:19:02 INFO     saving model related files
2023-12-16 20:19:02 INFO     saving model
2023-12-16 20:19:04 INFO     saving tokenizer
2023-12-16 20:19:04 INFO     saving optimizer
2023-12-16 20:19:08 INFO     remove old optimizer files
2023-12-16 20:19:18 INFO     	 * (global step 2800: loss: 0.17123450711369514, lr: 0.0001
2023-12-16 20:20:05 INFO     	 * (global step 2850: loss: 0.19707165658473969, lr: 0.0001
2023-12-16 20:20:52 INFO     	 * (global step 2900: loss: 0.18398868665099144, lr: 0.0001
2023-12-16 20:21:39 INFO     	 * (global step 2950: loss: 0.24617380276322365, lr: 0.0001
2023-12-16 20:22:26 INFO     	 * (global step 3000: loss: 0.19246955960988998, lr: 0.0001
2023-12-16 20:23:13 INFO     	 * (global step 3050: loss: 0.21906980499625206, lr: 0.0001
2023-12-16 20:24:00 INFO     	 * (global step 3100: loss: 0.23676327243447304, lr: 0.0001
2023-12-16 20:24:01 INFO     [epoch 9/15] average loss: 0.229, lr: 0.0001
2023-12-16 20:24:01 INFO     saving model related files
2023-12-16 20:24:01 INFO     saving model
2023-12-16 20:24:02 INFO     saving tokenizer
2023-12-16 20:24:03 INFO     saving optimizer
2023-12-16 20:24:06 INFO     remove old optimizer files
2023-12-16 20:24:06 INFO     complete training: model ckpt was saved at base_trained_ckpt/model_dpyopu
2023-12-16 20:24:06 INFO     ## 1st RUN: Configuration 3/12 ##
2023-12-16 20:24:06 INFO     initialize model trainer
2023-12-16 20:24:06 INFO     initialize checkpoint at base_trained_ckpt/model_mzgdpa
2023-12-16 20:24:06 INFO     hyperparameters
2023-12-16 20:24:06 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-16 20:24:06 INFO     	 * dataset_name: default
2023-12-16 20:24:06 INFO     	 * input_types: ['paragraph']
2023-12-16 20:24:06 INFO     	 * output_types: ['questions_answers']
2023-12-16 20:24:06 INFO     	 * prefix_types: ['qag']
2023-12-16 20:24:06 INFO     	 * model: t5-base
2023-12-16 20:24:06 INFO     	 * max_length: 512
2023-12-16 20:24:06 INFO     	 * max_length_output: 512
2023-12-16 20:24:06 INFO     	 * epoch: 15
2023-12-16 20:24:06 INFO     	 * batch: 2
2023-12-16 20:24:06 INFO     	 * lr: 0.0001
2023-12-16 20:24:06 INFO     	 * fp16: False
2023-12-16 20:24:06 INFO     	 * random_seed: 1
2023-12-16 20:24:06 INFO     	 * gradient_accumulation_steps: 2
2023-12-16 20:24:06 INFO     	 * label_smoothing: 0.0
2023-12-16 20:24:06 INFO     initialize checkpoint with t5-base
2023-12-16 20:24:09 INFO     use spaCy answer extraction model: positionrank
2023-12-16 20:24:09 INFO     Model `t5-base`
2023-12-16 20:24:09 INFO     	 * Num of GPU in use: 1
2023-12-16 20:24:09 INFO     	 * Prefix: True
2023-12-16 20:24:09 INFO     	 * Language: en (ignore at the training phase)
2023-12-16 20:24:09 INFO     dataset preprocessing
2023-12-16 20:24:10 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-base.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-16 20:24:10 INFO     start model training
2023-12-16 20:24:34 INFO     	 * (global step 50: loss: 0.49418579041957855, lr: 0.0001
2023-12-16 20:24:59 INFO     	 * (global step 100: loss: 0.5203526616096497, lr: 0.0001
2023-12-16 20:25:23 INFO     	 * (global step 150: loss: 0.4631658345460892, lr: 0.0001
2023-12-16 20:25:47 INFO     	 * (global step 200: loss: 0.3211568593978882, lr: 0.0001
2023-12-16 20:26:11 INFO     	 * (global step 250: loss: 0.3257419914007187, lr: 0.0001
2023-12-16 20:26:36 INFO     	 * (global step 300: loss: 0.6540381610393524, lr: 0.0001
2023-12-16 20:27:00 INFO     	 * (global step 350: loss: 0.3902387171983719, lr: 0.0001
2023-12-16 20:27:24 INFO     	 * (global step 400: loss: 0.32217617332935333, lr: 0.0001
2023-12-16 20:27:49 INFO     	 * (global step 450: loss: 0.36867697536945343, lr: 0.0001
2023-12-16 20:28:13 INFO     	 * (global step 500: loss: 0.3087269067764282, lr: 0.0001
2023-12-16 20:28:37 INFO     	 * (global step 550: loss: 0.756430983543396, lr: 0.0001
2023-12-16 20:29:02 INFO     	 * (global step 600: loss: 0.599729061126709, lr: 0.0001
2023-12-16 20:29:12 INFO     [epoch 0/15] average loss: 0.548, lr: 0.0001
2023-12-16 20:29:12 INFO     saving model related files
2023-12-16 20:29:12 INFO     saving model
2023-12-16 20:29:14 INFO     saving tokenizer
2023-12-16 20:29:14 INFO     saving optimizer
2023-12-16 20:29:16 INFO     remove old optimizer files
2023-12-16 20:29:30 INFO     	 * (global step 650: loss: 0.4929016977548599, lr: 0.0001
2023-12-16 20:29:55 INFO     	 * (global step 700: loss: 0.35122162103652954, lr: 0.0001
2023-12-16 20:30:19 INFO     	 * (global step 750: loss: 0.3768063634634018, lr: 0.0001
2023-12-16 20:30:43 INFO     	 * (global step 800: loss: 0.4411477744579315, lr: 0.0001
2023-12-16 20:31:07 INFO     	 * (global step 850: loss: 0.34015868604183197, lr: 0.0001
2023-12-16 20:31:32 INFO     	 * (global step 900: loss: 0.3403104990720749, lr: 0.0001
2023-12-16 20:31:56 INFO     	 * (global step 950: loss: 0.42871369421482086, lr: 0.0001
2023-12-16 20:32:20 INFO     	 * (global step 1000: loss: 0.3327028304338455, lr: 0.0001
2023-12-16 20:32:45 INFO     	 * (global step 1050: loss: 0.35794295370578766, lr: 0.0001
2023-12-16 20:33:09 INFO     	 * (global step 1100: loss: 0.32378941774368286, lr: 0.0001
2023-12-16 20:33:33 INFO     	 * (global step 1150: loss: 0.3940761238336563, lr: 0.0001
2023-12-16 20:33:57 INFO     	 * (global step 1200: loss: 0.31409725546836853, lr: 0.0001
2023-12-16 20:34:18 INFO     [epoch 1/15] average loss: 0.343, lr: 0.0001
2023-12-16 20:34:18 INFO     saving model related files
2023-12-16 20:34:18 INFO     saving model
2023-12-16 20:34:20 INFO     saving tokenizer
2023-12-16 20:34:20 INFO     saving optimizer
2023-12-16 20:34:23 INFO     remove old optimizer files
2023-12-16 20:34:27 INFO     	 * (global step 1250: loss: 0.25309765338897705, lr: 0.0001
2023-12-16 20:34:51 INFO     	 * (global step 1300: loss: 0.25012849271297455, lr: 0.0001
2023-12-16 20:35:15 INFO     	 * (global step 1350: loss: 0.2862309664487839, lr: 0.0001
2023-12-16 20:35:39 INFO     	 * (global step 1400: loss: 0.4213569164276123, lr: 0.0001
2023-12-16 20:36:04 INFO     	 * (global step 1450: loss: 0.37407152354717255, lr: 0.0001
2023-12-16 20:36:28 INFO     	 * (global step 1500: loss: 0.27324433624744415, lr: 0.0001
2023-12-16 20:36:52 INFO     	 * (global step 1550: loss: 0.2589186951518059, lr: 0.0001
2023-12-16 20:37:17 INFO     	 * (global step 1600: loss: 0.5157589614391327, lr: 0.0001
2023-12-16 20:37:41 INFO     	 * (global step 1650: loss: 0.2834719270467758, lr: 0.0001
2023-12-16 20:38:05 INFO     	 * (global step 1700: loss: 0.33662088215351105, lr: 0.0001
2023-12-16 20:38:29 INFO     	 * (global step 1750: loss: 0.26195765286684036, lr: 0.0001
2023-12-16 20:38:54 INFO     	 * (global step 1800: loss: 0.21240635961294174, lr: 0.0001
2023-12-16 20:39:18 INFO     	 * (global step 1850: loss: 0.3028103709220886, lr: 0.0001
2023-12-16 20:39:25 INFO     [epoch 2/15] average loss: 0.311, lr: 0.0001
2023-12-16 20:39:25 INFO     saving model related files
2023-12-16 20:39:25 INFO     saving model
2023-12-16 20:39:26 INFO     saving tokenizer
2023-12-16 20:39:26 INFO     saving optimizer
2023-12-16 20:39:30 INFO     remove old optimizer files
2023-12-16 20:39:48 INFO     	 * (global step 1900: loss: 0.2677692472934723, lr: 0.0001
2023-12-16 20:40:12 INFO     	 * (global step 1950: loss: 0.30841729044914246, lr: 0.0001
2023-12-16 20:40:36 INFO     	 * (global step 2000: loss: 0.23540370166301727, lr: 0.0001
2023-12-16 20:41:01 INFO     	 * (global step 2050: loss: 0.296060673892498, lr: 0.0001
2023-12-16 20:41:25 INFO     	 * (global step 2100: loss: 0.19964437186717987, lr: 0.0001
2023-12-16 20:41:50 INFO     	 * (global step 2150: loss: 0.2661914676427841, lr: 0.0001
2023-12-16 20:42:14 INFO     	 * (global step 2200: loss: 0.29411663115024567, lr: 0.0001
2023-12-16 20:42:38 INFO     	 * (global step 2250: loss: 0.24919825047254562, lr: 0.0001
2023-12-16 20:43:03 INFO     	 * (global step 2300: loss: 0.22244399040937424, lr: 0.0001
2023-12-16 20:43:27 INFO     	 * (global step 2350: loss: 0.2795008271932602, lr: 0.0001
2023-12-16 20:43:51 INFO     	 * (global step 2400: loss: 0.3154192864894867, lr: 0.0001
2023-12-16 20:44:16 INFO     	 * (global step 2450: loss: 0.3527766913175583, lr: 0.0001
2023-12-16 20:44:33 INFO     [epoch 3/15] average loss: 0.287, lr: 0.0001
2023-12-16 20:44:33 INFO     saving model related files
2023-12-16 20:44:33 INFO     saving model
2023-12-16 20:44:34 INFO     saving tokenizer
2023-12-16 20:44:34 INFO     saving optimizer
2023-12-16 20:44:37 INFO     remove old optimizer files
2023-12-16 20:44:45 INFO     	 * (global step 2500: loss: 0.2615421265363693, lr: 0.0001
2023-12-16 20:45:10 INFO     	 * (global step 2550: loss: 0.24940432608127594, lr: 0.0001
2023-12-16 20:45:34 INFO     	 * (global step 2600: loss: 0.3015890121459961, lr: 0.0001
2023-12-16 20:45:58 INFO     	 * (global step 2650: loss: 0.1576932892203331, lr: 0.0001
2023-12-16 20:46:23 INFO     	 * (global step 2700: loss: 0.23415205627679825, lr: 0.0001
2023-12-16 20:46:47 INFO     	 * (global step 2750: loss: 0.24991074576973915, lr: 0.0001
2023-12-16 20:47:11 INFO     	 * (global step 2800: loss: 0.22527548670768738, lr: 0.0001
2023-12-16 20:47:35 INFO     	 * (global step 2850: loss: 0.24551092088222504, lr: 0.0001
2023-12-16 20:48:00 INFO     	 * (global step 2900: loss: 0.13563117012381554, lr: 0.0001
2023-12-16 20:48:24 INFO     	 * (global step 2950: loss: 0.2781895101070404, lr: 0.0001
2023-12-16 20:48:49 INFO     	 * (global step 3000: loss: 0.28965824097394943, lr: 0.0001
2023-12-16 20:49:13 INFO     	 * (global step 3050: loss: 0.22606680542230606, lr: 0.0001
2023-12-16 20:49:37 INFO     	 * (global step 3100: loss: 0.24867833405733109, lr: 0.0001
2023-12-16 20:49:40 INFO     [epoch 4/15] average loss: 0.267, lr: 0.0001
2023-12-16 20:49:40 INFO     saving model related files
2023-12-16 20:49:40 INFO     saving model
2023-12-16 20:49:42 INFO     saving tokenizer
2023-12-16 20:49:42 INFO     saving optimizer
2023-12-16 20:49:45 INFO     remove old optimizer files
2023-12-16 20:50:07 INFO     	 * (global step 3150: loss: 0.18731175363063812, lr: 0.0001
2023-12-16 20:50:31 INFO     	 * (global step 3200: loss: 0.23560383915901184, lr: 0.0001
2023-12-16 20:50:56 INFO     	 * (global step 3250: loss: 0.29365983605384827, lr: 0.0001
2023-12-16 20:51:20 INFO     	 * (global step 3300: loss: 0.20363309234380722, lr: 0.0001
2023-12-16 20:51:45 INFO     	 * (global step 3350: loss: 0.31668469309806824, lr: 0.0001
2023-12-16 20:52:09 INFO     	 * (global step 3400: loss: 0.24753794074058533, lr: 0.0001
2023-12-16 20:52:33 INFO     	 * (global step 3450: loss: 0.2485618218779564, lr: 0.0001
2023-12-16 20:52:58 INFO     	 * (global step 3500: loss: 0.2761819362640381, lr: 0.0001
2023-12-16 20:53:22 INFO     	 * (global step 3550: loss: 0.29911942780017853, lr: 0.0001
2023-12-16 20:53:46 INFO     	 * (global step 3600: loss: 0.19538744539022446, lr: 0.0001
2023-12-16 20:54:11 INFO     	 * (global step 3650: loss: 0.35315240919589996, lr: 0.0001
2023-12-16 20:54:35 INFO     	 * (global step 3700: loss: 0.1845780685544014, lr: 0.0001
2023-12-16 20:54:48 INFO     [epoch 5/15] average loss: 0.249, lr: 0.0001
2023-12-16 20:54:48 INFO     saving model related files
2023-12-16 20:54:48 INFO     saving model
2023-12-16 20:54:50 INFO     saving tokenizer
2023-12-16 20:54:50 INFO     saving optimizer
2023-12-16 20:54:53 INFO     remove old optimizer files
2023-12-16 20:55:05 INFO     	 * (global step 3750: loss: 0.27562594413757324, lr: 0.0001
2023-12-16 20:55:29 INFO     	 * (global step 3800: loss: 0.2783967852592468, lr: 0.0001
2023-12-16 20:55:53 INFO     	 * (global step 3850: loss: 0.24765698611736298, lr: 0.0001
2023-12-16 20:56:18 INFO     	 * (global step 3900: loss: 0.281341090798378, lr: 0.0001
2023-12-16 20:56:42 INFO     	 * (global step 3950: loss: 0.22525851428508759, lr: 0.0001
2023-12-16 20:57:07 INFO     	 * (global step 4000: loss: 0.19879963248968124, lr: 0.0001
2023-12-16 20:57:31 INFO     	 * (global step 4050: loss: 0.2658332288265228, lr: 0.0001
2023-12-16 20:57:55 INFO     	 * (global step 4100: loss: 0.29403918981552124, lr: 0.0001
2023-12-16 20:58:20 INFO     	 * (global step 4150: loss: 0.1973194032907486, lr: 0.0001
2023-12-16 20:58:44 INFO     	 * (global step 4200: loss: 0.18010998144745827, lr: 0.0001
2023-12-16 20:59:08 INFO     	 * (global step 4250: loss: 0.1989646479487419, lr: 0.0001
2023-12-16 20:59:33 INFO     	 * (global step 4300: loss: 0.1847168579697609, lr: 0.0001
2023-12-16 20:59:56 INFO     [epoch 6/15] average loss: 0.232, lr: 0.0001
2023-12-16 20:59:56 INFO     saving model related files
2023-12-16 20:59:56 INFO     saving model
2023-12-16 20:59:57 INFO     saving tokenizer
2023-12-16 20:59:57 INFO     saving optimizer
2023-12-16 21:00:01 INFO     remove old optimizer files
2023-12-16 21:00:02 INFO     	 * (global step 4350: loss: 0.18821974098682404, lr: 0.0001
2023-12-16 21:00:27 INFO     	 * (global step 4400: loss: 0.2169208601117134, lr: 0.0001
2023-12-16 21:00:51 INFO     	 * (global step 4450: loss: 0.1989087238907814, lr: 0.0001
2023-12-16 21:01:15 INFO     	 * (global step 4500: loss: 0.2106582596898079, lr: 0.0001
2023-12-16 21:01:39 INFO     	 * (global step 4550: loss: 0.2133587971329689, lr: 0.0001
2023-12-16 21:02:04 INFO     	 * (global step 4600: loss: 0.24668803066015244, lr: 0.0001
2023-12-16 21:02:28 INFO     	 * (global step 4650: loss: 0.18238750100135803, lr: 0.0001
2023-12-16 21:02:52 INFO     	 * (global step 4700: loss: 0.2098023071885109, lr: 0.0001
2023-12-16 21:03:17 INFO     	 * (global step 4750: loss: 0.1715858355164528, lr: 0.0001
2023-12-16 21:03:41 INFO     	 * (global step 4800: loss: 0.12553144991397858, lr: 0.0001
2023-12-16 21:04:05 INFO     	 * (global step 4850: loss: 0.36634694039821625, lr: 0.0001
2023-12-16 21:04:29 INFO     	 * (global step 4900: loss: 0.21525400131940842, lr: 0.0001
2023-12-16 21:04:54 INFO     	 * (global step 4950: loss: 0.16520032659173012, lr: 0.0001
2023-12-16 21:05:03 INFO     [epoch 7/15] average loss: 0.218, lr: 0.0001
2023-12-16 21:05:03 INFO     saving model related files
2023-12-16 21:05:03 INFO     saving model
2023-12-16 21:05:04 INFO     saving tokenizer
2023-12-16 21:05:04 INFO     saving optimizer
2023-12-16 21:05:10 INFO     remove old optimizer files
2023-12-16 21:05:26 INFO     	 * (global step 5000: loss: 0.2622179388999939, lr: 0.0001
2023-12-16 21:05:50 INFO     	 * (global step 5050: loss: 0.1769808605313301, lr: 0.0001
2023-12-16 21:06:14 INFO     	 * (global step 5100: loss: 0.14724934473633766, lr: 0.0001
2023-12-16 21:06:38 INFO     	 * (global step 5150: loss: 0.14218832552433014, lr: 0.0001
2023-12-16 21:07:03 INFO     	 * (global step 5200: loss: 0.18075869046151638, lr: 0.0001
2023-12-16 21:07:27 INFO     	 * (global step 5250: loss: 0.2308674305677414, lr: 0.0001
2023-12-16 21:07:51 INFO     	 * (global step 5300: loss: 0.19653374701738358, lr: 0.0001
2023-12-16 21:08:15 INFO     	 * (global step 5350: loss: 0.150162935256958, lr: 0.0001
2023-12-16 21:08:40 INFO     	 * (global step 5400: loss: 0.23056413978338242, lr: 0.0001
2023-12-16 21:09:04 INFO     	 * (global step 5450: loss: 0.17484163492918015, lr: 0.0001
2023-12-16 21:09:28 INFO     	 * (global step 5500: loss: 0.13719802349805832, lr: 0.0001
2023-12-16 21:09:53 INFO     	 * (global step 5550: loss: 0.19636163860559464, lr: 0.0001
2023-12-16 21:10:12 INFO     [epoch 8/15] average loss: 0.204, lr: 0.0001
2023-12-16 21:10:12 INFO     saving model related files
2023-12-16 21:10:12 INFO     saving model
2023-12-16 21:10:13 INFO     saving tokenizer
2023-12-16 21:10:13 INFO     saving optimizer
2023-12-16 21:10:16 INFO     remove old optimizer files
2023-12-16 21:10:22 INFO     	 * (global step 5600: loss: 0.25256312638521194, lr: 0.0001
2023-12-16 21:10:46 INFO     	 * (global step 5650: loss: 0.19099213182926178, lr: 0.0001
2023-12-16 21:11:10 INFO     	 * (global step 5700: loss: 0.20025350898504257, lr: 0.0001
2023-12-16 21:11:35 INFO     	 * (global step 5750: loss: 0.16809700429439545, lr: 0.0001
2023-12-16 21:11:59 INFO     	 * (global step 5800: loss: 0.17965321987867355, lr: 0.0001
2023-12-16 21:12:23 INFO     	 * (global step 5850: loss: 0.1833951622247696, lr: 0.0001
2023-12-16 21:12:47 INFO     	 * (global step 5900: loss: 0.1619410142302513, lr: 0.0001
2023-12-16 21:13:12 INFO     	 * (global step 5950: loss: 0.12452447786927223, lr: 0.0001
2023-12-16 21:13:36 INFO     	 * (global step 6000: loss: 0.27730078995227814, lr: 0.0001
2023-12-16 21:14:00 INFO     	 * (global step 6050: loss: 0.22075244039297104, lr: 0.0001
2023-12-16 21:14:25 INFO     	 * (global step 6100: loss: 0.1945212110877037, lr: 0.0001
2023-12-16 21:14:49 INFO     	 * (global step 6150: loss: 0.19313987344503403, lr: 0.0001
2023-12-16 21:15:13 INFO     	 * (global step 6200: loss: 0.1721375435590744, lr: 0.0001
2023-12-16 21:15:18 INFO     [epoch 9/15] average loss: 0.191, lr: 0.0001
2023-12-16 21:15:18 INFO     saving model related files
2023-12-16 21:15:18 INFO     saving model
2023-12-16 21:15:20 INFO     saving tokenizer
2023-12-16 21:15:20 INFO     saving optimizer
2023-12-16 21:15:23 INFO     remove old optimizer files
2023-12-16 21:15:24 INFO     complete training: model ckpt was saved at base_trained_ckpt/model_mzgdpa
2023-12-16 21:15:24 INFO     ## 1st RUN: Configuration 4/12 ##
2023-12-16 21:15:24 INFO     initialize model trainer
2023-12-16 21:15:24 INFO     initialize checkpoint at base_trained_ckpt/model_mntyya
2023-12-16 21:15:24 INFO     hyperparameters
2023-12-16 21:15:24 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-16 21:15:24 INFO     	 * dataset_name: default
2023-12-16 21:15:24 INFO     	 * input_types: ['paragraph']
2023-12-16 21:15:24 INFO     	 * output_types: ['questions_answers']
2023-12-16 21:15:24 INFO     	 * prefix_types: ['qag']
2023-12-16 21:15:24 INFO     	 * model: t5-base
2023-12-16 21:15:24 INFO     	 * max_length: 512
2023-12-16 21:15:24 INFO     	 * max_length_output: 512
2023-12-16 21:15:24 INFO     	 * epoch: 15
2023-12-16 21:15:24 INFO     	 * batch: 2
2023-12-16 21:15:24 INFO     	 * lr: 5e-05
2023-12-16 21:15:24 INFO     	 * fp16: False
2023-12-16 21:15:24 INFO     	 * random_seed: 1
2023-12-16 21:15:24 INFO     	 * gradient_accumulation_steps: 4
2023-12-16 21:15:24 INFO     	 * label_smoothing: 0.15
2023-12-16 21:15:24 INFO     initialize checkpoint with t5-base
2023-12-16 21:15:26 INFO     use spaCy answer extraction model: positionrank
2023-12-16 21:15:26 INFO     Model `t5-base`
2023-12-16 21:15:26 INFO     	 * Num of GPU in use: 1
2023-12-16 21:15:26 INFO     	 * Prefix: True
2023-12-16 21:15:26 INFO     	 * Language: en (ignore at the training phase)
2023-12-16 21:15:26 INFO     dataset preprocessing
2023-12-16 21:15:27 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-base.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-16 21:15:27 INFO     start model training
2023-12-16 21:16:14 INFO     	 * (global step 50: loss: 0.8280775547027588, lr: 5e-05
2023-12-16 21:17:01 INFO     	 * (global step 100: loss: 0.461581289768219, lr: 5e-05
2023-12-16 21:17:48 INFO     	 * (global step 150: loss: 0.7385011613368988, lr: 5e-05
2023-12-16 21:18:35 INFO     	 * (global step 200: loss: 0.5216936841607094, lr: 5e-05
2023-12-16 21:19:22 INFO     	 * (global step 250: loss: 0.4498784989118576, lr: 5e-05
2023-12-16 21:20:09 INFO     	 * (global step 300: loss: 0.5558523163199425, lr: 5e-05
2023-12-16 21:20:19 INFO     [epoch 0/15] average loss: 0.882, lr: 5e-05
2023-12-16 21:20:19 INFO     saving model related files
2023-12-16 21:20:19 INFO     saving model
2023-12-16 21:20:21 INFO     saving tokenizer
2023-12-16 21:20:21 INFO     saving optimizer
2023-12-16 21:20:24 INFO     remove old optimizer files
2023-12-16 21:21:01 INFO     	 * (global step 350: loss: 0.41594740748405457, lr: 5e-05
2023-12-16 21:21:48 INFO     	 * (global step 400: loss: 0.4485616385936737, lr: 5e-05
2023-12-16 21:22:36 INFO     	 * (global step 450: loss: 0.4535547196865082, lr: 5e-05
2023-12-16 21:23:23 INFO     	 * (global step 500: loss: 0.32988353446125984, lr: 5e-05
2023-12-16 21:24:10 INFO     	 * (global step 550: loss: 0.3607261888682842, lr: 5e-05
2023-12-16 21:24:57 INFO     	 * (global step 600: loss: 0.3568142130970955, lr: 5e-05
2023-12-16 21:25:17 INFO     [epoch 1/15] average loss: 0.411, lr: 5e-05
2023-12-16 21:25:17 INFO     saving model related files
2023-12-16 21:25:17 INFO     saving model
2023-12-16 21:25:18 INFO     saving tokenizer
2023-12-16 21:25:18 INFO     saving optimizer
2023-12-16 21:25:22 INFO     remove old optimizer files
2023-12-16 21:25:50 INFO     	 * (global step 650: loss: 0.3035845458507538, lr: 5e-05
2023-12-16 21:26:37 INFO     	 * (global step 700: loss: 0.3330783173441887, lr: 5e-05
2023-12-16 21:27:24 INFO     	 * (global step 750: loss: 0.3295895382761955, lr: 5e-05
2023-12-16 21:28:11 INFO     	 * (global step 800: loss: 0.3288930729031563, lr: 5e-05
2023-12-16 21:28:58 INFO     	 * (global step 850: loss: 0.3608703464269638, lr: 5e-05
2023-12-16 21:29:45 INFO     	 * (global step 900: loss: 0.4769106097519398, lr: 5e-05
2023-12-16 21:30:14 INFO     [epoch 2/15] average loss: 0.371, lr: 5e-05
2023-12-16 21:30:14 INFO     saving model related files
2023-12-16 21:30:14 INFO     saving model
2023-12-16 21:30:16 INFO     saving tokenizer
2023-12-16 21:30:16 INFO     saving optimizer
2023-12-16 21:30:19 INFO     remove old optimizer files
2023-12-16 21:30:38 INFO     	 * (global step 950: loss: 0.35574790090322495, lr: 5e-05
2023-12-16 21:31:25 INFO     	 * (global step 1000: loss: 0.3858143761754036, lr: 5e-05
2023-12-16 21:32:12 INFO     	 * (global step 1050: loss: 0.38129355013370514, lr: 5e-05
2023-12-16 21:32:59 INFO     	 * (global step 1100: loss: 0.38084954395890236, lr: 5e-05
2023-12-16 21:33:46 INFO     	 * (global step 1150: loss: 0.3940477669239044, lr: 5e-05
2023-12-16 21:34:33 INFO     	 * (global step 1200: loss: 0.2831944599747658, lr: 5e-05
2023-12-16 21:35:11 INFO     [epoch 3/15] average loss: 0.349, lr: 5e-05
2023-12-16 21:35:11 INFO     saving model related files
2023-12-16 21:35:11 INFO     saving model
2023-12-16 21:35:13 INFO     saving tokenizer
2023-12-16 21:35:13 INFO     saving optimizer
2023-12-16 21:35:17 INFO     remove old optimizer files
2023-12-16 21:35:27 INFO     	 * (global step 1250: loss: 0.31687087565660477, lr: 5e-05
2023-12-16 21:36:14 INFO     	 * (global step 1300: loss: 0.33698845282197, lr: 5e-05
2023-12-16 21:37:01 INFO     	 * (global step 1350: loss: 0.37712788581848145, lr: 5e-05
2023-12-16 21:37:48 INFO     	 * (global step 1400: loss: 0.3694749027490616, lr: 5e-05
2023-12-16 21:38:34 INFO     	 * (global step 1450: loss: 0.3639034107327461, lr: 5e-05
2023-12-16 21:39:21 INFO     	 * (global step 1500: loss: 0.22078751400113106, lr: 5e-05
2023-12-16 21:40:08 INFO     	 * (global step 1550: loss: 0.41675903275609016, lr: 5e-05
2023-12-16 21:40:09 INFO     [epoch 4/15] average loss: 0.332, lr: 5e-05
2023-12-16 21:40:09 INFO     saving model related files
2023-12-16 21:40:09 INFO     saving model
2023-12-16 21:40:11 INFO     saving tokenizer
2023-12-16 21:40:11 INFO     saving optimizer
2023-12-16 21:40:14 INFO     remove old optimizer files
2023-12-16 21:41:01 INFO     	 * (global step 1600: loss: 0.374136108905077, lr: 5e-05
2023-12-16 21:41:48 INFO     	 * (global step 1650: loss: 0.3171672597527504, lr: 5e-05
2023-12-16 21:42:34 INFO     	 * (global step 1700: loss: 0.401181623339653, lr: 5e-05
2023-12-16 21:43:21 INFO     	 * (global step 1750: loss: 0.2950042560696602, lr: 5e-05
2023-12-16 21:44:08 INFO     	 * (global step 1800: loss: 0.3971583656966686, lr: 5e-05
2023-12-16 21:44:55 INFO     	 * (global step 1850: loss: 0.4622078612446785, lr: 5e-05
2023-12-16 21:45:05 INFO     [epoch 5/15] average loss: 0.319, lr: 5e-05
2023-12-16 21:45:05 INFO     saving model related files
2023-12-16 21:45:05 INFO     saving model
2023-12-16 21:45:07 INFO     saving tokenizer
2023-12-16 21:45:07 INFO     saving optimizer
2023-12-16 21:45:11 INFO     remove old optimizer files
2023-12-16 21:45:49 INFO     	 * (global step 1900: loss: 0.3034888803958893, lr: 5e-05
2023-12-16 21:46:35 INFO     	 * (global step 1950: loss: 0.2501729652285576, lr: 5e-05
2023-12-16 21:47:22 INFO     	 * (global step 2000: loss: 0.42458029091358185, lr: 5e-05
2023-12-16 21:48:09 INFO     	 * (global step 2050: loss: 0.23900076001882553, lr: 5e-05
2023-12-16 21:48:56 INFO     	 * (global step 2100: loss: 0.33945439383387566, lr: 5e-05
2023-12-16 21:49:43 INFO     	 * (global step 2150: loss: 0.31404007971286774, lr: 5e-05
2023-12-16 21:50:02 INFO     [epoch 6/15] average loss: 0.308, lr: 5e-05
2023-12-16 21:50:02 INFO     saving model related files
2023-12-16 21:50:02 INFO     saving model
2023-12-16 21:50:04 INFO     saving tokenizer
2023-12-16 21:50:04 INFO     saving optimizer
2023-12-16 21:50:07 INFO     remove old optimizer files
2023-12-16 21:50:35 INFO     	 * (global step 2200: loss: 0.22906381264328957, lr: 5e-05
2023-12-16 21:51:22 INFO     	 * (global step 2250: loss: 0.25822756066918373, lr: 5e-05
2023-12-16 21:52:09 INFO     	 * (global step 2300: loss: 0.2550586014986038, lr: 5e-05
2023-12-16 21:52:56 INFO     	 * (global step 2350: loss: 0.27812501043081284, lr: 5e-05
2023-12-16 21:53:43 INFO     	 * (global step 2400: loss: 0.23898673057556152, lr: 5e-05
2023-12-16 21:54:30 INFO     	 * (global step 2450: loss: 0.30016370117664337, lr: 5e-05
2023-12-16 21:54:59 INFO     [epoch 7/15] average loss: 0.297, lr: 5e-05
2023-12-16 21:54:59 INFO     saving model related files
2023-12-16 21:54:59 INFO     saving model
2023-12-16 21:55:00 INFO     saving tokenizer
2023-12-16 21:55:00 INFO     saving optimizer
2023-12-16 21:55:03 INFO     remove old optimizer files
2023-12-16 21:55:22 INFO     	 * (global step 2500: loss: 0.2760434113442898, lr: 5e-05
2023-12-16 21:56:09 INFO     	 * (global step 2550: loss: 0.257743027061224, lr: 5e-05
2023-12-16 21:56:56 INFO     	 * (global step 2600: loss: 0.2960270270705223, lr: 5e-05
2023-12-16 21:57:43 INFO     	 * (global step 2650: loss: 0.3219100162386894, lr: 5e-05
2023-12-16 21:58:30 INFO     	 * (global step 2700: loss: 0.24992365017533302, lr: 5e-05
2023-12-16 21:59:17 INFO     	 * (global step 2750: loss: 0.33389703556895256, lr: 5e-05
2023-12-16 21:59:56 INFO     [epoch 8/15] average loss: 0.287, lr: 5e-05
2023-12-16 21:59:56 INFO     saving model related files
2023-12-16 21:59:56 INFO     saving model
2023-12-16 21:59:57 INFO     saving tokenizer
2023-12-16 21:59:57 INFO     saving optimizer
2023-12-16 22:00:01 INFO     remove old optimizer files
2023-12-16 22:00:10 INFO     	 * (global step 2800: loss: 0.20554393529891968, lr: 5e-05
2023-12-16 22:00:57 INFO     	 * (global step 2850: loss: 0.2393493466079235, lr: 5e-05
2023-12-16 22:01:44 INFO     	 * (global step 2900: loss: 0.22163806855678558, lr: 5e-05
2023-12-16 22:02:31 INFO     	 * (global step 2950: loss: 0.2959030978381634, lr: 5e-05
2023-12-16 22:03:18 INFO     	 * (global step 3000: loss: 0.23886024206876755, lr: 5e-05
2023-12-16 22:04:05 INFO     	 * (global step 3050: loss: 0.2590324319899082, lr: 5e-05
2023-12-16 22:04:52 INFO     	 * (global step 3100: loss: 0.2843368202447891, lr: 5e-05
2023-12-16 22:04:53 INFO     [epoch 9/15] average loss: 0.278, lr: 5e-05
2023-12-16 22:04:53 INFO     saving model related files
2023-12-16 22:04:53 INFO     saving model
2023-12-16 22:04:54 INFO     saving tokenizer
2023-12-16 22:04:54 INFO     saving optimizer
2023-12-16 22:04:58 INFO     remove old optimizer files
2023-12-16 22:04:58 INFO     complete training: model ckpt was saved at base_trained_ckpt/model_mntyya
2023-12-16 22:04:58 INFO     ## 1st RUN: Configuration 5/12 ##
2023-12-16 22:04:58 INFO     initialize model trainer
2023-12-16 22:04:58 INFO     initialize checkpoint at base_trained_ckpt/model_woixzh
2023-12-16 22:04:58 INFO     hyperparameters
2023-12-16 22:04:58 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-16 22:04:58 INFO     	 * dataset_name: default
2023-12-16 22:04:58 INFO     	 * input_types: ['paragraph']
2023-12-16 22:04:58 INFO     	 * output_types: ['questions_answers']
2023-12-16 22:04:58 INFO     	 * prefix_types: ['qag']
2023-12-16 22:04:58 INFO     	 * model: t5-base
2023-12-16 22:04:58 INFO     	 * max_length: 512
2023-12-16 22:04:58 INFO     	 * max_length_output: 512
2023-12-16 22:04:58 INFO     	 * epoch: 15
2023-12-16 22:04:58 INFO     	 * batch: 2
2023-12-16 22:04:58 INFO     	 * lr: 5e-05
2023-12-16 22:04:58 INFO     	 * fp16: False
2023-12-16 22:04:58 INFO     	 * random_seed: 1
2023-12-16 22:04:58 INFO     	 * gradient_accumulation_steps: 2
2023-12-16 22:04:58 INFO     	 * label_smoothing: 0.15
2023-12-16 22:04:58 INFO     initialize checkpoint with t5-base
2023-12-16 22:05:01 INFO     use spaCy answer extraction model: positionrank
2023-12-16 22:05:01 INFO     Model `t5-base`
2023-12-16 22:05:01 INFO     	 * Num of GPU in use: 1
2023-12-16 22:05:01 INFO     	 * Prefix: True
2023-12-16 22:05:01 INFO     	 * Language: en (ignore at the training phase)
2023-12-16 22:05:01 INFO     dataset preprocessing
2023-12-16 22:05:03 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-base.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-16 22:05:03 INFO     start model training
2023-12-16 22:05:27 INFO     	 * (global step 50: loss: 0.6871725022792816, lr: 5e-05
2023-12-16 22:05:51 INFO     	 * (global step 100: loss: 0.6359577775001526, lr: 5e-05
2023-12-16 22:06:16 INFO     	 * (global step 150: loss: 0.5696761012077332, lr: 5e-05
2023-12-16 22:06:40 INFO     	 * (global step 200: loss: 0.38241273164749146, lr: 5e-05
2023-12-16 22:07:04 INFO     	 * (global step 250: loss: 0.36501258611679077, lr: 5e-05
2023-12-16 22:07:28 INFO     	 * (global step 300: loss: 0.7421885132789612, lr: 5e-05
2023-12-16 22:07:53 INFO     	 * (global step 350: loss: 0.43636225163936615, lr: 5e-05
2023-12-16 22:08:17 INFO     	 * (global step 400: loss: 0.36679428815841675, lr: 5e-05
2023-12-16 22:08:42 INFO     	 * (global step 450: loss: 0.402570903301239, lr: 5e-05
2023-12-16 22:09:06 INFO     	 * (global step 500: loss: 0.3343223035335541, lr: 5e-05
2023-12-16 22:09:30 INFO     	 * (global step 550: loss: 0.8545628786087036, lr: 5e-05
2023-12-16 22:09:55 INFO     	 * (global step 600: loss: 0.6647511422634125, lr: 5e-05
2023-12-16 22:10:05 INFO     [epoch 0/15] average loss: 0.679, lr: 5e-05
2023-12-16 22:10:05 INFO     saving model related files
2023-12-16 22:10:05 INFO     saving model
2023-12-16 22:10:07 INFO     saving tokenizer
2023-12-16 22:10:07 INFO     saving optimizer
2023-12-16 22:10:10 INFO     remove old optimizer files
2023-12-16 22:10:24 INFO     	 * (global step 650: loss: 0.5664253979921341, lr: 5e-05
2023-12-16 22:10:49 INFO     	 * (global step 700: loss: 0.3925643563270569, lr: 5e-05
2023-12-16 22:11:13 INFO     	 * (global step 750: loss: 0.4156379699707031, lr: 5e-05
2023-12-16 22:11:37 INFO     	 * (global step 800: loss: 0.4695606976747513, lr: 5e-05
2023-12-16 22:12:02 INFO     	 * (global step 850: loss: 0.36572377383708954, lr: 5e-05
2023-12-16 22:12:26 INFO     	 * (global step 900: loss: 0.3737815171480179, lr: 5e-05
2023-12-16 22:12:50 INFO     	 * (global step 950: loss: 0.4725956320762634, lr: 5e-05
2023-12-16 22:13:15 INFO     	 * (global step 1000: loss: 0.3639528900384903, lr: 5e-05
2023-12-16 22:13:39 INFO     	 * (global step 1050: loss: 0.3961043208837509, lr: 5e-05
2023-12-16 22:14:04 INFO     	 * (global step 1100: loss: 0.355608269572258, lr: 5e-05
2023-12-16 22:14:28 INFO     	 * (global step 1150: loss: 0.42178934812545776, lr: 5e-05
2023-12-16 22:14:52 INFO     	 * (global step 1200: loss: 0.3574370741844177, lr: 5e-05
2023-12-16 22:15:13 INFO     [epoch 1/15] average loss: 0.376, lr: 5e-05
2023-12-16 22:15:13 INFO     saving model related files
2023-12-16 22:15:13 INFO     saving model
2023-12-16 22:15:15 INFO     saving tokenizer
2023-12-16 22:15:15 INFO     saving optimizer
2023-12-16 22:15:19 INFO     remove old optimizer files
2023-12-16 22:15:23 INFO     	 * (global step 1250: loss: 0.27383138984441757, lr: 5e-05
2023-12-16 22:15:47 INFO     	 * (global step 1300: loss: 0.2814303934574127, lr: 5e-05
2023-12-16 22:16:11 INFO     	 * (global step 1350: loss: 0.31694571673870087, lr: 5e-05
2023-12-16 22:16:36 INFO     	 * (global step 1400: loss: 0.46402277052402496, lr: 5e-05
2023-12-16 22:17:00 INFO     	 * (global step 1450: loss: 0.4172986000776291, lr: 5e-05
2023-12-16 22:17:24 INFO     	 * (global step 1500: loss: 0.2994331419467926, lr: 5e-05
2023-12-16 22:17:48 INFO     	 * (global step 1550: loss: 0.2876729741692543, lr: 5e-05
2023-12-16 22:18:13 INFO     	 * (global step 1600: loss: 0.5668006241321564, lr: 5e-05
2023-12-16 22:18:37 INFO     	 * (global step 1650: loss: 0.29819363355636597, lr: 5e-05
2023-12-16 22:19:01 INFO     	 * (global step 1700: loss: 0.36594657599925995, lr: 5e-05
2023-12-16 22:19:26 INFO     	 * (global step 1750: loss: 0.28322765976190567, lr: 5e-05
2023-12-16 22:19:50 INFO     	 * (global step 1800: loss: 0.23460818827152252, lr: 5e-05
2023-12-16 22:20:14 INFO     	 * (global step 1850: loss: 0.3416052907705307, lr: 5e-05
2023-12-16 22:20:21 INFO     [epoch 2/15] average loss: 0.343, lr: 5e-05
2023-12-16 22:20:21 INFO     saving model related files
2023-12-16 22:20:21 INFO     saving model
2023-12-16 22:20:22 INFO     saving tokenizer
2023-12-16 22:20:22 INFO     saving optimizer
2023-12-16 22:20:26 INFO     remove old optimizer files
2023-12-16 22:20:44 INFO     	 * (global step 1900: loss: 0.30269868671894073, lr: 5e-05
2023-12-16 22:21:08 INFO     	 * (global step 1950: loss: 0.3513566553592682, lr: 5e-05
2023-12-16 22:21:32 INFO     	 * (global step 2000: loss: 0.2608822137117386, lr: 5e-05
2023-12-16 22:21:56 INFO     	 * (global step 2050: loss: 0.31940432637929916, lr: 5e-05
2023-12-16 22:22:21 INFO     	 * (global step 2100: loss: 0.21822991967201233, lr: 5e-05
2023-12-16 22:22:45 INFO     	 * (global step 2150: loss: 0.3087584972381592, lr: 5e-05
2023-12-16 22:23:09 INFO     	 * (global step 2200: loss: 0.3200618177652359, lr: 5e-05
2023-12-16 22:23:34 INFO     	 * (global step 2250: loss: 0.2839549258351326, lr: 5e-05
2023-12-16 22:23:58 INFO     	 * (global step 2300: loss: 0.24757856875658035, lr: 5e-05
2023-12-16 22:24:22 INFO     	 * (global step 2350: loss: 0.3120066672563553, lr: 5e-05
2023-12-16 22:24:46 INFO     	 * (global step 2400: loss: 0.3446824997663498, lr: 5e-05
2023-12-16 22:25:11 INFO     	 * (global step 2450: loss: 0.3816196322441101, lr: 5e-05
2023-12-16 22:25:27 INFO     [epoch 3/15] average loss: 0.321, lr: 5e-05
2023-12-16 22:25:27 INFO     saving model related files
2023-12-16 22:25:27 INFO     saving model
2023-12-16 22:25:29 INFO     saving tokenizer
2023-12-16 22:25:29 INFO     saving optimizer
2023-12-16 22:25:32 INFO     remove old optimizer files
2023-12-16 22:25:40 INFO     	 * (global step 2500: loss: 0.29563532769680023, lr: 5e-05
2023-12-16 22:26:04 INFO     	 * (global step 2550: loss: 0.29836881160736084, lr: 5e-05
2023-12-16 22:26:29 INFO     	 * (global step 2600: loss: 0.35229456424713135, lr: 5e-05
2023-12-16 22:26:53 INFO     	 * (global step 2650: loss: 0.18095438182353973, lr: 5e-05
2023-12-16 22:27:17 INFO     	 * (global step 2700: loss: 0.2799449488520622, lr: 5e-05
2023-12-16 22:27:41 INFO     	 * (global step 2750: loss: 0.2861691266298294, lr: 5e-05
2023-12-16 22:28:06 INFO     	 * (global step 2800: loss: 0.25849488377571106, lr: 5e-05
2023-12-16 22:28:30 INFO     	 * (global step 2850: loss: 0.27021482586860657, lr: 5e-05
2023-12-16 22:28:54 INFO     	 * (global step 2900: loss: 0.15185177698731422, lr: 5e-05
2023-12-16 22:29:19 INFO     	 * (global step 2950: loss: 0.30539385974407196, lr: 5e-05
2023-12-16 22:29:43 INFO     	 * (global step 3000: loss: 0.3216260001063347, lr: 5e-05
2023-12-16 22:30:07 INFO     	 * (global step 3050: loss: 0.25526370108127594, lr: 5e-05
2023-12-16 22:30:31 INFO     	 * (global step 3100: loss: 0.2799835056066513, lr: 5e-05
2023-12-16 22:30:34 INFO     [epoch 4/15] average loss: 0.304, lr: 5e-05
2023-12-16 22:30:34 INFO     saving model related files
2023-12-16 22:30:34 INFO     saving model
2023-12-16 22:30:36 INFO     saving tokenizer
2023-12-16 22:30:36 INFO     saving optimizer
2023-12-16 22:30:39 INFO     remove old optimizer files
2023-12-16 22:31:01 INFO     	 * (global step 3150: loss: 0.21583398431539536, lr: 5e-05
2023-12-16 22:31:25 INFO     	 * (global step 3200: loss: 0.27922800183296204, lr: 5e-05
2023-12-16 22:31:49 INFO     	 * (global step 3250: loss: 0.3328527361154556, lr: 5e-05
2023-12-16 22:32:14 INFO     	 * (global step 3300: loss: 0.23203808814287186, lr: 5e-05
2023-12-16 22:32:38 INFO     	 * (global step 3350: loss: 0.36563561111688614, lr: 5e-05
2023-12-16 22:33:03 INFO     	 * (global step 3400: loss: 0.29116959124803543, lr: 5e-05
2023-12-16 22:33:27 INFO     	 * (global step 3450: loss: 0.2820958122611046, lr: 5e-05
2023-12-16 22:33:51 INFO     	 * (global step 3500: loss: 0.31919117271900177, lr: 5e-05
2023-12-16 22:34:16 INFO     	 * (global step 3550: loss: 0.3480808436870575, lr: 5e-05
2023-12-16 22:34:40 INFO     	 * (global step 3600: loss: 0.22407332062721252, lr: 5e-05
2023-12-16 22:35:05 INFO     	 * (global step 3650: loss: 0.4005482345819473, lr: 5e-05
2023-12-16 22:35:29 INFO     	 * (global step 3700: loss: 0.21225640177726746, lr: 5e-05
2023-12-16 22:35:42 INFO     [epoch 5/15] average loss: 0.289, lr: 5e-05
2023-12-16 22:35:42 INFO     saving model related files
2023-12-16 22:35:42 INFO     saving model
2023-12-16 22:35:44 INFO     saving tokenizer
2023-12-16 22:35:44 INFO     saving optimizer
2023-12-16 22:35:47 INFO     remove old optimizer files
2023-12-16 22:35:59 INFO     	 * (global step 3750: loss: 0.3348909765481949, lr: 5e-05
2023-12-16 22:36:23 INFO     	 * (global step 3800: loss: 0.33648480474948883, lr: 5e-05
2023-12-16 22:36:48 INFO     	 * (global step 3850: loss: 0.2935011237859726, lr: 5e-05
2023-12-16 22:37:12 INFO     	 * (global step 3900: loss: 0.3166855275630951, lr: 5e-05
2023-12-16 22:37:36 INFO     	 * (global step 3950: loss: 0.27027418464422226, lr: 5e-05
2023-12-16 22:38:01 INFO     	 * (global step 4000: loss: 0.23542191088199615, lr: 5e-05
2023-12-16 22:38:25 INFO     	 * (global step 4050: loss: 0.31338535249233246, lr: 5e-05
2023-12-16 22:38:49 INFO     	 * (global step 4100: loss: 0.3507882356643677, lr: 5e-05
2023-12-16 22:39:14 INFO     	 * (global step 4150: loss: 0.23711340874433517, lr: 5e-05
2023-12-16 22:39:38 INFO     	 * (global step 4200: loss: 0.20537006855010986, lr: 5e-05
2023-12-16 22:40:03 INFO     	 * (global step 4250: loss: 0.23025672882795334, lr: 5e-05
2023-12-16 22:40:27 INFO     	 * (global step 4300: loss: 0.21732643246650696, lr: 5e-05
2023-12-16 22:40:50 INFO     [epoch 6/15] average loss: 0.275, lr: 5e-05
2023-12-16 22:40:50 INFO     saving model related files
2023-12-16 22:40:50 INFO     saving model
2023-12-16 22:40:52 INFO     saving tokenizer
2023-12-16 22:40:52 INFO     saving optimizer
2023-12-16 22:40:56 INFO     remove old optimizer files
2023-12-16 22:40:57 INFO     	 * (global step 4350: loss: 0.23439347743988037, lr: 5e-05
2023-12-16 22:41:22 INFO     	 * (global step 4400: loss: 0.26678653061389923, lr: 5e-05
2023-12-16 22:41:46 INFO     	 * (global step 4450: loss: 0.22753268480300903, lr: 5e-05
2023-12-16 22:42:10 INFO     	 * (global step 4500: loss: 0.2546560764312744, lr: 5e-05
2023-12-16 22:42:35 INFO     	 * (global step 4550: loss: 0.2595013901591301, lr: 5e-05
2023-12-16 22:42:59 INFO     	 * (global step 4600: loss: 0.2905869334936142, lr: 5e-05
2023-12-16 22:43:23 INFO     	 * (global step 4650: loss: 0.22177256643772125, lr: 5e-05
2023-12-16 22:43:48 INFO     	 * (global step 4700: loss: 0.25070298463106155, lr: 5e-05
2023-12-16 22:44:12 INFO     	 * (global step 4750: loss: 0.20297236740589142, lr: 5e-05
2023-12-16 22:44:36 INFO     	 * (global step 4800: loss: 0.1456352174282074, lr: 5e-05
2023-12-16 22:45:01 INFO     	 * (global step 4850: loss: 0.4388207495212555, lr: 5e-05
2023-12-16 22:45:25 INFO     	 * (global step 4900: loss: 0.25487925857305527, lr: 5e-05
2023-12-16 22:45:49 INFO     	 * (global step 4950: loss: 0.18879249691963196, lr: 5e-05
2023-12-16 22:45:58 INFO     [epoch 7/15] average loss: 0.262, lr: 5e-05
2023-12-16 22:45:58 INFO     saving model related files
2023-12-16 22:45:58 INFO     saving model
2023-12-16 22:46:00 INFO     saving tokenizer
2023-12-16 22:46:00 INFO     saving optimizer
2023-12-16 22:46:03 INFO     remove old optimizer files
2023-12-16 22:46:19 INFO     	 * (global step 5000: loss: 0.3298434764146805, lr: 5e-05
2023-12-16 22:46:43 INFO     	 * (global step 5050: loss: 0.21750490367412567, lr: 5e-05
2023-12-16 22:47:08 INFO     	 * (global step 5100: loss: 0.1872078999876976, lr: 5e-05
2023-12-16 22:47:32 INFO     	 * (global step 5150: loss: 0.1758241206407547, lr: 5e-05
2023-12-16 22:47:57 INFO     	 * (global step 5200: loss: 0.22080432251095772, lr: 5e-05
2023-12-16 22:48:21 INFO     	 * (global step 5250: loss: 0.273072712123394, lr: 5e-05
2023-12-16 22:48:45 INFO     	 * (global step 5300: loss: 0.25290512293577194, lr: 5e-05
2023-12-16 22:49:10 INFO     	 * (global step 5350: loss: 0.17640450596809387, lr: 5e-05
2023-12-16 22:49:34 INFO     	 * (global step 5400: loss: 0.2978115975856781, lr: 5e-05
2023-12-16 22:49:59 INFO     	 * (global step 5450: loss: 0.21310526132583618, lr: 5e-05
2023-12-16 22:50:23 INFO     	 * (global step 5500: loss: 0.15999316424131393, lr: 5e-05
2023-12-16 22:50:47 INFO     	 * (global step 5550: loss: 0.2444285899400711, lr: 5e-05
2023-12-16 22:51:07 INFO     [epoch 8/15] average loss: 0.25, lr: 5e-05
2023-12-16 22:51:07 INFO     saving model related files
2023-12-16 22:51:07 INFO     saving model
2023-12-16 22:51:08 INFO     saving tokenizer
2023-12-16 22:51:08 INFO     saving optimizer
2023-12-16 22:51:12 INFO     remove old optimizer files
2023-12-16 22:51:17 INFO     	 * (global step 5600: loss: 0.3295106291770935, lr: 5e-05
2023-12-16 22:51:41 INFO     	 * (global step 5650: loss: 0.23847753554582596, lr: 5e-05
2023-12-16 22:52:06 INFO     	 * (global step 5700: loss: 0.25935717672109604, lr: 5e-05
2023-12-16 22:52:30 INFO     	 * (global step 5750: loss: 0.21469905972480774, lr: 5e-05
2023-12-16 22:52:54 INFO     	 * (global step 5800: loss: 0.23237312585115433, lr: 5e-05
2023-12-16 22:53:19 INFO     	 * (global step 5850: loss: 0.24038086831569672, lr: 5e-05
2023-12-16 22:53:43 INFO     	 * (global step 5900: loss: 0.2047717198729515, lr: 5e-05
2023-12-16 22:54:07 INFO     	 * (global step 5950: loss: 0.18114402145147324, lr: 5e-05
2023-12-16 22:54:32 INFO     	 * (global step 6000: loss: 0.35990869998931885, lr: 5e-05
2023-12-16 22:54:56 INFO     	 * (global step 6050: loss: 0.2718778997659683, lr: 5e-05
2023-12-16 22:55:20 INFO     	 * (global step 6100: loss: 0.23378346115350723, lr: 5e-05
2023-12-16 22:55:45 INFO     	 * (global step 6150: loss: 0.23601652681827545, lr: 5e-05
2023-12-16 22:56:09 INFO     	 * (global step 6200: loss: 0.21060477942228317, lr: 5e-05
2023-12-16 22:56:14 INFO     [epoch 9/15] average loss: 0.24, lr: 5e-05
2023-12-16 22:56:14 INFO     saving model related files
2023-12-16 22:56:14 INFO     saving model
2023-12-16 22:56:15 INFO     saving tokenizer
2023-12-16 22:56:16 INFO     saving optimizer
2023-12-16 22:56:19 INFO     remove old optimizer files
2023-12-16 22:56:19 INFO     complete training: model ckpt was saved at base_trained_ckpt/model_woixzh
2023-12-16 22:56:19 INFO     ## 1st RUN: Configuration 6/12 ##
2023-12-16 22:56:19 INFO     initialize model trainer
2023-12-16 22:56:19 INFO     initialize checkpoint at base_trained_ckpt/model_sdkaaa
2023-12-16 22:56:19 INFO     hyperparameters
2023-12-16 22:56:19 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-16 22:56:19 INFO     	 * dataset_name: default
2023-12-16 22:56:19 INFO     	 * input_types: ['paragraph']
2023-12-16 22:56:19 INFO     	 * output_types: ['questions_answers']
2023-12-16 22:56:19 INFO     	 * prefix_types: ['qag']
2023-12-16 22:56:19 INFO     	 * model: t5-base
2023-12-16 22:56:19 INFO     	 * max_length: 512
2023-12-16 22:56:19 INFO     	 * max_length_output: 512
2023-12-16 22:56:19 INFO     	 * epoch: 15
2023-12-16 22:56:19 INFO     	 * batch: 2
2023-12-16 22:56:19 INFO     	 * lr: 5e-05
2023-12-16 22:56:19 INFO     	 * fp16: False
2023-12-16 22:56:19 INFO     	 * random_seed: 1
2023-12-16 22:56:19 INFO     	 * gradient_accumulation_steps: 4
2023-12-16 22:56:19 INFO     	 * label_smoothing: 0.0
2023-12-16 22:56:19 INFO     initialize checkpoint with t5-base
2023-12-16 22:56:21 INFO     use spaCy answer extraction model: positionrank
2023-12-16 22:56:21 INFO     Model `t5-base`
2023-12-16 22:56:21 INFO     	 * Num of GPU in use: 1
2023-12-16 22:56:21 INFO     	 * Prefix: True
2023-12-16 22:56:21 INFO     	 * Language: en (ignore at the training phase)
2023-12-16 22:56:21 INFO     dataset preprocessing
2023-12-16 22:56:23 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-base.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-16 22:56:23 INFO     start model training
2023-12-16 22:57:09 INFO     	 * (global step 50: loss: 0.8280775547027588, lr: 5e-05
2023-12-16 22:57:56 INFO     	 * (global step 100: loss: 0.461581289768219, lr: 5e-05
2023-12-16 22:58:43 INFO     	 * (global step 150: loss: 0.7385011613368988, lr: 5e-05
2023-12-16 22:59:30 INFO     	 * (global step 200: loss: 0.5216936841607094, lr: 5e-05
2023-12-16 23:00:17 INFO     	 * (global step 250: loss: 0.4498784989118576, lr: 5e-05
2023-12-16 23:01:04 INFO     	 * (global step 300: loss: 0.5558523163199425, lr: 5e-05
2023-12-16 23:01:14 INFO     [epoch 0/15] average loss: 0.882, lr: 5e-05
2023-12-16 23:01:14 INFO     saving model related files
2023-12-16 23:01:14 INFO     saving model
2023-12-16 23:01:15 INFO     saving tokenizer
2023-12-16 23:01:15 INFO     saving optimizer
2023-12-16 23:01:18 INFO     remove old optimizer files
2023-12-16 23:01:56 INFO     	 * (global step 350: loss: 0.41594740748405457, lr: 5e-05
2023-12-16 23:02:43 INFO     	 * (global step 400: loss: 0.4485616385936737, lr: 5e-05
2023-12-16 23:03:29 INFO     	 * (global step 450: loss: 0.4535547196865082, lr: 5e-05
2023-12-16 23:04:16 INFO     	 * (global step 500: loss: 0.32988353446125984, lr: 5e-05
2023-12-16 23:05:03 INFO     	 * (global step 550: loss: 0.3607261888682842, lr: 5e-05
2023-12-16 23:05:50 INFO     	 * (global step 600: loss: 0.3568142130970955, lr: 5e-05
2023-12-16 23:06:09 INFO     [epoch 1/15] average loss: 0.411, lr: 5e-05
2023-12-16 23:06:09 INFO     saving model related files
2023-12-16 23:06:09 INFO     saving model
2023-12-16 23:06:11 INFO     saving tokenizer
2023-12-16 23:06:11 INFO     saving optimizer
2023-12-16 23:06:14 INFO     remove old optimizer files
2023-12-16 23:06:42 INFO     	 * (global step 650: loss: 0.3035845458507538, lr: 5e-05
2023-12-16 23:07:29 INFO     	 * (global step 700: loss: 0.3330783173441887, lr: 5e-05
2023-12-16 23:08:16 INFO     	 * (global step 750: loss: 0.3295895382761955, lr: 5e-05
2023-12-16 23:09:03 INFO     	 * (global step 800: loss: 0.3288930729031563, lr: 5e-05
2023-12-16 23:09:50 INFO     	 * (global step 850: loss: 0.3608703464269638, lr: 5e-05
2023-12-16 23:10:37 INFO     	 * (global step 900: loss: 0.4769106097519398, lr: 5e-05
2023-12-16 23:11:06 INFO     [epoch 2/15] average loss: 0.371, lr: 5e-05
2023-12-16 23:11:06 INFO     saving model related files
2023-12-16 23:11:06 INFO     saving model
2023-12-16 23:11:07 INFO     saving tokenizer
2023-12-16 23:11:08 INFO     saving optimizer
2023-12-16 23:11:11 INFO     remove old optimizer files
2023-12-16 23:11:30 INFO     	 * (global step 950: loss: 0.35574790090322495, lr: 5e-05
2023-12-16 23:12:17 INFO     	 * (global step 1000: loss: 0.3858143761754036, lr: 5e-05
2023-12-16 23:13:04 INFO     	 * (global step 1050: loss: 0.38129355013370514, lr: 5e-05
2023-12-16 23:13:51 INFO     	 * (global step 1100: loss: 0.38084954395890236, lr: 5e-05
2023-12-16 23:14:38 INFO     	 * (global step 1150: loss: 0.3940477669239044, lr: 5e-05
2023-12-16 23:15:25 INFO     	 * (global step 1200: loss: 0.2831944599747658, lr: 5e-05
2023-12-16 23:16:03 INFO     [epoch 3/15] average loss: 0.349, lr: 5e-05
2023-12-16 23:16:03 INFO     saving model related files
2023-12-16 23:16:03 INFO     saving model
2023-12-16 23:16:04 INFO     saving tokenizer
2023-12-16 23:16:05 INFO     saving optimizer
2023-12-16 23:16:08 INFO     remove old optimizer files
2023-12-16 23:16:17 INFO     	 * (global step 1250: loss: 0.31687087565660477, lr: 5e-05
2023-12-16 23:17:04 INFO     	 * (global step 1300: loss: 0.33698845282197, lr: 5e-05
2023-12-16 23:17:52 INFO     	 * (global step 1350: loss: 0.37712788581848145, lr: 5e-05
2023-12-16 23:18:39 INFO     	 * (global step 1400: loss: 0.3694749027490616, lr: 5e-05
2023-12-16 23:19:26 INFO     	 * (global step 1450: loss: 0.3639034107327461, lr: 5e-05
2023-12-16 23:20:13 INFO     	 * (global step 1500: loss: 0.22078751400113106, lr: 5e-05
2023-12-16 23:21:00 INFO     	 * (global step 1550: loss: 0.41675903275609016, lr: 5e-05
2023-12-16 23:21:00 INFO     [epoch 4/15] average loss: 0.332, lr: 5e-05
2023-12-16 23:21:00 INFO     saving model related files
2023-12-16 23:21:00 INFO     saving model
2023-12-16 23:21:02 INFO     saving tokenizer
2023-12-16 23:21:02 INFO     saving optimizer
2023-12-16 23:21:06 INFO     remove old optimizer files
2023-12-16 23:21:53 INFO     	 * (global step 1600: loss: 0.374136108905077, lr: 5e-05
2023-12-16 23:22:40 INFO     	 * (global step 1650: loss: 0.3171672597527504, lr: 5e-05
2023-12-16 23:23:27 INFO     	 * (global step 1700: loss: 0.401181623339653, lr: 5e-05
2023-12-16 23:24:14 INFO     	 * (global step 1750: loss: 0.2950042560696602, lr: 5e-05
2023-12-16 23:25:01 INFO     	 * (global step 1800: loss: 0.3971583656966686, lr: 5e-05
2023-12-16 23:25:48 INFO     	 * (global step 1850: loss: 0.4622078612446785, lr: 5e-05
2023-12-16 23:25:58 INFO     [epoch 5/15] average loss: 0.319, lr: 5e-05
2023-12-16 23:25:58 INFO     saving model related files
2023-12-16 23:25:58 INFO     saving model
2023-12-16 23:26:00 INFO     saving tokenizer
2023-12-16 23:26:00 INFO     saving optimizer
2023-12-16 23:26:04 INFO     remove old optimizer files
2023-12-16 23:26:41 INFO     	 * (global step 1900: loss: 0.3034888803958893, lr: 5e-05
2023-12-16 23:27:28 INFO     	 * (global step 1950: loss: 0.2501729652285576, lr: 5e-05
2023-12-16 23:28:15 INFO     	 * (global step 2000: loss: 0.42458029091358185, lr: 5e-05
2023-12-16 23:29:02 INFO     	 * (global step 2050: loss: 0.23900076001882553, lr: 5e-05
2023-12-16 23:29:49 INFO     	 * (global step 2100: loss: 0.33945439383387566, lr: 5e-05
2023-12-16 23:30:36 INFO     	 * (global step 2150: loss: 0.31404007971286774, lr: 5e-05
2023-12-16 23:30:56 INFO     [epoch 6/15] average loss: 0.308, lr: 5e-05
2023-12-16 23:30:56 INFO     saving model related files
2023-12-16 23:30:56 INFO     saving model
2023-12-16 23:30:57 INFO     saving tokenizer
2023-12-16 23:30:57 INFO     saving optimizer
2023-12-16 23:31:00 INFO     remove old optimizer files
2023-12-16 23:31:29 INFO     	 * (global step 2200: loss: 0.22906381264328957, lr: 5e-05
2023-12-16 23:32:15 INFO     	 * (global step 2250: loss: 0.25822756066918373, lr: 5e-05
2023-12-16 23:33:02 INFO     	 * (global step 2300: loss: 0.2550586014986038, lr: 5e-05
2023-12-16 23:33:49 INFO     	 * (global step 2350: loss: 0.27812501043081284, lr: 5e-05
2023-12-16 23:34:36 INFO     	 * (global step 2400: loss: 0.23898673057556152, lr: 5e-05
2023-12-16 23:35:23 INFO     	 * (global step 2450: loss: 0.30016370117664337, lr: 5e-05
2023-12-16 23:35:52 INFO     [epoch 7/15] average loss: 0.297, lr: 5e-05
2023-12-16 23:35:52 INFO     saving model related files
2023-12-16 23:35:52 INFO     saving model
2023-12-16 23:35:53 INFO     saving tokenizer
2023-12-16 23:35:53 INFO     saving optimizer
2023-12-16 23:35:56 INFO     remove old optimizer files
2023-12-16 23:36:15 INFO     	 * (global step 2500: loss: 0.2760434113442898, lr: 5e-05
2023-12-16 23:37:02 INFO     	 * (global step 2550: loss: 0.257743027061224, lr: 5e-05
2023-12-16 23:37:49 INFO     	 * (global step 2600: loss: 0.2960270270705223, lr: 5e-05
2023-12-16 23:38:36 INFO     	 * (global step 2650: loss: 0.3219100162386894, lr: 5e-05
2023-12-16 23:39:23 INFO     	 * (global step 2700: loss: 0.24992365017533302, lr: 5e-05
2023-12-16 23:40:10 INFO     	 * (global step 2750: loss: 0.33389703556895256, lr: 5e-05
2023-12-16 23:40:48 INFO     [epoch 8/15] average loss: 0.287, lr: 5e-05
2023-12-16 23:40:48 INFO     saving model related files
2023-12-16 23:40:48 INFO     saving model
2023-12-16 23:40:49 INFO     saving tokenizer
2023-12-16 23:40:49 INFO     saving optimizer
2023-12-16 23:40:52 INFO     remove old optimizer files
2023-12-16 23:41:02 INFO     	 * (global step 2800: loss: 0.20554393529891968, lr: 5e-05
2023-12-16 23:41:48 INFO     	 * (global step 2850: loss: 0.2393493466079235, lr: 5e-05
2023-12-16 23:42:35 INFO     	 * (global step 2900: loss: 0.22163806855678558, lr: 5e-05
2023-12-16 23:43:22 INFO     	 * (global step 2950: loss: 0.2959030978381634, lr: 5e-05
2023-12-16 23:44:09 INFO     	 * (global step 3000: loss: 0.23886024206876755, lr: 5e-05
2023-12-16 23:44:56 INFO     	 * (global step 3050: loss: 0.2590324319899082, lr: 5e-05
2023-12-16 23:45:43 INFO     	 * (global step 3100: loss: 0.2843368202447891, lr: 5e-05
2023-12-16 23:45:43 INFO     [epoch 9/15] average loss: 0.278, lr: 5e-05
2023-12-16 23:45:43 INFO     saving model related files
2023-12-16 23:45:43 INFO     saving model
2023-12-16 23:45:45 INFO     saving tokenizer
2023-12-16 23:45:45 INFO     saving optimizer
2023-12-16 23:45:48 INFO     remove old optimizer files
2023-12-16 23:45:48 INFO     complete training: model ckpt was saved at base_trained_ckpt/model_sdkaaa
2023-12-16 23:45:48 INFO     ## 1st RUN: Configuration 7/12 ##
2023-12-16 23:45:48 INFO     initialize model trainer
2023-12-16 23:45:48 INFO     initialize checkpoint at base_trained_ckpt/model_uramvg
2023-12-16 23:45:48 INFO     hyperparameters
2023-12-16 23:45:48 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-16 23:45:48 INFO     	 * dataset_name: default
2023-12-16 23:45:48 INFO     	 * input_types: ['paragraph']
2023-12-16 23:45:48 INFO     	 * output_types: ['questions_answers']
2023-12-16 23:45:48 INFO     	 * prefix_types: ['qag']
2023-12-16 23:45:48 INFO     	 * model: t5-base
2023-12-16 23:45:48 INFO     	 * max_length: 512
2023-12-16 23:45:48 INFO     	 * max_length_output: 512
2023-12-16 23:45:48 INFO     	 * epoch: 15
2023-12-16 23:45:48 INFO     	 * batch: 2
2023-12-16 23:45:48 INFO     	 * lr: 5e-05
2023-12-16 23:45:48 INFO     	 * fp16: False
2023-12-16 23:45:48 INFO     	 * random_seed: 1
2023-12-16 23:45:48 INFO     	 * gradient_accumulation_steps: 2
2023-12-16 23:45:48 INFO     	 * label_smoothing: 0.0
2023-12-16 23:45:48 INFO     initialize checkpoint with t5-base
2023-12-16 23:45:50 INFO     use spaCy answer extraction model: positionrank
2023-12-16 23:45:51 INFO     Model `t5-base`
2023-12-16 23:45:51 INFO     	 * Num of GPU in use: 1
2023-12-16 23:45:51 INFO     	 * Prefix: True
2023-12-16 23:45:51 INFO     	 * Language: en (ignore at the training phase)
2023-12-16 23:45:51 INFO     dataset preprocessing
2023-12-16 23:45:52 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-base.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-16 23:45:52 INFO     start model training
2023-12-16 23:46:16 INFO     	 * (global step 50: loss: 0.6871725022792816, lr: 5e-05
2023-12-16 23:46:41 INFO     	 * (global step 100: loss: 0.6359577775001526, lr: 5e-05
2023-12-16 23:47:05 INFO     	 * (global step 150: loss: 0.5696761012077332, lr: 5e-05
2023-12-16 23:47:29 INFO     	 * (global step 200: loss: 0.38241273164749146, lr: 5e-05
2023-12-16 23:47:53 INFO     	 * (global step 250: loss: 0.36501258611679077, lr: 5e-05
2023-12-16 23:48:18 INFO     	 * (global step 300: loss: 0.7421885132789612, lr: 5e-05
2023-12-16 23:48:42 INFO     	 * (global step 350: loss: 0.43636225163936615, lr: 5e-05
2023-12-16 23:49:06 INFO     	 * (global step 400: loss: 0.36679428815841675, lr: 5e-05
2023-12-16 23:49:31 INFO     	 * (global step 450: loss: 0.402570903301239, lr: 5e-05
2023-12-16 23:49:55 INFO     	 * (global step 500: loss: 0.3343223035335541, lr: 5e-05
2023-12-16 23:50:19 INFO     	 * (global step 550: loss: 0.8545628786087036, lr: 5e-05
2023-12-16 23:50:44 INFO     	 * (global step 600: loss: 0.6647511422634125, lr: 5e-05
2023-12-16 23:50:54 INFO     [epoch 0/15] average loss: 0.679, lr: 5e-05
2023-12-16 23:50:54 INFO     saving model related files
2023-12-16 23:50:54 INFO     saving model
2023-12-16 23:50:56 INFO     saving tokenizer
2023-12-16 23:50:56 INFO     saving optimizer
2023-12-16 23:51:00 INFO     remove old optimizer files
2023-12-16 23:51:14 INFO     	 * (global step 650: loss: 0.5664253979921341, lr: 5e-05
2023-12-16 23:51:38 INFO     	 * (global step 700: loss: 0.3925643563270569, lr: 5e-05
2023-12-16 23:52:02 INFO     	 * (global step 750: loss: 0.4156379699707031, lr: 5e-05
2023-12-16 23:52:27 INFO     	 * (global step 800: loss: 0.4695606976747513, lr: 5e-05
2023-12-16 23:52:51 INFO     	 * (global step 850: loss: 0.36572377383708954, lr: 5e-05
2023-12-16 23:53:16 INFO     	 * (global step 900: loss: 0.3737815171480179, lr: 5e-05
2023-12-16 23:53:40 INFO     	 * (global step 950: loss: 0.4725956320762634, lr: 5e-05
2023-12-16 23:54:04 INFO     	 * (global step 1000: loss: 0.3639528900384903, lr: 5e-05
2023-12-16 23:54:29 INFO     	 * (global step 1050: loss: 0.3961043208837509, lr: 5e-05
2023-12-16 23:54:53 INFO     	 * (global step 1100: loss: 0.355608269572258, lr: 5e-05
2023-12-16 23:55:18 INFO     	 * (global step 1150: loss: 0.42178934812545776, lr: 5e-05
2023-12-16 23:55:42 INFO     	 * (global step 1200: loss: 0.3574370741844177, lr: 5e-05
2023-12-16 23:56:03 INFO     [epoch 1/15] average loss: 0.376, lr: 5e-05
2023-12-16 23:56:03 INFO     saving model related files
2023-12-16 23:56:03 INFO     saving model
2023-12-16 23:56:04 INFO     saving tokenizer
2023-12-16 23:56:04 INFO     saving optimizer
2023-12-16 23:56:08 INFO     remove old optimizer files
2023-12-16 23:56:12 INFO     	 * (global step 1250: loss: 0.27383138984441757, lr: 5e-05
2023-12-16 23:56:36 INFO     	 * (global step 1300: loss: 0.2814303934574127, lr: 5e-05
2023-12-16 23:57:00 INFO     	 * (global step 1350: loss: 0.31694571673870087, lr: 5e-05
2023-12-16 23:57:25 INFO     	 * (global step 1400: loss: 0.46402277052402496, lr: 5e-05
2023-12-16 23:57:49 INFO     	 * (global step 1450: loss: 0.4172986000776291, lr: 5e-05
2023-12-16 23:58:14 INFO     	 * (global step 1500: loss: 0.2994331419467926, lr: 5e-05
2023-12-16 23:58:38 INFO     	 * (global step 1550: loss: 0.2876729741692543, lr: 5e-05
2023-12-16 23:59:02 INFO     	 * (global step 1600: loss: 0.5668006241321564, lr: 5e-05
2023-12-16 23:59:27 INFO     	 * (global step 1650: loss: 0.29819363355636597, lr: 5e-05
2023-12-16 23:59:51 INFO     	 * (global step 1700: loss: 0.36594657599925995, lr: 5e-05
2023-12-17 00:00:15 INFO     	 * (global step 1750: loss: 0.28322765976190567, lr: 5e-05
2023-12-17 00:00:40 INFO     	 * (global step 1800: loss: 0.23460818827152252, lr: 5e-05
2023-12-17 00:01:04 INFO     	 * (global step 1850: loss: 0.3416052907705307, lr: 5e-05
2023-12-17 00:01:11 INFO     [epoch 2/15] average loss: 0.343, lr: 5e-05
2023-12-17 00:01:11 INFO     saving model related files
2023-12-17 00:01:11 INFO     saving model
2023-12-17 00:01:12 INFO     saving tokenizer
2023-12-17 00:01:13 INFO     saving optimizer
2023-12-17 00:01:16 INFO     remove old optimizer files
2023-12-17 00:01:34 INFO     	 * (global step 1900: loss: 0.30269868671894073, lr: 5e-05
2023-12-17 00:01:58 INFO     	 * (global step 1950: loss: 0.3513566553592682, lr: 5e-05
2023-12-17 00:02:23 INFO     	 * (global step 2000: loss: 0.2608822137117386, lr: 5e-05
2023-12-17 00:02:47 INFO     	 * (global step 2050: loss: 0.31940432637929916, lr: 5e-05
2023-12-17 00:03:11 INFO     	 * (global step 2100: loss: 0.21822991967201233, lr: 5e-05
2023-12-17 00:03:36 INFO     	 * (global step 2150: loss: 0.3087584972381592, lr: 5e-05
2023-12-17 00:04:00 INFO     	 * (global step 2200: loss: 0.3200618177652359, lr: 5e-05
2023-12-17 00:04:24 INFO     	 * (global step 2250: loss: 0.2839549258351326, lr: 5e-05
2023-12-17 00:04:49 INFO     	 * (global step 2300: loss: 0.24757856875658035, lr: 5e-05
2023-12-17 00:05:13 INFO     	 * (global step 2350: loss: 0.3120066672563553, lr: 5e-05
2023-12-17 00:05:37 INFO     	 * (global step 2400: loss: 0.3446824997663498, lr: 5e-05
2023-12-17 00:06:02 INFO     	 * (global step 2450: loss: 0.3816196322441101, lr: 5e-05
2023-12-17 00:06:18 INFO     [epoch 3/15] average loss: 0.321, lr: 5e-05
2023-12-17 00:06:18 INFO     saving model related files
2023-12-17 00:06:18 INFO     saving model
2023-12-17 00:06:20 INFO     saving tokenizer
2023-12-17 00:06:20 INFO     saving optimizer
2023-12-17 00:06:23 INFO     remove old optimizer files
2023-12-17 00:06:31 INFO     	 * (global step 2500: loss: 0.29563532769680023, lr: 5e-05
2023-12-17 00:06:55 INFO     	 * (global step 2550: loss: 0.29836881160736084, lr: 5e-05
2023-12-17 00:07:19 INFO     	 * (global step 2600: loss: 0.35229456424713135, lr: 5e-05
2023-12-17 00:07:44 INFO     	 * (global step 2650: loss: 0.18095438182353973, lr: 5e-05
2023-12-17 00:08:08 INFO     	 * (global step 2700: loss: 0.2799449488520622, lr: 5e-05
2023-12-17 00:08:32 INFO     	 * (global step 2750: loss: 0.2861691266298294, lr: 5e-05
2023-12-17 00:08:57 INFO     	 * (global step 2800: loss: 0.25849488377571106, lr: 5e-05
2023-12-17 00:09:21 INFO     	 * (global step 2850: loss: 0.27021482586860657, lr: 5e-05
2023-12-17 00:09:45 INFO     	 * (global step 2900: loss: 0.15185177698731422, lr: 5e-05
2023-12-17 00:10:09 INFO     	 * (global step 2950: loss: 0.30539385974407196, lr: 5e-05
2023-12-17 00:10:34 INFO     	 * (global step 3000: loss: 0.3216260001063347, lr: 5e-05
2023-12-17 00:10:58 INFO     	 * (global step 3050: loss: 0.25526370108127594, lr: 5e-05
2023-12-17 00:11:22 INFO     	 * (global step 3100: loss: 0.2799835056066513, lr: 5e-05
2023-12-17 00:11:25 INFO     [epoch 4/15] average loss: 0.304, lr: 5e-05
2023-12-17 00:11:25 INFO     saving model related files
2023-12-17 00:11:25 INFO     saving model
2023-12-17 00:11:26 INFO     saving tokenizer
2023-12-17 00:11:27 INFO     saving optimizer
2023-12-17 00:11:30 INFO     remove old optimizer files
2023-12-17 00:11:52 INFO     	 * (global step 3150: loss: 0.21583398431539536, lr: 5e-05
2023-12-17 00:12:16 INFO     	 * (global step 3200: loss: 0.27922800183296204, lr: 5e-05
2023-12-17 00:12:40 INFO     	 * (global step 3250: loss: 0.3328527361154556, lr: 5e-05
2023-12-17 00:13:04 INFO     	 * (global step 3300: loss: 0.23203808814287186, lr: 5e-05
2023-12-17 00:13:29 INFO     	 * (global step 3350: loss: 0.36563561111688614, lr: 5e-05
2023-12-17 00:13:53 INFO     	 * (global step 3400: loss: 0.29116959124803543, lr: 5e-05
2023-12-17 00:14:17 INFO     	 * (global step 3450: loss: 0.2820958122611046, lr: 5e-05
2023-12-17 00:14:42 INFO     	 * (global step 3500: loss: 0.31919117271900177, lr: 5e-05
2023-12-17 00:15:06 INFO     	 * (global step 3550: loss: 0.3480808436870575, lr: 5e-05
2023-12-17 00:15:30 INFO     	 * (global step 3600: loss: 0.22407332062721252, lr: 5e-05
2023-12-17 00:15:54 INFO     	 * (global step 3650: loss: 0.4005482345819473, lr: 5e-05
2023-12-17 00:16:19 INFO     	 * (global step 3700: loss: 0.21225640177726746, lr: 5e-05
2023-12-17 00:16:32 INFO     [epoch 5/15] average loss: 0.289, lr: 5e-05
2023-12-17 00:16:32 INFO     saving model related files
2023-12-17 00:16:32 INFO     saving model
2023-12-17 00:16:33 INFO     saving tokenizer
2023-12-17 00:16:33 INFO     saving optimizer
2023-12-17 00:16:37 INFO     remove old optimizer files
2023-12-17 00:16:49 INFO     	 * (global step 3750: loss: 0.3348909765481949, lr: 5e-05
2023-12-17 00:17:13 INFO     	 * (global step 3800: loss: 0.33648480474948883, lr: 5e-05
2023-12-17 00:17:38 INFO     	 * (global step 3850: loss: 0.2935011237859726, lr: 5e-05
2023-12-17 00:18:02 INFO     	 * (global step 3900: loss: 0.3166855275630951, lr: 5e-05
2023-12-17 00:18:26 INFO     	 * (global step 3950: loss: 0.27027418464422226, lr: 5e-05
2023-12-17 00:18:50 INFO     	 * (global step 4000: loss: 0.23542191088199615, lr: 5e-05
2023-12-17 00:19:15 INFO     	 * (global step 4050: loss: 0.31338535249233246, lr: 5e-05
2023-12-17 00:19:39 INFO     	 * (global step 4100: loss: 0.3507882356643677, lr: 5e-05
2023-12-17 00:20:03 INFO     	 * (global step 4150: loss: 0.23711340874433517, lr: 5e-05
2023-12-17 00:20:28 INFO     	 * (global step 4200: loss: 0.20537006855010986, lr: 5e-05
2023-12-17 00:20:52 INFO     	 * (global step 4250: loss: 0.23025672882795334, lr: 5e-05
2023-12-17 00:21:16 INFO     	 * (global step 4300: loss: 0.21732643246650696, lr: 5e-05
2023-12-17 00:21:39 INFO     [epoch 6/15] average loss: 0.275, lr: 5e-05
2023-12-17 00:21:39 INFO     saving model related files
2023-12-17 00:21:39 INFO     saving model
2023-12-17 00:21:41 INFO     saving tokenizer
2023-12-17 00:21:41 INFO     saving optimizer
2023-12-17 00:21:44 INFO     remove old optimizer files
2023-12-17 00:21:45 INFO     	 * (global step 4350: loss: 0.23439347743988037, lr: 5e-05
2023-12-17 00:22:10 INFO     	 * (global step 4400: loss: 0.26678653061389923, lr: 5e-05
2023-12-17 00:22:34 INFO     	 * (global step 4450: loss: 0.22753268480300903, lr: 5e-05
2023-12-17 00:22:58 INFO     	 * (global step 4500: loss: 0.2546560764312744, lr: 5e-05
2023-12-17 00:23:23 INFO     	 * (global step 4550: loss: 0.2595013901591301, lr: 5e-05
2023-12-17 00:23:47 INFO     	 * (global step 4600: loss: 0.2905869334936142, lr: 5e-05
2023-12-17 00:24:11 INFO     	 * (global step 4650: loss: 0.22177256643772125, lr: 5e-05
2023-12-17 00:24:36 INFO     	 * (global step 4700: loss: 0.25070298463106155, lr: 5e-05
2023-12-17 00:25:00 INFO     	 * (global step 4750: loss: 0.20297236740589142, lr: 5e-05
2023-12-17 00:25:24 INFO     	 * (global step 4800: loss: 0.1456352174282074, lr: 5e-05
2023-12-17 00:25:49 INFO     	 * (global step 4850: loss: 0.4388207495212555, lr: 5e-05
2023-12-17 00:26:13 INFO     	 * (global step 4900: loss: 0.25487925857305527, lr: 5e-05
2023-12-17 00:26:37 INFO     	 * (global step 4950: loss: 0.18879249691963196, lr: 5e-05
2023-12-17 00:26:46 INFO     [epoch 7/15] average loss: 0.262, lr: 5e-05
2023-12-17 00:26:46 INFO     saving model related files
2023-12-17 00:26:46 INFO     saving model
2023-12-17 00:26:48 INFO     saving tokenizer
2023-12-17 00:26:48 INFO     saving optimizer
2023-12-17 00:26:52 INFO     remove old optimizer files
2023-12-17 00:27:08 INFO     	 * (global step 5000: loss: 0.3298434764146805, lr: 5e-05
2023-12-17 00:27:32 INFO     	 * (global step 5050: loss: 0.21750490367412567, lr: 5e-05
2023-12-17 00:27:56 INFO     	 * (global step 5100: loss: 0.1872078999876976, lr: 5e-05
2023-12-17 00:28:21 INFO     	 * (global step 5150: loss: 0.1758241206407547, lr: 5e-05
2023-12-17 00:28:45 INFO     	 * (global step 5200: loss: 0.22080432251095772, lr: 5e-05
2023-12-17 00:29:09 INFO     	 * (global step 5250: loss: 0.273072712123394, lr: 5e-05
2023-12-17 00:29:34 INFO     	 * (global step 5300: loss: 0.25290512293577194, lr: 5e-05
2023-12-17 00:29:58 INFO     	 * (global step 5350: loss: 0.17640450596809387, lr: 5e-05
2023-12-17 00:30:22 INFO     	 * (global step 5400: loss: 0.2978115975856781, lr: 5e-05
2023-12-17 00:30:47 INFO     	 * (global step 5450: loss: 0.21310526132583618, lr: 5e-05
2023-12-17 00:31:11 INFO     	 * (global step 5500: loss: 0.15999316424131393, lr: 5e-05
2023-12-17 00:31:35 INFO     	 * (global step 5550: loss: 0.2444285899400711, lr: 5e-05
2023-12-17 00:31:55 INFO     [epoch 8/15] average loss: 0.25, lr: 5e-05
2023-12-17 00:31:55 INFO     saving model related files
2023-12-17 00:31:55 INFO     saving model
2023-12-17 00:31:56 INFO     saving tokenizer
2023-12-17 00:31:56 INFO     saving optimizer
2023-12-17 00:32:00 INFO     remove old optimizer files
2023-12-17 00:32:05 INFO     	 * (global step 5600: loss: 0.3295106291770935, lr: 5e-05
2023-12-17 00:32:29 INFO     	 * (global step 5650: loss: 0.23847753554582596, lr: 5e-05
2023-12-17 00:32:54 INFO     	 * (global step 5700: loss: 0.25935717672109604, lr: 5e-05
2023-12-17 00:33:18 INFO     	 * (global step 5750: loss: 0.21469905972480774, lr: 5e-05
2023-12-17 00:33:43 INFO     	 * (global step 5800: loss: 0.23237312585115433, lr: 5e-05
2023-12-17 00:34:07 INFO     	 * (global step 5850: loss: 0.24038086831569672, lr: 5e-05
2023-12-17 00:34:31 INFO     	 * (global step 5900: loss: 0.2047717198729515, lr: 5e-05
2023-12-17 00:34:56 INFO     	 * (global step 5950: loss: 0.18114402145147324, lr: 5e-05
2023-12-17 00:35:20 INFO     	 * (global step 6000: loss: 0.35990869998931885, lr: 5e-05
2023-12-17 00:35:44 INFO     	 * (global step 6050: loss: 0.2718778997659683, lr: 5e-05
2023-12-17 00:36:09 INFO     	 * (global step 6100: loss: 0.23378346115350723, lr: 5e-05
2023-12-17 00:36:33 INFO     	 * (global step 6150: loss: 0.23601652681827545, lr: 5e-05
2023-12-17 00:36:57 INFO     	 * (global step 6200: loss: 0.21060477942228317, lr: 5e-05
2023-12-17 00:37:03 INFO     [epoch 9/15] average loss: 0.24, lr: 5e-05
2023-12-17 00:37:03 INFO     saving model related files
2023-12-17 00:37:03 INFO     saving model
2023-12-17 00:37:04 INFO     saving tokenizer
2023-12-17 00:37:04 INFO     saving optimizer
2023-12-17 00:37:08 INFO     remove old optimizer files
2023-12-17 00:37:08 INFO     complete training: model ckpt was saved at base_trained_ckpt/model_uramvg
2023-12-17 00:37:08 INFO     ## 1st RUN: Configuration 8/12 ##
2023-12-17 00:37:08 INFO     initialize model trainer
2023-12-17 00:37:08 INFO     initialize checkpoint at base_trained_ckpt/model_nxaqhy
2023-12-17 00:37:08 INFO     hyperparameters
2023-12-17 00:37:08 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-17 00:37:08 INFO     	 * dataset_name: default
2023-12-17 00:37:08 INFO     	 * input_types: ['paragraph']
2023-12-17 00:37:08 INFO     	 * output_types: ['questions_answers']
2023-12-17 00:37:08 INFO     	 * prefix_types: ['qag']
2023-12-17 00:37:08 INFO     	 * model: t5-base
2023-12-17 00:37:08 INFO     	 * max_length: 512
2023-12-17 00:37:08 INFO     	 * max_length_output: 512
2023-12-17 00:37:08 INFO     	 * epoch: 15
2023-12-17 00:37:08 INFO     	 * batch: 2
2023-12-17 00:37:08 INFO     	 * lr: 1e-05
2023-12-17 00:37:08 INFO     	 * fp16: False
2023-12-17 00:37:08 INFO     	 * random_seed: 1
2023-12-17 00:37:08 INFO     	 * gradient_accumulation_steps: 4
2023-12-17 00:37:08 INFO     	 * label_smoothing: 0.15
2023-12-17 00:37:08 INFO     initialize checkpoint with t5-base
2023-12-17 00:37:10 INFO     use spaCy answer extraction model: positionrank
2023-12-17 00:37:10 INFO     Model `t5-base`
2023-12-17 00:37:10 INFO     	 * Num of GPU in use: 1
2023-12-17 00:37:10 INFO     	 * Prefix: True
2023-12-17 00:37:10 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 00:37:10 INFO     dataset preprocessing
2023-12-17 00:37:11 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-base.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-17 00:37:12 INFO     start model training
2023-12-17 00:37:58 INFO     	 * (global step 50: loss: 2.9942490458488464, lr: 1e-05
2023-12-17 00:38:45 INFO     	 * (global step 100: loss: 1.2009174078702927, lr: 1e-05
2023-12-17 00:39:32 INFO     	 * (global step 150: loss: 1.4088190495967865, lr: 1e-05
2023-12-17 00:40:19 INFO     	 * (global step 200: loss: 0.9229001104831696, lr: 1e-05
2023-12-17 00:41:06 INFO     	 * (global step 250: loss: 0.7685092091560364, lr: 1e-05
2023-12-17 00:41:53 INFO     	 * (global step 300: loss: 0.9140346199274063, lr: 1e-05
2023-12-17 00:42:03 INFO     [epoch 0/15] average loss: 2.194, lr: 1e-05
2023-12-17 00:42:03 INFO     saving model related files
2023-12-17 00:42:03 INFO     saving model
2023-12-17 00:42:05 INFO     saving tokenizer
2023-12-17 00:42:05 INFO     saving optimizer
2023-12-17 00:42:08 INFO     remove old optimizer files
2023-12-17 00:42:45 INFO     	 * (global step 350: loss: 0.6466420739889145, lr: 1e-05
2023-12-17 00:43:32 INFO     	 * (global step 400: loss: 0.6775780022144318, lr: 1e-05
2023-12-17 00:44:19 INFO     	 * (global step 450: loss: 0.6796678006649017, lr: 1e-05
2023-12-17 00:45:06 INFO     	 * (global step 500: loss: 0.5100077614188194, lr: 1e-05
2023-12-17 00:45:53 INFO     	 * (global step 550: loss: 0.5234061852097511, lr: 1e-05
2023-12-17 00:46:40 INFO     	 * (global step 600: loss: 0.5357197225093842, lr: 1e-05
2023-12-17 00:46:59 INFO     [epoch 1/15] average loss: 0.62, lr: 1e-05
2023-12-17 00:46:59 INFO     saving model related files
2023-12-17 00:46:59 INFO     saving model
2023-12-17 00:47:01 INFO     saving tokenizer
2023-12-17 00:47:01 INFO     saving optimizer
2023-12-17 00:47:05 INFO     remove old optimizer files
2023-12-17 00:47:33 INFO     	 * (global step 650: loss: 0.4067389592528343, lr: 1e-05
2023-12-17 00:48:20 INFO     	 * (global step 700: loss: 0.4917820990085602, lr: 1e-05
2023-12-17 00:49:07 INFO     	 * (global step 750: loss: 0.4636220261454582, lr: 1e-05
2023-12-17 00:49:53 INFO     	 * (global step 800: loss: 0.45757008343935013, lr: 1e-05
2023-12-17 00:50:40 INFO     	 * (global step 850: loss: 0.4905405417084694, lr: 1e-05
2023-12-17 00:51:27 INFO     	 * (global step 900: loss: 0.6511414088308811, lr: 1e-05
2023-12-17 00:51:56 INFO     [epoch 2/15] average loss: 0.518, lr: 1e-05
2023-12-17 00:51:56 INFO     saving model related files
2023-12-17 00:51:56 INFO     saving model
2023-12-17 00:51:58 INFO     saving tokenizer
2023-12-17 00:51:58 INFO     saving optimizer
2023-12-17 00:52:01 INFO     remove old optimizer files
2023-12-17 00:52:20 INFO     	 * (global step 950: loss: 0.45908667892217636, lr: 1e-05
2023-12-17 00:53:06 INFO     	 * (global step 1000: loss: 0.5289506018161774, lr: 1e-05
2023-12-17 00:53:53 INFO     	 * (global step 1050: loss: 0.4964752495288849, lr: 1e-05
2023-12-17 00:54:40 INFO     	 * (global step 1100: loss: 0.5297186970710754, lr: 1e-05
2023-12-17 00:55:27 INFO     	 * (global step 1150: loss: 0.5390617549419403, lr: 1e-05
2023-12-17 00:56:14 INFO     	 * (global step 1200: loss: 0.3894500806927681, lr: 1e-05
2023-12-17 00:56:52 INFO     [epoch 3/15] average loss: 0.47, lr: 1e-05
2023-12-17 00:56:52 INFO     saving model related files
2023-12-17 00:56:52 INFO     saving model
2023-12-17 00:56:53 INFO     saving tokenizer
2023-12-17 00:56:54 INFO     saving optimizer
2023-12-17 00:56:58 INFO     remove old optimizer files
2023-12-17 00:57:07 INFO     	 * (global step 1250: loss: 0.4397120922803879, lr: 1e-05
2023-12-17 00:57:54 INFO     	 * (global step 1300: loss: 0.4454526901245117, lr: 1e-05
2023-12-17 00:58:41 INFO     	 * (global step 1350: loss: 0.491821251809597, lr: 1e-05
2023-12-17 00:59:28 INFO     	 * (global step 1400: loss: 0.509926900267601, lr: 1e-05
2023-12-17 01:00:15 INFO     	 * (global step 1450: loss: 0.48442837595939636, lr: 1e-05
2023-12-17 01:01:02 INFO     	 * (global step 1500: loss: 0.2865889333188534, lr: 1e-05
2023-12-17 01:01:49 INFO     	 * (global step 1550: loss: 0.5556603223085403, lr: 1e-05
2023-12-17 01:01:50 INFO     [epoch 4/15] average loss: 0.441, lr: 1e-05
2023-12-17 01:01:50 INFO     saving model related files
2023-12-17 01:01:50 INFO     saving model
2023-12-17 01:01:51 INFO     saving tokenizer
2023-12-17 01:01:51 INFO     saving optimizer
2023-12-17 01:01:54 INFO     remove old optimizer files
2023-12-17 01:02:41 INFO     	 * (global step 1600: loss: 0.5112181901931763, lr: 1e-05
2023-12-17 01:03:28 INFO     	 * (global step 1650: loss: 0.41271771490573883, lr: 1e-05
2023-12-17 01:04:15 INFO     	 * (global step 1700: loss: 0.5261932983994484, lr: 1e-05
2023-12-17 01:05:02 INFO     	 * (global step 1750: loss: 0.3762963153421879, lr: 1e-05
2023-12-17 01:05:49 INFO     	 * (global step 1800: loss: 0.5312978401780128, lr: 1e-05
2023-12-17 01:06:36 INFO     	 * (global step 1850: loss: 0.6151441037654877, lr: 1e-05
2023-12-17 01:06:46 INFO     [epoch 5/15] average loss: 0.421, lr: 1e-05
2023-12-17 01:06:46 INFO     saving model related files
2023-12-17 01:06:46 INFO     saving model
2023-12-17 01:06:48 INFO     saving tokenizer
2023-12-17 01:06:48 INFO     saving optimizer
2023-12-17 01:06:51 INFO     remove old optimizer files
2023-12-17 01:07:29 INFO     	 * (global step 1900: loss: 0.46602992713451385, lr: 1e-05
2023-12-17 01:08:16 INFO     	 * (global step 1950: loss: 0.3224122002720833, lr: 1e-05
2023-12-17 01:09:03 INFO     	 * (global step 2000: loss: 0.5438224971294403, lr: 1e-05
2023-12-17 01:09:50 INFO     	 * (global step 2050: loss: 0.29640308022499084, lr: 1e-05
2023-12-17 01:10:37 INFO     	 * (global step 2100: loss: 0.4362150952219963, lr: 1e-05
2023-12-17 01:11:24 INFO     	 * (global step 2150: loss: 0.4041375294327736, lr: 1e-05
2023-12-17 01:11:44 INFO     [epoch 6/15] average loss: 0.405, lr: 1e-05
2023-12-17 01:11:44 INFO     saving model related files
2023-12-17 01:11:44 INFO     saving model
2023-12-17 01:11:45 INFO     saving tokenizer
2023-12-17 01:11:45 INFO     saving optimizer
2023-12-17 01:11:49 INFO     remove old optimizer files
2023-12-17 01:12:17 INFO     	 * (global step 2200: loss: 0.3072412461042404, lr: 1e-05
2023-12-17 01:13:04 INFO     	 * (global step 2250: loss: 0.33852943778038025, lr: 1e-05
2023-12-17 01:13:51 INFO     	 * (global step 2300: loss: 0.3311287686228752, lr: 1e-05
2023-12-17 01:14:38 INFO     	 * (global step 2350: loss: 0.36260948330163956, lr: 1e-05
2023-12-17 01:15:25 INFO     	 * (global step 2400: loss: 0.30947674065828323, lr: 1e-05
2023-12-17 01:16:12 INFO     	 * (global step 2450: loss: 0.3762238174676895, lr: 1e-05
2023-12-17 01:16:41 INFO     [epoch 7/15] average loss: 0.393, lr: 1e-05
2023-12-17 01:16:41 INFO     saving model related files
2023-12-17 01:16:41 INFO     saving model
2023-12-17 01:16:42 INFO     saving tokenizer
2023-12-17 01:16:43 INFO     saving optimizer
2023-12-17 01:16:46 INFO     remove old optimizer files
2023-12-17 01:17:05 INFO     	 * (global step 2500: loss: 0.35612035542726517, lr: 1e-05
2023-12-17 01:17:52 INFO     	 * (global step 2550: loss: 0.35669147968292236, lr: 1e-05
2023-12-17 01:18:39 INFO     	 * (global step 2600: loss: 0.3889514282345772, lr: 1e-05
2023-12-17 01:19:26 INFO     	 * (global step 2650: loss: 0.4216570630669594, lr: 1e-05
2023-12-17 01:20:13 INFO     	 * (global step 2700: loss: 0.32808515802025795, lr: 1e-05
2023-12-17 01:21:00 INFO     	 * (global step 2750: loss: 0.4502400606870651, lr: 1e-05
2023-12-17 01:21:38 INFO     [epoch 8/15] average loss: 0.383, lr: 1e-05
2023-12-17 01:21:38 INFO     saving model related files
2023-12-17 01:21:38 INFO     saving model
2023-12-17 01:21:40 INFO     saving tokenizer
2023-12-17 01:21:40 INFO     saving optimizer
2023-12-17 01:21:43 INFO     remove old optimizer files
2023-12-17 01:21:52 INFO     	 * (global step 2800: loss: 0.2757312171161175, lr: 1e-05
2023-12-17 01:22:39 INFO     	 * (global step 2850: loss: 0.3366474285721779, lr: 1e-05
2023-12-17 01:23:26 INFO     	 * (global step 2900: loss: 0.30305781587958336, lr: 1e-05
2023-12-17 01:24:13 INFO     	 * (global step 2950: loss: 0.38850177824497223, lr: 1e-05
2023-12-17 01:25:00 INFO     	 * (global step 3000: loss: 0.3200096972286701, lr: 1e-05
2023-12-17 01:25:47 INFO     	 * (global step 3050: loss: 0.3172386512160301, lr: 1e-05
2023-12-17 01:26:34 INFO     	 * (global step 3100: loss: 0.3715437352657318, lr: 1e-05
2023-12-17 01:26:35 INFO     [epoch 9/15] average loss: 0.374, lr: 1e-05
2023-12-17 01:26:35 INFO     saving model related files
2023-12-17 01:26:35 INFO     saving model
2023-12-17 01:26:36 INFO     saving tokenizer
2023-12-17 01:26:36 INFO     saving optimizer
2023-12-17 01:26:39 INFO     remove old optimizer files
2023-12-17 01:26:39 INFO     complete training: model ckpt was saved at base_trained_ckpt/model_nxaqhy
2023-12-17 01:26:39 INFO     ## 1st RUN: Configuration 9/12 ##
2023-12-17 01:26:39 INFO     initialize model trainer
2023-12-17 01:26:39 INFO     initialize checkpoint at base_trained_ckpt/model_oprhlh
2023-12-17 01:26:39 INFO     hyperparameters
2023-12-17 01:26:39 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-17 01:26:39 INFO     	 * dataset_name: default
2023-12-17 01:26:39 INFO     	 * input_types: ['paragraph']
2023-12-17 01:26:39 INFO     	 * output_types: ['questions_answers']
2023-12-17 01:26:39 INFO     	 * prefix_types: ['qag']
2023-12-17 01:26:39 INFO     	 * model: t5-base
2023-12-17 01:26:39 INFO     	 * max_length: 512
2023-12-17 01:26:39 INFO     	 * max_length_output: 512
2023-12-17 01:26:39 INFO     	 * epoch: 15
2023-12-17 01:26:39 INFO     	 * batch: 2
2023-12-17 01:26:39 INFO     	 * lr: 1e-05
2023-12-17 01:26:39 INFO     	 * fp16: False
2023-12-17 01:26:39 INFO     	 * random_seed: 1
2023-12-17 01:26:39 INFO     	 * gradient_accumulation_steps: 2
2023-12-17 01:26:39 INFO     	 * label_smoothing: 0.15
2023-12-17 01:26:39 INFO     initialize checkpoint with t5-base
2023-12-17 01:26:41 INFO     use spaCy answer extraction model: positionrank
2023-12-17 01:26:42 INFO     Model `t5-base`
2023-12-17 01:26:42 INFO     	 * Num of GPU in use: 1
2023-12-17 01:26:42 INFO     	 * Prefix: True
2023-12-17 01:26:42 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 01:26:42 INFO     dataset preprocessing
2023-12-17 01:26:43 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-base.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-17 01:26:43 INFO     start model training
2023-12-17 01:27:08 INFO     	 * (global step 50: loss: 2.858136534690857, lr: 1e-05
2023-12-17 01:27:32 INFO     	 * (global step 100: loss: 1.5121923089027405, lr: 1e-05
2023-12-17 01:27:56 INFO     	 * (global step 150: loss: 1.2367805242538452, lr: 1e-05
2023-12-17 01:28:21 INFO     	 * (global step 200: loss: 0.8288280665874481, lr: 1e-05
2023-12-17 01:28:45 INFO     	 * (global step 250: loss: 0.6718795895576477, lr: 1e-05
2023-12-17 01:29:09 INFO     	 * (global step 300: loss: 1.222614049911499, lr: 1e-05
2023-12-17 01:29:34 INFO     	 * (global step 350: loss: 0.7287163734436035, lr: 1e-05
2023-12-17 01:29:58 INFO     	 * (global step 400: loss: 0.5953767001628876, lr: 1e-05
2023-12-17 01:30:22 INFO     	 * (global step 450: loss: 0.6061260104179382, lr: 1e-05
2023-12-17 01:30:47 INFO     	 * (global step 500: loss: 0.5000347346067429, lr: 1e-05
2023-12-17 01:31:11 INFO     	 * (global step 550: loss: 1.197412371635437, lr: 1e-05
2023-12-17 01:31:35 INFO     	 * (global step 600: loss: 0.9937562048435211, lr: 1e-05
2023-12-17 01:31:46 INFO     [epoch 0/15] average loss: 1.549, lr: 1e-05
2023-12-17 01:31:46 INFO     saving model related files
2023-12-17 01:31:46 INFO     saving model
2023-12-17 01:31:47 INFO     saving tokenizer
2023-12-17 01:31:47 INFO     saving optimizer
2023-12-17 01:31:50 INFO     remove old optimizer files
2023-12-17 01:32:04 INFO     	 * (global step 650: loss: 0.8249156475067139, lr: 1e-05
2023-12-17 01:32:29 INFO     	 * (global step 700: loss: 0.5382688343524933, lr: 1e-05
2023-12-17 01:32:53 INFO     	 * (global step 750: loss: 0.6451774537563324, lr: 1e-05
2023-12-17 01:33:17 INFO     	 * (global step 800: loss: 0.6304422616958618, lr: 1e-05
2023-12-17 01:33:42 INFO     	 * (global step 850: loss: 0.48636382818222046, lr: 1e-05
2023-12-17 01:34:06 INFO     	 * (global step 900: loss: 0.5576684921979904, lr: 1e-05
2023-12-17 01:34:30 INFO     	 * (global step 950: loss: 0.6528532356023788, lr: 1e-05
2023-12-17 01:34:55 INFO     	 * (global step 1000: loss: 0.5209823846817017, lr: 1e-05
2023-12-17 01:35:19 INFO     	 * (global step 1050: loss: 0.5281393826007843, lr: 1e-05
2023-12-17 01:35:43 INFO     	 * (global step 1100: loss: 0.49037714302539825, lr: 1e-05
2023-12-17 01:36:08 INFO     	 * (global step 1150: loss: 0.5434968173503876, lr: 1e-05
2023-12-17 01:36:32 INFO     	 * (global step 1200: loss: 0.5467745512723923, lr: 1e-05
2023-12-17 01:36:53 INFO     [epoch 1/15] average loss: 0.525, lr: 1e-05
2023-12-17 01:36:53 INFO     saving model related files
2023-12-17 01:36:53 INFO     saving model
2023-12-17 01:36:54 INFO     saving tokenizer
2023-12-17 01:36:54 INFO     saving optimizer
2023-12-17 01:36:58 INFO     remove old optimizer files
2023-12-17 01:37:02 INFO     	 * (global step 1250: loss: 0.3555913120508194, lr: 1e-05
2023-12-17 01:37:26 INFO     	 * (global step 1300: loss: 0.39461587369441986, lr: 1e-05
2023-12-17 01:37:51 INFO     	 * (global step 1350: loss: 0.4106461703777313, lr: 1e-05
2023-12-17 01:38:15 INFO     	 * (global step 1400: loss: 0.5699094533920288, lr: 1e-05
2023-12-17 01:38:39 INFO     	 * (global step 1450: loss: 0.560036301612854, lr: 1e-05
2023-12-17 01:39:04 INFO     	 * (global step 1500: loss: 0.4085777997970581, lr: 1e-05
2023-12-17 01:39:28 INFO     	 * (global step 1550: loss: 0.3801253139972687, lr: 1e-05
2023-12-17 01:39:52 INFO     	 * (global step 1600: loss: 0.7873803228139877, lr: 1e-05
2023-12-17 01:40:17 INFO     	 * (global step 1650: loss: 0.3739551305770874, lr: 1e-05
2023-12-17 01:40:41 INFO     	 * (global step 1700: loss: 0.47428759932518005, lr: 1e-05
2023-12-17 01:41:05 INFO     	 * (global step 1750: loss: 0.3570575565099716, lr: 1e-05
2023-12-17 01:41:30 INFO     	 * (global step 1800: loss: 0.3031764626502991, lr: 1e-05
2023-12-17 01:41:54 INFO     	 * (global step 1850: loss: 0.4509284347295761, lr: 1e-05
2023-12-17 01:42:01 INFO     [epoch 2/15] average loss: 0.453, lr: 1e-05
2023-12-17 01:42:01 INFO     saving model related files
2023-12-17 01:42:01 INFO     saving model
2023-12-17 01:42:02 INFO     saving tokenizer
2023-12-17 01:42:02 INFO     saving optimizer
2023-12-17 01:42:05 INFO     remove old optimizer files
2023-12-17 01:42:24 INFO     	 * (global step 1900: loss: 0.38784366846084595, lr: 1e-05
2023-12-17 01:42:48 INFO     	 * (global step 1950: loss: 0.4745500683784485, lr: 1e-05
2023-12-17 01:43:12 INFO     	 * (global step 2000: loss: 0.3549709767103195, lr: 1e-05
2023-12-17 01:43:37 INFO     	 * (global step 2050: loss: 0.40927691012620926, lr: 1e-05
2023-12-17 01:44:01 INFO     	 * (global step 2100: loss: 0.29101647436618805, lr: 1e-05
2023-12-17 01:44:25 INFO     	 * (global step 2150: loss: 0.4119337499141693, lr: 1e-05
2023-12-17 01:44:50 INFO     	 * (global step 2200: loss: 0.41656357049942017, lr: 1e-05
2023-12-17 01:45:14 INFO     	 * (global step 2250: loss: 0.39003418385982513, lr: 1e-05
2023-12-17 01:45:39 INFO     	 * (global step 2300: loss: 0.3187633454799652, lr: 1e-05
2023-12-17 01:46:03 INFO     	 * (global step 2350: loss: 0.4129679650068283, lr: 1e-05
2023-12-17 01:46:27 INFO     	 * (global step 2400: loss: 0.4749503433704376, lr: 1e-05
2023-12-17 01:46:52 INFO     	 * (global step 2450: loss: 0.48587068915367126, lr: 1e-05
2023-12-17 01:47:08 INFO     [epoch 3/15] average loss: 0.418, lr: 1e-05
2023-12-17 01:47:08 INFO     saving model related files
2023-12-17 01:47:08 INFO     saving model
2023-12-17 01:47:10 INFO     saving tokenizer
2023-12-17 01:47:10 INFO     saving optimizer
2023-12-17 01:47:13 INFO     remove old optimizer files
2023-12-17 01:47:21 INFO     	 * (global step 2500: loss: 0.3809356689453125, lr: 1e-05
2023-12-17 01:47:45 INFO     	 * (global step 2550: loss: 0.3930549770593643, lr: 1e-05
2023-12-17 01:48:09 INFO     	 * (global step 2600: loss: 0.4710424989461899, lr: 1e-05
2023-12-17 01:48:34 INFO     	 * (global step 2650: loss: 0.24845394492149353, lr: 1e-05
2023-12-17 01:48:58 INFO     	 * (global step 2700: loss: 0.36981649696826935, lr: 1e-05
2023-12-17 01:49:22 INFO     	 * (global step 2750: loss: 0.3831675723195076, lr: 1e-05
2023-12-17 01:49:47 INFO     	 * (global step 2800: loss: 0.35112375020980835, lr: 1e-05
2023-12-17 01:50:11 INFO     	 * (global step 2850: loss: 0.3305811583995819, lr: 1e-05
2023-12-17 01:50:35 INFO     	 * (global step 2900: loss: 0.2137066125869751, lr: 1e-05
2023-12-17 01:50:59 INFO     	 * (global step 2950: loss: 0.4226893037557602, lr: 1e-05
2023-12-17 01:51:24 INFO     	 * (global step 3000: loss: 0.40630321204662323, lr: 1e-05
2023-12-17 01:51:48 INFO     	 * (global step 3050: loss: 0.3095177859067917, lr: 1e-05
2023-12-17 01:52:12 INFO     	 * (global step 3100: loss: 0.35765010118484497, lr: 1e-05
2023-12-17 01:52:15 INFO     [epoch 4/15] average loss: 0.396, lr: 1e-05
2023-12-17 01:52:15 INFO     saving model related files
2023-12-17 01:52:15 INFO     saving model
2023-12-17 01:52:16 INFO     saving tokenizer
2023-12-17 01:52:17 INFO     saving optimizer
2023-12-17 01:52:20 INFO     remove old optimizer files
2023-12-17 01:52:42 INFO     	 * (global step 3150: loss: 0.27908845990896225, lr: 1e-05
2023-12-17 01:53:06 INFO     	 * (global step 3200: loss: 0.34713082015514374, lr: 1e-05
2023-12-17 01:53:30 INFO     	 * (global step 3250: loss: 0.4054233282804489, lr: 1e-05
2023-12-17 01:53:54 INFO     	 * (global step 3300: loss: 0.30862027406692505, lr: 1e-05
2023-12-17 01:54:19 INFO     	 * (global step 3350: loss: 0.49232645332813263, lr: 1e-05
2023-12-17 01:54:43 INFO     	 * (global step 3400: loss: 0.38560914993286133, lr: 1e-05
2023-12-17 01:55:07 INFO     	 * (global step 3450: loss: 0.3468581587076187, lr: 1e-05
2023-12-17 01:55:32 INFO     	 * (global step 3500: loss: 0.4023872911930084, lr: 1e-05
2023-12-17 01:55:56 INFO     	 * (global step 3550: loss: 0.46703900396823883, lr: 1e-05
2023-12-17 01:56:20 INFO     	 * (global step 3600: loss: 0.283003993332386, lr: 1e-05
2023-12-17 01:56:44 INFO     	 * (global step 3650: loss: 0.5020252615213394, lr: 1e-05
2023-12-17 01:57:09 INFO     	 * (global step 3700: loss: 0.28732776641845703, lr: 1e-05
2023-12-17 01:57:22 INFO     [epoch 5/15] average loss: 0.379, lr: 1e-05
2023-12-17 01:57:22 INFO     saving model related files
2023-12-17 01:57:22 INFO     saving model
2023-12-17 01:57:23 INFO     saving tokenizer
2023-12-17 01:57:23 INFO     saving optimizer
2023-12-17 01:57:26 INFO     remove old optimizer files
2023-12-17 01:57:37 INFO     	 * (global step 3750: loss: 0.43687577545642853, lr: 1e-05
2023-12-17 01:58:02 INFO     	 * (global step 3800: loss: 0.46830372512340546, lr: 1e-05
2023-12-17 01:58:26 INFO     	 * (global step 3850: loss: 0.4159669876098633, lr: 1e-05
2023-12-17 01:58:50 INFO     	 * (global step 3900: loss: 0.4056762754917145, lr: 1e-05
2023-12-17 01:59:14 INFO     	 * (global step 3950: loss: 0.36108608543872833, lr: 1e-05
2023-12-17 01:59:39 INFO     	 * (global step 4000: loss: 0.3272126466035843, lr: 1e-05
2023-12-17 02:00:03 INFO     	 * (global step 4050: loss: 0.44796082377433777, lr: 1e-05
2023-12-17 02:00:27 INFO     	 * (global step 4100: loss: 0.45880869030952454, lr: 1e-05
2023-12-17 02:00:52 INFO     	 * (global step 4150: loss: 0.3138018846511841, lr: 1e-05
2023-12-17 02:01:16 INFO     	 * (global step 4200: loss: 0.27171310782432556, lr: 1e-05
2023-12-17 02:01:40 INFO     	 * (global step 4250: loss: 0.3010057955980301, lr: 1e-05
2023-12-17 02:02:04 INFO     	 * (global step 4300: loss: 0.27245690673589706, lr: 1e-05
2023-12-17 02:02:28 INFO     [epoch 6/15] average loss: 0.366, lr: 1e-05
2023-12-17 02:02:28 INFO     saving model related files
2023-12-17 02:02:28 INFO     saving model
2023-12-17 02:02:30 INFO     saving tokenizer
2023-12-17 02:02:30 INFO     saving optimizer
2023-12-17 02:02:33 INFO     remove old optimizer files
2023-12-17 02:02:35 INFO     	 * (global step 4350: loss: 0.29707858711481094, lr: 1e-05
2023-12-17 02:02:59 INFO     	 * (global step 4400: loss: 0.37089572846889496, lr: 1e-05
2023-12-17 02:03:23 INFO     	 * (global step 4450: loss: 0.3109568953514099, lr: 1e-05
2023-12-17 02:03:48 INFO     	 * (global step 4500: loss: 0.3362416923046112, lr: 1e-05
2023-12-17 02:04:12 INFO     	 * (global step 4550: loss: 0.35466335713863373, lr: 1e-05
2023-12-17 02:04:36 INFO     	 * (global step 4600: loss: 0.47555868327617645, lr: 1e-05
2023-12-17 02:05:01 INFO     	 * (global step 4650: loss: 0.2949419692158699, lr: 1e-05
2023-12-17 02:05:25 INFO     	 * (global step 4700: loss: 0.3213297724723816, lr: 1e-05
2023-12-17 02:05:49 INFO     	 * (global step 4750: loss: 0.2614954113960266, lr: 1e-05
2023-12-17 02:06:14 INFO     	 * (global step 4800: loss: 0.19175376743078232, lr: 1e-05
2023-12-17 02:06:38 INFO     	 * (global step 4850: loss: 0.5670909136533737, lr: 1e-05
2023-12-17 02:07:02 INFO     	 * (global step 4900: loss: 0.3122500628232956, lr: 1e-05
2023-12-17 02:07:27 INFO     	 * (global step 4950: loss: 0.23388055711984634, lr: 1e-05
2023-12-17 02:07:36 INFO     [epoch 7/15] average loss: 0.354, lr: 1e-05
2023-12-17 02:07:36 INFO     saving model related files
2023-12-17 02:07:36 INFO     saving model
2023-12-17 02:07:38 INFO     saving tokenizer
2023-12-17 02:07:38 INFO     saving optimizer
2023-12-17 02:07:42 INFO     remove old optimizer files
2023-12-17 02:07:58 INFO     	 * (global step 5000: loss: 0.45659565925598145, lr: 1e-05
2023-12-17 02:08:22 INFO     	 * (global step 5050: loss: 0.3469996899366379, lr: 1e-05
2023-12-17 02:08:46 INFO     	 * (global step 5100: loss: 0.24371275305747986, lr: 1e-05
2023-12-17 02:09:10 INFO     	 * (global step 5150: loss: 0.22494205087423325, lr: 1e-05
2023-12-17 02:09:35 INFO     	 * (global step 5200: loss: 0.29158303886651993, lr: 1e-05
2023-12-17 02:09:59 INFO     	 * (global step 5250: loss: 0.3768998831510544, lr: 1e-05
2023-12-17 02:10:23 INFO     	 * (global step 5300: loss: 0.3341876119375229, lr: 1e-05
2023-12-17 02:10:48 INFO     	 * (global step 5350: loss: 0.24284719675779343, lr: 1e-05
2023-12-17 02:11:12 INFO     	 * (global step 5400: loss: 0.40430887043476105, lr: 1e-05
2023-12-17 02:11:37 INFO     	 * (global step 5450: loss: 0.29139944911003113, lr: 1e-05
2023-12-17 02:12:01 INFO     	 * (global step 5500: loss: 0.20063981413841248, lr: 1e-05
2023-12-17 02:12:25 INFO     	 * (global step 5550: loss: 0.3249858170747757, lr: 1e-05
2023-12-17 02:12:44 INFO     [epoch 8/15] average loss: 0.345, lr: 1e-05
2023-12-17 02:12:44 INFO     saving model related files
2023-12-17 02:12:44 INFO     saving model
2023-12-17 02:12:47 INFO     saving tokenizer
2023-12-17 02:12:47 INFO     saving optimizer
2023-12-17 02:12:51 INFO     remove old optimizer files
2023-12-17 02:12:57 INFO     	 * (global step 5600: loss: 0.4728371948003769, lr: 1e-05
2023-12-17 02:13:21 INFO     	 * (global step 5650: loss: 0.32089006900787354, lr: 1e-05
2023-12-17 02:13:45 INFO     	 * (global step 5700: loss: 0.37526778876781464, lr: 1e-05
2023-12-17 02:14:10 INFO     	 * (global step 5750: loss: 0.3057931959629059, lr: 1e-05
2023-12-17 02:14:34 INFO     	 * (global step 5800: loss: 0.3184102177619934, lr: 1e-05
2023-12-17 02:14:58 INFO     	 * (global step 5850: loss: 0.33429811894893646, lr: 1e-05
2023-12-17 02:15:23 INFO     	 * (global step 5900: loss: 0.27205923199653625, lr: 1e-05
2023-12-17 02:15:47 INFO     	 * (global step 5950: loss: 0.27958981692790985, lr: 1e-05
2023-12-17 02:16:12 INFO     	 * (global step 6000: loss: 0.5348823070526123, lr: 1e-05
2023-12-17 02:16:36 INFO     	 * (global step 6050: loss: 0.4077741503715515, lr: 1e-05
2023-12-17 02:17:00 INFO     	 * (global step 6100: loss: 0.33171895146369934, lr: 1e-05
2023-12-17 02:17:25 INFO     	 * (global step 6150: loss: 0.3358250558376312, lr: 1e-05
2023-12-17 02:17:49 INFO     	 * (global step 6200: loss: 0.28733038157224655, lr: 1e-05
2023-12-17 02:17:54 INFO     [epoch 9/15] average loss: 0.336, lr: 1e-05
2023-12-17 02:17:54 INFO     saving model related files
2023-12-17 02:17:54 INFO     saving model
2023-12-17 02:17:56 INFO     saving tokenizer
2023-12-17 02:17:56 INFO     saving optimizer
2023-12-17 02:18:00 INFO     remove old optimizer files
2023-12-17 02:18:00 INFO     complete training: model ckpt was saved at base_trained_ckpt/model_oprhlh
2023-12-17 02:18:00 INFO     ## 1st RUN: Configuration 10/12 ##
2023-12-17 02:18:00 INFO     initialize model trainer
2023-12-17 02:18:00 INFO     initialize checkpoint at base_trained_ckpt/model_vhyoja
2023-12-17 02:18:00 INFO     hyperparameters
2023-12-17 02:18:00 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-17 02:18:00 INFO     	 * dataset_name: default
2023-12-17 02:18:00 INFO     	 * input_types: ['paragraph']
2023-12-17 02:18:00 INFO     	 * output_types: ['questions_answers']
2023-12-17 02:18:00 INFO     	 * prefix_types: ['qag']
2023-12-17 02:18:00 INFO     	 * model: t5-base
2023-12-17 02:18:00 INFO     	 * max_length: 512
2023-12-17 02:18:00 INFO     	 * max_length_output: 512
2023-12-17 02:18:00 INFO     	 * epoch: 15
2023-12-17 02:18:00 INFO     	 * batch: 2
2023-12-17 02:18:00 INFO     	 * lr: 1e-05
2023-12-17 02:18:00 INFO     	 * fp16: False
2023-12-17 02:18:00 INFO     	 * random_seed: 1
2023-12-17 02:18:00 INFO     	 * gradient_accumulation_steps: 4
2023-12-17 02:18:00 INFO     	 * label_smoothing: 0.0
2023-12-17 02:18:00 INFO     initialize checkpoint with t5-base
2023-12-17 02:18:02 INFO     use spaCy answer extraction model: positionrank
2023-12-17 02:18:03 INFO     Model `t5-base`
2023-12-17 02:18:03 INFO     	 * Num of GPU in use: 1
2023-12-17 02:18:03 INFO     	 * Prefix: True
2023-12-17 02:18:03 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 02:18:03 INFO     dataset preprocessing
2023-12-17 02:18:04 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-base.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-17 02:18:04 INFO     start model training
2023-12-17 02:18:51 INFO     	 * (global step 50: loss: 2.9942490458488464, lr: 1e-05
2023-12-17 02:19:38 INFO     	 * (global step 100: loss: 1.2009174078702927, lr: 1e-05
2023-12-17 02:20:25 INFO     	 * (global step 150: loss: 1.4088190495967865, lr: 1e-05
2023-12-17 02:21:12 INFO     	 * (global step 200: loss: 0.9229001104831696, lr: 1e-05
2023-12-17 02:21:59 INFO     	 * (global step 250: loss: 0.7685092091560364, lr: 1e-05
2023-12-17 02:22:46 INFO     	 * (global step 300: loss: 0.9140346199274063, lr: 1e-05
2023-12-17 02:22:56 INFO     [epoch 0/15] average loss: 2.194, lr: 1e-05
2023-12-17 02:22:56 INFO     saving model related files
2023-12-17 02:22:56 INFO     saving model
2023-12-17 02:22:58 INFO     saving tokenizer
2023-12-17 02:22:58 INFO     saving optimizer
2023-12-17 02:23:01 INFO     remove old optimizer files
2023-12-17 02:23:38 INFO     	 * (global step 350: loss: 0.6466420739889145, lr: 1e-05
2023-12-17 02:24:25 INFO     	 * (global step 400: loss: 0.6775780022144318, lr: 1e-05
2023-12-17 02:25:12 INFO     	 * (global step 450: loss: 0.6796678006649017, lr: 1e-05
2023-12-17 02:25:59 INFO     	 * (global step 500: loss: 0.5100077614188194, lr: 1e-05
2023-12-17 02:26:46 INFO     	 * (global step 550: loss: 0.5234061852097511, lr: 1e-05
2023-12-17 02:27:33 INFO     	 * (global step 600: loss: 0.5357197225093842, lr: 1e-05
2023-12-17 02:27:52 INFO     [epoch 1/15] average loss: 0.62, lr: 1e-05
2023-12-17 02:27:52 INFO     saving model related files
2023-12-17 02:27:52 INFO     saving model
2023-12-17 02:27:54 INFO     saving tokenizer
2023-12-17 02:27:54 INFO     saving optimizer
2023-12-17 02:27:58 INFO     remove old optimizer files
2023-12-17 02:28:26 INFO     	 * (global step 650: loss: 0.4067389592528343, lr: 1e-05
2023-12-17 02:29:13 INFO     	 * (global step 700: loss: 0.4917820990085602, lr: 1e-05
2023-12-17 02:30:00 INFO     	 * (global step 750: loss: 0.4636220261454582, lr: 1e-05
2023-12-17 02:30:46 INFO     	 * (global step 800: loss: 0.45757008343935013, lr: 1e-05
2023-12-17 02:31:33 INFO     	 * (global step 850: loss: 0.4905405417084694, lr: 1e-05
2023-12-17 02:32:20 INFO     	 * (global step 900: loss: 0.6511414088308811, lr: 1e-05
2023-12-17 02:32:49 INFO     [epoch 2/15] average loss: 0.518, lr: 1e-05
2023-12-17 02:32:49 INFO     saving model related files
2023-12-17 02:32:49 INFO     saving model
2023-12-17 02:32:51 INFO     saving tokenizer
2023-12-17 02:32:51 INFO     saving optimizer
2023-12-17 02:32:54 INFO     remove old optimizer files
2023-12-17 02:33:13 INFO     	 * (global step 950: loss: 0.45908667892217636, lr: 1e-05
2023-12-17 02:34:00 INFO     	 * (global step 1000: loss: 0.5289506018161774, lr: 1e-05
2023-12-17 02:34:46 INFO     	 * (global step 1050: loss: 0.4964752495288849, lr: 1e-05
2023-12-17 02:35:33 INFO     	 * (global step 1100: loss: 0.5297186970710754, lr: 1e-05
2023-12-17 02:36:20 INFO     	 * (global step 1150: loss: 0.5390617549419403, lr: 1e-05
2023-12-17 02:37:07 INFO     	 * (global step 1200: loss: 0.3894500806927681, lr: 1e-05
2023-12-17 02:37:45 INFO     [epoch 3/15] average loss: 0.47, lr: 1e-05
2023-12-17 02:37:45 INFO     saving model related files
2023-12-17 02:37:45 INFO     saving model
2023-12-17 02:37:47 INFO     saving tokenizer
2023-12-17 02:37:47 INFO     saving optimizer
2023-12-17 02:37:50 INFO     remove old optimizer files
2023-12-17 02:37:59 INFO     	 * (global step 1250: loss: 0.4397120922803879, lr: 1e-05
2023-12-17 02:38:46 INFO     	 * (global step 1300: loss: 0.4454526901245117, lr: 1e-05
2023-12-17 02:39:33 INFO     	 * (global step 1350: loss: 0.491821251809597, lr: 1e-05
2023-12-17 02:40:20 INFO     	 * (global step 1400: loss: 0.509926900267601, lr: 1e-05
2023-12-17 02:41:07 INFO     	 * (global step 1450: loss: 0.48442837595939636, lr: 1e-05
2023-12-17 02:41:54 INFO     	 * (global step 1500: loss: 0.2865889333188534, lr: 1e-05
2023-12-17 02:42:41 INFO     	 * (global step 1550: loss: 0.5556603223085403, lr: 1e-05
2023-12-17 02:42:42 INFO     [epoch 4/15] average loss: 0.441, lr: 1e-05
2023-12-17 02:42:42 INFO     saving model related files
2023-12-17 02:42:42 INFO     saving model
2023-12-17 02:42:44 INFO     saving tokenizer
2023-12-17 02:42:44 INFO     saving optimizer
2023-12-17 02:42:47 INFO     remove old optimizer files
2023-12-17 02:43:34 INFO     	 * (global step 1600: loss: 0.5112181901931763, lr: 1e-05
2023-12-17 02:44:21 INFO     	 * (global step 1650: loss: 0.41271771490573883, lr: 1e-05
2023-12-17 02:45:08 INFO     	 * (global step 1700: loss: 0.5261932983994484, lr: 1e-05
2023-12-17 02:45:55 INFO     	 * (global step 1750: loss: 0.3762963153421879, lr: 1e-05
2023-12-17 02:46:42 INFO     	 * (global step 1800: loss: 0.5312978401780128, lr: 1e-05
2023-12-17 02:47:29 INFO     	 * (global step 1850: loss: 0.6151441037654877, lr: 1e-05
2023-12-17 02:47:39 INFO     [epoch 5/15] average loss: 0.421, lr: 1e-05
2023-12-17 02:47:39 INFO     saving model related files
2023-12-17 02:47:39 INFO     saving model
2023-12-17 02:47:41 INFO     saving tokenizer
2023-12-17 02:47:41 INFO     saving optimizer
2023-12-17 02:47:46 INFO     remove old optimizer files
2023-12-17 02:48:23 INFO     	 * (global step 1900: loss: 0.46602992713451385, lr: 1e-05
2023-12-17 02:49:10 INFO     	 * (global step 1950: loss: 0.3224122002720833, lr: 1e-05
2023-12-17 02:49:57 INFO     	 * (global step 2000: loss: 0.5438224971294403, lr: 1e-05
2023-12-17 02:50:44 INFO     	 * (global step 2050: loss: 0.29640308022499084, lr: 1e-05
2023-12-17 02:51:31 INFO     	 * (global step 2100: loss: 0.4362150952219963, lr: 1e-05
2023-12-17 02:52:18 INFO     	 * (global step 2150: loss: 0.4041375294327736, lr: 1e-05
2023-12-17 02:52:38 INFO     [epoch 6/15] average loss: 0.405, lr: 1e-05
2023-12-17 02:52:38 INFO     saving model related files
2023-12-17 02:52:38 INFO     saving model
2023-12-17 02:52:40 INFO     saving tokenizer
2023-12-17 02:52:40 INFO     saving optimizer
2023-12-17 02:52:44 INFO     remove old optimizer files
2023-12-17 02:53:12 INFO     	 * (global step 2200: loss: 0.3072412461042404, lr: 1e-05
2023-12-17 02:53:59 INFO     	 * (global step 2250: loss: 0.33852943778038025, lr: 1e-05
2023-12-17 02:54:46 INFO     	 * (global step 2300: loss: 0.3311287686228752, lr: 1e-05
2023-12-17 02:55:33 INFO     	 * (global step 2350: loss: 0.36260948330163956, lr: 1e-05
2023-12-17 02:56:20 INFO     	 * (global step 2400: loss: 0.30947674065828323, lr: 1e-05
2023-12-17 02:57:07 INFO     	 * (global step 2450: loss: 0.3762238174676895, lr: 1e-05
2023-12-17 02:57:36 INFO     [epoch 7/15] average loss: 0.393, lr: 1e-05
2023-12-17 02:57:36 INFO     saving model related files
2023-12-17 02:57:36 INFO     saving model
2023-12-17 02:57:39 INFO     saving tokenizer
2023-12-17 02:57:39 INFO     saving optimizer
2023-12-17 02:57:43 INFO     remove old optimizer files
2023-12-17 02:58:02 INFO     	 * (global step 2500: loss: 0.35612035542726517, lr: 1e-05
2023-12-17 02:58:48 INFO     	 * (global step 2550: loss: 0.35669147968292236, lr: 1e-05
2023-12-17 02:59:35 INFO     	 * (global step 2600: loss: 0.3889514282345772, lr: 1e-05
2023-12-17 03:00:22 INFO     	 * (global step 2650: loss: 0.4216570630669594, lr: 1e-05
2023-12-17 03:01:09 INFO     	 * (global step 2700: loss: 0.32808515802025795, lr: 1e-05
2023-12-17 03:01:56 INFO     	 * (global step 2750: loss: 0.4502400606870651, lr: 1e-05
2023-12-17 03:02:34 INFO     [epoch 8/15] average loss: 0.383, lr: 1e-05
2023-12-17 03:02:34 INFO     saving model related files
2023-12-17 03:02:34 INFO     saving model
2023-12-17 03:02:36 INFO     saving tokenizer
2023-12-17 03:02:36 INFO     saving optimizer
2023-12-17 03:02:39 INFO     remove old optimizer files
2023-12-17 03:02:49 INFO     	 * (global step 2800: loss: 0.2757312171161175, lr: 1e-05
2023-12-17 03:03:36 INFO     	 * (global step 2850: loss: 0.3366474285721779, lr: 1e-05
2023-12-17 03:04:23 INFO     	 * (global step 2900: loss: 0.30305781587958336, lr: 1e-05
2023-12-17 03:05:09 INFO     	 * (global step 2950: loss: 0.38850177824497223, lr: 1e-05
2023-12-17 03:05:56 INFO     	 * (global step 3000: loss: 0.3200096972286701, lr: 1e-05
2023-12-17 03:06:43 INFO     	 * (global step 3050: loss: 0.3172386512160301, lr: 1e-05
2023-12-17 03:07:30 INFO     	 * (global step 3100: loss: 0.3715437352657318, lr: 1e-05
2023-12-17 03:07:31 INFO     [epoch 9/15] average loss: 0.374, lr: 1e-05
2023-12-17 03:07:31 INFO     saving model related files
2023-12-17 03:07:31 INFO     saving model
2023-12-17 03:07:34 INFO     saving tokenizer
2023-12-17 03:07:34 INFO     saving optimizer
2023-12-17 03:07:40 INFO     remove old optimizer files
2023-12-17 03:07:40 INFO     complete training: model ckpt was saved at base_trained_ckpt/model_vhyoja
2023-12-17 03:07:40 INFO     ## 1st RUN: Configuration 11/12 ##
2023-12-17 03:07:40 INFO     initialize model trainer
2023-12-17 03:07:40 INFO     initialize checkpoint at base_trained_ckpt/model_nrudfu
2023-12-17 03:07:40 INFO     hyperparameters
2023-12-17 03:07:40 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-17 03:07:40 INFO     	 * dataset_name: default
2023-12-17 03:07:40 INFO     	 * input_types: ['paragraph']
2023-12-17 03:07:40 INFO     	 * output_types: ['questions_answers']
2023-12-17 03:07:40 INFO     	 * prefix_types: ['qag']
2023-12-17 03:07:40 INFO     	 * model: t5-base
2023-12-17 03:07:40 INFO     	 * max_length: 512
2023-12-17 03:07:40 INFO     	 * max_length_output: 512
2023-12-17 03:07:40 INFO     	 * epoch: 15
2023-12-17 03:07:40 INFO     	 * batch: 2
2023-12-17 03:07:40 INFO     	 * lr: 1e-05
2023-12-17 03:07:40 INFO     	 * fp16: False
2023-12-17 03:07:40 INFO     	 * random_seed: 1
2023-12-17 03:07:40 INFO     	 * gradient_accumulation_steps: 2
2023-12-17 03:07:40 INFO     	 * label_smoothing: 0.0
2023-12-17 03:07:40 INFO     initialize checkpoint with t5-base
2023-12-17 03:07:42 INFO     use spaCy answer extraction model: positionrank
2023-12-17 03:07:42 INFO     Model `t5-base`
2023-12-17 03:07:42 INFO     	 * Num of GPU in use: 1
2023-12-17 03:07:42 INFO     	 * Prefix: True
2023-12-17 03:07:42 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 03:07:42 INFO     dataset preprocessing
2023-12-17 03:07:43 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-base.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-17 03:07:44 INFO     start model training
2023-12-17 03:08:08 INFO     	 * (global step 50: loss: 2.858136534690857, lr: 1e-05
2023-12-17 03:08:32 INFO     	 * (global step 100: loss: 1.5121923089027405, lr: 1e-05
2023-12-17 03:08:56 INFO     	 * (global step 150: loss: 1.2367805242538452, lr: 1e-05
2023-12-17 03:09:20 INFO     	 * (global step 200: loss: 0.8288280665874481, lr: 1e-05
2023-12-17 03:09:45 INFO     	 * (global step 250: loss: 0.6718795895576477, lr: 1e-05
2023-12-17 03:10:09 INFO     	 * (global step 300: loss: 1.222614049911499, lr: 1e-05
2023-12-17 03:10:33 INFO     	 * (global step 350: loss: 0.7287163734436035, lr: 1e-05
2023-12-17 03:10:57 INFO     	 * (global step 400: loss: 0.5953767001628876, lr: 1e-05
2023-12-17 03:11:22 INFO     	 * (global step 450: loss: 0.6061260104179382, lr: 1e-05
2023-12-17 03:11:46 INFO     	 * (global step 500: loss: 0.5000347346067429, lr: 1e-05
2023-12-17 03:12:10 INFO     	 * (global step 550: loss: 1.197412371635437, lr: 1e-05
2023-12-17 03:12:35 INFO     	 * (global step 600: loss: 0.9937562048435211, lr: 1e-05
2023-12-17 03:12:45 INFO     [epoch 0/15] average loss: 1.549, lr: 1e-05
2023-12-17 03:12:45 INFO     saving model related files
2023-12-17 03:12:45 INFO     saving model
2023-12-17 03:12:47 INFO     saving tokenizer
2023-12-17 03:12:47 INFO     saving optimizer
2023-12-17 03:12:51 INFO     remove old optimizer files
2023-12-17 03:13:05 INFO     	 * (global step 650: loss: 0.8249156475067139, lr: 1e-05
2023-12-17 03:13:29 INFO     	 * (global step 700: loss: 0.5382688343524933, lr: 1e-05
2023-12-17 03:13:53 INFO     	 * (global step 750: loss: 0.6451774537563324, lr: 1e-05
2023-12-17 03:14:17 INFO     	 * (global step 800: loss: 0.6304422616958618, lr: 1e-05
2023-12-17 03:14:42 INFO     	 * (global step 850: loss: 0.48636382818222046, lr: 1e-05
2023-12-17 03:15:06 INFO     	 * (global step 900: loss: 0.5576684921979904, lr: 1e-05
2023-12-17 03:15:30 INFO     	 * (global step 950: loss: 0.6528532356023788, lr: 1e-05
2023-12-17 03:15:55 INFO     	 * (global step 1000: loss: 0.5209823846817017, lr: 1e-05
2023-12-17 03:16:19 INFO     	 * (global step 1050: loss: 0.5281393826007843, lr: 1e-05
2023-12-17 03:16:43 INFO     	 * (global step 1100: loss: 0.49037714302539825, lr: 1e-05
2023-12-17 03:17:08 INFO     	 * (global step 1150: loss: 0.5434968173503876, lr: 1e-05
2023-12-17 03:17:32 INFO     	 * (global step 1200: loss: 0.5467745512723923, lr: 1e-05
2023-12-17 03:17:53 INFO     [epoch 1/15] average loss: 0.525, lr: 1e-05
2023-12-17 03:17:53 INFO     saving model related files
2023-12-17 03:17:53 INFO     saving model
2023-12-17 03:17:55 INFO     saving tokenizer
2023-12-17 03:17:55 INFO     saving optimizer
2023-12-17 03:17:59 INFO     remove old optimizer files
2023-12-17 03:18:03 INFO     	 * (global step 1250: loss: 0.3555913120508194, lr: 1e-05
2023-12-17 03:18:28 INFO     	 * (global step 1300: loss: 0.39461587369441986, lr: 1e-05
2023-12-17 03:18:52 INFO     	 * (global step 1350: loss: 0.4106461703777313, lr: 1e-05
2023-12-17 03:19:16 INFO     	 * (global step 1400: loss: 0.5699094533920288, lr: 1e-05
2023-12-17 03:19:41 INFO     	 * (global step 1450: loss: 0.560036301612854, lr: 1e-05
2023-12-17 03:20:05 INFO     	 * (global step 1500: loss: 0.4085777997970581, lr: 1e-05
2023-12-17 03:20:29 INFO     	 * (global step 1550: loss: 0.3801253139972687, lr: 1e-05
2023-12-17 03:20:54 INFO     	 * (global step 1600: loss: 0.7873803228139877, lr: 1e-05
2023-12-17 03:21:18 INFO     	 * (global step 1650: loss: 0.3739551305770874, lr: 1e-05
2023-12-17 03:21:42 INFO     	 * (global step 1700: loss: 0.47428759932518005, lr: 1e-05
2023-12-17 03:22:07 INFO     	 * (global step 1750: loss: 0.3570575565099716, lr: 1e-05
2023-12-17 03:22:31 INFO     	 * (global step 1800: loss: 0.3031764626502991, lr: 1e-05
2023-12-17 03:22:56 INFO     	 * (global step 1850: loss: 0.4509284347295761, lr: 1e-05
2023-12-17 03:23:02 INFO     [epoch 2/15] average loss: 0.453, lr: 1e-05
2023-12-17 03:23:02 INFO     saving model related files
2023-12-17 03:23:02 INFO     saving model
2023-12-17 03:23:04 INFO     saving tokenizer
2023-12-17 03:23:04 INFO     saving optimizer
2023-12-17 03:23:07 INFO     remove old optimizer files
2023-12-17 03:23:26 INFO     	 * (global step 1900: loss: 0.38784366846084595, lr: 1e-05
2023-12-17 03:23:50 INFO     	 * (global step 1950: loss: 0.4745500683784485, lr: 1e-05
2023-12-17 03:24:14 INFO     	 * (global step 2000: loss: 0.3549709767103195, lr: 1e-05
2023-12-17 03:24:38 INFO     	 * (global step 2050: loss: 0.40927691012620926, lr: 1e-05
2023-12-17 03:25:03 INFO     	 * (global step 2100: loss: 0.29101647436618805, lr: 1e-05
2023-12-17 03:25:27 INFO     	 * (global step 2150: loss: 0.4119337499141693, lr: 1e-05
2023-12-17 03:25:51 INFO     	 * (global step 2200: loss: 0.41656357049942017, lr: 1e-05
2023-12-17 03:26:16 INFO     	 * (global step 2250: loss: 0.39003418385982513, lr: 1e-05
2023-12-17 03:26:40 INFO     	 * (global step 2300: loss: 0.3187633454799652, lr: 1e-05
2023-12-17 03:27:04 INFO     	 * (global step 2350: loss: 0.4129679650068283, lr: 1e-05
2023-12-17 03:27:29 INFO     	 * (global step 2400: loss: 0.4749503433704376, lr: 1e-05
2023-12-17 03:27:53 INFO     	 * (global step 2450: loss: 0.48587068915367126, lr: 1e-05
2023-12-17 03:28:10 INFO     [epoch 3/15] average loss: 0.418, lr: 1e-05
2023-12-17 03:28:10 INFO     saving model related files
2023-12-17 03:28:10 INFO     saving model
2023-12-17 03:28:12 INFO     saving tokenizer
2023-12-17 03:28:12 INFO     saving optimizer
2023-12-17 03:28:15 INFO     remove old optimizer files
2023-12-17 03:28:23 INFO     	 * (global step 2500: loss: 0.3809356689453125, lr: 1e-05
2023-12-17 03:28:47 INFO     	 * (global step 2550: loss: 0.3930549770593643, lr: 1e-05
2023-12-17 03:29:12 INFO     	 * (global step 2600: loss: 0.4710424989461899, lr: 1e-05
2023-12-17 03:29:36 INFO     	 * (global step 2650: loss: 0.24845394492149353, lr: 1e-05
2023-12-17 03:30:00 INFO     	 * (global step 2700: loss: 0.36981649696826935, lr: 1e-05
2023-12-17 03:30:25 INFO     	 * (global step 2750: loss: 0.3831675723195076, lr: 1e-05
2023-12-17 03:30:49 INFO     	 * (global step 2800: loss: 0.35112375020980835, lr: 1e-05
2023-12-17 03:31:13 INFO     	 * (global step 2850: loss: 0.3305811583995819, lr: 1e-05
2023-12-17 03:31:38 INFO     	 * (global step 2900: loss: 0.2137066125869751, lr: 1e-05
2023-12-17 03:32:02 INFO     	 * (global step 2950: loss: 0.4226893037557602, lr: 1e-05
2023-12-17 03:32:27 INFO     	 * (global step 3000: loss: 0.40630321204662323, lr: 1e-05
2023-12-17 03:32:51 INFO     	 * (global step 3050: loss: 0.3095177859067917, lr: 1e-05
2023-12-17 03:33:15 INFO     	 * (global step 3100: loss: 0.35765010118484497, lr: 1e-05
2023-12-17 03:33:18 INFO     [epoch 4/15] average loss: 0.396, lr: 1e-05
2023-12-17 03:33:18 INFO     saving model related files
2023-12-17 03:33:18 INFO     saving model
2023-12-17 03:33:20 INFO     saving tokenizer
2023-12-17 03:33:20 INFO     saving optimizer
2023-12-17 03:33:23 INFO     remove old optimizer files
2023-12-17 03:33:45 INFO     	 * (global step 3150: loss: 0.27908845990896225, lr: 1e-05
2023-12-17 03:34:10 INFO     	 * (global step 3200: loss: 0.34713082015514374, lr: 1e-05
2023-12-17 03:34:34 INFO     	 * (global step 3250: loss: 0.4054233282804489, lr: 1e-05
2023-12-17 03:34:58 INFO     	 * (global step 3300: loss: 0.30862027406692505, lr: 1e-05
2023-12-17 03:35:23 INFO     	 * (global step 3350: loss: 0.49232645332813263, lr: 1e-05
2023-12-17 03:35:47 INFO     	 * (global step 3400: loss: 0.38560914993286133, lr: 1e-05
2023-12-17 03:36:11 INFO     	 * (global step 3450: loss: 0.3468581587076187, lr: 1e-05
2023-12-17 03:36:36 INFO     	 * (global step 3500: loss: 0.4023872911930084, lr: 1e-05
2023-12-17 03:37:00 INFO     	 * (global step 3550: loss: 0.46703900396823883, lr: 1e-05
2023-12-17 03:37:24 INFO     	 * (global step 3600: loss: 0.283003993332386, lr: 1e-05
2023-12-17 03:37:49 INFO     	 * (global step 3650: loss: 0.5020252615213394, lr: 1e-05
2023-12-17 03:38:13 INFO     	 * (global step 3700: loss: 0.28732776641845703, lr: 1e-05
2023-12-17 03:38:26 INFO     [epoch 5/15] average loss: 0.379, lr: 1e-05
2023-12-17 03:38:26 INFO     saving model related files
2023-12-17 03:38:26 INFO     saving model
2023-12-17 03:38:27 INFO     saving tokenizer
2023-12-17 03:38:27 INFO     saving optimizer
2023-12-17 03:38:31 INFO     remove old optimizer files
2023-12-17 03:38:42 INFO     	 * (global step 3750: loss: 0.43687577545642853, lr: 1e-05
2023-12-17 03:39:07 INFO     	 * (global step 3800: loss: 0.46830372512340546, lr: 1e-05
2023-12-17 03:39:31 INFO     	 * (global step 3850: loss: 0.4159669876098633, lr: 1e-05
2023-12-17 03:39:55 INFO     	 * (global step 3900: loss: 0.4056762754917145, lr: 1e-05
2023-12-17 03:40:19 INFO     	 * (global step 3950: loss: 0.36108608543872833, lr: 1e-05
2023-12-17 03:40:44 INFO     	 * (global step 4000: loss: 0.3272126466035843, lr: 1e-05
2023-12-17 03:41:08 INFO     	 * (global step 4050: loss: 0.44796082377433777, lr: 1e-05
2023-12-17 03:41:32 INFO     	 * (global step 4100: loss: 0.45880869030952454, lr: 1e-05
2023-12-17 03:41:57 INFO     	 * (global step 4150: loss: 0.3138018846511841, lr: 1e-05
2023-12-17 03:42:21 INFO     	 * (global step 4200: loss: 0.27171310782432556, lr: 1e-05
2023-12-17 03:42:45 INFO     	 * (global step 4250: loss: 0.3010057955980301, lr: 1e-05
2023-12-17 03:43:09 INFO     	 * (global step 4300: loss: 0.27245690673589706, lr: 1e-05
2023-12-17 03:43:33 INFO     [epoch 6/15] average loss: 0.366, lr: 1e-05
2023-12-17 03:43:33 INFO     saving model related files
2023-12-17 03:43:33 INFO     saving model
2023-12-17 03:43:34 INFO     saving tokenizer
2023-12-17 03:43:34 INFO     saving optimizer
2023-12-17 03:43:37 INFO     remove old optimizer files
2023-12-17 03:43:39 INFO     	 * (global step 4350: loss: 0.29707858711481094, lr: 1e-05
2023-12-17 03:44:03 INFO     	 * (global step 4400: loss: 0.37089572846889496, lr: 1e-05
2023-12-17 03:44:27 INFO     	 * (global step 4450: loss: 0.3109568953514099, lr: 1e-05
2023-12-17 03:44:51 INFO     	 * (global step 4500: loss: 0.3362416923046112, lr: 1e-05
2023-12-17 03:45:16 INFO     	 * (global step 4550: loss: 0.35466335713863373, lr: 1e-05
2023-12-17 03:45:40 INFO     	 * (global step 4600: loss: 0.47555868327617645, lr: 1e-05
2023-12-17 03:46:04 INFO     	 * (global step 4650: loss: 0.2949419692158699, lr: 1e-05
2023-12-17 03:46:29 INFO     	 * (global step 4700: loss: 0.3213297724723816, lr: 1e-05
2023-12-17 03:46:53 INFO     	 * (global step 4750: loss: 0.2614954113960266, lr: 1e-05
2023-12-17 03:47:17 INFO     	 * (global step 4800: loss: 0.19175376743078232, lr: 1e-05
2023-12-17 03:47:41 INFO     	 * (global step 4850: loss: 0.5670909136533737, lr: 1e-05
2023-12-17 03:48:06 INFO     	 * (global step 4900: loss: 0.3122500628232956, lr: 1e-05
2023-12-17 03:48:30 INFO     	 * (global step 4950: loss: 0.23388055711984634, lr: 1e-05
2023-12-17 03:48:39 INFO     [epoch 7/15] average loss: 0.354, lr: 1e-05
2023-12-17 03:48:39 INFO     saving model related files
2023-12-17 03:48:39 INFO     saving model
2023-12-17 03:48:41 INFO     saving tokenizer
2023-12-17 03:48:41 INFO     saving optimizer
2023-12-17 03:48:44 INFO     remove old optimizer files
2023-12-17 03:48:59 INFO     	 * (global step 5000: loss: 0.45659565925598145, lr: 1e-05
2023-12-17 03:49:24 INFO     	 * (global step 5050: loss: 0.3469996899366379, lr: 1e-05
2023-12-17 03:49:48 INFO     	 * (global step 5100: loss: 0.24371275305747986, lr: 1e-05
2023-12-17 03:50:12 INFO     	 * (global step 5150: loss: 0.22494205087423325, lr: 1e-05
2023-12-17 03:50:36 INFO     	 * (global step 5200: loss: 0.29158303886651993, lr: 1e-05
2023-12-17 03:51:01 INFO     	 * (global step 5250: loss: 0.3768998831510544, lr: 1e-05
2023-12-17 03:51:25 INFO     	 * (global step 5300: loss: 0.3341876119375229, lr: 1e-05
2023-12-17 03:51:49 INFO     	 * (global step 5350: loss: 0.24284719675779343, lr: 1e-05
2023-12-17 03:52:14 INFO     	 * (global step 5400: loss: 0.40430887043476105, lr: 1e-05
2023-12-17 03:52:38 INFO     	 * (global step 5450: loss: 0.29139944911003113, lr: 1e-05
2023-12-17 03:53:03 INFO     	 * (global step 5500: loss: 0.20063981413841248, lr: 1e-05
2023-12-17 03:53:27 INFO     	 * (global step 5550: loss: 0.3249858170747757, lr: 1e-05
2023-12-17 03:53:46 INFO     [epoch 8/15] average loss: 0.345, lr: 1e-05
2023-12-17 03:53:46 INFO     saving model related files
2023-12-17 03:53:46 INFO     saving model
2023-12-17 03:53:48 INFO     saving tokenizer
2023-12-17 03:53:48 INFO     saving optimizer
2023-12-17 03:53:52 INFO     remove old optimizer files
2023-12-17 03:53:57 INFO     	 * (global step 5600: loss: 0.4728371948003769, lr: 1e-05
2023-12-17 03:54:21 INFO     	 * (global step 5650: loss: 0.32089006900787354, lr: 1e-05
2023-12-17 03:54:46 INFO     	 * (global step 5700: loss: 0.37526778876781464, lr: 1e-05
2023-12-17 03:55:10 INFO     	 * (global step 5750: loss: 0.3057931959629059, lr: 1e-05
2023-12-17 03:55:34 INFO     	 * (global step 5800: loss: 0.3184102177619934, lr: 1e-05
2023-12-17 03:55:59 INFO     	 * (global step 5850: loss: 0.33429811894893646, lr: 1e-05
2023-12-17 03:56:23 INFO     	 * (global step 5900: loss: 0.27205923199653625, lr: 1e-05
2023-12-17 03:56:48 INFO     	 * (global step 5950: loss: 0.27958981692790985, lr: 1e-05
2023-12-17 03:57:12 INFO     	 * (global step 6000: loss: 0.5348823070526123, lr: 1e-05
2023-12-17 03:57:36 INFO     	 * (global step 6050: loss: 0.4077741503715515, lr: 1e-05
2023-12-17 03:58:01 INFO     	 * (global step 6100: loss: 0.33171895146369934, lr: 1e-05
2023-12-17 03:58:25 INFO     	 * (global step 6150: loss: 0.3358250558376312, lr: 1e-05
2023-12-17 03:58:49 INFO     	 * (global step 6200: loss: 0.28733038157224655, lr: 1e-05
2023-12-17 03:58:55 INFO     [epoch 9/15] average loss: 0.336, lr: 1e-05
2023-12-17 03:58:55 INFO     saving model related files
2023-12-17 03:58:55 INFO     saving model
2023-12-17 03:58:56 INFO     saving tokenizer
2023-12-17 03:58:56 INFO     saving optimizer
2023-12-17 03:58:59 INFO     remove old optimizer files
2023-12-17 03:59:00 INFO     complete training: model ckpt was saved at base_trained_ckpt/model_nrudfu
2023-12-17 03:59:00 INFO     ## 1st RUN (EVAL): Configuration 0/12 ##
2023-12-17 03:59:26 INFO     use spaCy answer extraction model: positionrank
2023-12-17 03:59:26 INFO     Model `base_trained_ckpt/model_rillvb/epoch_10`
2023-12-17 03:59:26 INFO     	 * Num of GPU in use: 1
2023-12-17 03:59:26 INFO     	 * Prefix: True
2023-12-17 03:59:26 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 03:59:27 INFO     encode all the data       : 574
  0%|          | 0/574 [00:00<?, ?it/s]  0%|          | 1/574 [00:00<01:37,  5.86it/s]  5%|▍         | 26/574 [00:00<00:04, 116.97it/s] 18%|█▊        | 101/574 [00:00<00:01, 364.89it/s] 32%|███▏      | 181/574 [00:00<00:00, 520.03it/s] 45%|████▌     | 259/574 [00:00<00:00, 608.02it/s] 59%|█████▉    | 338/574 [00:00<00:00, 667.28it/s] 71%|███████   | 408/574 [00:00<00:00, 656.02it/s] 83%|████████▎ | 476/574 [00:00<00:00, 656.70it/s] 95%|█████████▍| 543/574 [00:01<00:00, 605.87it/s]100%|██████████| 574/574 [00:01<00:00, 538.74it/s]
2023-12-17 03:59:29 INFO     after remove the overflow : 574
2023-12-17 03:59:29 INFO     preprocessed feature is saved at /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 04:11:18 INFO     encode all the data       : 574
  0%|          | 0/574 [00:00<?, ?it/s] 12%|█▏        | 69/574 [00:00<00:00, 682.67it/s] 26%|██▌       | 148/574 [00:00<00:00, 743.86it/s] 40%|████      | 231/574 [00:00<00:00, 780.29it/s] 54%|█████▍    | 312/574 [00:00<00:00, 791.50it/s] 68%|██████▊   | 392/574 [00:00<00:00, 771.22it/s] 82%|████████▏ | 473/574 [00:00<00:00, 780.33it/s] 96%|█████████▌| 552/574 [00:00<00:00, 778.99it/s]100%|██████████| 574/574 [00:00<00:00, 776.66it/s]
2023-12-17 04:11:19 INFO     after remove the overflow : 574
2023-12-17 04:11:20 INFO     preprocessed feature is saved at /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 04:22:48 INFO     	Bleu_1: 0.12478373126779976
2023-12-17 04:22:48 INFO     	Bleu_2: 0.06851062765182969
2023-12-17 04:22:48 INFO     	Bleu_3: 0.03613428319955351
2023-12-17 04:22:48 INFO     	Bleu_4: 0.022800842017452354
2023-12-17 04:22:49 INFO     	Bleu_1: 0.12168045571680373
2023-12-17 04:22:49 INFO     	Bleu_2: 0.06584087275036252
2023-12-17 04:22:49 INFO     	Bleu_3: 0.03414838427585448
2023-12-17 04:22:49 INFO     	Bleu_4: 0.021506753583609782
2023-12-17 04:22:49 INFO     ## 1st RUN (EVAL): Configuration 1/12 ##
2023-12-17 04:23:03 INFO     use spaCy answer extraction model: positionrank
2023-12-17 04:23:03 INFO     Model `base_trained_ckpt/model_eszyci/epoch_10`
2023-12-17 04:23:03 INFO     	 * Num of GPU in use: 1
2023-12-17 04:23:03 INFO     	 * Prefix: True
2023-12-17 04:23:03 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 04:23:04 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 04:34:48 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 04:46:40 INFO     	Bleu_1: 0.12259282327668314
2023-12-17 04:46:40 INFO     	Bleu_2: 0.06801131766092577
2023-12-17 04:46:40 INFO     	Bleu_3: 0.03727599803564107
2023-12-17 04:46:40 INFO     	Bleu_4: 0.024054985672499824
2023-12-17 04:46:40 INFO     	Bleu_1: 0.11955430065755739
2023-12-17 04:46:40 INFO     	Bleu_2: 0.06573954085926015
2023-12-17 04:46:40 INFO     	Bleu_3: 0.03535393759484511
2023-12-17 04:46:40 INFO     	Bleu_4: 0.022664155778241036
2023-12-17 04:46:40 INFO     ## 1st RUN (EVAL): Configuration 2/12 ##
2023-12-17 04:46:54 INFO     use spaCy answer extraction model: positionrank
2023-12-17 04:46:54 INFO     Model `base_trained_ckpt/model_dpyopu/epoch_10`
2023-12-17 04:46:54 INFO     	 * Num of GPU in use: 1
2023-12-17 04:46:54 INFO     	 * Prefix: True
2023-12-17 04:46:54 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 04:46:55 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 04:58:35 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 05:10:09 INFO     	Bleu_1: 0.12478373126779976
2023-12-17 05:10:09 INFO     	Bleu_2: 0.06851062765182969
2023-12-17 05:10:09 INFO     	Bleu_3: 0.03613428319955351
2023-12-17 05:10:09 INFO     	Bleu_4: 0.022800842017452354
2023-12-17 05:10:10 INFO     	Bleu_1: 0.12168045571680373
2023-12-17 05:10:10 INFO     	Bleu_2: 0.06584087275036252
2023-12-17 05:10:10 INFO     	Bleu_3: 0.03414838427585448
2023-12-17 05:10:10 INFO     	Bleu_4: 0.021506753583609782
2023-12-17 05:10:10 INFO     ## 1st RUN (EVAL): Configuration 3/12 ##
2023-12-17 05:10:24 INFO     use spaCy answer extraction model: positionrank
2023-12-17 05:10:24 INFO     Model `base_trained_ckpt/model_mzgdpa/epoch_10`
2023-12-17 05:10:24 INFO     	 * Num of GPU in use: 1
2023-12-17 05:10:24 INFO     	 * Prefix: True
2023-12-17 05:10:24 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 05:10:26 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 05:22:13 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 05:33:55 INFO     	Bleu_1: 0.12259282327668314
2023-12-17 05:33:55 INFO     	Bleu_2: 0.06801131766092577
2023-12-17 05:33:55 INFO     	Bleu_3: 0.03727599803564107
2023-12-17 05:33:55 INFO     	Bleu_4: 0.024054985672499824
2023-12-17 05:33:56 INFO     	Bleu_1: 0.11955430065755739
2023-12-17 05:33:56 INFO     	Bleu_2: 0.06573954085926015
2023-12-17 05:33:56 INFO     	Bleu_3: 0.03535393759484511
2023-12-17 05:33:56 INFO     	Bleu_4: 0.022664155778241036
2023-12-17 05:33:56 INFO     ## 1st RUN (EVAL): Configuration 4/12 ##
2023-12-17 05:34:10 INFO     use spaCy answer extraction model: positionrank
2023-12-17 05:34:11 INFO     Model `base_trained_ckpt/model_mntyya/epoch_10`
2023-12-17 05:34:11 INFO     	 * Num of GPU in use: 1
2023-12-17 05:34:11 INFO     	 * Prefix: True
2023-12-17 05:34:11 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 05:34:12 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 05:45:56 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 05:57:52 INFO     	Bleu_1: 0.11416640256520129
2023-12-17 05:57:52 INFO     	Bleu_2: 0.06287816616209928
2023-12-17 05:57:52 INFO     	Bleu_3: 0.03308098339735222
2023-12-17 05:57:52 INFO     	Bleu_4: 0.020868386581288704
2023-12-17 05:57:52 INFO     	Bleu_1: 0.1116191844712208
2023-12-17 05:57:52 INFO     	Bleu_2: 0.06099462227396811
2023-12-17 05:57:52 INFO     	Bleu_3: 0.031772245849292814
2023-12-17 05:57:52 INFO     	Bleu_4: 0.02000790331482763
2023-12-17 05:57:53 INFO     ## 1st RUN (EVAL): Configuration 5/12 ##
2023-12-17 05:58:06 INFO     use spaCy answer extraction model: positionrank
2023-12-17 05:58:06 INFO     Model `base_trained_ckpt/model_woixzh/epoch_10`
2023-12-17 05:58:06 INFO     	 * Num of GPU in use: 1
2023-12-17 05:58:06 INFO     	 * Prefix: True
2023-12-17 05:58:06 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 05:58:07 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 06:09:54 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 06:21:28 INFO     	Bleu_1: 0.1201260905392848
2023-12-17 06:21:28 INFO     	Bleu_2: 0.06543694482517014
2023-12-17 06:21:28 INFO     	Bleu_3: 0.03373785907999485
2023-12-17 06:21:28 INFO     	Bleu_4: 0.020898815226956035
2023-12-17 06:21:28 INFO     	Bleu_1: 0.11685513921954174
2023-12-17 06:21:28 INFO     	Bleu_2: 0.06389872964029243
2023-12-17 06:21:28 INFO     	Bleu_3: 0.033588963663628235
2023-12-17 06:21:28 INFO     	Bleu_4: 0.02129202091043798
2023-12-17 06:21:28 INFO     ## 1st RUN (EVAL): Configuration 6/12 ##
2023-12-17 06:21:43 INFO     use spaCy answer extraction model: positionrank
2023-12-17 06:21:43 INFO     Model `base_trained_ckpt/model_sdkaaa/epoch_10`
2023-12-17 06:21:43 INFO     	 * Num of GPU in use: 1
2023-12-17 06:21:43 INFO     	 * Prefix: True
2023-12-17 06:21:43 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 06:21:44 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 06:33:40 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 06:45:24 INFO     	Bleu_1: 0.11416640256520129
2023-12-17 06:45:24 INFO     	Bleu_2: 0.06287816616209928
2023-12-17 06:45:24 INFO     	Bleu_3: 0.03308098339735222
2023-12-17 06:45:24 INFO     	Bleu_4: 0.020868386581288704
2023-12-17 06:45:24 INFO     	Bleu_1: 0.1116191844712208
2023-12-17 06:45:24 INFO     	Bleu_2: 0.06099462227396811
2023-12-17 06:45:24 INFO     	Bleu_3: 0.031772245849292814
2023-12-17 06:45:24 INFO     	Bleu_4: 0.02000790331482763
2023-12-17 06:45:24 INFO     ## 1st RUN (EVAL): Configuration 7/12 ##
2023-12-17 06:45:40 INFO     use spaCy answer extraction model: positionrank
2023-12-17 06:45:40 INFO     Model `base_trained_ckpt/model_uramvg/epoch_10`
2023-12-17 06:45:40 INFO     	 * Num of GPU in use: 1
2023-12-17 06:45:40 INFO     	 * Prefix: True
2023-12-17 06:45:40 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 06:45:42 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 06:57:17 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 07:08:59 INFO     	Bleu_1: 0.1201260905392848
2023-12-17 07:08:59 INFO     	Bleu_2: 0.06543694482517014
2023-12-17 07:08:59 INFO     	Bleu_3: 0.03373785907999485
2023-12-17 07:08:59 INFO     	Bleu_4: 0.020898815226956035
2023-12-17 07:09:00 INFO     	Bleu_1: 0.11685513921954174
2023-12-17 07:09:00 INFO     	Bleu_2: 0.06389872964029243
2023-12-17 07:09:00 INFO     	Bleu_3: 0.033588963663628235
2023-12-17 07:09:00 INFO     	Bleu_4: 0.02129202091043798
2023-12-17 07:09:00 INFO     ## 1st RUN (EVAL): Configuration 8/12 ##
2023-12-17 07:09:14 INFO     use spaCy answer extraction model: positionrank
2023-12-17 07:09:15 INFO     Model `base_trained_ckpt/model_nxaqhy/epoch_10`
2023-12-17 07:09:15 INFO     	 * Num of GPU in use: 1
2023-12-17 07:09:15 INFO     	 * Prefix: True
2023-12-17 07:09:15 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 07:09:16 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 07:21:07 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 07:32:56 INFO     	Bleu_1: 0.12320423976303857
2023-12-17 07:32:56 INFO     	Bleu_2: 0.06763996290192302
2023-12-17 07:32:56 INFO     	Bleu_3: 0.035462282398871905
2023-12-17 07:32:56 INFO     	Bleu_4: 0.02203525356928333
2023-12-17 07:32:57 INFO     	Bleu_1: 0.12201972331602523
2023-12-17 07:32:57 INFO     	Bleu_2: 0.06658087997374813
2023-12-17 07:32:57 INFO     	Bleu_3: 0.03461745684126431
2023-12-17 07:32:57 INFO     	Bleu_4: 0.021644420447928272
2023-12-17 07:32:57 INFO     ## 1st RUN (EVAL): Configuration 9/12 ##
2023-12-17 07:33:13 INFO     use spaCy answer extraction model: positionrank
2023-12-17 07:33:13 INFO     Model `base_trained_ckpt/model_oprhlh/epoch_10`
2023-12-17 07:33:13 INFO     	 * Num of GPU in use: 1
2023-12-17 07:33:13 INFO     	 * Prefix: True
2023-12-17 07:33:13 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 07:33:15 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 07:45:13 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 07:57:03 INFO     	Bleu_1: 0.1167260471834369
2023-12-17 07:57:03 INFO     	Bleu_2: 0.06381692327675838
2023-12-17 07:57:03 INFO     	Bleu_3: 0.03295605991669857
2023-12-17 07:57:03 INFO     	Bleu_4: 0.020492472774531374
2023-12-17 07:57:04 INFO     	Bleu_1: 0.11711117149992807
2023-12-17 07:57:04 INFO     	Bleu_2: 0.06405642980825495
2023-12-17 07:57:04 INFO     	Bleu_3: 0.03333580616200328
2023-12-17 07:57:04 INFO     	Bleu_4: 0.020791855202947893
2023-12-17 07:57:04 INFO     ## 1st RUN (EVAL): Configuration 10/12 ##
2023-12-17 07:57:18 INFO     use spaCy answer extraction model: positionrank
2023-12-17 07:57:19 INFO     Model `base_trained_ckpt/model_vhyoja/epoch_10`
2023-12-17 07:57:19 INFO     	 * Num of GPU in use: 1
2023-12-17 07:57:19 INFO     	 * Prefix: True
2023-12-17 07:57:19 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 07:57:20 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 08:09:10 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 08:21:08 INFO     	Bleu_1: 0.12320423976303857
2023-12-17 08:21:08 INFO     	Bleu_2: 0.06763996290192302
2023-12-17 08:21:08 INFO     	Bleu_3: 0.035462282398871905
2023-12-17 08:21:08 INFO     	Bleu_4: 0.02203525356928333
2023-12-17 08:21:08 INFO     	Bleu_1: 0.12201972331602523
2023-12-17 08:21:08 INFO     	Bleu_2: 0.06658087997374813
2023-12-17 08:21:08 INFO     	Bleu_3: 0.03461745684126431
2023-12-17 08:21:08 INFO     	Bleu_4: 0.021644420447928272
2023-12-17 08:21:08 INFO     ## 1st RUN (EVAL): Configuration 11/12 ##
2023-12-17 08:21:22 INFO     use spaCy answer extraction model: positionrank
2023-12-17 08:21:22 INFO     Model `base_trained_ckpt/model_nrudfu/epoch_10`
2023-12-17 08:21:22 INFO     	 * Num of GPU in use: 1
2023-12-17 08:21:22 INFO     	 * Prefix: True
2023-12-17 08:21:22 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 08:21:23 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 08:33:08 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 08:44:57 INFO     	Bleu_1: 0.1167260471834369
2023-12-17 08:44:57 INFO     	Bleu_2: 0.06381692327675838
2023-12-17 08:44:57 INFO     	Bleu_3: 0.03295605991669857
2023-12-17 08:44:57 INFO     	Bleu_4: 0.020492472774531374
2023-12-17 08:44:57 INFO     	Bleu_1: 0.11711117149992807
2023-12-17 08:44:57 INFO     	Bleu_2: 0.06405642980825495
2023-12-17 08:44:57 INFO     	Bleu_3: 0.03333580616200328
2023-12-17 08:44:57 INFO     	Bleu_4: 0.020791855202947893
2023-12-17 08:44:57 INFO     1st RUN RESULTS (validation/Bleu_4)
2023-12-17 08:44:57 INFO     	 * rank: 0 | metric: 0.024 | model: base_trained_ckpt/model_eszyci/epoch_10 |
2023-12-17 08:44:57 INFO     	 * rank: 1 | metric: 0.024 | model: base_trained_ckpt/model_mzgdpa/epoch_10 |
2023-12-17 08:44:57 INFO     	 * rank: 2 | metric: 0.023 | model: base_trained_ckpt/model_rillvb/epoch_10 |
2023-12-17 08:44:57 INFO     	 * rank: 3 | metric: 0.023 | model: base_trained_ckpt/model_dpyopu/epoch_10 |
2023-12-17 08:44:57 INFO     	 * rank: 4 | metric: 0.022 | model: base_trained_ckpt/model_nxaqhy/epoch_10 |
2023-12-17 08:44:57 INFO     	 * rank: 5 | metric: 0.022 | model: base_trained_ckpt/model_vhyoja/epoch_10 |
2023-12-17 08:44:57 INFO     	 * rank: 6 | metric: 0.021 | model: base_trained_ckpt/model_woixzh/epoch_10 |
2023-12-17 08:44:57 INFO     	 * rank: 7 | metric: 0.021 | model: base_trained_ckpt/model_uramvg/epoch_10 |
2023-12-17 08:44:57 INFO     	 * rank: 8 | metric: 0.021 | model: base_trained_ckpt/model_mntyya/epoch_10 |
2023-12-17 08:44:57 INFO     	 * rank: 9 | metric: 0.021 | model: base_trained_ckpt/model_sdkaaa/epoch_10 |
2023-12-17 08:44:57 INFO     	 * rank: 10 | metric: 0.02 | model: base_trained_ckpt/model_oprhlh/epoch_10 |
2023-12-17 08:44:57 INFO     	 * rank: 11 | metric: 0.02 | model: base_trained_ckpt/model_nrudfu/epoch_10 |
2023-12-17 08:44:57 INFO     ## 2nd RUN: Configuration 0/5: validation/Bleu_4 = 0.024054985672499824
2023-12-17 08:44:57 INFO     initialize model trainer
2023-12-17 08:44:57 INFO     load config from existing checkpoint at base_trained_ckpt/model_eszyci
2023-12-17 08:44:57 INFO     hyperparameters
2023-12-17 08:44:57 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-17 08:44:57 INFO     	 * dataset_name: default
2023-12-17 08:44:57 INFO     	 * input_types: ['paragraph']
2023-12-17 08:44:57 INFO     	 * output_types: ['questions_answers']
2023-12-17 08:44:57 INFO     	 * prefix_types: ['qag']
2023-12-17 08:44:57 INFO     	 * model: t5-base
2023-12-17 08:44:57 INFO     	 * max_length: 512
2023-12-17 08:44:57 INFO     	 * max_length_output: 512
2023-12-17 08:44:57 INFO     	 * epoch: 15
2023-12-17 08:44:57 INFO     	 * batch: 2
2023-12-17 08:44:57 INFO     	 * lr: 0.0001
2023-12-17 08:44:57 INFO     	 * fp16: False
2023-12-17 08:44:57 INFO     	 * random_seed: 1
2023-12-17 08:44:57 INFO     	 * gradient_accumulation_steps: 2
2023-12-17 08:44:57 INFO     	 * label_smoothing: 0.15
2023-12-17 08:44:57 INFO     load checkpoint from base_trained_ckpt/model_eszyci/epoch_10
2023-12-17 08:44:59 INFO     use spaCy answer extraction model: positionrank
2023-12-17 08:44:59 INFO     Model `base_trained_ckpt/model_eszyci/epoch_10`
2023-12-17 08:44:59 INFO     	 * Num of GPU in use: 1
2023-12-17 08:44:59 INFO     	 * Prefix: True
2023-12-17 08:44:59 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 08:44:59 INFO     load optimizer from base_trained_ckpt/model_eszyci/optimizers/optimizer.10.pt
2023-12-17 08:44:59 INFO     optimizer is loading on cuda
2023-12-17 08:45:24 INFO     dataset preprocessing
2023-12-17 08:45:25 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-base.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-17 08:45:26 INFO     start model training
2023-12-17 08:45:51 INFO     	 * (global step 50: loss: 2.3966405391693115, lr: 0.0001
2023-12-17 08:46:15 INFO     	 * (global step 100: loss: 2.36001980304718, lr: 0.0001
2023-12-17 08:46:40 INFO     	 * (global step 150: loss: 2.3187566995620728, lr: 0.0001
2023-12-17 08:47:05 INFO     	 * (global step 200: loss: 2.3584107160568237, lr: 0.0001
2023-12-17 08:47:30 INFO     	 * (global step 250: loss: 2.348168969154358, lr: 0.0001
2023-12-17 08:47:55 INFO     	 * (global step 300: loss: 2.263358235359192, lr: 0.0001
2023-12-17 08:48:20 INFO     	 * (global step 350: loss: 2.3206595182418823, lr: 0.0001
2023-12-17 08:48:45 INFO     	 * (global step 400: loss: 2.305180549621582, lr: 0.0001
2023-12-17 08:49:10 INFO     	 * (global step 450: loss: 2.3153923749923706, lr: 0.0001
2023-12-17 08:49:36 INFO     	 * (global step 500: loss: 2.3247708082199097, lr: 0.0001
2023-12-17 08:50:01 INFO     	 * (global step 550: loss: 2.382885456085205, lr: 0.0001
2023-12-17 08:50:26 INFO     	 * (global step 600: loss: 2.2383464574813843, lr: 0.0001
2023-12-17 08:50:37 INFO     [epoch 10/15] average loss: 2.36, lr: 0.0001
2023-12-17 08:50:37 INFO     saving model related files
2023-12-17 08:50:37 INFO     saving model
2023-12-17 08:50:39 INFO     saving tokenizer
2023-12-17 08:50:39 INFO     saving optimizer
2023-12-17 08:50:42 INFO     remove old optimizer files
2023-12-17 08:50:57 INFO     	 * (global step 650: loss: 2.2340712547302246, lr: 0.0001
2023-12-17 08:51:22 INFO     	 * (global step 700: loss: 2.244364023208618, lr: 0.0001
2023-12-17 08:51:46 INFO     	 * (global step 750: loss: 2.2192355394363403, lr: 0.0001
2023-12-17 08:52:11 INFO     	 * (global step 800: loss: 2.2315781116485596, lr: 0.0001
2023-12-17 08:52:36 INFO     	 * (global step 850: loss: 2.2231959104537964, lr: 0.0001
2023-12-17 08:53:01 INFO     	 * (global step 900: loss: 2.2570765018463135, lr: 0.0001
2023-12-17 08:53:26 INFO     	 * (global step 950: loss: 2.255845069885254, lr: 0.0001
2023-12-17 08:53:52 INFO     	 * (global step 1000: loss: 2.240539789199829, lr: 0.0001
2023-12-17 08:54:17 INFO     	 * (global step 1050: loss: 2.2418935298919678, lr: 0.0001
2023-12-17 08:54:42 INFO     	 * (global step 1100: loss: 2.246895909309387, lr: 0.0001
2023-12-17 08:55:07 INFO     	 * (global step 1150: loss: 2.184564471244812, lr: 0.0001
2023-12-17 08:55:32 INFO     	 * (global step 1200: loss: 2.3105998039245605, lr: 0.0001
2023-12-17 08:55:53 INFO     [epoch 11/15] average loss: 2.253, lr: 0.0001
2023-12-17 08:55:53 INFO     saving model related files
2023-12-17 08:55:53 INFO     saving model
2023-12-17 08:55:55 INFO     saving tokenizer
2023-12-17 08:55:55 INFO     saving optimizer
2023-12-17 08:55:58 INFO     remove old optimizer files
2023-12-17 08:56:02 INFO     	 * (global step 1250: loss: 2.2781379222869873, lr: 0.0001
2023-12-17 08:56:27 INFO     	 * (global step 1300: loss: 2.1897257566452026, lr: 0.0001
2023-12-17 08:56:52 INFO     	 * (global step 1350: loss: 2.2207255363464355, lr: 0.0001
2023-12-17 08:57:17 INFO     	 * (global step 1400: loss: 2.308404803276062, lr: 0.0001
2023-12-17 08:57:42 INFO     	 * (global step 1450: loss: 2.149168372154236, lr: 0.0001
2023-12-17 08:58:07 INFO     	 * (global step 1500: loss: 2.295396566390991, lr: 0.0001
2023-12-17 08:58:32 INFO     	 * (global step 1550: loss: 2.229544162750244, lr: 0.0001
2023-12-17 08:58:57 INFO     	 * (global step 1600: loss: 2.1887673139572144, lr: 0.0001
2023-12-17 08:59:22 INFO     	 * (global step 1650: loss: 2.2530550956726074, lr: 0.0001
2023-12-17 08:59:47 INFO     	 * (global step 1700: loss: 2.2860116958618164, lr: 0.0001
2023-12-17 09:00:11 INFO     	 * (global step 1750: loss: 2.159570574760437, lr: 0.0001
2023-12-17 09:00:36 INFO     	 * (global step 1800: loss: 2.2938296794891357, lr: 0.0001
2023-12-17 09:01:01 INFO     	 * (global step 1850: loss: 2.2172731161117554, lr: 0.0001
2023-12-17 09:01:08 INFO     [epoch 12/15] average loss: 2.232, lr: 0.0001
2023-12-17 09:01:08 INFO     saving model related files
2023-12-17 09:01:08 INFO     saving model
2023-12-17 09:01:09 INFO     saving tokenizer
2023-12-17 09:01:09 INFO     saving optimizer
2023-12-17 09:01:13 INFO     remove old optimizer files
2023-12-17 09:01:32 INFO     	 * (global step 1900: loss: 2.18688428401947, lr: 0.0001
2023-12-17 09:01:56 INFO     	 * (global step 1950: loss: 2.3286396265029907, lr: 0.0001
2023-12-17 09:02:21 INFO     	 * (global step 2000: loss: 2.2122875452041626, lr: 0.0001
2023-12-17 09:02:46 INFO     	 * (global step 2050: loss: 2.228694200515747, lr: 0.0001
2023-12-17 09:03:11 INFO     	 * (global step 2100: loss: 2.2249865531921387, lr: 0.0001
2023-12-17 09:03:35 INFO     	 * (global step 2150: loss: 2.166115641593933, lr: 0.0001
2023-12-17 09:04:00 INFO     	 * (global step 2200: loss: 2.197351813316345, lr: 0.0001
2023-12-17 09:04:25 INFO     	 * (global step 2250: loss: 2.2049049139022827, lr: 0.0001
2023-12-17 09:04:50 INFO     	 * (global step 2300: loss: 2.204002022743225, lr: 0.0001
2023-12-17 09:05:15 INFO     	 * (global step 2350: loss: 2.1579126119613647, lr: 0.0001
2023-12-17 09:05:40 INFO     	 * (global step 2400: loss: 2.2173086404800415, lr: 0.0001
2023-12-17 09:06:05 INFO     	 * (global step 2450: loss: 2.165943145751953, lr: 0.0001
2023-12-17 09:06:22 INFO     [epoch 13/15] average loss: 2.217, lr: 0.0001
2023-12-17 09:06:22 INFO     saving model related files
2023-12-17 09:06:22 INFO     saving model
2023-12-17 09:06:23 INFO     saving tokenizer
2023-12-17 09:06:23 INFO     saving optimizer
2023-12-17 09:06:26 INFO     remove old optimizer files
2023-12-17 09:06:35 INFO     	 * (global step 2500: loss: 2.3060226440429688, lr: 0.0001
2023-12-17 09:06:59 INFO     	 * (global step 2550: loss: 2.2537155151367188, lr: 0.0001
2023-12-17 09:07:24 INFO     	 * (global step 2600: loss: 2.220964789390564, lr: 0.0001
2023-12-17 09:07:49 INFO     	 * (global step 2650: loss: 2.1754653453826904, lr: 0.0001
2023-12-17 09:08:14 INFO     	 * (global step 2700: loss: 2.1759426593780518, lr: 0.0001
2023-12-17 09:08:38 INFO     	 * (global step 2750: loss: 2.1927120685577393, lr: 0.0001
2023-12-17 09:09:03 INFO     	 * (global step 2800: loss: 2.200513005256653, lr: 0.0001
2023-12-17 09:09:28 INFO     	 * (global step 2850: loss: 2.1730509996414185, lr: 0.0001
2023-12-17 09:09:53 INFO     	 * (global step 2900: loss: 2.1654226779937744, lr: 0.0001
2023-12-17 09:10:18 INFO     	 * (global step 2950: loss: 2.214365839958191, lr: 0.0001
2023-12-17 09:10:42 INFO     	 * (global step 3000: loss: 2.1796047687530518, lr: 0.0001
2023-12-17 09:11:07 INFO     	 * (global step 3050: loss: 2.1885677576065063, lr: 0.0001
2023-12-17 09:11:32 INFO     	 * (global step 3100: loss: 2.208772897720337, lr: 0.0001
2023-12-17 09:11:35 INFO     [epoch 14/15] average loss: 2.205, lr: 0.0001
2023-12-17 09:11:35 INFO     saving model related files
2023-12-17 09:11:35 INFO     saving model
2023-12-17 09:11:36 INFO     saving tokenizer
2023-12-17 09:11:36 INFO     saving optimizer
2023-12-17 09:11:39 INFO     remove old optimizer files
2023-12-17 09:11:40 INFO     complete training: model ckpt was saved at base_trained_ckpt/model_eszyci
2023-12-17 09:11:40 INFO     ## 2nd RUN: Configuration 1/5: validation/Bleu_4 = 0.024054985672499824
2023-12-17 09:11:40 INFO     initialize model trainer
2023-12-17 09:11:40 INFO     load config from existing checkpoint at base_trained_ckpt/model_mzgdpa
2023-12-17 09:11:40 INFO     hyperparameters
2023-12-17 09:11:40 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-17 09:11:40 INFO     	 * dataset_name: default
2023-12-17 09:11:40 INFO     	 * input_types: ['paragraph']
2023-12-17 09:11:40 INFO     	 * output_types: ['questions_answers']
2023-12-17 09:11:40 INFO     	 * prefix_types: ['qag']
2023-12-17 09:11:40 INFO     	 * model: t5-base
2023-12-17 09:11:40 INFO     	 * max_length: 512
2023-12-17 09:11:40 INFO     	 * max_length_output: 512
2023-12-17 09:11:40 INFO     	 * epoch: 15
2023-12-17 09:11:40 INFO     	 * batch: 2
2023-12-17 09:11:40 INFO     	 * lr: 0.0001
2023-12-17 09:11:40 INFO     	 * fp16: False
2023-12-17 09:11:40 INFO     	 * random_seed: 1
2023-12-17 09:11:40 INFO     	 * gradient_accumulation_steps: 2
2023-12-17 09:11:40 INFO     	 * label_smoothing: 0.0
2023-12-17 09:11:40 INFO     load checkpoint from base_trained_ckpt/model_mzgdpa/epoch_10
2023-12-17 09:11:53 INFO     use spaCy answer extraction model: positionrank
2023-12-17 09:11:53 INFO     Model `base_trained_ckpt/model_mzgdpa/epoch_10`
2023-12-17 09:11:53 INFO     	 * Num of GPU in use: 1
2023-12-17 09:11:53 INFO     	 * Prefix: True
2023-12-17 09:11:53 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 09:11:53 INFO     load optimizer from base_trained_ckpt/model_mzgdpa/optimizers/optimizer.10.pt
2023-12-17 09:11:53 INFO     optimizer is loading on cuda
2023-12-17 09:12:13 INFO     dataset preprocessing
2023-12-17 09:12:14 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-base.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-17 09:12:15 INFO     start model training
2023-12-17 09:12:39 INFO     	 * (global step 50: loss: 0.11940713971853256, lr: 0.0001
2023-12-17 09:13:03 INFO     	 * (global step 100: loss: 0.15376268327236176, lr: 0.0001
2023-12-17 09:13:27 INFO     	 * (global step 150: loss: 0.13922832906246185, lr: 0.0001
2023-12-17 09:13:52 INFO     	 * (global step 200: loss: 0.21893306076526642, lr: 0.0001
2023-12-17 09:14:17 INFO     	 * (global step 250: loss: 0.2010175660252571, lr: 0.0001
2023-12-17 09:14:41 INFO     	 * (global step 300: loss: 0.14613600820302963, lr: 0.0001
2023-12-17 09:15:06 INFO     	 * (global step 350: loss: 0.18983934819698334, lr: 0.0001
2023-12-17 09:15:30 INFO     	 * (global step 400: loss: 0.1938570961356163, lr: 0.0001
2023-12-17 09:15:55 INFO     	 * (global step 450: loss: 0.20661043375730515, lr: 0.0001
2023-12-17 09:16:20 INFO     	 * (global step 500: loss: 0.20763897150754929, lr: 0.0001
2023-12-17 09:16:44 INFO     	 * (global step 550: loss: 0.27633725106716156, lr: 0.0001
2023-12-17 09:17:09 INFO     	 * (global step 600: loss: 0.14320475235581398, lr: 0.0001
2023-12-17 09:17:19 INFO     [epoch 10/15] average loss: 0.18, lr: 0.0001
2023-12-17 09:17:19 INFO     saving model related files
2023-12-17 09:17:19 INFO     saving model
2023-12-17 09:17:21 INFO     saving tokenizer
2023-12-17 09:17:21 INFO     saving optimizer
2023-12-17 09:17:25 INFO     remove old optimizer files
2023-12-17 09:17:39 INFO     	 * (global step 650: loss: 0.13043947517871857, lr: 0.0001
2023-12-17 09:18:04 INFO     	 * (global step 700: loss: 0.1546272560954094, lr: 0.0001
2023-12-17 09:18:29 INFO     	 * (global step 750: loss: 0.13429025560617447, lr: 0.0001
2023-12-17 09:18:53 INFO     	 * (global step 800: loss: 0.14591453596949577, lr: 0.0001
2023-12-17 09:19:18 INFO     	 * (global step 850: loss: 0.14636564999818802, lr: 0.0001
2023-12-17 09:19:43 INFO     	 * (global step 900: loss: 0.18034743517637253, lr: 0.0001
2023-12-17 09:20:07 INFO     	 * (global step 950: loss: 0.16405704617500305, lr: 0.0001
2023-12-17 09:20:32 INFO     	 * (global step 1000: loss: 0.16603348031640053, lr: 0.0001
2023-12-17 09:20:57 INFO     	 * (global step 1050: loss: 0.16731256246566772, lr: 0.0001
2023-12-17 09:21:22 INFO     	 * (global step 1100: loss: 0.1678347885608673, lr: 0.0001
2023-12-17 09:21:46 INFO     	 * (global step 1150: loss: 0.12124864757061005, lr: 0.0001
2023-12-17 09:22:11 INFO     	 * (global step 1200: loss: 0.228452667593956, lr: 0.0001
2023-12-17 09:22:32 INFO     [epoch 11/15] average loss: 0.17, lr: 0.0001
2023-12-17 09:22:32 INFO     saving model related files
2023-12-17 09:22:32 INFO     saving model
2023-12-17 09:22:34 INFO     saving tokenizer
2023-12-17 09:22:34 INFO     saving optimizer
2023-12-17 09:22:37 INFO     remove old optimizer files
2023-12-17 09:22:42 INFO     	 * (global step 1250: loss: 0.18603383749723434, lr: 0.0001
2023-12-17 09:23:06 INFO     	 * (global step 1300: loss: 0.11682026460766792, lr: 0.0001
2023-12-17 09:23:31 INFO     	 * (global step 1350: loss: 0.14399442076683044, lr: 0.0001
2023-12-17 09:23:56 INFO     	 * (global step 1400: loss: 0.22304093092679977, lr: 0.0001
2023-12-17 09:24:20 INFO     	 * (global step 1450: loss: 0.09131782874464989, lr: 0.0001
2023-12-17 09:24:45 INFO     	 * (global step 1500: loss: 0.223374143242836, lr: 0.0001
2023-12-17 09:25:10 INFO     	 * (global step 1550: loss: 0.15454672276973724, lr: 0.0001
2023-12-17 09:25:35 INFO     	 * (global step 1600: loss: 0.13001694530248642, lr: 0.0001
2023-12-17 09:26:00 INFO     	 * (global step 1650: loss: 0.16382432729005814, lr: 0.0001
2023-12-17 09:26:24 INFO     	 * (global step 1700: loss: 0.20330026000738144, lr: 0.0001
2023-12-17 09:26:49 INFO     	 * (global step 1750: loss: 0.09897156432271004, lr: 0.0001
2023-12-17 09:27:14 INFO     	 * (global step 1800: loss: 0.21727126836776733, lr: 0.0001
2023-12-17 09:27:39 INFO     	 * (global step 1850: loss: 0.1482665203511715, lr: 0.0001
2023-12-17 09:27:46 INFO     [epoch 12/15] average loss: 0.161, lr: 0.0001
2023-12-17 09:27:46 INFO     saving model related files
2023-12-17 09:27:46 INFO     saving model
2023-12-17 09:27:47 INFO     saving tokenizer
2023-12-17 09:27:47 INFO     saving optimizer
2023-12-17 09:27:51 INFO     remove old optimizer files
2023-12-17 09:28:10 INFO     	 * (global step 1900: loss: 0.11802627891302109, lr: 0.0001
2023-12-17 09:28:34 INFO     	 * (global step 1950: loss: 0.2472730055451393, lr: 0.0001
2023-12-17 09:28:59 INFO     	 * (global step 2000: loss: 0.1468379721045494, lr: 0.0001
2023-12-17 09:29:24 INFO     	 * (global step 2050: loss: 0.15370740741491318, lr: 0.0001
2023-12-17 09:29:49 INFO     	 * (global step 2100: loss: 0.15540281683206558, lr: 0.0001
2023-12-17 09:30:13 INFO     	 * (global step 2150: loss: 0.11457202583551407, lr: 0.0001
2023-12-17 09:30:38 INFO     	 * (global step 2200: loss: 0.1416843831539154, lr: 0.0001
2023-12-17 09:31:03 INFO     	 * (global step 2250: loss: 0.14619170501828194, lr: 0.0001
2023-12-17 09:31:28 INFO     	 * (global step 2300: loss: 0.15029159933328629, lr: 0.0001
2023-12-17 09:31:52 INFO     	 * (global step 2350: loss: 0.10513399541378021, lr: 0.0001
2023-12-17 09:32:17 INFO     	 * (global step 2400: loss: 0.14535601437091827, lr: 0.0001
2023-12-17 09:32:42 INFO     	 * (global step 2450: loss: 0.10236688703298569, lr: 0.0001
2023-12-17 09:32:59 INFO     [epoch 13/15] average loss: 0.152, lr: 0.0001
2023-12-17 09:32:59 INFO     saving model related files
2023-12-17 09:32:59 INFO     saving model
2023-12-17 09:33:01 INFO     saving tokenizer
2023-12-17 09:33:01 INFO     saving optimizer
2023-12-17 09:33:04 INFO     remove old optimizer files
2023-12-17 09:33:12 INFO     	 * (global step 2500: loss: 0.24044649675488472, lr: 0.0001
2023-12-17 09:33:37 INFO     	 * (global step 2550: loss: 0.17040523141622543, lr: 0.0001
2023-12-17 09:34:02 INFO     	 * (global step 2600: loss: 0.17015857994556427, lr: 0.0001
2023-12-17 09:34:26 INFO     	 * (global step 2650: loss: 0.12985409796237946, lr: 0.0001
2023-12-17 09:34:51 INFO     	 * (global step 2700: loss: 0.11327336728572845, lr: 0.0001
2023-12-17 09:35:16 INFO     	 * (global step 2750: loss: 0.13526912406086922, lr: 0.0001
2023-12-17 09:35:41 INFO     	 * (global step 2800: loss: 0.13490764424204826, lr: 0.0001
2023-12-17 09:36:05 INFO     	 * (global step 2850: loss: 0.10927009955048561, lr: 0.0001
2023-12-17 09:36:30 INFO     	 * (global step 2900: loss: 0.11311659589409828, lr: 0.0001
2023-12-17 09:36:55 INFO     	 * (global step 2950: loss: 0.1647758111357689, lr: 0.0001
2023-12-17 09:37:20 INFO     	 * (global step 3000: loss: 0.10770388692617416, lr: 0.0001
2023-12-17 09:37:44 INFO     	 * (global step 3050: loss: 0.12535925209522247, lr: 0.0001
2023-12-17 09:38:09 INFO     	 * (global step 3100: loss: 0.14955496788024902, lr: 0.0001
2023-12-17 09:38:11 INFO     [epoch 14/15] average loss: 0.144, lr: 0.0001
2023-12-17 09:38:11 INFO     saving model related files
2023-12-17 09:38:11 INFO     saving model
2023-12-17 09:38:13 INFO     saving tokenizer
2023-12-17 09:38:13 INFO     saving optimizer
2023-12-17 09:38:16 INFO     remove old optimizer files
2023-12-17 09:38:16 INFO     complete training: model ckpt was saved at base_trained_ckpt/model_mzgdpa
2023-12-17 09:38:16 INFO     ## 2nd RUN: Configuration 2/5: validation/Bleu_4 = 0.022800842017452354
2023-12-17 09:38:16 INFO     initialize model trainer
2023-12-17 09:38:16 INFO     load config from existing checkpoint at base_trained_ckpt/model_rillvb
2023-12-17 09:38:16 INFO     hyperparameters
2023-12-17 09:38:16 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-17 09:38:16 INFO     	 * dataset_name: default
2023-12-17 09:38:16 INFO     	 * input_types: ['paragraph']
2023-12-17 09:38:16 INFO     	 * output_types: ['questions_answers']
2023-12-17 09:38:16 INFO     	 * prefix_types: ['qag']
2023-12-17 09:38:16 INFO     	 * model: t5-base
2023-12-17 09:38:16 INFO     	 * max_length: 512
2023-12-17 09:38:16 INFO     	 * max_length_output: 512
2023-12-17 09:38:16 INFO     	 * epoch: 15
2023-12-17 09:38:16 INFO     	 * batch: 2
2023-12-17 09:38:16 INFO     	 * lr: 0.0001
2023-12-17 09:38:16 INFO     	 * fp16: False
2023-12-17 09:38:16 INFO     	 * random_seed: 1
2023-12-17 09:38:16 INFO     	 * gradient_accumulation_steps: 4
2023-12-17 09:38:16 INFO     	 * label_smoothing: 0.15
2023-12-17 09:38:16 INFO     load checkpoint from base_trained_ckpt/model_rillvb/epoch_10
2023-12-17 09:38:27 INFO     use spaCy answer extraction model: positionrank
2023-12-17 09:38:28 INFO     Model `base_trained_ckpt/model_rillvb/epoch_10`
2023-12-17 09:38:28 INFO     	 * Num of GPU in use: 1
2023-12-17 09:38:28 INFO     	 * Prefix: True
2023-12-17 09:38:28 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 09:38:28 INFO     load optimizer from base_trained_ckpt/model_rillvb/optimizers/optimizer.10.pt
2023-12-17 09:38:28 INFO     optimizer is loading on cuda
2023-12-17 09:38:49 INFO     dataset preprocessing
2023-12-17 09:38:50 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-base.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-17 09:38:50 INFO     start model training
2023-12-17 09:39:37 INFO     	 * (global step 50: loss: 2.4710254073143005, lr: 0.0001
2023-12-17 09:40:25 INFO     	 * (global step 100: loss: 2.4597795009613037, lr: 0.0001
2023-12-17 09:41:12 INFO     	 * (global step 150: loss: 2.3465856313705444, lr: 0.0001
2023-12-17 09:42:00 INFO     	 * (global step 200: loss: 2.3568792939186096, lr: 0.0001
2023-12-17 09:42:48 INFO     	 * (global step 250: loss: 2.35142719745636, lr: 0.0001
2023-12-17 09:43:35 INFO     	 * (global step 300: loss: 2.280101180076599, lr: 0.0001
2023-12-17 09:43:46 INFO     [epoch 10/15] average loss: 2.419, lr: 0.0001
2023-12-17 09:43:46 INFO     saving model related files
2023-12-17 09:43:46 INFO     saving model
2023-12-17 09:43:47 INFO     saving tokenizer
2023-12-17 09:43:47 INFO     saving optimizer
2023-12-17 09:43:50 INFO     remove old optimizer files
2023-12-17 09:44:28 INFO     	 * (global step 350: loss: 2.2542447447776794, lr: 0.0001
2023-12-17 09:45:16 INFO     	 * (global step 400: loss: 2.2682695388793945, lr: 0.0001
2023-12-17 09:46:04 INFO     	 * (global step 450: loss: 2.2820961475372314, lr: 0.0001
2023-12-17 09:46:51 INFO     	 * (global step 500: loss: 2.27741676568985, lr: 0.0001
2023-12-17 09:47:39 INFO     	 * (global step 550: loss: 2.266827940940857, lr: 0.0001
2023-12-17 09:48:26 INFO     	 * (global step 600: loss: 2.3732038140296936, lr: 0.0001
2023-12-17 09:48:46 INFO     [epoch 11/15] average loss: 2.291, lr: 0.0001
2023-12-17 09:48:46 INFO     saving model related files
2023-12-17 09:48:46 INFO     saving model
2023-12-17 09:48:48 INFO     saving tokenizer
2023-12-17 09:48:48 INFO     saving optimizer
2023-12-17 09:48:51 INFO     remove old optimizer files
2023-12-17 09:49:20 INFO     	 * (global step 650: loss: 2.2824788689613342, lr: 0.0001
2023-12-17 09:50:07 INFO     	 * (global step 700: loss: 2.1946833729743958, lr: 0.0001
2023-12-17 09:50:55 INFO     	 * (global step 750: loss: 2.236973226070404, lr: 0.0001
2023-12-17 09:51:42 INFO     	 * (global step 800: loss: 2.258317768573761, lr: 0.0001
2023-12-17 09:52:30 INFO     	 * (global step 850: loss: 2.239055871963501, lr: 0.0001
2023-12-17 09:53:17 INFO     	 * (global step 900: loss: 2.224853515625, lr: 0.0001
2023-12-17 09:53:47 INFO     [epoch 12/15] average loss: 2.269, lr: 0.0001
2023-12-17 09:53:47 INFO     saving model related files
2023-12-17 09:53:47 INFO     saving model
2023-12-17 09:53:48 INFO     saving tokenizer
2023-12-17 09:53:48 INFO     saving optimizer
2023-12-17 09:53:52 INFO     remove old optimizer files
2023-12-17 09:54:11 INFO     	 * (global step 950: loss: 2.286631166934967, lr: 0.0001
2023-12-17 09:54:59 INFO     	 * (global step 1000: loss: 2.2837948203086853, lr: 0.0001
2023-12-17 09:55:46 INFO     	 * (global step 1050: loss: 2.2292388677597046, lr: 0.0001
2023-12-17 09:56:34 INFO     	 * (global step 1100: loss: 2.2203568816184998, lr: 0.0001
2023-12-17 09:57:22 INFO     	 * (global step 1150: loss: 2.292638838291168, lr: 0.0001
2023-12-17 09:58:10 INFO     	 * (global step 1200: loss: 2.187317728996277, lr: 0.0001
2023-12-17 09:58:49 INFO     [epoch 13/15] average loss: 2.254, lr: 0.0001
2023-12-17 09:58:49 INFO     saving model related files
2023-12-17 09:58:49 INFO     saving model
2023-12-17 09:58:50 INFO     saving tokenizer
2023-12-17 09:58:50 INFO     saving optimizer
2023-12-17 09:58:53 INFO     remove old optimizer files
2023-12-17 09:59:03 INFO     	 * (global step 1250: loss: 2.197782039642334, lr: 0.0001
2023-12-17 09:59:51 INFO     	 * (global step 1300: loss: 2.222128748893738, lr: 0.0001
2023-12-17 10:00:38 INFO     	 * (global step 1350: loss: 2.179601550102234, lr: 0.0001
2023-12-17 10:01:26 INFO     	 * (global step 1400: loss: 2.236766815185547, lr: 0.0001
2023-12-17 10:02:14 INFO     	 * (global step 1450: loss: 2.2223487496376038, lr: 0.0001
2023-12-17 10:03:02 INFO     	 * (global step 1500: loss: 2.237441837787628, lr: 0.0001
2023-12-17 10:03:50 INFO     	 * (global step 1550: loss: 2.2376158833503723, lr: 0.0001
2023-12-17 10:03:51 INFO     [epoch 14/15] average loss: 2.242, lr: 0.0001
2023-12-17 10:03:51 INFO     saving model related files
2023-12-17 10:03:51 INFO     saving model
2023-12-17 10:03:53 INFO     saving tokenizer
2023-12-17 10:03:53 INFO     saving optimizer
2023-12-17 10:03:56 INFO     remove old optimizer files
2023-12-17 10:03:57 INFO     complete training: model ckpt was saved at base_trained_ckpt/model_rillvb
2023-12-17 10:03:57 INFO     ## 2nd RUN: Configuration 3/5: validation/Bleu_4 = 0.022800842017452354
2023-12-17 10:03:57 INFO     initialize model trainer
2023-12-17 10:03:57 INFO     load config from existing checkpoint at base_trained_ckpt/model_dpyopu
2023-12-17 10:03:57 INFO     hyperparameters
2023-12-17 10:03:57 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-17 10:03:57 INFO     	 * dataset_name: default
2023-12-17 10:03:57 INFO     	 * input_types: ['paragraph']
2023-12-17 10:03:57 INFO     	 * output_types: ['questions_answers']
2023-12-17 10:03:57 INFO     	 * prefix_types: ['qag']
2023-12-17 10:03:57 INFO     	 * model: t5-base
2023-12-17 10:03:57 INFO     	 * max_length: 512
2023-12-17 10:03:57 INFO     	 * max_length_output: 512
2023-12-17 10:03:57 INFO     	 * epoch: 15
2023-12-17 10:03:57 INFO     	 * batch: 2
2023-12-17 10:03:57 INFO     	 * lr: 0.0001
2023-12-17 10:03:57 INFO     	 * fp16: False
2023-12-17 10:03:57 INFO     	 * random_seed: 1
2023-12-17 10:03:57 INFO     	 * gradient_accumulation_steps: 4
2023-12-17 10:03:57 INFO     	 * label_smoothing: 0.0
2023-12-17 10:03:57 INFO     load checkpoint from base_trained_ckpt/model_dpyopu/epoch_10
2023-12-17 10:04:09 INFO     use spaCy answer extraction model: positionrank
2023-12-17 10:04:10 INFO     Model `base_trained_ckpt/model_dpyopu/epoch_10`
2023-12-17 10:04:10 INFO     	 * Num of GPU in use: 1
2023-12-17 10:04:10 INFO     	 * Prefix: True
2023-12-17 10:04:10 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 10:04:10 INFO     load optimizer from base_trained_ckpt/model_dpyopu/optimizers/optimizer.10.pt
2023-12-17 10:04:10 INFO     optimizer is loading on cuda
2023-12-17 10:04:33 INFO     dataset preprocessing
2023-12-17 10:04:34 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-base.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-17 10:04:34 INFO     start model training
2023-12-17 10:05:21 INFO     	 * (global step 50: loss: 0.22593478485941887, lr: 0.0001
2023-12-17 10:06:08 INFO     	 * (global step 100: loss: 0.2915703058242798, lr: 0.0001
2023-12-17 10:06:55 INFO     	 * (global step 150: loss: 0.21583499014377594, lr: 0.0001
2023-12-17 10:07:42 INFO     	 * (global step 200: loss: 0.2386346273124218, lr: 0.0001
2023-12-17 10:08:30 INFO     	 * (global step 250: loss: 0.23938094079494476, lr: 0.0001
2023-12-17 10:09:17 INFO     	 * (global step 300: loss: 0.19010027311742306, lr: 0.0001
2023-12-17 10:09:27 INFO     [epoch 10/15] average loss: 0.218, lr: 0.0001
2023-12-17 10:09:27 INFO     saving model related files
2023-12-17 10:09:27 INFO     saving model
2023-12-17 10:09:29 INFO     saving tokenizer
2023-12-17 10:09:29 INFO     saving optimizer
2023-12-17 10:09:33 INFO     remove old optimizer files
2023-12-17 10:10:11 INFO     	 * (global step 350: loss: 0.16659412533044815, lr: 0.0001
2023-12-17 10:10:58 INFO     	 * (global step 400: loss: 0.18319011479616165, lr: 0.0001
2023-12-17 10:11:45 INFO     	 * (global step 450: loss: 0.19927959144115448, lr: 0.0001
2023-12-17 10:12:33 INFO     	 * (global step 500: loss: 0.2007295973598957, lr: 0.0001
2023-12-17 10:13:20 INFO     	 * (global step 550: loss: 0.18999365344643593, lr: 0.0001
2023-12-17 10:14:07 INFO     	 * (global step 600: loss: 0.29562075436115265, lr: 0.0001
2023-12-17 10:14:26 INFO     [epoch 11/15] average loss: 0.209, lr: 0.0001
2023-12-17 10:14:26 INFO     saving model related files
2023-12-17 10:14:26 INFO     saving model
2023-12-17 10:14:28 INFO     saving tokenizer
2023-12-17 10:14:28 INFO     saving optimizer
2023-12-17 10:14:32 INFO     remove old optimizer files
2023-12-17 10:15:00 INFO     	 * (global step 650: loss: 0.20499098673462868, lr: 0.0001
2023-12-17 10:15:47 INFO     	 * (global step 700: loss: 0.13031124137341976, lr: 0.0001
2023-12-17 10:16:34 INFO     	 * (global step 750: loss: 0.17542376928031445, lr: 0.0001
2023-12-17 10:17:21 INFO     	 * (global step 800: loss: 0.1871098130941391, lr: 0.0001
2023-12-17 10:18:08 INFO     	 * (global step 850: loss: 0.16818953678011894, lr: 0.0001
2023-12-17 10:18:56 INFO     	 * (global step 900: loss: 0.15365256741642952, lr: 0.0001
2023-12-17 10:19:25 INFO     [epoch 12/15] average loss: 0.199, lr: 0.0001
2023-12-17 10:19:25 INFO     saving model related files
2023-12-17 10:19:25 INFO     saving model
2023-12-17 10:19:26 INFO     saving tokenizer
2023-12-17 10:19:26 INFO     saving optimizer
2023-12-17 10:19:29 INFO     remove old optimizer files
2023-12-17 10:19:48 INFO     	 * (global step 950: loss: 0.21073151379823685, lr: 0.0001
2023-12-17 10:20:35 INFO     	 * (global step 1000: loss: 0.21777218580245972, lr: 0.0001
2023-12-17 10:21:22 INFO     	 * (global step 1050: loss: 0.16513985209167004, lr: 0.0001
2023-12-17 10:22:10 INFO     	 * (global step 1100: loss: 0.16096151061356068, lr: 0.0001
2023-12-17 10:22:57 INFO     	 * (global step 1150: loss: 0.23180443421006203, lr: 0.0001
2023-12-17 10:23:44 INFO     	 * (global step 1200: loss: 0.1396432127803564, lr: 0.0001
2023-12-17 10:24:22 INFO     [epoch 13/15] average loss: 0.19, lr: 0.0001
2023-12-17 10:24:22 INFO     saving model related files
2023-12-17 10:24:22 INFO     saving model
2023-12-17 10:24:24 INFO     saving tokenizer
2023-12-17 10:24:24 INFO     saving optimizer
2023-12-17 10:24:27 INFO     remove old optimizer files
2023-12-17 10:24:37 INFO     	 * (global step 1250: loss: 0.1367673445492983, lr: 0.0001
2023-12-17 10:25:24 INFO     	 * (global step 1300: loss: 0.15783161111176014, lr: 0.0001
2023-12-17 10:26:11 INFO     	 * (global step 1350: loss: 0.13184989616274834, lr: 0.0001
2023-12-17 10:26:58 INFO     	 * (global step 1400: loss: 0.1717176605015993, lr: 0.0001
2023-12-17 10:27:45 INFO     	 * (global step 1450: loss: 0.15456349775195122, lr: 0.0001
2023-12-17 10:28:32 INFO     	 * (global step 1500: loss: 0.1639912836253643, lr: 0.0001
2023-12-17 10:29:20 INFO     	 * (global step 1550: loss: 0.1799664981663227, lr: 0.0001
2023-12-17 10:29:20 INFO     [epoch 14/15] average loss: 0.181, lr: 0.0001
2023-12-17 10:29:20 INFO     saving model related files
2023-12-17 10:29:20 INFO     saving model
2023-12-17 10:29:22 INFO     saving tokenizer
2023-12-17 10:29:22 INFO     saving optimizer
2023-12-17 10:29:26 INFO     remove old optimizer files
2023-12-17 10:29:26 INFO     complete training: model ckpt was saved at base_trained_ckpt/model_dpyopu
2023-12-17 10:29:26 INFO     ## 2nd RUN: Configuration 4/5: validation/Bleu_4 = 0.02203525356928333
2023-12-17 10:29:26 INFO     initialize model trainer
2023-12-17 10:29:26 INFO     load config from existing checkpoint at base_trained_ckpt/model_nxaqhy
2023-12-17 10:29:26 INFO     hyperparameters
2023-12-17 10:29:26 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-17 10:29:26 INFO     	 * dataset_name: default
2023-12-17 10:29:26 INFO     	 * input_types: ['paragraph']
2023-12-17 10:29:26 INFO     	 * output_types: ['questions_answers']
2023-12-17 10:29:26 INFO     	 * prefix_types: ['qag']
2023-12-17 10:29:26 INFO     	 * model: t5-base
2023-12-17 10:29:26 INFO     	 * max_length: 512
2023-12-17 10:29:26 INFO     	 * max_length_output: 512
2023-12-17 10:29:26 INFO     	 * epoch: 15
2023-12-17 10:29:26 INFO     	 * batch: 2
2023-12-17 10:29:26 INFO     	 * lr: 1e-05
2023-12-17 10:29:26 INFO     	 * fp16: False
2023-12-17 10:29:26 INFO     	 * random_seed: 1
2023-12-17 10:29:26 INFO     	 * gradient_accumulation_steps: 4
2023-12-17 10:29:26 INFO     	 * label_smoothing: 0.15
2023-12-17 10:29:26 INFO     load checkpoint from base_trained_ckpt/model_nxaqhy/epoch_10
2023-12-17 10:29:48 INFO     use spaCy answer extraction model: positionrank
2023-12-17 10:29:49 INFO     Model `base_trained_ckpt/model_nxaqhy/epoch_10`
2023-12-17 10:29:49 INFO     	 * Num of GPU in use: 1
2023-12-17 10:29:49 INFO     	 * Prefix: True
2023-12-17 10:29:49 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 10:29:49 INFO     load optimizer from base_trained_ckpt/model_nxaqhy/optimizers/optimizer.10.pt
2023-12-17 10:29:49 INFO     optimizer is loading on cuda
2023-12-17 10:30:36 INFO     dataset preprocessing
2023-12-17 10:30:38 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-base.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-17 10:30:38 INFO     start model training
2023-12-17 10:31:25 INFO     	 * (global step 50: loss: 2.8620190024375916, lr: 1e-05
2023-12-17 10:32:12 INFO     	 * (global step 100: loss: 2.836602747440338, lr: 1e-05
2023-12-17 10:33:00 INFO     	 * (global step 150: loss: 2.6820008754730225, lr: 1e-05
2023-12-17 10:33:47 INFO     	 * (global step 200: loss: 2.6764031052589417, lr: 1e-05
2023-12-17 10:34:35 INFO     	 * (global step 250: loss: 2.664858400821686, lr: 1e-05
2023-12-17 10:35:23 INFO     	 * (global step 300: loss: 2.540528953075409, lr: 1e-05
2023-12-17 10:35:34 INFO     [epoch 10/15] average loss: 2.787, lr: 1e-05
2023-12-17 10:35:34 INFO     saving model related files
2023-12-17 10:35:34 INFO     saving model
2023-12-17 10:35:35 INFO     saving tokenizer
2023-12-17 10:35:36 INFO     saving optimizer
2023-12-17 10:35:39 INFO     remove old optimizer files
2023-12-17 10:36:17 INFO     	 * (global step 350: loss: 2.511246681213379, lr: 1e-05
2023-12-17 10:37:05 INFO     	 * (global step 400: loss: 2.5356019139289856, lr: 1e-05
2023-12-17 10:37:53 INFO     	 * (global step 450: loss: 2.552517294883728, lr: 1e-05
2023-12-17 10:38:41 INFO     	 * (global step 500: loss: 2.513253927230835, lr: 1e-05
2023-12-17 10:39:29 INFO     	 * (global step 550: loss: 2.5148828625679016, lr: 1e-05
2023-12-17 10:40:17 INFO     	 * (global step 600: loss: 2.683872878551483, lr: 1e-05
2023-12-17 10:40:36 INFO     [epoch 11/15] average loss: 2.556, lr: 1e-05
2023-12-17 10:40:36 INFO     saving model related files
2023-12-17 10:40:36 INFO     saving model
2023-12-17 10:40:38 INFO     saving tokenizer
2023-12-17 10:40:38 INFO     saving optimizer
2023-12-17 10:40:41 INFO     remove old optimizer files
2023-12-17 10:41:10 INFO     	 * (global step 650: loss: 2.535865843296051, lr: 1e-05
2023-12-17 10:41:58 INFO     	 * (global step 700: loss: 2.3991532921791077, lr: 1e-05
2023-12-17 10:42:46 INFO     	 * (global step 750: loss: 2.4547977447509766, lr: 1e-05
2023-12-17 10:43:34 INFO     	 * (global step 800: loss: 2.4751322269439697, lr: 1e-05
2023-12-17 10:44:22 INFO     	 * (global step 850: loss: 2.469595491886139, lr: 1e-05
2023-12-17 10:45:09 INFO     	 * (global step 900: loss: 2.443904936313629, lr: 1e-05
2023-12-17 10:45:39 INFO     [epoch 12/15] average loss: 2.504, lr: 1e-05
2023-12-17 10:45:39 INFO     saving model related files
2023-12-17 10:45:39 INFO     saving model
2023-12-17 10:45:41 INFO     saving tokenizer
2023-12-17 10:45:41 INFO     saving optimizer
2023-12-17 10:45:44 INFO     remove old optimizer files
2023-12-17 10:46:03 INFO     	 * (global step 950: loss: 2.53612744808197, lr: 1e-05
2023-12-17 10:46:51 INFO     	 * (global step 1000: loss: 2.516112804412842, lr: 1e-05
2023-12-17 10:47:39 INFO     	 * (global step 1050: loss: 2.450599730014801, lr: 1e-05
2023-12-17 10:48:27 INFO     	 * (global step 1100: loss: 2.419208526611328, lr: 1e-05
2023-12-17 10:49:15 INFO     	 * (global step 1150: loss: 2.5298889875411987, lr: 1e-05
2023-12-17 10:50:02 INFO     	 * (global step 1200: loss: 2.345090389251709, lr: 1e-05
2023-12-17 10:50:41 INFO     [epoch 13/15] average loss: 2.474, lr: 1e-05
2023-12-17 10:50:41 INFO     saving model related files
2023-12-17 10:50:41 INFO     saving model
2023-12-17 10:50:43 INFO     saving tokenizer
2023-12-17 10:50:43 INFO     saving optimizer
2023-12-17 10:50:46 INFO     remove old optimizer files
2023-12-17 10:50:55 INFO     	 * (global step 1250: loss: 2.3715943098068237, lr: 1e-05
2023-12-17 10:51:43 INFO     	 * (global step 1300: loss: 2.414361596107483, lr: 1e-05
2023-12-17 10:52:30 INFO     	 * (global step 1350: loss: 2.3524329662323, lr: 1e-05
2023-12-17 10:53:18 INFO     	 * (global step 1400: loss: 2.442231774330139, lr: 1e-05
2023-12-17 10:54:06 INFO     	 * (global step 1450: loss: 2.4130253195762634, lr: 1e-05
2023-12-17 10:54:53 INFO     	 * (global step 1500: loss: 2.4477899074554443, lr: 1e-05
2023-12-17 10:55:41 INFO     	 * (global step 1550: loss: 2.4479336738586426, lr: 1e-05
2023-12-17 10:55:42 INFO     [epoch 14/15] average loss: 2.452, lr: 1e-05
2023-12-17 10:55:42 INFO     saving model related files
2023-12-17 10:55:42 INFO     saving model
2023-12-17 10:55:43 INFO     saving tokenizer
2023-12-17 10:55:43 INFO     saving optimizer
2023-12-17 10:55:46 INFO     remove old optimizer files
2023-12-17 10:55:46 INFO     complete training: model ckpt was saved at base_trained_ckpt/model_nxaqhy
2023-12-17 10:55:46 INFO     ## 2nd RUN (EVAL): Configuration 0/5 ##
2023-12-17 10:56:01 INFO     use spaCy answer extraction model: positionrank
2023-12-17 10:56:01 INFO     Model `base_trained_ckpt/model_eszyci/epoch_1`
2023-12-17 10:56:01 INFO     	 * Num of GPU in use: 1
2023-12-17 10:56:01 INFO     	 * Prefix: True
2023-12-17 10:56:01 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 10:56:03 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 11:07:50 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 11:19:37 INFO     	Bleu_1: 0.12792125302504398
2023-12-17 11:19:37 INFO     	Bleu_2: 0.07074204657644073
2023-12-17 11:19:37 INFO     	Bleu_3: 0.037863859983735086
2023-12-17 11:19:37 INFO     	Bleu_4: 0.024009352858761487
2023-12-17 11:19:37 INFO     	Bleu_1: 0.12255029157505072
2023-12-17 11:19:37 INFO     	Bleu_2: 0.06649483163439583
2023-12-17 11:19:37 INFO     	Bleu_3: 0.03419562166224399
2023-12-17 11:19:37 INFO     	Bleu_4: 0.02106760274842686
2023-12-17 11:19:53 INFO     use spaCy answer extraction model: positionrank
2023-12-17 11:19:53 INFO     Model `base_trained_ckpt/model_eszyci/epoch_11`
2023-12-17 11:19:53 INFO     	 * Num of GPU in use: 1
2023-12-17 11:19:53 INFO     	 * Prefix: True
2023-12-17 11:19:53 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 11:19:54 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 11:31:09 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 11:42:38 INFO     	Bleu_1: 0.13296398891966668
2023-12-17 11:42:38 INFO     	Bleu_2: 0.07472833604038842
2023-12-17 11:42:38 INFO     	Bleu_3: 0.041564795342759524
2023-12-17 11:42:38 INFO     	Bleu_4: 0.027318237753903536
2023-12-17 11:42:39 INFO     	Bleu_1: 0.13485916245662194
2023-12-17 11:42:39 INFO     	Bleu_2: 0.0749509453963611
2023-12-17 11:42:39 INFO     	Bleu_3: 0.040922320629579914
2023-12-17 11:42:39 INFO     	Bleu_4: 0.02651649187416279
2023-12-17 11:42:54 INFO     use spaCy answer extraction model: positionrank
2023-12-17 11:42:55 INFO     Model `base_trained_ckpt/model_eszyci/epoch_12`
2023-12-17 11:42:55 INFO     	 * Num of GPU in use: 1
2023-12-17 11:42:55 INFO     	 * Prefix: True
2023-12-17 11:42:55 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 11:42:56 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 11:54:41 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 12:06:16 INFO     	Bleu_1: 0.12910548107517922
2023-12-17 12:06:16 INFO     	Bleu_2: 0.07211368679576603
2023-12-17 12:06:16 INFO     	Bleu_3: 0.03968745791387857
2023-12-17 12:06:16 INFO     	Bleu_4: 0.02580254335493972
2023-12-17 12:06:17 INFO     	Bleu_1: 0.12611039454972303
2023-12-17 12:06:17 INFO     	Bleu_2: 0.07014955841510871
2023-12-17 12:06:17 INFO     	Bleu_3: 0.03851529429393234
2023-12-17 12:06:17 INFO     	Bleu_4: 0.02503743128390812
2023-12-17 12:06:33 INFO     use spaCy answer extraction model: positionrank
2023-12-17 12:06:33 INFO     Model `base_trained_ckpt/model_eszyci/epoch_13`
2023-12-17 12:06:33 INFO     	 * Num of GPU in use: 1
2023-12-17 12:06:33 INFO     	 * Prefix: True
2023-12-17 12:06:33 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 12:06:34 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 12:18:06 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 12:29:44 INFO     	Bleu_1: 0.13642748049964454
2023-12-17 12:29:44 INFO     	Bleu_2: 0.0770186548984504
2023-12-17 12:29:44 INFO     	Bleu_3: 0.04368424234650454
2023-12-17 12:29:44 INFO     	Bleu_4: 0.028865019879046532
2023-12-17 12:29:45 INFO     	Bleu_1: 0.13400677573954628
2023-12-17 12:29:45 INFO     	Bleu_2: 0.07546943339358575
2023-12-17 12:29:45 INFO     	Bleu_3: 0.042855568975511946
2023-12-17 12:29:45 INFO     	Bleu_4: 0.028665232842611202
2023-12-17 12:30:09 INFO     use spaCy answer extraction model: positionrank
2023-12-17 12:30:10 INFO     Model `base_trained_ckpt/model_eszyci/epoch_14`
2023-12-17 12:30:10 INFO     	 * Num of GPU in use: 1
2023-12-17 12:30:10 INFO     	 * Prefix: True
2023-12-17 12:30:10 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 12:30:11 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 12:41:14 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 12:52:10 INFO     	Bleu_1: 0.15397307233306246
2023-12-17 12:52:10 INFO     	Bleu_2: 0.08622542152672097
2023-12-17 12:52:10 INFO     	Bleu_3: 0.04784213921227775
2023-12-17 12:52:10 INFO     	Bleu_4: 0.03095930277951126
2023-12-17 12:52:11 INFO     	Bleu_1: 0.15451340263933838
2023-12-17 12:52:11 INFO     	Bleu_2: 0.08557350772043439
2023-12-17 12:52:11 INFO     	Bleu_3: 0.046927774372706496
2023-12-17 12:52:11 INFO     	Bleu_4: 0.030260711728189865
2023-12-17 12:52:29 INFO     use spaCy answer extraction model: positionrank
2023-12-17 12:52:29 INFO     Model `base_trained_ckpt/model_eszyci/epoch_15`
2023-12-17 12:52:29 INFO     	 * Num of GPU in use: 1
2023-12-17 12:52:29 INFO     	 * Prefix: True
2023-12-17 12:52:29 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 12:52:30 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 13:03:35 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 13:14:30 INFO     	Bleu_1: 0.15157720944261757
2023-12-17 13:14:30 INFO     	Bleu_2: 0.08565420298764033
2023-12-17 13:14:30 INFO     	Bleu_3: 0.04852366288904204
2023-12-17 13:14:30 INFO     	Bleu_4: 0.03201097272182527
2023-12-17 13:14:30 INFO     	Bleu_1: 0.15247126898657756
2023-12-17 13:14:30 INFO     	Bleu_2: 0.08604332690833895
2023-12-17 13:14:30 INFO     	Bleu_3: 0.04860424504021666
2023-12-17 13:14:30 INFO     	Bleu_4: 0.031895264911693276
2023-12-17 13:14:43 INFO     use spaCy answer extraction model: positionrank
2023-12-17 13:14:44 INFO     Model `base_trained_ckpt/model_eszyci/epoch_2`
2023-12-17 13:14:44 INFO     	 * Num of GPU in use: 1
2023-12-17 13:14:44 INFO     	 * Prefix: True
2023-12-17 13:14:44 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 13:14:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 13:26:37 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 13:38:33 INFO     	Bleu_1: 0.11310230094557663
2023-12-17 13:38:33 INFO     	Bleu_2: 0.062566842066739
2023-12-17 13:38:33 INFO     	Bleu_3: 0.03323152539652735
2023-12-17 13:38:33 INFO     	Bleu_4: 0.02130959300377921
2023-12-17 13:38:33 INFO     	Bleu_1: 0.11051581302533646
2023-12-17 13:38:33 INFO     	Bleu_2: 0.06039464152634137
2023-12-17 13:38:33 INFO     	Bleu_3: 0.031632009400344156
2023-12-17 13:38:33 INFO     	Bleu_4: 0.019870151148697127
2023-12-17 13:38:47 INFO     use spaCy answer extraction model: positionrank
2023-12-17 13:38:48 INFO     Model `base_trained_ckpt/model_eszyci/epoch_3`
2023-12-17 13:38:48 INFO     	 * Num of GPU in use: 1
2023-12-17 13:38:48 INFO     	 * Prefix: True
2023-12-17 13:38:48 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 13:38:49 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 13:50:26 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 14:01:58 INFO     	Bleu_1: 0.11973465349138335
2023-12-17 14:01:58 INFO     	Bleu_2: 0.06537028730892434
2023-12-17 14:01:58 INFO     	Bleu_3: 0.034263820610961594
2023-12-17 14:01:58 INFO     	Bleu_4: 0.02175253935748025
2023-12-17 14:01:59 INFO     	Bleu_1: 0.11957414627149728
2023-12-17 14:01:59 INFO     	Bleu_2: 0.06578130427129503
2023-12-17 14:01:59 INFO     	Bleu_3: 0.035265437186670016
2023-12-17 14:01:59 INFO     	Bleu_4: 0.02272343895344681
2023-12-17 14:02:14 INFO     use spaCy answer extraction model: positionrank
2023-12-17 14:02:14 INFO     Model `base_trained_ckpt/model_eszyci/epoch_4`
2023-12-17 14:02:14 INFO     	 * Num of GPU in use: 1
2023-12-17 14:02:14 INFO     	 * Prefix: True
2023-12-17 14:02:14 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 14:02:15 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 14:13:31 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 14:24:49 INFO     	Bleu_1: 0.12717719109374576
2023-12-17 14:24:49 INFO     	Bleu_2: 0.0699740511513461
2023-12-17 14:24:49 INFO     	Bleu_3: 0.036981770233893284
2023-12-17 14:24:49 INFO     	Bleu_4: 0.023469185402777953
2023-12-17 14:24:49 INFO     	Bleu_1: 0.12401494898030511
2023-12-17 14:24:49 INFO     	Bleu_2: 0.06763880250302493
2023-12-17 14:24:49 INFO     	Bleu_3: 0.03539680670080418
2023-12-17 14:24:49 INFO     	Bleu_4: 0.022199180766664786
2023-12-17 14:25:08 INFO     use spaCy answer extraction model: positionrank
2023-12-17 14:25:09 INFO     Model `base_trained_ckpt/model_eszyci/epoch_5`
2023-12-17 14:25:09 INFO     	 * Num of GPU in use: 1
2023-12-17 14:25:09 INFO     	 * Prefix: True
2023-12-17 14:25:09 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 14:25:10 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 14:36:58 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 14:48:56 INFO     	Bleu_1: 0.10642410592489766
2023-12-17 14:48:56 INFO     	Bleu_2: 0.05812117337239955
2023-12-17 14:48:56 INFO     	Bleu_3: 0.030118300259374963
2023-12-17 14:48:56 INFO     	Bleu_4: 0.018795056726771793
2023-12-17 14:48:56 INFO     	Bleu_1: 0.10600165418263513
2023-12-17 14:48:56 INFO     	Bleu_2: 0.05789683918367118
2023-12-17 14:48:56 INFO     	Bleu_3: 0.030038916933140352
2023-12-17 14:48:56 INFO     	Bleu_4: 0.018676913228298002
2023-12-17 14:49:10 INFO     use spaCy answer extraction model: positionrank
2023-12-17 14:49:11 INFO     Model `base_trained_ckpt/model_eszyci/epoch_6`
2023-12-17 14:49:11 INFO     	 * Num of GPU in use: 1
2023-12-17 14:49:11 INFO     	 * Prefix: True
2023-12-17 14:49:11 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 14:49:12 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 15:01:07 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 15:12:54 INFO     	Bleu_1: 0.10996195694905553
2023-12-17 15:12:54 INFO     	Bleu_2: 0.06076718016668417
2023-12-17 15:12:54 INFO     	Bleu_3: 0.03180847278171052
2023-12-17 15:12:54 INFO     	Bleu_4: 0.020076824325041213
2023-12-17 15:12:54 INFO     	Bleu_1: 0.10608219700809028
2023-12-17 15:12:54 INFO     	Bleu_2: 0.057855495940194776
2023-12-17 15:12:54 INFO     	Bleu_3: 0.02997309215277685
2023-12-17 15:12:54 INFO     	Bleu_4: 0.018613649501967318
2023-12-17 15:13:08 INFO     use spaCy answer extraction model: positionrank
2023-12-17 15:13:09 INFO     Model `base_trained_ckpt/model_eszyci/epoch_7`
2023-12-17 15:13:09 INFO     	 * Num of GPU in use: 1
2023-12-17 15:13:09 INFO     	 * Prefix: True
2023-12-17 15:13:09 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 15:13:10 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 15:25:11 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 15:37:12 INFO     	Bleu_1: 0.11027035548198882
2023-12-17 15:37:12 INFO     	Bleu_2: 0.061578074201953
2023-12-17 15:37:12 INFO     	Bleu_3: 0.033918168074726335
2023-12-17 15:37:12 INFO     	Bleu_4: 0.022175168296980078
2023-12-17 15:37:12 INFO     	Bleu_1: 0.10948992032663184
2023-12-17 15:37:12 INFO     	Bleu_2: 0.06150059812721133
2023-12-17 15:37:12 INFO     	Bleu_3: 0.03415764365678497
2023-12-17 15:37:12 INFO     	Bleu_4: 0.022515166555915775
2023-12-17 15:37:26 INFO     use spaCy answer extraction model: positionrank
2023-12-17 15:37:27 INFO     Model `base_trained_ckpt/model_eszyci/epoch_8`
2023-12-17 15:37:27 INFO     	 * Num of GPU in use: 1
2023-12-17 15:37:27 INFO     	 * Prefix: True
2023-12-17 15:37:27 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 15:37:28 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 15:49:14 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 16:01:03 INFO     	Bleu_1: 0.11462567670618118
2023-12-17 16:01:03 INFO     	Bleu_2: 0.06307305615722278
2023-12-17 16:01:03 INFO     	Bleu_3: 0.03332155770101888
2023-12-17 16:01:03 INFO     	Bleu_4: 0.021212958422077884
2023-12-17 16:01:04 INFO     	Bleu_1: 0.11416338969108072
2023-12-17 16:01:04 INFO     	Bleu_2: 0.06264941253010167
2023-12-17 16:01:04 INFO     	Bleu_3: 0.032888811299890235
2023-12-17 16:01:04 INFO     	Bleu_4: 0.02064757576424302
2023-12-17 16:01:17 INFO     use spaCy answer extraction model: positionrank
2023-12-17 16:01:17 INFO     Model `base_trained_ckpt/model_eszyci/epoch_9`
2023-12-17 16:01:17 INFO     	 * Num of GPU in use: 1
2023-12-17 16:01:17 INFO     	 * Prefix: True
2023-12-17 16:01:17 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 16:01:19 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 16:12:58 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 16:24:36 INFO     	Bleu_1: 0.12401280822230402
2023-12-17 16:24:36 INFO     	Bleu_2: 0.0691420283705955
2023-12-17 16:24:36 INFO     	Bleu_3: 0.03813835762672572
2023-12-17 16:24:36 INFO     	Bleu_4: 0.02496695879641108
2023-12-17 16:24:37 INFO     	Bleu_1: 0.12076192944377777
2023-12-17 16:24:37 INFO     	Bleu_2: 0.0672105177518717
2023-12-17 16:24:37 INFO     	Bleu_3: 0.037080001082916356
2023-12-17 16:24:37 INFO     	Bleu_4: 0.024359028293561815
2023-12-17 16:24:37 INFO     ## 2nd RUN (EVAL): Configuration 1/5 ##
2023-12-17 16:24:50 INFO     use spaCy answer extraction model: positionrank
2023-12-17 16:24:50 INFO     Model `base_trained_ckpt/model_mzgdpa/epoch_1`
2023-12-17 16:24:50 INFO     	 * Num of GPU in use: 1
2023-12-17 16:24:50 INFO     	 * Prefix: True
2023-12-17 16:24:50 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 16:24:52 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 16:36:41 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 16:48:23 INFO     	Bleu_1: 0.12792125302504398
2023-12-17 16:48:23 INFO     	Bleu_2: 0.07074204657644073
2023-12-17 16:48:23 INFO     	Bleu_3: 0.037863859983735086
2023-12-17 16:48:23 INFO     	Bleu_4: 0.024009352858761487
2023-12-17 16:48:23 INFO     	Bleu_1: 0.12255029157505072
2023-12-17 16:48:23 INFO     	Bleu_2: 0.06649483163439583
2023-12-17 16:48:23 INFO     	Bleu_3: 0.03419562166224399
2023-12-17 16:48:23 INFO     	Bleu_4: 0.02106760274842686
2023-12-17 16:48:39 INFO     use spaCy answer extraction model: positionrank
2023-12-17 16:48:39 INFO     Model `base_trained_ckpt/model_mzgdpa/epoch_11`
2023-12-17 16:48:39 INFO     	 * Num of GPU in use: 1
2023-12-17 16:48:39 INFO     	 * Prefix: True
2023-12-17 16:48:39 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 16:48:41 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 17:00:03 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 17:11:24 INFO     	Bleu_1: 0.13231253815840505
2023-12-17 17:11:24 INFO     	Bleu_2: 0.0737218651413916
2023-12-17 17:11:24 INFO     	Bleu_3: 0.039866631839197725
2023-12-17 17:11:24 INFO     	Bleu_4: 0.025484203963500755
2023-12-17 17:11:25 INFO     	Bleu_1: 0.12998688249878007
2023-12-17 17:11:25 INFO     	Bleu_2: 0.07238452925416747
2023-12-17 17:11:25 INFO     	Bleu_3: 0.0394829489465723
2023-12-17 17:11:25 INFO     	Bleu_4: 0.025432439504419653
2023-12-17 17:11:36 INFO     use spaCy answer extraction model: positionrank
2023-12-17 17:11:37 INFO     Model `base_trained_ckpt/model_mzgdpa/epoch_12`
2023-12-17 17:11:37 INFO     	 * Num of GPU in use: 1
2023-12-17 17:11:37 INFO     	 * Prefix: True
2023-12-17 17:11:37 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 17:11:38 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 17:23:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 17:35:12 INFO     	Bleu_1: 0.13352864759533323
2023-12-17 17:35:12 INFO     	Bleu_2: 0.07559718226930047
2023-12-17 17:35:12 INFO     	Bleu_3: 0.043096814504546846
2023-12-17 17:35:12 INFO     	Bleu_4: 0.02865861252330175
2023-12-17 17:35:12 INFO     	Bleu_1: 0.12446834327462146
2023-12-17 17:35:12 INFO     	Bleu_2: 0.06927368144011412
2023-12-17 17:35:12 INFO     	Bleu_3: 0.03857325090019121
2023-12-17 17:35:12 INFO     	Bleu_4: 0.02534827214644819
2023-12-17 17:35:24 INFO     use spaCy answer extraction model: positionrank
2023-12-17 17:35:24 INFO     Model `base_trained_ckpt/model_mzgdpa/epoch_13`
2023-12-17 17:35:24 INFO     	 * Num of GPU in use: 1
2023-12-17 17:35:24 INFO     	 * Prefix: True
2023-12-17 17:35:24 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 17:35:25 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 17:46:53 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 17:58:20 INFO     	Bleu_1: 0.14627394529952278
2023-12-17 17:58:20 INFO     	Bleu_2: 0.0826012001650716
2023-12-17 17:58:20 INFO     	Bleu_3: 0.04652876035933702
2023-12-17 17:58:20 INFO     	Bleu_4: 0.030539986616525256
2023-12-17 17:58:20 INFO     	Bleu_1: 0.14363536114684697
2023-12-17 17:58:20 INFO     	Bleu_2: 0.08069732267328548
2023-12-17 17:58:20 INFO     	Bleu_3: 0.0453910252030828
2023-12-17 17:58:20 INFO     	Bleu_4: 0.02986878955633688
2023-12-17 17:58:33 INFO     use spaCy answer extraction model: positionrank
2023-12-17 17:58:33 INFO     Model `base_trained_ckpt/model_mzgdpa/epoch_14`
2023-12-17 17:58:33 INFO     	 * Num of GPU in use: 1
2023-12-17 17:58:33 INFO     	 * Prefix: True
2023-12-17 17:58:33 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 17:58:34 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 18:09:54 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 18:21:04 INFO     	Bleu_1: 0.15702426818127532
2023-12-17 18:21:04 INFO     	Bleu_2: 0.08949673140047287
2023-12-17 18:21:04 INFO     	Bleu_3: 0.05081984253888556
2023-12-17 18:21:04 INFO     	Bleu_4: 0.03349692010094845
2023-12-17 18:21:04 INFO     	Bleu_1: 0.1533027226689547
2023-12-17 18:21:04 INFO     	Bleu_2: 0.08643644704818675
2023-12-17 18:21:04 INFO     	Bleu_3: 0.04860676852739581
2023-12-17 18:21:04 INFO     	Bleu_4: 0.031812096301029304
2023-12-17 18:21:17 INFO     use spaCy answer extraction model: positionrank
2023-12-17 18:21:18 INFO     Model `base_trained_ckpt/model_mzgdpa/epoch_15`
2023-12-17 18:21:18 INFO     	 * Num of GPU in use: 1
2023-12-17 18:21:18 INFO     	 * Prefix: True
2023-12-17 18:21:18 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 18:21:19 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 18:32:34 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 18:43:42 INFO     	Bleu_1: 0.15262986348510804
2023-12-17 18:43:42 INFO     	Bleu_2: 0.08716100273924746
2023-12-17 18:43:42 INFO     	Bleu_3: 0.04983051118086129
2023-12-17 18:43:42 INFO     	Bleu_4: 0.03291405046612744
2023-12-17 18:43:42 INFO     	Bleu_1: 0.1480402222490628
2023-12-17 18:43:42 INFO     	Bleu_2: 0.08309829124966989
2023-12-17 18:43:42 INFO     	Bleu_3: 0.046271738123504255
2023-12-17 18:43:42 INFO     	Bleu_4: 0.029837745141535834
2023-12-17 18:43:56 INFO     use spaCy answer extraction model: positionrank
2023-12-17 18:43:57 INFO     Model `base_trained_ckpt/model_mzgdpa/epoch_2`
2023-12-17 18:43:57 INFO     	 * Num of GPU in use: 1
2023-12-17 18:43:57 INFO     	 * Prefix: True
2023-12-17 18:43:57 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 18:43:58 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 18:55:49 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 19:07:45 INFO     	Bleu_1: 0.11310230094557663
2023-12-17 19:07:45 INFO     	Bleu_2: 0.062566842066739
2023-12-17 19:07:45 INFO     	Bleu_3: 0.03323152539652735
2023-12-17 19:07:45 INFO     	Bleu_4: 0.02130959300377921
2023-12-17 19:07:46 INFO     	Bleu_1: 0.11051581302533646
2023-12-17 19:07:46 INFO     	Bleu_2: 0.06039464152634137
2023-12-17 19:07:46 INFO     	Bleu_3: 0.031632009400344156
2023-12-17 19:07:46 INFO     	Bleu_4: 0.019870151148697127
2023-12-17 19:07:59 INFO     use spaCy answer extraction model: positionrank
2023-12-17 19:07:59 INFO     Model `base_trained_ckpt/model_mzgdpa/epoch_3`
2023-12-17 19:07:59 INFO     	 * Num of GPU in use: 1
2023-12-17 19:07:59 INFO     	 * Prefix: True
2023-12-17 19:07:59 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 19:08:00 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 19:19:36 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 19:31:12 INFO     	Bleu_1: 0.11973465349138335
2023-12-17 19:31:12 INFO     	Bleu_2: 0.06537028730892434
2023-12-17 19:31:12 INFO     	Bleu_3: 0.034263820610961594
2023-12-17 19:31:12 INFO     	Bleu_4: 0.02175253935748025
2023-12-17 19:31:13 INFO     	Bleu_1: 0.11957414627149728
2023-12-17 19:31:13 INFO     	Bleu_2: 0.06578130427129503
2023-12-17 19:31:13 INFO     	Bleu_3: 0.035265437186670016
2023-12-17 19:31:13 INFO     	Bleu_4: 0.02272343895344681
2023-12-17 19:31:29 INFO     use spaCy answer extraction model: positionrank
2023-12-17 19:31:30 INFO     Model `base_trained_ckpt/model_mzgdpa/epoch_4`
2023-12-17 19:31:30 INFO     	 * Num of GPU in use: 1
2023-12-17 19:31:30 INFO     	 * Prefix: True
2023-12-17 19:31:30 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 19:31:31 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 19:42:43 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 19:53:59 INFO     	Bleu_1: 0.12717719109374576
2023-12-17 19:53:59 INFO     	Bleu_2: 0.0699740511513461
2023-12-17 19:53:59 INFO     	Bleu_3: 0.036981770233893284
2023-12-17 19:53:59 INFO     	Bleu_4: 0.023469185402777953
2023-12-17 19:53:59 INFO     	Bleu_1: 0.12401494898030511
2023-12-17 19:53:59 INFO     	Bleu_2: 0.06763880250302493
2023-12-17 19:53:59 INFO     	Bleu_3: 0.03539680670080418
2023-12-17 19:53:59 INFO     	Bleu_4: 0.022199180766664786
2023-12-17 19:54:12 INFO     use spaCy answer extraction model: positionrank
2023-12-17 19:54:12 INFO     Model `base_trained_ckpt/model_mzgdpa/epoch_5`
2023-12-17 19:54:12 INFO     	 * Num of GPU in use: 1
2023-12-17 19:54:12 INFO     	 * Prefix: True
2023-12-17 19:54:12 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 19:54:13 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 20:06:03 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 20:18:05 INFO     	Bleu_1: 0.10642410592489766
2023-12-17 20:18:05 INFO     	Bleu_2: 0.05812117337239955
2023-12-17 20:18:05 INFO     	Bleu_3: 0.030118300259374963
2023-12-17 20:18:05 INFO     	Bleu_4: 0.018795056726771793
2023-12-17 20:18:05 INFO     	Bleu_1: 0.10600165418263513
2023-12-17 20:18:05 INFO     	Bleu_2: 0.05789683918367118
2023-12-17 20:18:05 INFO     	Bleu_3: 0.030038916933140352
2023-12-17 20:18:05 INFO     	Bleu_4: 0.018676913228298002
2023-12-17 20:18:17 INFO     use spaCy answer extraction model: positionrank
2023-12-17 20:18:18 INFO     Model `base_trained_ckpt/model_mzgdpa/epoch_6`
2023-12-17 20:18:18 INFO     	 * Num of GPU in use: 1
2023-12-17 20:18:18 INFO     	 * Prefix: True
2023-12-17 20:18:18 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 20:18:19 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 20:30:10 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 20:42:03 INFO     	Bleu_1: 0.10996195694905553
2023-12-17 20:42:03 INFO     	Bleu_2: 0.06076718016668417
2023-12-17 20:42:03 INFO     	Bleu_3: 0.03180847278171052
2023-12-17 20:42:03 INFO     	Bleu_4: 0.020076824325041213
2023-12-17 20:42:03 INFO     	Bleu_1: 0.10608219700809028
2023-12-17 20:42:03 INFO     	Bleu_2: 0.057855495940194776
2023-12-17 20:42:03 INFO     	Bleu_3: 0.02997309215277685
2023-12-17 20:42:03 INFO     	Bleu_4: 0.018613649501967318
2023-12-17 20:42:24 INFO     use spaCy answer extraction model: positionrank
2023-12-17 20:42:24 INFO     Model `base_trained_ckpt/model_mzgdpa/epoch_7`
2023-12-17 20:42:24 INFO     	 * Num of GPU in use: 1
2023-12-17 20:42:24 INFO     	 * Prefix: True
2023-12-17 20:42:24 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 20:42:26 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 20:54:24 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 21:06:16 INFO     	Bleu_1: 0.11027035548198882
2023-12-17 21:06:16 INFO     	Bleu_2: 0.061578074201953
2023-12-17 21:06:16 INFO     	Bleu_3: 0.033918168074726335
2023-12-17 21:06:16 INFO     	Bleu_4: 0.022175168296980078
2023-12-17 21:06:17 INFO     	Bleu_1: 0.10948992032663184
2023-12-17 21:06:17 INFO     	Bleu_2: 0.06150059812721133
2023-12-17 21:06:17 INFO     	Bleu_3: 0.03415764365678497
2023-12-17 21:06:17 INFO     	Bleu_4: 0.022515166555915775
2023-12-17 21:06:29 INFO     use spaCy answer extraction model: positionrank
2023-12-17 21:06:29 INFO     Model `base_trained_ckpt/model_mzgdpa/epoch_8`
2023-12-17 21:06:29 INFO     	 * Num of GPU in use: 1
2023-12-17 21:06:29 INFO     	 * Prefix: True
2023-12-17 21:06:29 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 21:06:31 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 21:18:16 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 21:29:57 INFO     	Bleu_1: 0.11462567670618118
2023-12-17 21:29:57 INFO     	Bleu_2: 0.06307305615722278
2023-12-17 21:29:57 INFO     	Bleu_3: 0.03332155770101888
2023-12-17 21:29:57 INFO     	Bleu_4: 0.021212958422077884
2023-12-17 21:29:57 INFO     	Bleu_1: 0.11416338969108072
2023-12-17 21:29:57 INFO     	Bleu_2: 0.06264941253010167
2023-12-17 21:29:57 INFO     	Bleu_3: 0.032888811299890235
2023-12-17 21:29:57 INFO     	Bleu_4: 0.02064757576424302
2023-12-17 21:30:09 INFO     use spaCy answer extraction model: positionrank
2023-12-17 21:30:09 INFO     Model `base_trained_ckpt/model_mzgdpa/epoch_9`
2023-12-17 21:30:09 INFO     	 * Num of GPU in use: 1
2023-12-17 21:30:09 INFO     	 * Prefix: True
2023-12-17 21:30:09 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 21:30:11 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 21:41:44 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 21:53:21 INFO     	Bleu_1: 0.12401280822230402
2023-12-17 21:53:21 INFO     	Bleu_2: 0.0691420283705955
2023-12-17 21:53:21 INFO     	Bleu_3: 0.03813835762672572
2023-12-17 21:53:21 INFO     	Bleu_4: 0.02496695879641108
2023-12-17 21:53:21 INFO     	Bleu_1: 0.12076192944377777
2023-12-17 21:53:21 INFO     	Bleu_2: 0.0672105177518717
2023-12-17 21:53:21 INFO     	Bleu_3: 0.037080001082916356
2023-12-17 21:53:21 INFO     	Bleu_4: 0.024359028293561815
2023-12-17 21:53:21 INFO     ## 2nd RUN (EVAL): Configuration 2/5 ##
2023-12-17 21:53:34 INFO     use spaCy answer extraction model: positionrank
2023-12-17 21:53:34 INFO     Model `base_trained_ckpt/model_rillvb/epoch_1`
2023-12-17 21:53:34 INFO     	 * Num of GPU in use: 1
2023-12-17 21:53:34 INFO     	 * Prefix: True
2023-12-17 21:53:34 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 21:53:36 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 22:05:25 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 22:17:13 INFO     	Bleu_1: 0.1131366343888366
2023-12-17 22:17:13 INFO     	Bleu_2: 0.06193380825707731
2023-12-17 22:17:13 INFO     	Bleu_3: 0.03203483143460465
2023-12-17 22:17:13 INFO     	Bleu_4: 0.019806198468704184
2023-12-17 22:17:13 INFO     	Bleu_1: 0.11438167623983785
2023-12-17 22:17:13 INFO     	Bleu_2: 0.062346244378643785
2023-12-17 22:17:13 INFO     	Bleu_3: 0.03221227040954329
2023-12-17 22:17:13 INFO     	Bleu_4: 0.019874152848845777
2023-12-17 22:17:30 INFO     use spaCy answer extraction model: positionrank
2023-12-17 22:17:30 INFO     Model `base_trained_ckpt/model_rillvb/epoch_11`
2023-12-17 22:17:30 INFO     	 * Num of GPU in use: 1
2023-12-17 22:17:30 INFO     	 * Prefix: True
2023-12-17 22:17:30 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 22:17:31 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 22:28:46 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 22:40:10 INFO     	Bleu_1: 0.1285100963414876
2023-12-17 22:40:10 INFO     	Bleu_2: 0.07099851768451369
2023-12-17 22:40:10 INFO     	Bleu_3: 0.03820553000148
2023-12-17 22:40:10 INFO     	Bleu_4: 0.02451032152090238
2023-12-17 22:40:10 INFO     	Bleu_1: 0.1278024864296962
2023-12-17 22:40:10 INFO     	Bleu_2: 0.06980041906007849
2023-12-17 22:40:10 INFO     	Bleu_3: 0.036666720228871005
2023-12-17 22:40:10 INFO     	Bleu_4: 0.023059272267580887
2023-12-17 22:40:22 INFO     use spaCy answer extraction model: positionrank
2023-12-17 22:40:23 INFO     Model `base_trained_ckpt/model_rillvb/epoch_12`
2023-12-17 22:40:23 INFO     	 * Num of GPU in use: 1
2023-12-17 22:40:23 INFO     	 * Prefix: True
2023-12-17 22:40:23 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 22:40:24 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 22:51:47 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 23:03:22 INFO     	Bleu_1: 0.12530680281930381
2023-12-17 23:03:22 INFO     	Bleu_2: 0.06933561947877379
2023-12-17 23:03:22 INFO     	Bleu_3: 0.03739500395125837
2023-12-17 23:03:22 INFO     	Bleu_4: 0.023988366371189437
2023-12-17 23:03:22 INFO     	Bleu_1: 0.12524663407195277
2023-12-17 23:03:22 INFO     	Bleu_2: 0.06893526802540563
2023-12-17 23:03:22 INFO     	Bleu_3: 0.03713641530395941
2023-12-17 23:03:22 INFO     	Bleu_4: 0.023902906558026873
2023-12-17 23:03:37 INFO     use spaCy answer extraction model: positionrank
2023-12-17 23:03:38 INFO     Model `base_trained_ckpt/model_rillvb/epoch_13`
2023-12-17 23:03:38 INFO     	 * Num of GPU in use: 1
2023-12-17 23:03:38 INFO     	 * Prefix: True
2023-12-17 23:03:38 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 23:03:39 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 23:14:56 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 23:26:10 INFO     	Bleu_1: 0.13122632559363376
2023-12-17 23:26:10 INFO     	Bleu_2: 0.07265582349124615
2023-12-17 23:26:10 INFO     	Bleu_3: 0.03911424840489513
2023-12-17 23:26:10 INFO     	Bleu_4: 0.024977194531936288
2023-12-17 23:26:10 INFO     	Bleu_1: 0.131442693236284
2023-12-17 23:26:10 INFO     	Bleu_2: 0.07259115761270012
2023-12-17 23:26:10 INFO     	Bleu_3: 0.03964682668774298
2023-12-17 23:26:10 INFO     	Bleu_4: 0.025786630347290944
2023-12-17 23:26:22 INFO     use spaCy answer extraction model: positionrank
2023-12-17 23:26:22 INFO     Model `base_trained_ckpt/model_rillvb/epoch_14`
2023-12-17 23:26:22 INFO     	 * Num of GPU in use: 1
2023-12-17 23:26:22 INFO     	 * Prefix: True
2023-12-17 23:26:22 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 23:26:23 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 23:37:15 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-17 23:48:18 INFO     	Bleu_1: 0.1407259732932094
2023-12-17 23:48:18 INFO     	Bleu_2: 0.07867914241234
2023-12-17 23:48:18 INFO     	Bleu_3: 0.04315620122761481
2023-12-17 23:48:18 INFO     	Bleu_4: 0.02810175286994642
2023-12-17 23:48:18 INFO     	Bleu_1: 0.13922717449816163
2023-12-17 23:48:18 INFO     	Bleu_2: 0.07551145899179078
2023-12-17 23:48:18 INFO     	Bleu_3: 0.04001105409916698
2023-12-17 23:48:18 INFO     	Bleu_4: 0.02535547879431821
2023-12-17 23:48:31 INFO     use spaCy answer extraction model: positionrank
2023-12-17 23:48:31 INFO     Model `base_trained_ckpt/model_rillvb/epoch_15`
2023-12-17 23:48:31 INFO     	 * Num of GPU in use: 1
2023-12-17 23:48:31 INFO     	 * Prefix: True
2023-12-17 23:48:31 INFO     	 * Language: en (ignore at the training phase)
2023-12-17 23:48:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-17 23:59:51 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 00:11:17 INFO     	Bleu_1: 0.12918673455345997
2023-12-18 00:11:17 INFO     	Bleu_2: 0.07193078146309531
2023-12-18 00:11:17 INFO     	Bleu_3: 0.038943896955421546
2023-12-18 00:11:17 INFO     	Bleu_4: 0.025176825966023778
2023-12-18 00:11:18 INFO     	Bleu_1: 0.1298934412217211
2023-12-18 00:11:18 INFO     	Bleu_2: 0.07088764672795125
2023-12-18 00:11:18 INFO     	Bleu_3: 0.03735172014686881
2023-12-18 00:11:18 INFO     	Bleu_4: 0.02360382239790324
2023-12-18 00:11:30 INFO     use spaCy answer extraction model: positionrank
2023-12-18 00:11:31 INFO     Model `base_trained_ckpt/model_rillvb/epoch_2`
2023-12-18 00:11:31 INFO     	 * Num of GPU in use: 1
2023-12-18 00:11:31 INFO     	 * Prefix: True
2023-12-18 00:11:31 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 00:11:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 00:23:04 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 00:34:40 INFO     	Bleu_1: 0.12376574273046186
2023-12-18 00:34:40 INFO     	Bleu_2: 0.06780369918085911
2023-12-18 00:34:40 INFO     	Bleu_3: 0.03548449289062585
2023-12-18 00:34:40 INFO     	Bleu_4: 0.022248703802259007
2023-12-18 00:34:40 INFO     	Bleu_1: 0.12241012202112767
2023-12-18 00:34:40 INFO     	Bleu_2: 0.06701537484411949
2023-12-18 00:34:40 INFO     	Bleu_3: 0.035225965831616995
2023-12-18 00:34:40 INFO     	Bleu_4: 0.022041910367092243
2023-12-18 00:34:52 INFO     use spaCy answer extraction model: positionrank
2023-12-18 00:34:53 INFO     Model `base_trained_ckpt/model_rillvb/epoch_3`
2023-12-18 00:34:53 INFO     	 * Num of GPU in use: 1
2023-12-18 00:34:53 INFO     	 * Prefix: True
2023-12-18 00:34:53 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 00:34:54 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 00:46:35 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 00:58:07 INFO     	Bleu_1: 0.11523003601334131
2023-12-18 00:58:07 INFO     	Bleu_2: 0.0629691997944532
2023-12-18 00:58:07 INFO     	Bleu_3: 0.03292580717124412
2023-12-18 00:58:07 INFO     	Bleu_4: 0.020834996558894217
2023-12-18 00:58:08 INFO     	Bleu_1: 0.11547276663555661
2023-12-18 00:58:08 INFO     	Bleu_2: 0.06344731340352801
2023-12-18 00:58:08 INFO     	Bleu_3: 0.0338535145660075
2023-12-18 00:58:08 INFO     	Bleu_4: 0.021618219227139017
2023-12-18 00:58:21 INFO     use spaCy answer extraction model: positionrank
2023-12-18 00:58:22 INFO     Model `base_trained_ckpt/model_rillvb/epoch_4`
2023-12-18 00:58:22 INFO     	 * Num of GPU in use: 1
2023-12-18 00:58:22 INFO     	 * Prefix: True
2023-12-18 00:58:22 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 00:58:23 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 01:09:12 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 01:20:15 INFO     	Bleu_1: 0.12966886524769503
2023-12-18 01:20:15 INFO     	Bleu_2: 0.07050125505807822
2023-12-18 01:20:15 INFO     	Bleu_3: 0.035737262572235626
2023-12-18 01:20:15 INFO     	Bleu_4: 0.021829697221114913
2023-12-18 01:20:15 INFO     	Bleu_1: 0.13007660172225746
2023-12-18 01:20:15 INFO     	Bleu_2: 0.07083355065121785
2023-12-18 01:20:15 INFO     	Bleu_3: 0.03673425050085254
2023-12-18 01:20:15 INFO     	Bleu_4: 0.022947726356418067
2023-12-18 01:20:29 INFO     use spaCy answer extraction model: positionrank
2023-12-18 01:20:29 INFO     Model `base_trained_ckpt/model_rillvb/epoch_5`
2023-12-18 01:20:29 INFO     	 * Num of GPU in use: 1
2023-12-18 01:20:29 INFO     	 * Prefix: True
2023-12-18 01:20:29 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 01:20:30 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 01:32:16 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 01:44:04 INFO     	Bleu_1: 0.10808919166408999
2023-12-18 01:44:04 INFO     	Bleu_2: 0.05946415243335169
2023-12-18 01:44:04 INFO     	Bleu_3: 0.031322034317359235
2023-12-18 01:44:04 INFO     	Bleu_4: 0.01977124734567227
2023-12-18 01:44:04 INFO     	Bleu_1: 0.10620846110876717
2023-12-18 01:44:04 INFO     	Bleu_2: 0.05850794521097311
2023-12-18 01:44:04 INFO     	Bleu_3: 0.031170746171225328
2023-12-18 01:44:04 INFO     	Bleu_4: 0.01972333608227653
2023-12-18 01:44:18 INFO     use spaCy answer extraction model: positionrank
2023-12-18 01:44:19 INFO     Model `base_trained_ckpt/model_rillvb/epoch_6`
2023-12-18 01:44:19 INFO     	 * Num of GPU in use: 1
2023-12-18 01:44:19 INFO     	 * Prefix: True
2023-12-18 01:44:19 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 01:44:20 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 01:56:01 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 02:07:45 INFO     	Bleu_1: 0.11216371466661684
2023-12-18 02:07:45 INFO     	Bleu_2: 0.06154264446940876
2023-12-18 02:07:45 INFO     	Bleu_3: 0.031982434617724716
2023-12-18 02:07:45 INFO     	Bleu_4: 0.019995755397349612
2023-12-18 02:07:46 INFO     	Bleu_1: 0.11092467850581685
2023-12-18 02:07:46 INFO     	Bleu_2: 0.06094583320152495
2023-12-18 02:07:46 INFO     	Bleu_3: 0.031988900926451146
2023-12-18 02:07:46 INFO     	Bleu_4: 0.02011438563928875
2023-12-18 02:07:59 INFO     use spaCy answer extraction model: positionrank
2023-12-18 02:08:00 INFO     Model `base_trained_ckpt/model_rillvb/epoch_7`
2023-12-18 02:08:00 INFO     	 * Num of GPU in use: 1
2023-12-18 02:08:00 INFO     	 * Prefix: True
2023-12-18 02:08:00 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 02:08:01 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 02:19:52 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 02:31:44 INFO     	Bleu_1: 0.10690702108506094
2023-12-18 02:31:44 INFO     	Bleu_2: 0.059270011481593436
2023-12-18 02:31:44 INFO     	Bleu_3: 0.031730394021842086
2023-12-18 02:31:44 INFO     	Bleu_4: 0.02029170763936699
2023-12-18 02:31:45 INFO     	Bleu_1: 0.10449199922884077
2023-12-18 02:31:45 INFO     	Bleu_2: 0.05737365884366434
2023-12-18 02:31:45 INFO     	Bleu_3: 0.030375889156233876
2023-12-18 02:31:45 INFO     	Bleu_4: 0.019271754628466475
2023-12-18 02:32:01 INFO     use spaCy answer extraction model: positionrank
2023-12-18 02:32:01 INFO     Model `base_trained_ckpt/model_rillvb/epoch_8`
2023-12-18 02:32:01 INFO     	 * Num of GPU in use: 1
2023-12-18 02:32:01 INFO     	 * Prefix: True
2023-12-18 02:32:01 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 02:32:03 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 02:43:46 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 02:55:33 INFO     	Bleu_1: 0.11156360582978402
2023-12-18 02:55:33 INFO     	Bleu_2: 0.06119874440858285
2023-12-18 02:55:33 INFO     	Bleu_3: 0.03193582433127853
2023-12-18 02:55:33 INFO     	Bleu_4: 0.020064028553459422
2023-12-18 02:55:34 INFO     	Bleu_1: 0.11031054713171004
2023-12-18 02:55:34 INFO     	Bleu_2: 0.06061515223438515
2023-12-18 02:55:34 INFO     	Bleu_3: 0.03201047721785157
2023-12-18 02:55:34 INFO     	Bleu_4: 0.020146310766423745
2023-12-18 02:55:58 INFO     use spaCy answer extraction model: positionrank
2023-12-18 02:55:59 INFO     Model `base_trained_ckpt/model_rillvb/epoch_9`
2023-12-18 02:55:59 INFO     	 * Num of GPU in use: 1
2023-12-18 02:55:59 INFO     	 * Prefix: True
2023-12-18 02:55:59 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 02:56:00 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 03:07:10 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 03:18:36 INFO     	Bleu_1: 0.12741361256544415
2023-12-18 03:18:36 INFO     	Bleu_2: 0.07028040918803258
2023-12-18 03:18:36 INFO     	Bleu_3: 0.03761385572063542
2023-12-18 03:18:36 INFO     	Bleu_4: 0.023901779174541823
2023-12-18 03:18:36 INFO     	Bleu_1: 0.12807780579718267
2023-12-18 03:18:36 INFO     	Bleu_2: 0.07053801203209645
2023-12-18 03:18:36 INFO     	Bleu_3: 0.037474254634184426
2023-12-18 03:18:36 INFO     	Bleu_4: 0.023621561874017236
2023-12-18 03:18:36 INFO     ## 2nd RUN (EVAL): Configuration 3/5 ##
2023-12-18 03:18:57 INFO     use spaCy answer extraction model: positionrank
2023-12-18 03:18:57 INFO     Model `base_trained_ckpt/model_dpyopu/epoch_1`
2023-12-18 03:18:57 INFO     	 * Num of GPU in use: 1
2023-12-18 03:18:57 INFO     	 * Prefix: True
2023-12-18 03:18:57 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 03:18:58 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 03:30:48 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 03:42:38 INFO     	Bleu_1: 0.1131366343888366
2023-12-18 03:42:38 INFO     	Bleu_2: 0.06193380825707731
2023-12-18 03:42:38 INFO     	Bleu_3: 0.03203483143460465
2023-12-18 03:42:38 INFO     	Bleu_4: 0.019806198468704184
2023-12-18 03:42:38 INFO     	Bleu_1: 0.11438167623983785
2023-12-18 03:42:38 INFO     	Bleu_2: 0.062346244378643785
2023-12-18 03:42:38 INFO     	Bleu_3: 0.03221227040954329
2023-12-18 03:42:38 INFO     	Bleu_4: 0.019874152848845777
2023-12-18 03:42:57 INFO     use spaCy answer extraction model: positionrank
2023-12-18 03:42:57 INFO     Model `base_trained_ckpt/model_dpyopu/epoch_11`
2023-12-18 03:42:57 INFO     	 * Num of GPU in use: 1
2023-12-18 03:42:57 INFO     	 * Prefix: True
2023-12-18 03:42:57 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 03:42:59 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 03:54:42 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 04:06:18 INFO     	Bleu_1: 0.1176125220172133
2023-12-18 04:06:18 INFO     	Bleu_2: 0.06504687449798542
2023-12-18 04:06:18 INFO     	Bleu_3: 0.03436769587239514
2023-12-18 04:06:18 INFO     	Bleu_4: 0.021742868985802265
2023-12-18 04:06:18 INFO     	Bleu_1: 0.11565571267426498
2023-12-18 04:06:18 INFO     	Bleu_2: 0.0644538838409705
2023-12-18 04:06:18 INFO     	Bleu_3: 0.03521026481139698
2023-12-18 04:06:18 INFO     	Bleu_4: 0.022962555307308372
2023-12-18 04:06:31 INFO     use spaCy answer extraction model: positionrank
2023-12-18 04:06:32 INFO     Model `base_trained_ckpt/model_dpyopu/epoch_12`
2023-12-18 04:06:32 INFO     	 * Num of GPU in use: 1
2023-12-18 04:06:32 INFO     	 * Prefix: True
2023-12-18 04:06:32 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 04:06:33 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 04:18:16 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 04:29:59 INFO     	Bleu_1: 0.1168165172216605
2023-12-18 04:29:59 INFO     	Bleu_2: 0.06406777818259603
2023-12-18 04:29:59 INFO     	Bleu_3: 0.03417980021101771
2023-12-18 04:29:59 INFO     	Bleu_4: 0.021740254612422908
2023-12-18 04:29:59 INFO     	Bleu_1: 0.11573859619182221
2023-12-18 04:29:59 INFO     	Bleu_2: 0.06417084047405294
2023-12-18 04:29:59 INFO     	Bleu_3: 0.03527093112009915
2023-12-18 04:29:59 INFO     	Bleu_4: 0.02302558273834126
2023-12-18 04:30:12 INFO     use spaCy answer extraction model: positionrank
2023-12-18 04:30:13 INFO     Model `base_trained_ckpt/model_dpyopu/epoch_13`
2023-12-18 04:30:13 INFO     	 * Num of GPU in use: 1
2023-12-18 04:30:13 INFO     	 * Prefix: True
2023-12-18 04:30:13 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 04:30:14 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 04:41:43 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 04:53:20 INFO     	Bleu_1: 0.12162906241196429
2023-12-18 04:53:20 INFO     	Bleu_2: 0.06767732029931103
2023-12-18 04:53:20 INFO     	Bleu_3: 0.036647063612643645
2023-12-18 04:53:20 INFO     	Bleu_4: 0.023498987264044636
2023-12-18 04:53:21 INFO     	Bleu_1: 0.11773109820386854
2023-12-18 04:53:21 INFO     	Bleu_2: 0.06495649119107913
2023-12-18 04:53:21 INFO     	Bleu_3: 0.03523565307280707
2023-12-18 04:53:21 INFO     	Bleu_4: 0.022750363968319164
2023-12-18 04:53:34 INFO     use spaCy answer extraction model: positionrank
2023-12-18 04:53:34 INFO     Model `base_trained_ckpt/model_dpyopu/epoch_14`
2023-12-18 04:53:34 INFO     	 * Num of GPU in use: 1
2023-12-18 04:53:34 INFO     	 * Prefix: True
2023-12-18 04:53:34 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 04:53:35 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 05:05:18 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 05:16:57 INFO     	Bleu_1: 0.1263155807065784
2023-12-18 05:16:57 INFO     	Bleu_2: 0.07011650386820185
2023-12-18 05:16:57 INFO     	Bleu_3: 0.03828906666984042
2023-12-18 05:16:57 INFO     	Bleu_4: 0.024887313176799545
2023-12-18 05:16:57 INFO     	Bleu_1: 0.12170509252415063
2023-12-18 05:16:57 INFO     	Bleu_2: 0.06722044511177001
2023-12-18 05:16:57 INFO     	Bleu_3: 0.036638986999222384
2023-12-18 05:16:57 INFO     	Bleu_4: 0.023742336887150876
2023-12-18 05:17:10 INFO     use spaCy answer extraction model: positionrank
2023-12-18 05:17:10 INFO     Model `base_trained_ckpt/model_dpyopu/epoch_15`
2023-12-18 05:17:10 INFO     	 * Num of GPU in use: 1
2023-12-18 05:17:10 INFO     	 * Prefix: True
2023-12-18 05:17:10 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 05:17:11 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 05:28:35 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 05:40:05 INFO     	Bleu_1: 0.13136792616856788
2023-12-18 05:40:05 INFO     	Bleu_2: 0.07259993997863505
2023-12-18 05:40:05 INFO     	Bleu_3: 0.039021751666100626
2023-12-18 05:40:05 INFO     	Bleu_4: 0.024885744077057813
2023-12-18 05:40:05 INFO     	Bleu_1: 0.1282369549946454
2023-12-18 05:40:05 INFO     	Bleu_2: 0.07123242094369647
2023-12-18 05:40:05 INFO     	Bleu_3: 0.039312839896726806
2023-12-18 05:40:05 INFO     	Bleu_4: 0.02559168484548206
2023-12-18 05:40:17 INFO     use spaCy answer extraction model: positionrank
2023-12-18 05:40:17 INFO     Model `base_trained_ckpt/model_dpyopu/epoch_2`
2023-12-18 05:40:17 INFO     	 * Num of GPU in use: 1
2023-12-18 05:40:17 INFO     	 * Prefix: True
2023-12-18 05:40:17 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 05:40:18 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 05:51:51 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 06:03:26 INFO     	Bleu_1: 0.12376574273046186
2023-12-18 06:03:26 INFO     	Bleu_2: 0.06780369918085911
2023-12-18 06:03:26 INFO     	Bleu_3: 0.03548449289062585
2023-12-18 06:03:26 INFO     	Bleu_4: 0.022248703802259007
2023-12-18 06:03:27 INFO     	Bleu_1: 0.12241012202112767
2023-12-18 06:03:27 INFO     	Bleu_2: 0.06701537484411949
2023-12-18 06:03:27 INFO     	Bleu_3: 0.035225965831616995
2023-12-18 06:03:27 INFO     	Bleu_4: 0.022041910367092243
2023-12-18 06:03:38 INFO     use spaCy answer extraction model: positionrank
2023-12-18 06:03:38 INFO     Model `base_trained_ckpt/model_dpyopu/epoch_3`
2023-12-18 06:03:38 INFO     	 * Num of GPU in use: 1
2023-12-18 06:03:38 INFO     	 * Prefix: True
2023-12-18 06:03:38 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 06:03:40 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 06:15:21 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 06:26:53 INFO     	Bleu_1: 0.11523003601334131
2023-12-18 06:26:53 INFO     	Bleu_2: 0.0629691997944532
2023-12-18 06:26:53 INFO     	Bleu_3: 0.03292580717124412
2023-12-18 06:26:53 INFO     	Bleu_4: 0.020834996558894217
2023-12-18 06:26:53 INFO     	Bleu_1: 0.11547276663555661
2023-12-18 06:26:53 INFO     	Bleu_2: 0.06344731340352801
2023-12-18 06:26:53 INFO     	Bleu_3: 0.0338535145660075
2023-12-18 06:26:53 INFO     	Bleu_4: 0.021618219227139017
2023-12-18 06:27:06 INFO     use spaCy answer extraction model: positionrank
2023-12-18 06:27:06 INFO     Model `base_trained_ckpt/model_dpyopu/epoch_4`
2023-12-18 06:27:06 INFO     	 * Num of GPU in use: 1
2023-12-18 06:27:06 INFO     	 * Prefix: True
2023-12-18 06:27:06 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 06:27:07 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 06:37:57 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 06:49:01 INFO     	Bleu_1: 0.12966886524769503
2023-12-18 06:49:01 INFO     	Bleu_2: 0.07050125505807822
2023-12-18 06:49:01 INFO     	Bleu_3: 0.035737262572235626
2023-12-18 06:49:01 INFO     	Bleu_4: 0.021829697221114913
2023-12-18 06:49:01 INFO     	Bleu_1: 0.13007660172225746
2023-12-18 06:49:01 INFO     	Bleu_2: 0.07083355065121785
2023-12-18 06:49:01 INFO     	Bleu_3: 0.03673425050085254
2023-12-18 06:49:01 INFO     	Bleu_4: 0.022947726356418067
2023-12-18 06:49:13 INFO     use spaCy answer extraction model: positionrank
2023-12-18 06:49:14 INFO     Model `base_trained_ckpt/model_dpyopu/epoch_5`
2023-12-18 06:49:14 INFO     	 * Num of GPU in use: 1
2023-12-18 06:49:14 INFO     	 * Prefix: True
2023-12-18 06:49:14 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 06:49:15 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 07:01:02 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 07:12:49 INFO     	Bleu_1: 0.10808919166408999
2023-12-18 07:12:49 INFO     	Bleu_2: 0.05946415243335169
2023-12-18 07:12:49 INFO     	Bleu_3: 0.031322034317359235
2023-12-18 07:12:49 INFO     	Bleu_4: 0.01977124734567227
2023-12-18 07:12:50 INFO     	Bleu_1: 0.10620846110876717
2023-12-18 07:12:50 INFO     	Bleu_2: 0.05850794521097311
2023-12-18 07:12:50 INFO     	Bleu_3: 0.031170746171225328
2023-12-18 07:12:50 INFO     	Bleu_4: 0.01972333608227653
2023-12-18 07:13:04 INFO     use spaCy answer extraction model: positionrank
2023-12-18 07:13:04 INFO     Model `base_trained_ckpt/model_dpyopu/epoch_6`
2023-12-18 07:13:04 INFO     	 * Num of GPU in use: 1
2023-12-18 07:13:04 INFO     	 * Prefix: True
2023-12-18 07:13:04 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 07:13:05 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 07:24:47 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 07:36:33 INFO     	Bleu_1: 0.11216371466661684
2023-12-18 07:36:33 INFO     	Bleu_2: 0.06154264446940876
2023-12-18 07:36:33 INFO     	Bleu_3: 0.031982434617724716
2023-12-18 07:36:33 INFO     	Bleu_4: 0.019995755397349612
2023-12-18 07:36:33 INFO     	Bleu_1: 0.11092467850581685
2023-12-18 07:36:33 INFO     	Bleu_2: 0.06094583320152495
2023-12-18 07:36:33 INFO     	Bleu_3: 0.031988900926451146
2023-12-18 07:36:33 INFO     	Bleu_4: 0.02011438563928875
2023-12-18 07:36:46 INFO     use spaCy answer extraction model: positionrank
2023-12-18 07:36:46 INFO     Model `base_trained_ckpt/model_dpyopu/epoch_7`
2023-12-18 07:36:46 INFO     	 * Num of GPU in use: 1
2023-12-18 07:36:46 INFO     	 * Prefix: True
2023-12-18 07:36:46 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 07:36:47 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 07:48:39 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 08:00:31 INFO     	Bleu_1: 0.10690702108506094
2023-12-18 08:00:31 INFO     	Bleu_2: 0.059270011481593436
2023-12-18 08:00:31 INFO     	Bleu_3: 0.031730394021842086
2023-12-18 08:00:31 INFO     	Bleu_4: 0.02029170763936699
2023-12-18 08:00:31 INFO     	Bleu_1: 0.10449199922884077
2023-12-18 08:00:31 INFO     	Bleu_2: 0.05737365884366434
2023-12-18 08:00:31 INFO     	Bleu_3: 0.030375889156233876
2023-12-18 08:00:31 INFO     	Bleu_4: 0.019271754628466475
2023-12-18 08:00:49 INFO     use spaCy answer extraction model: positionrank
2023-12-18 08:00:50 INFO     Model `base_trained_ckpt/model_dpyopu/epoch_8`
2023-12-18 08:00:50 INFO     	 * Num of GPU in use: 1
2023-12-18 08:00:50 INFO     	 * Prefix: True
2023-12-18 08:00:50 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 08:00:51 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 08:12:34 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 08:24:21 INFO     	Bleu_1: 0.11156360582978402
2023-12-18 08:24:21 INFO     	Bleu_2: 0.06119874440858285
2023-12-18 08:24:21 INFO     	Bleu_3: 0.03193582433127853
2023-12-18 08:24:21 INFO     	Bleu_4: 0.020064028553459422
2023-12-18 08:24:22 INFO     	Bleu_1: 0.11031054713171004
2023-12-18 08:24:22 INFO     	Bleu_2: 0.06061515223438515
2023-12-18 08:24:22 INFO     	Bleu_3: 0.03201047721785157
2023-12-18 08:24:22 INFO     	Bleu_4: 0.020146310766423745
2023-12-18 08:24:34 INFO     use spaCy answer extraction model: positionrank
2023-12-18 08:24:34 INFO     Model `base_trained_ckpt/model_dpyopu/epoch_9`
2023-12-18 08:24:34 INFO     	 * Num of GPU in use: 1
2023-12-18 08:24:34 INFO     	 * Prefix: True
2023-12-18 08:24:34 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 08:24:35 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 08:35:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 08:47:10 INFO     	Bleu_1: 0.12741361256544415
2023-12-18 08:47:10 INFO     	Bleu_2: 0.07028040918803258
2023-12-18 08:47:10 INFO     	Bleu_3: 0.03761385572063542
2023-12-18 08:47:10 INFO     	Bleu_4: 0.023901779174541823
2023-12-18 08:47:11 INFO     	Bleu_1: 0.12807780579718267
2023-12-18 08:47:11 INFO     	Bleu_2: 0.07053801203209645
2023-12-18 08:47:11 INFO     	Bleu_3: 0.037474254634184426
2023-12-18 08:47:11 INFO     	Bleu_4: 0.023621561874017236
2023-12-18 08:47:11 INFO     ## 2nd RUN (EVAL): Configuration 4/5 ##
2023-12-18 08:47:23 INFO     use spaCy answer extraction model: positionrank
2023-12-18 08:47:23 INFO     Model `base_trained_ckpt/model_nxaqhy/epoch_1`
2023-12-18 08:47:23 INFO     	 * Num of GPU in use: 1
2023-12-18 08:47:23 INFO     	 * Prefix: True
2023-12-18 08:47:23 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 08:47:24 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 08:59:12 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 09:11:02 WARNING  prediction not found at the evaluation
2023-12-18 09:11:02 WARNING  prediction not found at the evaluation
2023-12-18 09:11:02 INFO     	Bleu_1: 0.0
2023-12-18 09:11:02 INFO     	Bleu_2: 0.0
2023-12-18 09:11:02 INFO     	Bleu_3: 0.0
2023-12-18 09:11:02 INFO     	Bleu_4: 0.0
2023-12-18 09:11:02 WARNING  prediction not found at the evaluation
2023-12-18 09:11:02 WARNING  prediction not found at the evaluation
2023-12-18 09:11:02 INFO     	Bleu_1: 0.0
2023-12-18 09:11:02 INFO     	Bleu_2: 0.0
2023-12-18 09:11:02 INFO     	Bleu_3: 0.0
2023-12-18 09:11:02 INFO     	Bleu_4: 0.0
2023-12-18 09:11:17 INFO     use spaCy answer extraction model: positionrank
2023-12-18 09:11:18 INFO     Model `base_trained_ckpt/model_nxaqhy/epoch_11`
2023-12-18 09:11:18 INFO     	 * Num of GPU in use: 1
2023-12-18 09:11:18 INFO     	 * Prefix: True
2023-12-18 09:11:18 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 09:11:19 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 09:22:03 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 09:32:59 INFO     	Bleu_1: 0.16195967049453974
2023-12-18 09:32:59 INFO     	Bleu_2: 0.08879284340447476
2023-12-18 09:32:59 INFO     	Bleu_3: 0.04720964480498562
2023-12-18 09:32:59 INFO     	Bleu_4: 0.02985331129729402
2023-12-18 09:33:00 INFO     	Bleu_1: 0.1575696015513167
2023-12-18 09:33:00 INFO     	Bleu_2: 0.08629538660116547
2023-12-18 09:33:00 INFO     	Bleu_3: 0.04589577834791806
2023-12-18 09:33:00 INFO     	Bleu_4: 0.02890935489225893
2023-12-18 09:33:18 INFO     use spaCy answer extraction model: positionrank
2023-12-18 09:33:19 INFO     Model `base_trained_ckpt/model_nxaqhy/epoch_12`
2023-12-18 09:33:19 INFO     	 * Num of GPU in use: 1
2023-12-18 09:33:19 INFO     	 * Prefix: True
2023-12-18 09:33:19 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 09:33:20 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 09:44:43 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 09:56:17 INFO     	Bleu_1: 0.14177921858534392
2023-12-18 09:56:17 INFO     	Bleu_2: 0.07777299419305386
2023-12-18 09:56:17 INFO     	Bleu_3: 0.04067213447730167
2023-12-18 09:56:17 INFO     	Bleu_4: 0.025218830845913746
2023-12-18 09:56:18 INFO     	Bleu_1: 0.13828790189350507
2023-12-18 09:56:18 INFO     	Bleu_2: 0.07587812861412173
2023-12-18 09:56:18 INFO     	Bleu_3: 0.040202970686383836
2023-12-18 09:56:18 INFO     	Bleu_4: 0.025483713088263523
2023-12-18 09:56:31 INFO     use spaCy answer extraction model: positionrank
2023-12-18 09:56:32 INFO     Model `base_trained_ckpt/model_nxaqhy/epoch_13`
2023-12-18 09:56:32 INFO     	 * Num of GPU in use: 1
2023-12-18 09:56:32 INFO     	 * Prefix: True
2023-12-18 09:56:32 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 09:56:33 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 10:07:57 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 10:19:39 INFO     	Bleu_1: 0.13251816123055107
2023-12-18 10:19:39 INFO     	Bleu_2: 0.07306635492726825
2023-12-18 10:19:39 INFO     	Bleu_3: 0.038495461495095584
2023-12-18 10:19:39 INFO     	Bleu_4: 0.02411465008672035
2023-12-18 10:19:39 INFO     	Bleu_1: 0.13129424536907797
2023-12-18 10:19:39 INFO     	Bleu_2: 0.07165043083578644
2023-12-18 10:19:39 INFO     	Bleu_3: 0.03712894051759844
2023-12-18 10:19:39 INFO     	Bleu_4: 0.02309758598481529
2023-12-18 10:19:56 INFO     use spaCy answer extraction model: positionrank
2023-12-18 10:19:57 INFO     Model `base_trained_ckpt/model_nxaqhy/epoch_14`
2023-12-18 10:19:57 INFO     	 * Num of GPU in use: 1
2023-12-18 10:19:57 INFO     	 * Prefix: True
2023-12-18 10:19:57 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 10:19:58 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 10:31:46 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 10:43:39 INFO     	Bleu_1: 0.12636392640646513
2023-12-18 10:43:39 INFO     	Bleu_2: 0.06947092508627759
2023-12-18 10:43:39 INFO     	Bleu_3: 0.03628684241516132
2023-12-18 10:43:39 INFO     	Bleu_4: 0.022631927251742935
2023-12-18 10:43:40 INFO     	Bleu_1: 0.12716377196418893
2023-12-18 10:43:40 INFO     	Bleu_2: 0.0685679868563184
2023-12-18 10:43:40 INFO     	Bleu_3: 0.03448213145731623
2023-12-18 10:43:40 INFO     	Bleu_4: 0.02071231228384249
2023-12-18 10:43:53 INFO     use spaCy answer extraction model: positionrank
2023-12-18 10:43:54 INFO     Model `base_trained_ckpt/model_nxaqhy/epoch_15`
2023-12-18 10:43:54 INFO     	 * Num of GPU in use: 1
2023-12-18 10:43:54 INFO     	 * Prefix: True
2023-12-18 10:43:54 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 10:43:55 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 10:55:47 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 11:07:42 INFO     	Bleu_1: 0.12154937922490561
2023-12-18 11:07:42 INFO     	Bleu_2: 0.06672960146784948
2023-12-18 11:07:42 INFO     	Bleu_3: 0.03496677500297688
2023-12-18 11:07:42 INFO     	Bleu_4: 0.022004305265312496
2023-12-18 11:07:43 INFO     	Bleu_1: 0.12275683590424544
2023-12-18 11:07:43 INFO     	Bleu_2: 0.06692789642053776
2023-12-18 11:07:43 INFO     	Bleu_3: 0.03471766900794886
2023-12-18 11:07:43 INFO     	Bleu_4: 0.021441853888820212
2023-12-18 11:07:58 INFO     use spaCy answer extraction model: positionrank
2023-12-18 11:07:58 INFO     Model `base_trained_ckpt/model_nxaqhy/epoch_2`
2023-12-18 11:07:58 INFO     	 * Num of GPU in use: 1
2023-12-18 11:07:58 INFO     	 * Prefix: True
2023-12-18 11:07:58 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 11:07:59 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 11:20:10 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 11:32:20 WARNING  prediction not found at the evaluation
2023-12-18 11:32:20 WARNING  prediction not found at the evaluation
2023-12-18 11:32:20 INFO     	Bleu_1: 0.0
2023-12-18 11:32:20 INFO     	Bleu_2: 0.0
2023-12-18 11:32:20 INFO     	Bleu_3: 0.0
2023-12-18 11:32:20 INFO     	Bleu_4: 0.0
2023-12-18 11:32:20 WARNING  prediction not found at the evaluation
2023-12-18 11:32:20 WARNING  prediction not found at the evaluation
2023-12-18 11:32:21 INFO     	Bleu_1: 8.190734988555322e-266
2023-12-18 11:32:21 INFO     	Bleu_2: 5.136436144636343e-266
2023-12-18 11:32:21 INFO     	Bleu_3: 4.0967614247773315e-266
2023-12-18 11:32:21 INFO     	Bleu_4: 3.4145716821779944e-266
2023-12-18 11:32:36 INFO     use spaCy answer extraction model: positionrank
2023-12-18 11:32:36 INFO     Model `base_trained_ckpt/model_nxaqhy/epoch_3`
2023-12-18 11:32:36 INFO     	 * Num of GPU in use: 1
2023-12-18 11:32:36 INFO     	 * Prefix: True
2023-12-18 11:32:36 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 11:32:37 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 11:44:51 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 11:57:07 INFO     	Bleu_1: 0.10532027151420732
2023-12-18 11:57:07 INFO     	Bleu_2: 0.057475228489198206
2023-12-18 11:57:07 INFO     	Bleu_3: 0.029875055053490203
2023-12-18 11:57:07 INFO     	Bleu_4: 0.01838668808940368
2023-12-18 11:57:07 INFO     	Bleu_1: 0.102709580063171
2023-12-18 11:57:07 INFO     	Bleu_2: 0.05594622615671249
2023-12-18 11:57:07 INFO     	Bleu_3: 0.028671543764335564
2023-12-18 11:57:07 INFO     	Bleu_4: 0.017386421110865483
2023-12-18 11:57:21 INFO     use spaCy answer extraction model: positionrank
2023-12-18 11:57:21 INFO     Model `base_trained_ckpt/model_nxaqhy/epoch_4`
2023-12-18 11:57:21 INFO     	 * Num of GPU in use: 1
2023-12-18 11:57:21 INFO     	 * Prefix: True
2023-12-18 11:57:21 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 11:57:22 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 12:09:33 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 12:21:46 INFO     	Bleu_1: 0.10251546499273129
2023-12-18 12:21:46 INFO     	Bleu_2: 0.055510601870542395
2023-12-18 12:21:46 INFO     	Bleu_3: 0.028300352015759166
2023-12-18 12:21:46 INFO     	Bleu_4: 0.017179973705215304
2023-12-18 12:21:46 INFO     	Bleu_1: 0.10191577219426158
2023-12-18 12:21:46 INFO     	Bleu_2: 0.05613634682006456
2023-12-18 12:21:46 INFO     	Bleu_3: 0.02946092062279426
2023-12-18 12:21:46 INFO     	Bleu_4: 0.01832821531414814
2023-12-18 12:22:03 INFO     use spaCy answer extraction model: positionrank
2023-12-18 12:22:04 INFO     Model `base_trained_ckpt/model_nxaqhy/epoch_5`
2023-12-18 12:22:04 INFO     	 * Num of GPU in use: 1
2023-12-18 12:22:04 INFO     	 * Prefix: True
2023-12-18 12:22:04 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 12:22:05 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 12:34:13 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 12:46:22 INFO     	Bleu_1: 0.1101524799595861
2023-12-18 12:46:22 INFO     	Bleu_2: 0.06034133152242571
2023-12-18 12:46:22 INFO     	Bleu_3: 0.030944539361890313
2023-12-18 12:46:22 INFO     	Bleu_4: 0.01870947199048214
2023-12-18 12:46:22 INFO     	Bleu_1: 0.10852541611607568
2023-12-18 12:46:22 INFO     	Bleu_2: 0.059858403685317654
2023-12-18 12:46:22 INFO     	Bleu_3: 0.03168765453435526
2023-12-18 12:46:22 INFO     	Bleu_4: 0.020000955905753435
2023-12-18 12:46:35 INFO     use spaCy answer extraction model: positionrank
2023-12-18 12:46:36 INFO     Model `base_trained_ckpt/model_nxaqhy/epoch_6`
2023-12-18 12:46:36 INFO     	 * Num of GPU in use: 1
2023-12-18 12:46:36 INFO     	 * Prefix: True
2023-12-18 12:46:36 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 12:46:37 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 12:58:43 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 13:10:50 INFO     	Bleu_1: 0.1137523610497119
2023-12-18 13:10:50 INFO     	Bleu_2: 0.06231509519418358
2023-12-18 13:10:50 INFO     	Bleu_3: 0.03220214238313305
2023-12-18 13:10:50 INFO     	Bleu_4: 0.019870758548568235
2023-12-18 13:10:50 INFO     	Bleu_1: 0.11007753972106621
2023-12-18 13:10:50 INFO     	Bleu_2: 0.06039978671058758
2023-12-18 13:10:50 INFO     	Bleu_3: 0.03126990872443033
2023-12-18 13:10:50 INFO     	Bleu_4: 0.019412417824598918
2023-12-18 13:11:05 INFO     use spaCy answer extraction model: positionrank
2023-12-18 13:11:06 INFO     Model `base_trained_ckpt/model_nxaqhy/epoch_7`
2023-12-18 13:11:06 INFO     	 * Num of GPU in use: 1
2023-12-18 13:11:06 INFO     	 * Prefix: True
2023-12-18 13:11:06 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 13:11:07 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 13:23:09 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 13:35:16 INFO     	Bleu_1: 0.11850181376808515
2023-12-18 13:35:16 INFO     	Bleu_2: 0.0646940681860688
2023-12-18 13:35:16 INFO     	Bleu_3: 0.03289105567293533
2023-12-18 13:35:16 INFO     	Bleu_4: 0.019911908775500524
2023-12-18 13:35:17 INFO     	Bleu_1: 0.11922429034817754
2023-12-18 13:35:17 INFO     	Bleu_2: 0.06530266529058204
2023-12-18 13:35:17 INFO     	Bleu_3: 0.03413315412789124
2023-12-18 13:35:17 INFO     	Bleu_4: 0.021280600871961878
2023-12-18 13:35:31 INFO     use spaCy answer extraction model: positionrank
2023-12-18 13:35:32 INFO     Model `base_trained_ckpt/model_nxaqhy/epoch_8`
2023-12-18 13:35:32 INFO     	 * Num of GPU in use: 1
2023-12-18 13:35:32 INFO     	 * Prefix: True
2023-12-18 13:35:32 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 13:35:33 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 13:47:33 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 13:59:34 INFO     	Bleu_1: 0.12298943258498407
2023-12-18 13:59:34 INFO     	Bleu_2: 0.0674339321396219
2023-12-18 13:59:34 INFO     	Bleu_3: 0.034918875833083084
2023-12-18 13:59:34 INFO     	Bleu_4: 0.021531662797743895
2023-12-18 13:59:34 INFO     	Bleu_1: 0.12483420171330913
2023-12-18 13:59:34 INFO     	Bleu_2: 0.06812546972602779
2023-12-18 13:59:34 INFO     	Bleu_3: 0.03499960716837846
2023-12-18 13:59:34 INFO     	Bleu_4: 0.02140720812432713
2023-12-18 13:59:51 INFO     use spaCy answer extraction model: positionrank
2023-12-18 13:59:52 INFO     Model `base_trained_ckpt/model_nxaqhy/epoch_9`
2023-12-18 13:59:52 INFO     	 * Num of GPU in use: 1
2023-12-18 13:59:52 INFO     	 * Prefix: True
2023-12-18 13:59:52 INFO     	 * Language: en (ignore at the training phase)
2023-12-18 13:59:53 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-18 14:11:48 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-base.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-18 14:23:48 INFO     	Bleu_1: 0.1253772030846821
2023-12-18 14:23:48 INFO     	Bleu_2: 0.06866049515002691
2023-12-18 14:23:48 INFO     	Bleu_3: 0.035763005159168644
2023-12-18 14:23:48 INFO     	Bleu_4: 0.022235544045002973
2023-12-18 14:23:49 INFO     	Bleu_1: 0.12696368412244421
2023-12-18 14:23:49 INFO     	Bleu_2: 0.06964860105028393
2023-12-18 14:23:49 INFO     	Bleu_3: 0.03654304481779398
2023-12-18 14:23:49 INFO     	Bleu_4: 0.022848994159637284
2023-12-18 14:23:49 INFO     2nd RUN RESULTS: 
[('base_trained_ckpt/model_mzgdpa/epoch_14', 0.03349692010094845), ('base_trained_ckpt/model_mzgdpa/epoch_15', 0.03291405046612744), ('base_trained_ckpt/model_eszyci/epoch_15', 0.03201097272182527), ('base_trained_ckpt/model_eszyci/epoch_14', 0.03095930277951126), ('base_trained_ckpt/model_mzgdpa/epoch_13', 0.030539986616525256), ('base_trained_ckpt/model_nxaqhy/epoch_11', 0.02985331129729402), ('base_trained_ckpt/model_eszyci/epoch_13', 0.028865019879046532), ('base_trained_ckpt/model_mzgdpa/epoch_12', 0.02865861252330175), ('base_trained_ckpt/model_rillvb/epoch_14', 0.02810175286994642), ('base_trained_ckpt/model_eszyci/epoch_11', 0.027318237753903536), ('base_trained_ckpt/model_eszyci/epoch_12', 0.02580254335493972), ('base_trained_ckpt/model_mzgdpa/epoch_11', 0.025484203963500755), ('base_trained_ckpt/model_nxaqhy/epoch_12', 0.025218830845913746), ('base_trained_ckpt/model_rillvb/epoch_15', 0.025176825966023778), ('base_trained_ckpt/model_rillvb/epoch_13', 0.024977194531936288), ('base_trained_ckpt/model_eszyci/epoch_9', 0.02496695879641108), ('base_trained_ckpt/model_mzgdpa/epoch_9', 0.02496695879641108), ('base_trained_ckpt/model_dpyopu/epoch_14', 0.024887313176799545), ('base_trained_ckpt/model_dpyopu/epoch_15', 0.024885744077057813), ('base_trained_ckpt/model_rillvb/epoch_11', 0.02451032152090238), ('base_trained_ckpt/model_nxaqhy/epoch_13', 0.02411465008672035), ('base_trained_ckpt/model_eszyci/epoch_10', 0.024054985672499824), ('base_trained_ckpt/model_mzgdpa/epoch_10', 0.024054985672499824), ('base_trained_ckpt/model_eszyci/epoch_1', 0.024009352858761487), ('base_trained_ckpt/model_mzgdpa/epoch_1', 0.024009352858761487), ('base_trained_ckpt/model_rillvb/epoch_12', 0.023988366371189437), ('base_trained_ckpt/model_rillvb/epoch_9', 0.023901779174541823), ('base_trained_ckpt/model_dpyopu/epoch_9', 0.023901779174541823), ('base_trained_ckpt/model_dpyopu/epoch_13', 0.023498987264044636), ('base_trained_ckpt/model_eszyci/epoch_4', 0.023469185402777953), ('base_trained_ckpt/model_mzgdpa/epoch_4', 0.023469185402777953), ('base_trained_ckpt/model_rillvb/epoch_10', 0.022800842017452354), ('base_trained_ckpt/model_dpyopu/epoch_10', 0.022800842017452354), ('base_trained_ckpt/model_nxaqhy/epoch_14', 0.022631927251742935), ('base_trained_ckpt/model_rillvb/epoch_2', 0.022248703802259007), ('base_trained_ckpt/model_dpyopu/epoch_2', 0.022248703802259007), ('base_trained_ckpt/model_nxaqhy/epoch_9', 0.022235544045002973), ('base_trained_ckpt/model_eszyci/epoch_7', 0.022175168296980078), ('base_trained_ckpt/model_mzgdpa/epoch_7', 0.022175168296980078), ('base_trained_ckpt/model_nxaqhy/epoch_10', 0.02203525356928333), ('base_trained_ckpt/model_nxaqhy/epoch_15', 0.022004305265312496), ('base_trained_ckpt/model_rillvb/epoch_4', 0.021829697221114913), ('base_trained_ckpt/model_dpyopu/epoch_4', 0.021829697221114913), ('base_trained_ckpt/model_eszyci/epoch_3', 0.02175253935748025), ('base_trained_ckpt/model_mzgdpa/epoch_3', 0.02175253935748025), ('base_trained_ckpt/model_dpyopu/epoch_11', 0.021742868985802265), ('base_trained_ckpt/model_dpyopu/epoch_12', 0.021740254612422908), ('base_trained_ckpt/model_nxaqhy/epoch_8', 0.021531662797743895), ('base_trained_ckpt/model_eszyci/epoch_2', 0.02130959300377921), ('base_trained_ckpt/model_mzgdpa/epoch_2', 0.02130959300377921), ('base_trained_ckpt/model_eszyci/epoch_8', 0.021212958422077884), ('base_trained_ckpt/model_mzgdpa/epoch_8', 0.021212958422077884), ('base_trained_ckpt/model_rillvb/epoch_3', 0.020834996558894217), ('base_trained_ckpt/model_dpyopu/epoch_3', 0.020834996558894217), ('base_trained_ckpt/model_rillvb/epoch_7', 0.02029170763936699), ('base_trained_ckpt/model_dpyopu/epoch_7', 0.02029170763936699), ('base_trained_ckpt/model_eszyci/epoch_6', 0.020076824325041213), ('base_trained_ckpt/model_mzgdpa/epoch_6', 0.020076824325041213), ('base_trained_ckpt/model_rillvb/epoch_8', 0.020064028553459422), ('base_trained_ckpt/model_dpyopu/epoch_8', 0.020064028553459422), ('base_trained_ckpt/model_rillvb/epoch_6', 0.019995755397349612), ('base_trained_ckpt/model_dpyopu/epoch_6', 0.019995755397349612), ('base_trained_ckpt/model_nxaqhy/epoch_7', 0.019911908775500524), ('base_trained_ckpt/model_nxaqhy/epoch_6', 0.019870758548568235), ('base_trained_ckpt/model_rillvb/epoch_1', 0.019806198468704184), ('base_trained_ckpt/model_dpyopu/epoch_1', 0.019806198468704184), ('base_trained_ckpt/model_rillvb/epoch_5', 0.01977124734567227), ('base_trained_ckpt/model_dpyopu/epoch_5', 0.01977124734567227), ('base_trained_ckpt/model_eszyci/epoch_5', 0.018795056726771793), ('base_trained_ckpt/model_mzgdpa/epoch_5', 0.018795056726771793), ('base_trained_ckpt/model_nxaqhy/epoch_5', 0.01870947199048214), ('base_trained_ckpt/model_nxaqhy/epoch_3', 0.01838668808940368), ('base_trained_ckpt/model_nxaqhy/epoch_4', 0.017179973705215304), ('base_trained_ckpt/model_nxaqhy/epoch_1', 0.0), ('base_trained_ckpt/model_nxaqhy/epoch_2', 0.0)]
2023-12-18 14:23:49 INFO     	 * rank: 0 | metric: 0.033 | model: base_trained_ckpt/model_mzgdpa/epoch_14 |
2023-12-18 14:23:49 INFO     	 * rank: 1 | metric: 0.033 | model: base_trained_ckpt/model_mzgdpa/epoch_15 |
2023-12-18 14:23:49 INFO     	 * rank: 2 | metric: 0.032 | model: base_trained_ckpt/model_eszyci/epoch_15 |
2023-12-18 14:23:49 INFO     	 * rank: 3 | metric: 0.031 | model: base_trained_ckpt/model_eszyci/epoch_14 |
2023-12-18 14:23:49 INFO     	 * rank: 4 | metric: 0.031 | model: base_trained_ckpt/model_mzgdpa/epoch_13 |
2023-12-18 14:23:49 INFO     	 * rank: 5 | metric: 0.03 | model: base_trained_ckpt/model_nxaqhy/epoch_11 |
2023-12-18 14:23:49 INFO     	 * rank: 6 | metric: 0.029 | model: base_trained_ckpt/model_eszyci/epoch_13 |
2023-12-18 14:23:49 INFO     	 * rank: 7 | metric: 0.029 | model: base_trained_ckpt/model_mzgdpa/epoch_12 |
2023-12-18 14:23:49 INFO     	 * rank: 8 | metric: 0.028 | model: base_trained_ckpt/model_rillvb/epoch_14 |
2023-12-18 14:23:49 INFO     	 * rank: 9 | metric: 0.027 | model: base_trained_ckpt/model_eszyci/epoch_11 |
2023-12-18 14:23:49 INFO     	 * rank: 10 | metric: 0.026 | model: base_trained_ckpt/model_eszyci/epoch_12 |
2023-12-18 14:23:49 INFO     	 * rank: 11 | metric: 0.025 | model: base_trained_ckpt/model_mzgdpa/epoch_11 |
2023-12-18 14:23:49 INFO     	 * rank: 12 | metric: 0.025 | model: base_trained_ckpt/model_nxaqhy/epoch_12 |
2023-12-18 14:23:49 INFO     	 * rank: 13 | metric: 0.025 | model: base_trained_ckpt/model_rillvb/epoch_15 |
2023-12-18 14:23:49 INFO     	 * rank: 14 | metric: 0.025 | model: base_trained_ckpt/model_rillvb/epoch_13 |
2023-12-18 14:23:49 INFO     	 * rank: 15 | metric: 0.025 | model: base_trained_ckpt/model_eszyci/epoch_9 |
2023-12-18 14:23:49 INFO     	 * rank: 16 | metric: 0.025 | model: base_trained_ckpt/model_mzgdpa/epoch_9 |
2023-12-18 14:23:49 INFO     	 * rank: 17 | metric: 0.025 | model: base_trained_ckpt/model_dpyopu/epoch_14 |
2023-12-18 14:23:49 INFO     	 * rank: 18 | metric: 0.025 | model: base_trained_ckpt/model_dpyopu/epoch_15 |
2023-12-18 14:23:49 INFO     	 * rank: 19 | metric: 0.025 | model: base_trained_ckpt/model_rillvb/epoch_11 |
2023-12-18 14:23:49 INFO     	 * rank: 20 | metric: 0.024 | model: base_trained_ckpt/model_nxaqhy/epoch_13 |
2023-12-18 14:23:49 INFO     	 * rank: 21 | metric: 0.024 | model: base_trained_ckpt/model_eszyci/epoch_10 |
2023-12-18 14:23:49 INFO     	 * rank: 22 | metric: 0.024 | model: base_trained_ckpt/model_mzgdpa/epoch_10 |
2023-12-18 14:23:49 INFO     	 * rank: 23 | metric: 0.024 | model: base_trained_ckpt/model_eszyci/epoch_1 |
2023-12-18 14:23:49 INFO     	 * rank: 24 | metric: 0.024 | model: base_trained_ckpt/model_mzgdpa/epoch_1 |
2023-12-18 14:23:49 INFO     	 * rank: 25 | metric: 0.024 | model: base_trained_ckpt/model_rillvb/epoch_12 |
2023-12-18 14:23:49 INFO     	 * rank: 26 | metric: 0.024 | model: base_trained_ckpt/model_rillvb/epoch_9 |
2023-12-18 14:23:49 INFO     	 * rank: 27 | metric: 0.024 | model: base_trained_ckpt/model_dpyopu/epoch_9 |
2023-12-18 14:23:49 INFO     	 * rank: 28 | metric: 0.023 | model: base_trained_ckpt/model_dpyopu/epoch_13 |
2023-12-18 14:23:49 INFO     	 * rank: 29 | metric: 0.023 | model: base_trained_ckpt/model_eszyci/epoch_4 |
2023-12-18 14:23:49 INFO     	 * rank: 30 | metric: 0.023 | model: base_trained_ckpt/model_mzgdpa/epoch_4 |
2023-12-18 14:23:49 INFO     	 * rank: 31 | metric: 0.023 | model: base_trained_ckpt/model_rillvb/epoch_10 |
2023-12-18 14:23:49 INFO     	 * rank: 32 | metric: 0.023 | model: base_trained_ckpt/model_dpyopu/epoch_10 |
2023-12-18 14:23:49 INFO     	 * rank: 33 | metric: 0.023 | model: base_trained_ckpt/model_nxaqhy/epoch_14 |
2023-12-18 14:23:49 INFO     	 * rank: 34 | metric: 0.022 | model: base_trained_ckpt/model_rillvb/epoch_2 |
2023-12-18 14:23:49 INFO     	 * rank: 35 | metric: 0.022 | model: base_trained_ckpt/model_dpyopu/epoch_2 |
2023-12-18 14:23:49 INFO     	 * rank: 36 | metric: 0.022 | model: base_trained_ckpt/model_nxaqhy/epoch_9 |
2023-12-18 14:23:49 INFO     	 * rank: 37 | metric: 0.022 | model: base_trained_ckpt/model_eszyci/epoch_7 |
2023-12-18 14:23:49 INFO     	 * rank: 38 | metric: 0.022 | model: base_trained_ckpt/model_mzgdpa/epoch_7 |
2023-12-18 14:23:49 INFO     	 * rank: 39 | metric: 0.022 | model: base_trained_ckpt/model_nxaqhy/epoch_10 |
2023-12-18 14:23:49 INFO     	 * rank: 40 | metric: 0.022 | model: base_trained_ckpt/model_nxaqhy/epoch_15 |
2023-12-18 14:23:49 INFO     	 * rank: 41 | metric: 0.022 | model: base_trained_ckpt/model_rillvb/epoch_4 |
2023-12-18 14:23:49 INFO     	 * rank: 42 | metric: 0.022 | model: base_trained_ckpt/model_dpyopu/epoch_4 |
2023-12-18 14:23:49 INFO     	 * rank: 43 | metric: 0.022 | model: base_trained_ckpt/model_eszyci/epoch_3 |
2023-12-18 14:23:49 INFO     	 * rank: 44 | metric: 0.022 | model: base_trained_ckpt/model_mzgdpa/epoch_3 |
2023-12-18 14:23:49 INFO     	 * rank: 45 | metric: 0.022 | model: base_trained_ckpt/model_dpyopu/epoch_11 |
2023-12-18 14:23:49 INFO     	 * rank: 46 | metric: 0.022 | model: base_trained_ckpt/model_dpyopu/epoch_12 |
2023-12-18 14:23:49 INFO     	 * rank: 47 | metric: 0.022 | model: base_trained_ckpt/model_nxaqhy/epoch_8 |
2023-12-18 14:23:49 INFO     	 * rank: 48 | metric: 0.021 | model: base_trained_ckpt/model_eszyci/epoch_2 |
2023-12-18 14:23:49 INFO     	 * rank: 49 | metric: 0.021 | model: base_trained_ckpt/model_mzgdpa/epoch_2 |
2023-12-18 14:23:49 INFO     	 * rank: 50 | metric: 0.021 | model: base_trained_ckpt/model_eszyci/epoch_8 |
2023-12-18 14:23:49 INFO     	 * rank: 51 | metric: 0.021 | model: base_trained_ckpt/model_mzgdpa/epoch_8 |
2023-12-18 14:23:49 INFO     	 * rank: 52 | metric: 0.021 | model: base_trained_ckpt/model_rillvb/epoch_3 |
2023-12-18 14:23:49 INFO     	 * rank: 53 | metric: 0.021 | model: base_trained_ckpt/model_dpyopu/epoch_3 |
2023-12-18 14:23:49 INFO     	 * rank: 54 | metric: 0.02 | model: base_trained_ckpt/model_rillvb/epoch_7 |
2023-12-18 14:23:49 INFO     	 * rank: 55 | metric: 0.02 | model: base_trained_ckpt/model_dpyopu/epoch_7 |
2023-12-18 14:23:49 INFO     	 * rank: 56 | metric: 0.02 | model: base_trained_ckpt/model_eszyci/epoch_6 |
2023-12-18 14:23:49 INFO     	 * rank: 57 | metric: 0.02 | model: base_trained_ckpt/model_mzgdpa/epoch_6 |
2023-12-18 14:23:49 INFO     	 * rank: 58 | metric: 0.02 | model: base_trained_ckpt/model_rillvb/epoch_8 |
2023-12-18 14:23:49 INFO     	 * rank: 59 | metric: 0.02 | model: base_trained_ckpt/model_dpyopu/epoch_8 |
2023-12-18 14:23:49 INFO     	 * rank: 60 | metric: 0.02 | model: base_trained_ckpt/model_rillvb/epoch_6 |
2023-12-18 14:23:49 INFO     	 * rank: 61 | metric: 0.02 | model: base_trained_ckpt/model_dpyopu/epoch_6 |
2023-12-18 14:23:49 INFO     	 * rank: 62 | metric: 0.02 | model: base_trained_ckpt/model_nxaqhy/epoch_7 |
2023-12-18 14:23:49 INFO     	 * rank: 63 | metric: 0.02 | model: base_trained_ckpt/model_nxaqhy/epoch_6 |
2023-12-18 14:23:49 INFO     	 * rank: 64 | metric: 0.02 | model: base_trained_ckpt/model_rillvb/epoch_1 |
2023-12-18 14:23:49 INFO     	 * rank: 65 | metric: 0.02 | model: base_trained_ckpt/model_dpyopu/epoch_1 |
2023-12-18 14:23:49 INFO     	 * rank: 66 | metric: 0.02 | model: base_trained_ckpt/model_rillvb/epoch_5 |
2023-12-18 14:23:49 INFO     	 * rank: 67 | metric: 0.02 | model: base_trained_ckpt/model_dpyopu/epoch_5 |
2023-12-18 14:23:49 INFO     	 * rank: 68 | metric: 0.019 | model: base_trained_ckpt/model_eszyci/epoch_5 |
2023-12-18 14:23:49 INFO     	 * rank: 69 | metric: 0.019 | model: base_trained_ckpt/model_mzgdpa/epoch_5 |
2023-12-18 14:23:49 INFO     	 * rank: 70 | metric: 0.019 | model: base_trained_ckpt/model_nxaqhy/epoch_5 |
2023-12-18 14:23:49 INFO     	 * rank: 71 | metric: 0.018 | model: base_trained_ckpt/model_nxaqhy/epoch_3 |
2023-12-18 14:23:49 INFO     	 * rank: 72 | metric: 0.017 | model: base_trained_ckpt/model_nxaqhy/epoch_4 |
2023-12-18 14:23:49 INFO     	 * rank: 73 | metric: 0.0 | model: base_trained_ckpt/model_nxaqhy/epoch_1 |
2023-12-18 14:23:49 INFO     	 * rank: 74 | metric: 0.0 | model: base_trained_ckpt/model_nxaqhy/epoch_2 |
2023-12-18 14:23:49 INFO     creating base_trained_ckpt/best_model
2023-12-18 14:23:49 INFO     copying base_trained_ckpt/model_mzgdpa/epoch_14/config.json -> base_trained_ckpt/best_model
2023-12-18 14:23:49 INFO     copying base_trained_ckpt/model_mzgdpa/epoch_14/generation_config.json -> base_trained_ckpt/best_model
2023-12-18 14:23:49 INFO     copying base_trained_ckpt/model_mzgdpa/epoch_14/pytorch_model.bin -> base_trained_ckpt/best_model
2023-12-18 14:23:50 INFO     copying base_trained_ckpt/model_mzgdpa/epoch_14/tokenizer_config.json -> base_trained_ckpt/best_model
2023-12-18 14:23:50 INFO     copying base_trained_ckpt/model_mzgdpa/epoch_14/special_tokens_map.json -> base_trained_ckpt/best_model
2023-12-18 14:23:50 INFO     copying base_trained_ckpt/model_mzgdpa/epoch_14/added_tokens.json -> base_trained_ckpt/best_model
2023-12-18 14:23:50 INFO     copying base_trained_ckpt/model_mzgdpa/epoch_14/spiece.model -> base_trained_ckpt/best_model
2023-12-18 14:23:51 INFO     copying base_trained_ckpt/model_mzgdpa/epoch_14/tokenizer.json -> base_trained_ckpt/best_model
2023-12-18 14:23:51 INFO     copying base_trained_ckpt/model_mzgdpa/epoch_14/trainer_config.json -> base_trained_ckpt/best_model
2023-12-18 14:23:51 INFO     creating base_trained_ckpt/best_model/eval
2023-12-18 14:23:51 INFO     copying base_trained_ckpt/model_mzgdpa/epoch_14/eval/samples.test.hyp.paragraph.questions_answers.StellarMilk_newsqa.default.txt -> base_trained_ckpt/best_model/eval
2023-12-18 14:23:51 INFO     copying base_trained_ckpt/model_mzgdpa/epoch_14/eval/samples.validation.hyp.paragraph.questions_answers.StellarMilk_newsqa.default.txt -> base_trained_ckpt/best_model/eval
2023-12-18 14:23:51 INFO     copying base_trained_ckpt/model_mzgdpa/epoch_14/eval/metric.first.answer.paragraph.questions_answers.StellarMilk_newsqa.default.json -> base_trained_ckpt/best_model/eval
