2023-12-20 10:46:12 INFO     INITIALIZE GRID SEARCHER: 4 configs to try
2023-12-20 10:46:12 INFO     ## 1st RUN: Configuration 0/4 ##
2023-12-20 10:46:12 INFO     initialize model trainer
2023-12-20 10:46:12 INFO     initialize checkpoint at small_finetuned_ckpt/model_yhgxcx
2023-12-20 10:46:12 INFO     hyperparameters
2023-12-20 10:46:12 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-20 10:46:12 INFO     	 * dataset_name: default
2023-12-20 10:46:12 INFO     	 * input_types: ['paragraph']
2023-12-20 10:46:12 INFO     	 * output_types: ['questions_answers']
2023-12-20 10:46:12 INFO     	 * prefix_types: ['qag']
2023-12-20 10:46:12 INFO     	 * model: lmqg/t5-small-squad-qag
2023-12-20 10:46:12 INFO     	 * max_length: 512
2023-12-20 10:46:12 INFO     	 * max_length_output: 512
2023-12-20 10:46:12 INFO     	 * epoch: 10
2023-12-20 10:46:12 INFO     	 * batch: 2
2023-12-20 10:46:12 INFO     	 * lr: 1e-05
2023-12-20 10:46:12 INFO     	 * fp16: False
2023-12-20 10:46:12 INFO     	 * random_seed: 1
2023-12-20 10:46:12 INFO     	 * gradient_accumulation_steps: 4
2023-12-20 10:46:12 INFO     	 * label_smoothing: 0.15
2023-12-20 10:46:12 INFO     initialize checkpoint with lmqg/t5-small-squad-qag
2023-12-20 10:46:14 INFO     use spaCy answer extraction model: positionrank
2023-12-20 10:46:15 INFO     Model `lmqg/t5-small-squad-qag`
2023-12-20 10:46:15 INFO     	 * Num of GPU in use: 1
2023-12-20 10:46:15 INFO     	 * Prefix: True
2023-12-20 10:46:15 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 10:46:15 INFO     dataset preprocessing
/home2/g.torresgamez/.local/lib/python3.10/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=False' instead.
  warnings.warn(
2023-12-20 10:46:18 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-20 10:46:18 INFO     start model training
2023-12-20 10:46:34 INFO     	 * (global step 50: loss: 0.4174450859427452, lr: 1e-05
2023-12-20 10:46:49 INFO     	 * (global step 100: loss: 0.44526612013578415, lr: 1e-05
2023-12-20 10:47:04 INFO     	 * (global step 150: loss: 0.4049782007932663, lr: 1e-05
2023-12-20 10:47:19 INFO     	 * (global step 200: loss: 0.3935903534293175, lr: 1e-05
2023-12-20 10:47:34 INFO     	 * (global step 250: loss: 0.48380047082901, lr: 1e-05
2023-12-20 10:47:50 INFO     	 * (global step 300: loss: 0.5217370912432671, lr: 1e-05
2023-12-20 10:47:53 INFO     [epoch 0/10] average loss: 0.447, lr: 1e-05
2023-12-20 10:47:53 INFO     saving model related files
2023-12-20 10:47:53 INFO     saving model
2023-12-20 10:47:54 INFO     saving tokenizer
2023-12-20 10:47:54 INFO     saving optimizer
2023-12-20 10:47:55 INFO     remove old optimizer files
2023-12-20 10:48:07 INFO     	 * (global step 350: loss: 0.38673805817961693, lr: 1e-05
2023-12-20 10:48:22 INFO     	 * (global step 400: loss: 0.3854989558458328, lr: 1e-05
2023-12-20 10:48:38 INFO     	 * (global step 450: loss: 0.5139632523059845, lr: 1e-05
2023-12-20 10:48:53 INFO     	 * (global step 500: loss: 0.5006912723183632, lr: 1e-05
2023-12-20 10:49:09 INFO     	 * (global step 550: loss: 0.5277958959341049, lr: 1e-05
2023-12-20 10:49:25 INFO     	 * (global step 600: loss: 0.45590126514434814, lr: 1e-05
2023-12-20 10:49:31 INFO     [epoch 1/10] average loss: 0.415, lr: 1e-05
2023-12-20 10:49:31 INFO     saving model related files
2023-12-20 10:49:31 INFO     saving model
2023-12-20 10:49:32 INFO     saving tokenizer
2023-12-20 10:49:32 INFO     saving optimizer
2023-12-20 10:49:32 INFO     remove old optimizer files
2023-12-20 10:49:42 INFO     	 * (global step 650: loss: 0.3872021660208702, lr: 1e-05
2023-12-20 10:49:57 INFO     	 * (global step 700: loss: 0.3514437712728977, lr: 1e-05
2023-12-20 10:50:13 INFO     	 * (global step 750: loss: 0.36614473164081573, lr: 1e-05
2023-12-20 10:50:29 INFO     	 * (global step 800: loss: 0.38412441313266754, lr: 1e-05
2023-12-20 10:50:44 INFO     	 * (global step 850: loss: 0.5429331362247467, lr: 1e-05
2023-12-20 10:51:00 INFO     	 * (global step 900: loss: 0.541653111577034, lr: 1e-05
2023-12-20 10:51:09 INFO     [epoch 2/10] average loss: 0.405, lr: 1e-05
2023-12-20 10:51:09 INFO     saving model related files
2023-12-20 10:51:09 INFO     saving model
2023-12-20 10:51:10 INFO     saving tokenizer
2023-12-20 10:51:10 INFO     saving optimizer
2023-12-20 10:51:11 INFO     remove old optimizer files
2023-12-20 10:51:17 INFO     	 * (global step 950: loss: 0.5032149404287338, lr: 1e-05
2023-12-20 10:51:33 INFO     	 * (global step 1000: loss: 0.3875398226082325, lr: 1e-05
2023-12-20 10:51:48 INFO     	 * (global step 1050: loss: 0.3773440942168236, lr: 1e-05
2023-12-20 10:52:04 INFO     	 * (global step 1100: loss: 0.33245617151260376, lr: 1e-05
2023-12-20 10:52:20 INFO     	 * (global step 1150: loss: 0.3028443232178688, lr: 1e-05
2023-12-20 10:52:35 INFO     	 * (global step 1200: loss: 0.37584253400564194, lr: 1e-05
2023-12-20 10:52:48 INFO     [epoch 3/10] average loss: 0.399, lr: 1e-05
2023-12-20 10:52:48 INFO     saving model related files
2023-12-20 10:52:48 INFO     saving model
2023-12-20 10:52:49 INFO     saving tokenizer
2023-12-20 10:52:49 INFO     saving optimizer
2023-12-20 10:52:50 INFO     remove old optimizer files
2023-12-20 10:52:53 INFO     	 * (global step 1250: loss: 0.40696991980075836, lr: 1e-05
2023-12-20 10:53:08 INFO     	 * (global step 1300: loss: 0.2805713154375553, lr: 1e-05
2023-12-20 10:53:24 INFO     	 * (global step 1350: loss: 0.3866093009710312, lr: 1e-05
2023-12-20 10:53:40 INFO     	 * (global step 1400: loss: 0.30510955303907394, lr: 1e-05
2023-12-20 10:53:55 INFO     	 * (global step 1450: loss: 0.34071657061576843, lr: 1e-05
2023-12-20 10:54:11 INFO     	 * (global step 1500: loss: 0.5018950253725052, lr: 1e-05
2023-12-20 10:54:27 INFO     	 * (global step 1550: loss: 0.3489021733403206, lr: 1e-05
2023-12-20 10:54:27 INFO     [epoch 4/10] average loss: 0.394, lr: 1e-05
2023-12-20 10:54:27 INFO     saving model related files
2023-12-20 10:54:27 INFO     saving model
2023-12-20 10:54:27 INFO     saving tokenizer
2023-12-20 10:54:27 INFO     saving optimizer
2023-12-20 10:54:28 INFO     remove old optimizer files
2023-12-20 10:54:28 INFO     complete training: model ckpt was saved at small_finetuned_ckpt/model_yhgxcx
2023-12-20 10:54:28 INFO     ## 1st RUN: Configuration 1/4 ##
2023-12-20 10:54:28 INFO     initialize model trainer
2023-12-20 10:54:28 INFO     initialize checkpoint at small_finetuned_ckpt/model_eszyci
2023-12-20 10:54:28 INFO     hyperparameters
2023-12-20 10:54:28 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-20 10:54:28 INFO     	 * dataset_name: default
2023-12-20 10:54:28 INFO     	 * input_types: ['paragraph']
2023-12-20 10:54:28 INFO     	 * output_types: ['questions_answers']
2023-12-20 10:54:28 INFO     	 * prefix_types: ['qag']
2023-12-20 10:54:28 INFO     	 * model: lmqg/t5-small-squad-qag
2023-12-20 10:54:28 INFO     	 * max_length: 512
2023-12-20 10:54:28 INFO     	 * max_length_output: 512
2023-12-20 10:54:28 INFO     	 * epoch: 10
2023-12-20 10:54:28 INFO     	 * batch: 2
2023-12-20 10:54:28 INFO     	 * lr: 1e-05
2023-12-20 10:54:28 INFO     	 * fp16: False
2023-12-20 10:54:28 INFO     	 * random_seed: 1
2023-12-20 10:54:28 INFO     	 * gradient_accumulation_steps: 2
2023-12-20 10:54:28 INFO     	 * label_smoothing: 0.15
2023-12-20 10:54:28 INFO     initialize checkpoint with lmqg/t5-small-squad-qag
2023-12-20 10:54:30 INFO     use spaCy answer extraction model: positionrank
2023-12-20 10:54:30 INFO     Model `lmqg/t5-small-squad-qag`
2023-12-20 10:54:30 INFO     	 * Num of GPU in use: 1
2023-12-20 10:54:30 INFO     	 * Prefix: True
2023-12-20 10:54:30 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 10:54:30 INFO     dataset preprocessing
2023-12-20 10:54:31 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-20 10:54:31 INFO     start model training
2023-12-20 10:54:39 INFO     	 * (global step 50: loss: 0.4581184983253479, lr: 1e-05
2023-12-20 10:54:47 INFO     	 * (global step 100: loss: 0.31322166323661804, lr: 1e-05
2023-12-20 10:54:56 INFO     	 * (global step 150: loss: 0.4114130139350891, lr: 1e-05
2023-12-20 10:55:04 INFO     	 * (global step 200: loss: 0.4368223398923874, lr: 1e-05
2023-12-20 10:55:12 INFO     	 * (global step 250: loss: 0.45259372889995575, lr: 1e-05
2023-12-20 10:55:20 INFO     	 * (global step 300: loss: 0.3870916813611984, lr: 1e-05
2023-12-20 10:55:28 INFO     	 * (global step 350: loss: 0.6222658753395081, lr: 1e-05
2023-12-20 10:55:36 INFO     	 * (global step 400: loss: 0.3737747520208359, lr: 1e-05
2023-12-20 10:55:44 INFO     	 * (global step 450: loss: 0.3162582144141197, lr: 1e-05
2023-12-20 10:55:52 INFO     	 * (global step 500: loss: 0.44897033274173737, lr: 1e-05
2023-12-20 10:56:00 INFO     	 * (global step 550: loss: 0.3220909982919693, lr: 1e-05
2023-12-20 10:56:08 INFO     	 * (global step 600: loss: 0.6511140465736389, lr: 1e-05
2023-12-20 10:56:12 INFO     [epoch 0/10] average loss: 0.437, lr: 1e-05
2023-12-20 10:56:12 INFO     saving model related files
2023-12-20 10:56:12 INFO     saving model
2023-12-20 10:56:12 INFO     saving tokenizer
2023-12-20 10:56:12 INFO     saving optimizer
2023-12-20 10:56:13 INFO     remove old optimizer files
2023-12-20 10:56:18 INFO     	 * (global step 650: loss: 0.4795816093683243, lr: 1e-05
2023-12-20 10:56:26 INFO     	 * (global step 700: loss: 0.43261465430259705, lr: 1e-05
2023-12-20 10:56:34 INFO     	 * (global step 750: loss: 0.350529745221138, lr: 1e-05
2023-12-20 10:56:42 INFO     	 * (global step 800: loss: 0.4104063808917999, lr: 1e-05
2023-12-20 10:56:50 INFO     	 * (global step 850: loss: 0.6829293370246887, lr: 1e-05
2023-12-20 10:56:58 INFO     	 * (global step 900: loss: 0.5406392365694046, lr: 1e-05
2023-12-20 10:57:06 INFO     	 * (global step 950: loss: 0.3603086471557617, lr: 1e-05
2023-12-20 10:57:15 INFO     	 * (global step 1000: loss: 0.6313239485025406, lr: 1e-05
2023-12-20 10:57:23 INFO     	 * (global step 1050: loss: 0.3617406040430069, lr: 1e-05
2023-12-20 10:57:31 INFO     	 * (global step 1100: loss: 0.4810079038143158, lr: 1e-05
2023-12-20 10:57:39 INFO     	 * (global step 1150: loss: 0.25516095757484436, lr: 1e-05
2023-12-20 10:57:47 INFO     	 * (global step 1200: loss: 0.6278806626796722, lr: 1e-05
2023-12-20 10:57:54 INFO     [epoch 1/10] average loss: 0.408, lr: 1e-05
2023-12-20 10:57:54 INFO     saving model related files
2023-12-20 10:57:54 INFO     saving model
2023-12-20 10:57:54 INFO     saving tokenizer
2023-12-20 10:57:54 INFO     saving optimizer
2023-12-20 10:57:55 INFO     remove old optimizer files
2023-12-20 10:57:56 INFO     	 * (global step 1250: loss: 0.4877290427684784, lr: 1e-05
2023-12-20 10:58:05 INFO     	 * (global step 1300: loss: 0.29171570390462875, lr: 1e-05
2023-12-20 10:58:13 INFO     	 * (global step 1350: loss: 0.2023996114730835, lr: 1e-05
2023-12-20 10:58:21 INFO     	 * (global step 1400: loss: 0.489560142159462, lr: 1e-05
2023-12-20 10:58:29 INFO     	 * (global step 1450: loss: 0.42874380946159363, lr: 1e-05
2023-12-20 10:58:37 INFO     	 * (global step 1500: loss: 0.2634795233607292, lr: 1e-05
2023-12-20 10:58:45 INFO     	 * (global step 1550: loss: 0.31569740921258926, lr: 1e-05
2023-12-20 10:58:53 INFO     	 * (global step 1600: loss: 0.3451329618692398, lr: 1e-05
2023-12-20 10:59:01 INFO     	 * (global step 1650: loss: 0.3560454994440079, lr: 1e-05
2023-12-20 10:59:09 INFO     	 * (global step 1700: loss: 0.34665510058403015, lr: 1e-05
2023-12-20 10:59:17 INFO     	 * (global step 1750: loss: 0.3040071651339531, lr: 1e-05
2023-12-20 10:59:25 INFO     	 * (global step 1800: loss: 0.4170057624578476, lr: 1e-05
2023-12-20 10:59:33 INFO     	 * (global step 1850: loss: 0.3199593350291252, lr: 1e-05
2023-12-20 10:59:35 INFO     [epoch 2/10] average loss: 0.398, lr: 1e-05
2023-12-20 10:59:35 INFO     saving model related files
2023-12-20 10:59:35 INFO     saving model
2023-12-20 10:59:36 INFO     saving tokenizer
2023-12-20 10:59:36 INFO     saving optimizer
2023-12-20 10:59:37 INFO     remove old optimizer files
2023-12-20 10:59:43 INFO     	 * (global step 1900: loss: 0.2564830780029297, lr: 1e-05
2023-12-20 10:59:51 INFO     	 * (global step 1950: loss: 0.3277565538883209, lr: 1e-05
2023-12-20 10:59:59 INFO     	 * (global step 2000: loss: 0.46879102289676666, lr: 1e-05
2023-12-20 11:00:07 INFO     	 * (global step 2050: loss: 0.523597776889801, lr: 1e-05
2023-12-20 11:00:15 INFO     	 * (global step 2100: loss: 0.3621535748243332, lr: 1e-05
2023-12-20 11:00:23 INFO     	 * (global step 2150: loss: 0.33858779072761536, lr: 1e-05
2023-12-20 11:00:31 INFO     	 * (global step 2200: loss: 0.4529307335615158, lr: 1e-05
2023-12-20 11:00:39 INFO     	 * (global step 2250: loss: 0.4112921953201294, lr: 1e-05
2023-12-20 11:00:47 INFO     	 * (global step 2300: loss: 0.41829873621463776, lr: 1e-05
2023-12-20 11:00:55 INFO     	 * (global step 2350: loss: 0.37792278826236725, lr: 1e-05
2023-12-20 11:01:03 INFO     	 * (global step 2400: loss: 0.4924331605434418, lr: 1e-05
2023-12-20 11:01:11 INFO     	 * (global step 2450: loss: 0.34562264382839203, lr: 1e-05
2023-12-20 11:01:17 INFO     [epoch 3/10] average loss: 0.392, lr: 1e-05
2023-12-20 11:01:17 INFO     saving model related files
2023-12-20 11:01:17 INFO     saving model
2023-12-20 11:01:17 INFO     saving tokenizer
2023-12-20 11:01:17 INFO     saving optimizer
2023-12-20 11:01:18 INFO     remove old optimizer files
2023-12-20 11:01:21 INFO     	 * (global step 2500: loss: 0.32126055657863617, lr: 1e-05
2023-12-20 11:01:29 INFO     	 * (global step 2550: loss: 0.28897348791360855, lr: 1e-05
2023-12-20 11:01:37 INFO     	 * (global step 2600: loss: 0.5761749446392059, lr: 1e-05
2023-12-20 11:01:45 INFO     	 * (global step 2650: loss: 0.4577275365591049, lr: 1e-05
2023-12-20 11:01:53 INFO     	 * (global step 2700: loss: 0.3312246948480606, lr: 1e-05
2023-12-20 11:02:01 INFO     	 * (global step 2750: loss: 0.3803910166025162, lr: 1e-05
2023-12-20 11:02:09 INFO     	 * (global step 2800: loss: 0.7083421945571899, lr: 1e-05
2023-12-20 11:02:17 INFO     	 * (global step 2850: loss: 0.43387313187122345, lr: 1e-05
2023-12-20 11:02:25 INFO     	 * (global step 2900: loss: 0.25521960854530334, lr: 1e-05
2023-12-20 11:02:33 INFO     	 * (global step 2950: loss: 0.35302241146564484, lr: 1e-05
2023-12-20 11:02:41 INFO     	 * (global step 3000: loss: 0.5151858180761337, lr: 1e-05
2023-12-20 11:02:49 INFO     	 * (global step 3050: loss: 0.3347122222185135, lr: 1e-05
2023-12-20 11:02:57 INFO     	 * (global step 3100: loss: 0.323006272315979, lr: 1e-05
2023-12-20 11:02:58 INFO     [epoch 4/10] average loss: 0.387, lr: 1e-05
2023-12-20 11:02:58 INFO     saving model related files
2023-12-20 11:02:58 INFO     saving model
2023-12-20 11:02:59 INFO     saving tokenizer
2023-12-20 11:02:59 INFO     saving optimizer
2023-12-20 11:03:00 INFO     remove old optimizer files
2023-12-20 11:03:00 INFO     complete training: model ckpt was saved at small_finetuned_ckpt/model_eszyci
2023-12-20 11:03:00 INFO     ## 1st RUN: Configuration 2/4 ##
2023-12-20 11:03:00 INFO     initialize model trainer
2023-12-20 11:03:00 INFO     initialize checkpoint at small_finetuned_ckpt/model_dpyopu
2023-12-20 11:03:00 INFO     hyperparameters
2023-12-20 11:03:00 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-20 11:03:00 INFO     	 * dataset_name: default
2023-12-20 11:03:00 INFO     	 * input_types: ['paragraph']
2023-12-20 11:03:00 INFO     	 * output_types: ['questions_answers']
2023-12-20 11:03:00 INFO     	 * prefix_types: ['qag']
2023-12-20 11:03:00 INFO     	 * model: lmqg/t5-small-squad-qag
2023-12-20 11:03:00 INFO     	 * max_length: 512
2023-12-20 11:03:00 INFO     	 * max_length_output: 512
2023-12-20 11:03:00 INFO     	 * epoch: 10
2023-12-20 11:03:00 INFO     	 * batch: 2
2023-12-20 11:03:00 INFO     	 * lr: 1e-05
2023-12-20 11:03:00 INFO     	 * fp16: False
2023-12-20 11:03:00 INFO     	 * random_seed: 1
2023-12-20 11:03:00 INFO     	 * gradient_accumulation_steps: 4
2023-12-20 11:03:00 INFO     	 * label_smoothing: 0.0
2023-12-20 11:03:00 INFO     initialize checkpoint with lmqg/t5-small-squad-qag
2023-12-20 11:03:01 INFO     use spaCy answer extraction model: positionrank
2023-12-20 11:03:01 INFO     Model `lmqg/t5-small-squad-qag`
2023-12-20 11:03:01 INFO     	 * Num of GPU in use: 1
2023-12-20 11:03:01 INFO     	 * Prefix: True
2023-12-20 11:03:01 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 11:03:01 INFO     dataset preprocessing
2023-12-20 11:03:02 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-20 11:03:03 INFO     start model training
2023-12-20 11:03:18 INFO     	 * (global step 50: loss: 0.4174450859427452, lr: 1e-05
2023-12-20 11:03:34 INFO     	 * (global step 100: loss: 0.44526612013578415, lr: 1e-05
2023-12-20 11:03:49 INFO     	 * (global step 150: loss: 0.4049782007932663, lr: 1e-05
2023-12-20 11:04:05 INFO     	 * (global step 200: loss: 0.3935903534293175, lr: 1e-05
2023-12-20 11:04:21 INFO     	 * (global step 250: loss: 0.48380047082901, lr: 1e-05
2023-12-20 11:04:36 INFO     	 * (global step 300: loss: 0.5217370912432671, lr: 1e-05
2023-12-20 11:04:40 INFO     [epoch 0/10] average loss: 0.447, lr: 1e-05
2023-12-20 11:04:40 INFO     saving model related files
2023-12-20 11:04:40 INFO     saving model
2023-12-20 11:04:40 INFO     saving tokenizer
2023-12-20 11:04:40 INFO     saving optimizer
2023-12-20 11:04:41 INFO     remove old optimizer files
2023-12-20 11:04:53 INFO     	 * (global step 350: loss: 0.38673805817961693, lr: 1e-05
2023-12-20 11:05:09 INFO     	 * (global step 400: loss: 0.3854989558458328, lr: 1e-05
2023-12-20 11:05:25 INFO     	 * (global step 450: loss: 0.5139632523059845, lr: 1e-05
2023-12-20 11:05:40 INFO     	 * (global step 500: loss: 0.5006912723183632, lr: 1e-05
2023-12-20 11:05:56 INFO     	 * (global step 550: loss: 0.5277958959341049, lr: 1e-05
2023-12-20 11:06:12 INFO     	 * (global step 600: loss: 0.45590126514434814, lr: 1e-05
2023-12-20 11:06:18 INFO     [epoch 1/10] average loss: 0.415, lr: 1e-05
2023-12-20 11:06:18 INFO     saving model related files
2023-12-20 11:06:18 INFO     saving model
2023-12-20 11:06:19 INFO     saving tokenizer
2023-12-20 11:06:19 INFO     saving optimizer
2023-12-20 11:06:20 INFO     remove old optimizer files
2023-12-20 11:06:29 INFO     	 * (global step 650: loss: 0.3872021660208702, lr: 1e-05
2023-12-20 11:06:45 INFO     	 * (global step 700: loss: 0.3514437712728977, lr: 1e-05
2023-12-20 11:07:00 INFO     	 * (global step 750: loss: 0.36614473164081573, lr: 1e-05
2023-12-20 11:07:16 INFO     	 * (global step 800: loss: 0.38412441313266754, lr: 1e-05
2023-12-20 11:07:32 INFO     	 * (global step 850: loss: 0.5429331362247467, lr: 1e-05
2023-12-20 11:07:47 INFO     	 * (global step 900: loss: 0.541653111577034, lr: 1e-05
2023-12-20 11:07:57 INFO     [epoch 2/10] average loss: 0.405, lr: 1e-05
2023-12-20 11:07:57 INFO     saving model related files
2023-12-20 11:07:57 INFO     saving model
2023-12-20 11:07:58 INFO     saving tokenizer
2023-12-20 11:07:58 INFO     saving optimizer
2023-12-20 11:07:58 INFO     remove old optimizer files
2023-12-20 11:08:05 INFO     	 * (global step 950: loss: 0.5032149404287338, lr: 1e-05
2023-12-20 11:08:21 INFO     	 * (global step 1000: loss: 0.3875398226082325, lr: 1e-05
2023-12-20 11:08:36 INFO     	 * (global step 1050: loss: 0.3773440942168236, lr: 1e-05
2023-12-20 11:08:52 INFO     	 * (global step 1100: loss: 0.33245617151260376, lr: 1e-05
2023-12-20 11:09:08 INFO     	 * (global step 1150: loss: 0.3028443232178688, lr: 1e-05
2023-12-20 11:09:23 INFO     	 * (global step 1200: loss: 0.37584253400564194, lr: 1e-05
2023-12-20 11:09:36 INFO     [epoch 3/10] average loss: 0.399, lr: 1e-05
2023-12-20 11:09:36 INFO     saving model related files
2023-12-20 11:09:36 INFO     saving model
2023-12-20 11:09:37 INFO     saving tokenizer
2023-12-20 11:09:37 INFO     saving optimizer
2023-12-20 11:09:37 INFO     remove old optimizer files
2023-12-20 11:09:41 INFO     	 * (global step 1250: loss: 0.40696991980075836, lr: 1e-05
2023-12-20 11:09:56 INFO     	 * (global step 1300: loss: 0.2805713154375553, lr: 1e-05
2023-12-20 11:10:12 INFO     	 * (global step 1350: loss: 0.3866093009710312, lr: 1e-05
2023-12-20 11:10:28 INFO     	 * (global step 1400: loss: 0.30510955303907394, lr: 1e-05
2023-12-20 11:10:43 INFO     	 * (global step 1450: loss: 0.34071657061576843, lr: 1e-05
2023-12-20 11:10:59 INFO     	 * (global step 1500: loss: 0.5018950253725052, lr: 1e-05
2023-12-20 11:11:15 INFO     	 * (global step 1550: loss: 0.3489021733403206, lr: 1e-05
2023-12-20 11:11:15 INFO     [epoch 4/10] average loss: 0.394, lr: 1e-05
2023-12-20 11:11:15 INFO     saving model related files
2023-12-20 11:11:15 INFO     saving model
2023-12-20 11:11:15 INFO     saving tokenizer
2023-12-20 11:11:15 INFO     saving optimizer
2023-12-20 11:11:16 INFO     remove old optimizer files
2023-12-20 11:11:16 INFO     complete training: model ckpt was saved at small_finetuned_ckpt/model_dpyopu
2023-12-20 11:11:16 INFO     ## 1st RUN: Configuration 3/4 ##
2023-12-20 11:11:16 INFO     initialize model trainer
2023-12-20 11:11:16 INFO     initialize checkpoint at small_finetuned_ckpt/model_mzgdpa
2023-12-20 11:11:16 INFO     hyperparameters
2023-12-20 11:11:16 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-20 11:11:16 INFO     	 * dataset_name: default
2023-12-20 11:11:16 INFO     	 * input_types: ['paragraph']
2023-12-20 11:11:16 INFO     	 * output_types: ['questions_answers']
2023-12-20 11:11:16 INFO     	 * prefix_types: ['qag']
2023-12-20 11:11:16 INFO     	 * model: lmqg/t5-small-squad-qag
2023-12-20 11:11:16 INFO     	 * max_length: 512
2023-12-20 11:11:16 INFO     	 * max_length_output: 512
2023-12-20 11:11:16 INFO     	 * epoch: 10
2023-12-20 11:11:16 INFO     	 * batch: 2
2023-12-20 11:11:16 INFO     	 * lr: 1e-05
2023-12-20 11:11:16 INFO     	 * fp16: False
2023-12-20 11:11:16 INFO     	 * random_seed: 1
2023-12-20 11:11:16 INFO     	 * gradient_accumulation_steps: 2
2023-12-20 11:11:16 INFO     	 * label_smoothing: 0.0
2023-12-20 11:11:16 INFO     initialize checkpoint with lmqg/t5-small-squad-qag
2023-12-20 11:11:17 INFO     use spaCy answer extraction model: positionrank
2023-12-20 11:11:18 INFO     Model `lmqg/t5-small-squad-qag`
2023-12-20 11:11:18 INFO     	 * Num of GPU in use: 1
2023-12-20 11:11:18 INFO     	 * Prefix: True
2023-12-20 11:11:18 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 11:11:18 INFO     dataset preprocessing
2023-12-20 11:11:19 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-20 11:11:19 INFO     start model training
2023-12-20 11:11:27 INFO     	 * (global step 50: loss: 0.4581184983253479, lr: 1e-05
2023-12-20 11:11:35 INFO     	 * (global step 100: loss: 0.31322166323661804, lr: 1e-05
2023-12-20 11:11:43 INFO     	 * (global step 150: loss: 0.4114130139350891, lr: 1e-05
2023-12-20 11:11:51 INFO     	 * (global step 200: loss: 0.4368223398923874, lr: 1e-05
2023-12-20 11:11:59 INFO     	 * (global step 250: loss: 0.45259372889995575, lr: 1e-05
2023-12-20 11:12:07 INFO     	 * (global step 300: loss: 0.3870916813611984, lr: 1e-05
2023-12-20 11:12:15 INFO     	 * (global step 350: loss: 0.6222658753395081, lr: 1e-05
2023-12-20 11:12:23 INFO     	 * (global step 400: loss: 0.3737747520208359, lr: 1e-05
2023-12-20 11:12:31 INFO     	 * (global step 450: loss: 0.3162582144141197, lr: 1e-05
2023-12-20 11:12:39 INFO     	 * (global step 500: loss: 0.44897033274173737, lr: 1e-05
2023-12-20 11:12:47 INFO     	 * (global step 550: loss: 0.3220909982919693, lr: 1e-05
2023-12-20 11:12:56 INFO     	 * (global step 600: loss: 0.6511140465736389, lr: 1e-05
2023-12-20 11:12:59 INFO     [epoch 0/10] average loss: 0.437, lr: 1e-05
2023-12-20 11:12:59 INFO     saving model related files
2023-12-20 11:12:59 INFO     saving model
2023-12-20 11:12:59 INFO     saving tokenizer
2023-12-20 11:12:59 INFO     saving optimizer
2023-12-20 11:13:00 INFO     remove old optimizer files
2023-12-20 11:13:05 INFO     	 * (global step 650: loss: 0.4795816093683243, lr: 1e-05
2023-12-20 11:13:13 INFO     	 * (global step 700: loss: 0.43261465430259705, lr: 1e-05
2023-12-20 11:13:21 INFO     	 * (global step 750: loss: 0.350529745221138, lr: 1e-05
2023-12-20 11:13:29 INFO     	 * (global step 800: loss: 0.4104063808917999, lr: 1e-05
2023-12-20 11:13:37 INFO     	 * (global step 850: loss: 0.6829293370246887, lr: 1e-05
2023-12-20 11:13:45 INFO     	 * (global step 900: loss: 0.5406392365694046, lr: 1e-05
2023-12-20 11:13:53 INFO     	 * (global step 950: loss: 0.3603086471557617, lr: 1e-05
2023-12-20 11:14:01 INFO     	 * (global step 1000: loss: 0.6313239485025406, lr: 1e-05
2023-12-20 11:14:09 INFO     	 * (global step 1050: loss: 0.3617406040430069, lr: 1e-05
2023-12-20 11:14:17 INFO     	 * (global step 1100: loss: 0.4810079038143158, lr: 1e-05
2023-12-20 11:14:25 INFO     	 * (global step 1150: loss: 0.25516095757484436, lr: 1e-05
2023-12-20 11:14:34 INFO     	 * (global step 1200: loss: 0.6278806626796722, lr: 1e-05
2023-12-20 11:14:40 INFO     [epoch 1/10] average loss: 0.408, lr: 1e-05
2023-12-20 11:14:40 INFO     saving model related files
2023-12-20 11:14:40 INFO     saving model
2023-12-20 11:14:41 INFO     saving tokenizer
2023-12-20 11:14:41 INFO     saving optimizer
2023-12-20 11:14:42 INFO     remove old optimizer files
2023-12-20 11:14:43 INFO     	 * (global step 1250: loss: 0.4877290427684784, lr: 1e-05
2023-12-20 11:14:51 INFO     	 * (global step 1300: loss: 0.29171570390462875, lr: 1e-05
2023-12-20 11:14:59 INFO     	 * (global step 1350: loss: 0.2023996114730835, lr: 1e-05
2023-12-20 11:15:07 INFO     	 * (global step 1400: loss: 0.489560142159462, lr: 1e-05
2023-12-20 11:15:15 INFO     	 * (global step 1450: loss: 0.42874380946159363, lr: 1e-05
2023-12-20 11:15:23 INFO     	 * (global step 1500: loss: 0.2634795233607292, lr: 1e-05
2023-12-20 11:15:31 INFO     	 * (global step 1550: loss: 0.31569740921258926, lr: 1e-05
2023-12-20 11:15:39 INFO     	 * (global step 1600: loss: 0.3451329618692398, lr: 1e-05
2023-12-20 11:15:47 INFO     	 * (global step 1650: loss: 0.3560454994440079, lr: 1e-05
2023-12-20 11:15:56 INFO     	 * (global step 1700: loss: 0.34665510058403015, lr: 1e-05
2023-12-20 11:16:04 INFO     	 * (global step 1750: loss: 0.3040071651339531, lr: 1e-05
2023-12-20 11:16:12 INFO     	 * (global step 1800: loss: 0.4170057624578476, lr: 1e-05
2023-12-20 11:16:20 INFO     	 * (global step 1850: loss: 0.3199593350291252, lr: 1e-05
2023-12-20 11:16:22 INFO     [epoch 2/10] average loss: 0.398, lr: 1e-05
2023-12-20 11:16:22 INFO     saving model related files
2023-12-20 11:16:22 INFO     saving model
2023-12-20 11:16:22 INFO     saving tokenizer
2023-12-20 11:16:22 INFO     saving optimizer
2023-12-20 11:16:23 INFO     remove old optimizer files
2023-12-20 11:16:29 INFO     	 * (global step 1900: loss: 0.2564830780029297, lr: 1e-05
2023-12-20 11:16:37 INFO     	 * (global step 1950: loss: 0.3277565538883209, lr: 1e-05
2023-12-20 11:16:45 INFO     	 * (global step 2000: loss: 0.46879102289676666, lr: 1e-05
2023-12-20 11:16:53 INFO     	 * (global step 2050: loss: 0.523597776889801, lr: 1e-05
2023-12-20 11:17:01 INFO     	 * (global step 2100: loss: 0.3621535748243332, lr: 1e-05
2023-12-20 11:17:09 INFO     	 * (global step 2150: loss: 0.33858779072761536, lr: 1e-05
2023-12-20 11:17:17 INFO     	 * (global step 2200: loss: 0.4529307335615158, lr: 1e-05
2023-12-20 11:17:26 INFO     	 * (global step 2250: loss: 0.4112921953201294, lr: 1e-05
2023-12-20 11:17:34 INFO     	 * (global step 2300: loss: 0.41829873621463776, lr: 1e-05
2023-12-20 11:17:42 INFO     	 * (global step 2350: loss: 0.37792278826236725, lr: 1e-05
2023-12-20 11:17:50 INFO     	 * (global step 2400: loss: 0.4924331605434418, lr: 1e-05
2023-12-20 11:17:58 INFO     	 * (global step 2450: loss: 0.34562264382839203, lr: 1e-05
2023-12-20 11:18:03 INFO     [epoch 3/10] average loss: 0.392, lr: 1e-05
2023-12-20 11:18:03 INFO     saving model related files
2023-12-20 11:18:03 INFO     saving model
2023-12-20 11:18:04 INFO     saving tokenizer
2023-12-20 11:18:04 INFO     saving optimizer
2023-12-20 11:18:05 INFO     remove old optimizer files
2023-12-20 11:18:07 INFO     	 * (global step 2500: loss: 0.32126055657863617, lr: 1e-05
2023-12-20 11:18:15 INFO     	 * (global step 2550: loss: 0.28897348791360855, lr: 1e-05
2023-12-20 11:18:23 INFO     	 * (global step 2600: loss: 0.5761749446392059, lr: 1e-05
2023-12-20 11:18:31 INFO     	 * (global step 2650: loss: 0.4577275365591049, lr: 1e-05
2023-12-20 11:18:39 INFO     	 * (global step 2700: loss: 0.3312246948480606, lr: 1e-05
2023-12-20 11:18:47 INFO     	 * (global step 2750: loss: 0.3803910166025162, lr: 1e-05
2023-12-20 11:18:56 INFO     	 * (global step 2800: loss: 0.7083421945571899, lr: 1e-05
2023-12-20 11:19:04 INFO     	 * (global step 2850: loss: 0.43387313187122345, lr: 1e-05
2023-12-20 11:19:12 INFO     	 * (global step 2900: loss: 0.25521960854530334, lr: 1e-05
2023-12-20 11:19:20 INFO     	 * (global step 2950: loss: 0.35302241146564484, lr: 1e-05
2023-12-20 11:19:28 INFO     	 * (global step 3000: loss: 0.5151858180761337, lr: 1e-05
2023-12-20 11:19:36 INFO     	 * (global step 3050: loss: 0.3347122222185135, lr: 1e-05
2023-12-20 11:19:44 INFO     	 * (global step 3100: loss: 0.323006272315979, lr: 1e-05
2023-12-20 11:19:45 INFO     [epoch 4/10] average loss: 0.387, lr: 1e-05
2023-12-20 11:19:45 INFO     saving model related files
2023-12-20 11:19:45 INFO     saving model
2023-12-20 11:19:45 INFO     saving tokenizer
2023-12-20 11:19:45 INFO     saving optimizer
2023-12-20 11:19:46 INFO     remove old optimizer files
2023-12-20 11:19:46 INFO     complete training: model ckpt was saved at small_finetuned_ckpt/model_mzgdpa
2023-12-20 11:19:46 INFO     ## 1st RUN (EVAL): Configuration 0/4 ##
2023-12-20 11:19:52 INFO     use spaCy answer extraction model: positionrank
2023-12-20 11:19:52 INFO     Model `small_finetuned_ckpt/model_yhgxcx/epoch_5`
2023-12-20 11:19:52 INFO     	 * Num of GPU in use: 1
2023-12-20 11:19:52 INFO     	 * Prefix: True
2023-12-20 11:19:52 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 11:19:53 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 11:25:08 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 11:30:17 INFO     	Bleu_1: 0.14497009145288134
2023-12-20 11:30:17 INFO     	Bleu_2: 0.07890852120871021
2023-12-20 11:30:17 INFO     	Bleu_3: 0.04102389534299622
2023-12-20 11:30:17 INFO     	Bleu_4: 0.025754485750608378
2023-12-20 11:30:17 INFO     	Bleu_1: 0.13601018187789846
2023-12-20 11:30:17 INFO     	Bleu_2: 0.07467052278879864
2023-12-20 11:30:17 INFO     	Bleu_3: 0.0399351841085161
2023-12-20 11:30:17 INFO     	Bleu_4: 0.025435115922939844
2023-12-20 11:30:17 INFO     ## 1st RUN (EVAL): Configuration 1/4 ##
2023-12-20 11:30:22 INFO     use spaCy answer extraction model: positionrank
2023-12-20 11:30:22 INFO     Model `small_finetuned_ckpt/model_eszyci/epoch_5`
2023-12-20 11:30:22 INFO     	 * Num of GPU in use: 1
2023-12-20 11:30:22 INFO     	 * Prefix: True
2023-12-20 11:30:22 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 11:30:23 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 11:35:38 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 11:40:48 INFO     	Bleu_1: 0.1352631360676802
2023-12-20 11:40:48 INFO     	Bleu_2: 0.07350537613758125
2023-12-20 11:40:48 INFO     	Bleu_3: 0.03789767436150744
2023-12-20 11:40:48 INFO     	Bleu_4: 0.023658109832308528
2023-12-20 11:40:48 INFO     	Bleu_1: 0.1256035036845479
2023-12-20 11:40:48 INFO     	Bleu_2: 0.06876728634280045
2023-12-20 11:40:48 INFO     	Bleu_3: 0.03660614135969604
2023-12-20 11:40:48 INFO     	Bleu_4: 0.023295631944150054
2023-12-20 11:40:48 INFO     ## 1st RUN (EVAL): Configuration 2/4 ##
2023-12-20 11:40:53 INFO     use spaCy answer extraction model: positionrank
2023-12-20 11:40:54 INFO     Model `small_finetuned_ckpt/model_dpyopu/epoch_5`
2023-12-20 11:40:54 INFO     	 * Num of GPU in use: 1
2023-12-20 11:40:54 INFO     	 * Prefix: True
2023-12-20 11:40:54 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 11:40:55 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 11:46:10 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 11:51:19 INFO     	Bleu_1: 0.14497009145288134
2023-12-20 11:51:19 INFO     	Bleu_2: 0.07890852120871021
2023-12-20 11:51:19 INFO     	Bleu_3: 0.04102389534299622
2023-12-20 11:51:19 INFO     	Bleu_4: 0.025754485750608378
2023-12-20 11:51:19 INFO     	Bleu_1: 0.13601018187789846
2023-12-20 11:51:19 INFO     	Bleu_2: 0.07467052278879864
2023-12-20 11:51:19 INFO     	Bleu_3: 0.0399351841085161
2023-12-20 11:51:19 INFO     	Bleu_4: 0.025435115922939844
2023-12-20 11:51:19 INFO     ## 1st RUN (EVAL): Configuration 3/4 ##
2023-12-20 11:51:24 INFO     use spaCy answer extraction model: positionrank
2023-12-20 11:51:25 INFO     Model `small_finetuned_ckpt/model_mzgdpa/epoch_5`
2023-12-20 11:51:25 INFO     	 * Num of GPU in use: 1
2023-12-20 11:51:25 INFO     	 * Prefix: True
2023-12-20 11:51:25 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 11:51:26 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 11:56:40 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 12:01:49 INFO     	Bleu_1: 0.1352631360676802
2023-12-20 12:01:49 INFO     	Bleu_2: 0.07350537613758125
2023-12-20 12:01:49 INFO     	Bleu_3: 0.03789767436150744
2023-12-20 12:01:49 INFO     	Bleu_4: 0.023658109832308528
2023-12-20 12:01:50 INFO     	Bleu_1: 0.1256035036845479
2023-12-20 12:01:50 INFO     	Bleu_2: 0.06876728634280045
2023-12-20 12:01:50 INFO     	Bleu_3: 0.03660614135969604
2023-12-20 12:01:50 INFO     	Bleu_4: 0.023295631944150054
2023-12-20 12:01:50 INFO     1st RUN RESULTS (validation/Bleu_4)
2023-12-20 12:01:50 INFO     	 * rank: 0 | metric: 0.026 | model: small_finetuned_ckpt/model_yhgxcx/epoch_5 |
2023-12-20 12:01:50 INFO     	 * rank: 1 | metric: 0.026 | model: small_finetuned_ckpt/model_dpyopu/epoch_5 |
2023-12-20 12:01:50 INFO     	 * rank: 2 | metric: 0.024 | model: small_finetuned_ckpt/model_eszyci/epoch_5 |
2023-12-20 12:01:50 INFO     	 * rank: 3 | metric: 0.024 | model: small_finetuned_ckpt/model_mzgdpa/epoch_5 |
2023-12-20 12:01:50 INFO     ## 2nd RUN: Configuration 0/4: validation/Bleu_4 = 0.025754485750608378
2023-12-20 12:01:50 INFO     initialize model trainer
2023-12-20 12:01:50 INFO     load config from existing checkpoint at small_finetuned_ckpt/model_yhgxcx
2023-12-20 12:01:50 INFO     hyperparameters
2023-12-20 12:01:50 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-20 12:01:50 INFO     	 * dataset_name: default
2023-12-20 12:01:50 INFO     	 * input_types: ['paragraph']
2023-12-20 12:01:50 INFO     	 * output_types: ['questions_answers']
2023-12-20 12:01:50 INFO     	 * prefix_types: ['qag']
2023-12-20 12:01:50 INFO     	 * model: lmqg/t5-small-squad-qag
2023-12-20 12:01:50 INFO     	 * max_length: 512
2023-12-20 12:01:50 INFO     	 * max_length_output: 512
2023-12-20 12:01:50 INFO     	 * epoch: 10
2023-12-20 12:01:50 INFO     	 * batch: 2
2023-12-20 12:01:50 INFO     	 * lr: 1e-05
2023-12-20 12:01:50 INFO     	 * fp16: False
2023-12-20 12:01:50 INFO     	 * random_seed: 1
2023-12-20 12:01:50 INFO     	 * gradient_accumulation_steps: 4
2023-12-20 12:01:50 INFO     	 * label_smoothing: 0.15
2023-12-20 12:01:50 INFO     load checkpoint from small_finetuned_ckpt/model_yhgxcx/epoch_5
2023-12-20 12:01:50 INFO     use spaCy answer extraction model: positionrank
2023-12-20 12:01:51 INFO     Model `small_finetuned_ckpt/model_yhgxcx/epoch_5`
2023-12-20 12:01:51 INFO     	 * Num of GPU in use: 1
2023-12-20 12:01:51 INFO     	 * Prefix: True
2023-12-20 12:01:51 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 12:01:51 INFO     load optimizer from small_finetuned_ckpt/model_yhgxcx/optimizers/optimizer.5.pt
2023-12-20 12:01:51 INFO     optimizer is loading on cuda
2023-12-20 12:01:55 INFO     dataset preprocessing
2023-12-20 12:01:57 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-20 12:01:57 INFO     start model training
2023-12-20 12:02:13 INFO     	 * (global step 50: loss: 3.1729416251182556, lr: 1e-05
2023-12-20 12:02:29 INFO     	 * (global step 100: loss: 3.005919873714447, lr: 1e-05
2023-12-20 12:02:46 INFO     	 * (global step 150: loss: 2.896347463130951, lr: 1e-05
2023-12-20 12:03:02 INFO     	 * (global step 200: loss: 2.805174171924591, lr: 1e-05
2023-12-20 12:03:18 INFO     	 * (global step 250: loss: 2.8284191489219666, lr: 1e-05
2023-12-20 12:03:34 INFO     	 * (global step 300: loss: 2.8440467715263367, lr: 1e-05
2023-12-20 12:03:38 INFO     [epoch 5/10] average loss: 3.08, lr: 1e-05
2023-12-20 12:03:38 INFO     saving model related files
2023-12-20 12:03:38 INFO     saving model
2023-12-20 12:03:38 INFO     saving tokenizer
2023-12-20 12:03:39 INFO     saving optimizer
2023-12-20 12:03:39 INFO     remove old optimizer files
2023-12-20 12:03:52 INFO     	 * (global step 350: loss: 2.7070798873901367, lr: 1e-05
2023-12-20 12:04:09 INFO     	 * (global step 400: loss: 2.6850701570510864, lr: 1e-05
2023-12-20 12:04:25 INFO     	 * (global step 450: loss: 2.7756646275520325, lr: 1e-05
2023-12-20 12:04:41 INFO     	 * (global step 500: loss: 2.741749584674835, lr: 1e-05
2023-12-20 12:04:57 INFO     	 * (global step 550: loss: 2.7645897269248962, lr: 1e-05
2023-12-20 12:05:14 INFO     	 * (global step 600: loss: 2.7023456692695618, lr: 1e-05
2023-12-20 12:05:20 INFO     [epoch 6/10] average loss: 2.695, lr: 1e-05
2023-12-20 12:05:20 INFO     saving model related files
2023-12-20 12:05:20 INFO     saving model
2023-12-20 12:05:21 INFO     saving tokenizer
2023-12-20 12:05:21 INFO     saving optimizer
2023-12-20 12:05:22 INFO     remove old optimizer files
2023-12-20 12:05:32 INFO     	 * (global step 650: loss: 2.604285180568695, lr: 1e-05
2023-12-20 12:05:48 INFO     	 * (global step 700: loss: 2.588522255420685, lr: 1e-05
2023-12-20 12:06:04 INFO     	 * (global step 750: loss: 2.596880793571472, lr: 1e-05
2023-12-20 12:06:20 INFO     	 * (global step 800: loss: 2.600050151348114, lr: 1e-05
2023-12-20 12:06:37 INFO     	 * (global step 850: loss: 2.7459980845451355, lr: 1e-05
2023-12-20 12:06:53 INFO     	 * (global step 900: loss: 2.7223235964775085, lr: 1e-05
2023-12-20 12:07:03 INFO     [epoch 7/10] average loss: 2.625, lr: 1e-05
2023-12-20 12:07:03 INFO     saving model related files
2023-12-20 12:07:03 INFO     saving model
2023-12-20 12:07:03 INFO     saving tokenizer
2023-12-20 12:07:03 INFO     saving optimizer
2023-12-20 12:07:04 INFO     remove old optimizer files
2023-12-20 12:07:11 INFO     	 * (global step 950: loss: 2.691940426826477, lr: 1e-05
2023-12-20 12:07:27 INFO     	 * (global step 1000: loss: 2.5745434165000916, lr: 1e-05
2023-12-20 12:07:43 INFO     	 * (global step 1050: loss: 2.584013819694519, lr: 1e-05
2023-12-20 12:08:00 INFO     	 * (global step 1100: loss: 2.5228121876716614, lr: 1e-05
2023-12-20 12:08:16 INFO     	 * (global step 1150: loss: 2.4918858408927917, lr: 1e-05
2023-12-20 12:08:32 INFO     	 * (global step 1200: loss: 2.57187557220459, lr: 1e-05
2023-12-20 12:08:45 INFO     [epoch 8/10] average loss: 2.589, lr: 1e-05
2023-12-20 12:08:45 INFO     saving model related files
2023-12-20 12:08:45 INFO     saving model
2023-12-20 12:08:46 INFO     saving tokenizer
2023-12-20 12:08:46 INFO     saving optimizer
2023-12-20 12:08:47 INFO     remove old optimizer files
2023-12-20 12:08:50 INFO     	 * (global step 1250: loss: 2.5806312561035156, lr: 1e-05
2023-12-20 12:09:06 INFO     	 * (global step 1300: loss: 2.4699066281318665, lr: 1e-05
2023-12-20 12:09:23 INFO     	 * (global step 1350: loss: 2.5764312744140625, lr: 1e-05
2023-12-20 12:09:39 INFO     	 * (global step 1400: loss: 2.4919973015785217, lr: 1e-05
2023-12-20 12:09:55 INFO     	 * (global step 1450: loss: 2.512939751148224, lr: 1e-05
2023-12-20 12:10:12 INFO     	 * (global step 1500: loss: 2.6705645322799683, lr: 1e-05
2023-12-20 12:10:28 INFO     	 * (global step 1550: loss: 2.506294310092926, lr: 1e-05
2023-12-20 12:10:28 INFO     [epoch 9/10] average loss: 2.563, lr: 1e-05
2023-12-20 12:10:28 INFO     saving model related files
2023-12-20 12:10:28 INFO     saving model
2023-12-20 12:10:29 INFO     saving tokenizer
2023-12-20 12:10:29 INFO     saving optimizer
2023-12-20 12:10:30 INFO     remove old optimizer files
2023-12-20 12:10:30 INFO     complete training: model ckpt was saved at small_finetuned_ckpt/model_yhgxcx
2023-12-20 12:10:30 INFO     ## 2nd RUN: Configuration 1/4: validation/Bleu_4 = 0.025754485750608378
2023-12-20 12:10:30 INFO     initialize model trainer
2023-12-20 12:10:30 INFO     load config from existing checkpoint at small_finetuned_ckpt/model_dpyopu
2023-12-20 12:10:30 INFO     hyperparameters
2023-12-20 12:10:30 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-20 12:10:30 INFO     	 * dataset_name: default
2023-12-20 12:10:30 INFO     	 * input_types: ['paragraph']
2023-12-20 12:10:30 INFO     	 * output_types: ['questions_answers']
2023-12-20 12:10:30 INFO     	 * prefix_types: ['qag']
2023-12-20 12:10:30 INFO     	 * model: lmqg/t5-small-squad-qag
2023-12-20 12:10:30 INFO     	 * max_length: 512
2023-12-20 12:10:30 INFO     	 * max_length_output: 512
2023-12-20 12:10:30 INFO     	 * epoch: 10
2023-12-20 12:10:30 INFO     	 * batch: 2
2023-12-20 12:10:30 INFO     	 * lr: 1e-05
2023-12-20 12:10:30 INFO     	 * fp16: False
2023-12-20 12:10:30 INFO     	 * random_seed: 1
2023-12-20 12:10:30 INFO     	 * gradient_accumulation_steps: 4
2023-12-20 12:10:30 INFO     	 * label_smoothing: 0.0
2023-12-20 12:10:30 INFO     load checkpoint from small_finetuned_ckpt/model_dpyopu/epoch_5
2023-12-20 12:10:30 INFO     use spaCy answer extraction model: positionrank
2023-12-20 12:10:30 INFO     Model `small_finetuned_ckpt/model_dpyopu/epoch_5`
2023-12-20 12:10:30 INFO     	 * Num of GPU in use: 1
2023-12-20 12:10:30 INFO     	 * Prefix: True
2023-12-20 12:10:30 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 12:10:30 INFO     load optimizer from small_finetuned_ckpt/model_dpyopu/optimizers/optimizer.5.pt
2023-12-20 12:10:30 INFO     optimizer is loading on cuda
2023-12-20 12:10:37 INFO     dataset preprocessing
2023-12-20 12:10:38 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-20 12:10:39 INFO     start model training
2023-12-20 12:10:54 INFO     	 * (global step 50: loss: 0.3444925621151924, lr: 1e-05
2023-12-20 12:11:10 INFO     	 * (global step 100: loss: 0.3867633119225502, lr: 1e-05
2023-12-20 12:11:26 INFO     	 * (global step 150: loss: 0.35944393277168274, lr: 1e-05
2023-12-20 12:11:42 INFO     	 * (global step 200: loss: 0.3453115224838257, lr: 1e-05
2023-12-20 12:11:58 INFO     	 * (global step 250: loss: 0.4395882338285446, lr: 1e-05
2023-12-20 12:12:14 INFO     	 * (global step 300: loss: 0.4812997803092003, lr: 1e-05
2023-12-20 12:12:17 INFO     [epoch 5/10] average loss: 0.388, lr: 1e-05
2023-12-20 12:12:17 INFO     saving model related files
2023-12-20 12:12:17 INFO     saving model
2023-12-20 12:12:18 INFO     saving tokenizer
2023-12-20 12:12:18 INFO     saving optimizer
2023-12-20 12:12:19 INFO     remove old optimizer files
2023-12-20 12:12:31 INFO     	 * (global step 350: loss: 0.3640522286295891, lr: 1e-05
2023-12-20 12:12:47 INFO     	 * (global step 400: loss: 0.35470760613679886, lr: 1e-05
2023-12-20 12:13:03 INFO     	 * (global step 450: loss: 0.4764527827501297, lr: 1e-05
2023-12-20 12:13:19 INFO     	 * (global step 500: loss: 0.4676702916622162, lr: 1e-05
2023-12-20 12:13:35 INFO     	 * (global step 550: loss: 0.4926901236176491, lr: 1e-05
2023-12-20 12:13:51 INFO     	 * (global step 600: loss: 0.42801520600914955, lr: 1e-05
2023-12-20 12:13:58 INFO     [epoch 6/10] average loss: 0.384, lr: 1e-05
2023-12-20 12:13:58 INFO     saving model related files
2023-12-20 12:13:58 INFO     saving model
2023-12-20 12:13:58 INFO     saving tokenizer
2023-12-20 12:13:58 INFO     saving optimizer
2023-12-20 12:13:59 INFO     remove old optimizer files
2023-12-20 12:14:09 INFO     	 * (global step 650: loss: 0.36117201298475266, lr: 1e-05
2023-12-20 12:14:24 INFO     	 * (global step 700: loss: 0.3258570395410061, lr: 1e-05
2023-12-20 12:14:40 INFO     	 * (global step 750: loss: 0.3419213965535164, lr: 1e-05
2023-12-20 12:14:56 INFO     	 * (global step 800: loss: 0.3603220358490944, lr: 1e-05
2023-12-20 12:15:12 INFO     	 * (global step 850: loss: 0.5133506879210472, lr: 1e-05
2023-12-20 12:15:28 INFO     	 * (global step 900: loss: 0.5095861852169037, lr: 1e-05
2023-12-20 12:15:38 INFO     [epoch 7/10] average loss: 0.38, lr: 1e-05
2023-12-20 12:15:38 INFO     saving model related files
2023-12-20 12:15:38 INFO     saving model
2023-12-20 12:15:38 INFO     saving tokenizer
2023-12-20 12:15:38 INFO     saving optimizer
2023-12-20 12:15:39 INFO     remove old optimizer files
2023-12-20 12:15:46 INFO     	 * (global step 950: loss: 0.47614045441150665, lr: 1e-05
2023-12-20 12:16:02 INFO     	 * (global step 1000: loss: 0.3657836951315403, lr: 1e-05
2023-12-20 12:16:18 INFO     	 * (global step 1050: loss: 0.36333950608968735, lr: 1e-05
2023-12-20 12:16:33 INFO     	 * (global step 1100: loss: 0.31902408599853516, lr: 1e-05
2023-12-20 12:16:49 INFO     	 * (global step 1150: loss: 0.28529759868979454, lr: 1e-05
2023-12-20 12:17:05 INFO     	 * (global step 1200: loss: 0.35795193910598755, lr: 1e-05
2023-12-20 12:17:18 INFO     [epoch 8/10] average loss: 0.378, lr: 1e-05
2023-12-20 12:17:18 INFO     saving model related files
2023-12-20 12:17:18 INFO     saving model
2023-12-20 12:17:19 INFO     saving tokenizer
2023-12-20 12:17:19 INFO     saving optimizer
2023-12-20 12:17:20 INFO     remove old optimizer files
2023-12-20 12:17:23 INFO     	 * (global step 1250: loss: 0.3884100504219532, lr: 1e-05
2023-12-20 12:17:39 INFO     	 * (global step 1300: loss: 0.26604167744517326, lr: 1e-05
2023-12-20 12:17:55 INFO     	 * (global step 1350: loss: 0.3687392398715019, lr: 1e-05
2023-12-20 12:18:11 INFO     	 * (global step 1400: loss: 0.2905567139387131, lr: 1e-05
2023-12-20 12:18:27 INFO     	 * (global step 1450: loss: 0.3267405033111572, lr: 1e-05
2023-12-20 12:18:43 INFO     	 * (global step 1500: loss: 0.4773299880325794, lr: 1e-05
2023-12-20 12:18:59 INFO     	 * (global step 1550: loss: 0.3336133658885956, lr: 1e-05
2023-12-20 12:18:59 INFO     [epoch 9/10] average loss: 0.375, lr: 1e-05
2023-12-20 12:18:59 INFO     saving model related files
2023-12-20 12:18:59 INFO     saving model
2023-12-20 12:18:59 INFO     saving tokenizer
2023-12-20 12:18:59 INFO     saving optimizer
2023-12-20 12:19:00 INFO     remove old optimizer files
2023-12-20 12:19:00 INFO     complete training: model ckpt was saved at small_finetuned_ckpt/model_dpyopu
2023-12-20 12:19:00 INFO     ## 2nd RUN: Configuration 2/4: validation/Bleu_4 = 0.023658109832308528
2023-12-20 12:19:00 INFO     initialize model trainer
2023-12-20 12:19:00 INFO     load config from existing checkpoint at small_finetuned_ckpt/model_eszyci
2023-12-20 12:19:00 INFO     hyperparameters
2023-12-20 12:19:00 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-20 12:19:00 INFO     	 * dataset_name: default
2023-12-20 12:19:00 INFO     	 * input_types: ['paragraph']
2023-12-20 12:19:00 INFO     	 * output_types: ['questions_answers']
2023-12-20 12:19:00 INFO     	 * prefix_types: ['qag']
2023-12-20 12:19:00 INFO     	 * model: lmqg/t5-small-squad-qag
2023-12-20 12:19:00 INFO     	 * max_length: 512
2023-12-20 12:19:00 INFO     	 * max_length_output: 512
2023-12-20 12:19:00 INFO     	 * epoch: 10
2023-12-20 12:19:00 INFO     	 * batch: 2
2023-12-20 12:19:00 INFO     	 * lr: 1e-05
2023-12-20 12:19:00 INFO     	 * fp16: False
2023-12-20 12:19:00 INFO     	 * random_seed: 1
2023-12-20 12:19:00 INFO     	 * gradient_accumulation_steps: 2
2023-12-20 12:19:00 INFO     	 * label_smoothing: 0.15
2023-12-20 12:19:00 INFO     load checkpoint from small_finetuned_ckpt/model_eszyci/epoch_5
2023-12-20 12:19:01 INFO     use spaCy answer extraction model: positionrank
2023-12-20 12:19:01 INFO     Model `small_finetuned_ckpt/model_eszyci/epoch_5`
2023-12-20 12:19:01 INFO     	 * Num of GPU in use: 1
2023-12-20 12:19:01 INFO     	 * Prefix: True
2023-12-20 12:19:01 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 12:19:01 INFO     load optimizer from small_finetuned_ckpt/model_eszyci/optimizers/optimizer.5.pt
2023-12-20 12:19:01 INFO     optimizer is loading on cuda
2023-12-20 12:19:10 INFO     dataset preprocessing
2023-12-20 12:19:11 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-20 12:19:11 INFO     start model training
2023-12-20 12:19:19 INFO     	 * (global step 50: loss: 3.2204099893569946, lr: 1e-05
2023-12-20 12:19:28 INFO     	 * (global step 100: loss: 2.9224566221237183, lr: 1e-05
2023-12-20 12:19:36 INFO     	 * (global step 150: loss: 2.84264874458313, lr: 1e-05
2023-12-20 12:19:45 INFO     	 * (global step 200: loss: 2.865183472633362, lr: 1e-05
2023-12-20 12:19:53 INFO     	 * (global step 250: loss: 2.8196682929992676, lr: 1e-05
2023-12-20 12:20:01 INFO     	 * (global step 300: loss: 2.7353893518447876, lr: 1e-05
2023-12-20 12:20:10 INFO     	 * (global step 350: loss: 2.922388195991516, lr: 1e-05
2023-12-20 12:20:18 INFO     	 * (global step 400: loss: 2.6836533546447754, lr: 1e-05
2023-12-20 12:20:27 INFO     	 * (global step 450: loss: 2.6000194549560547, lr: 1e-05
2023-12-20 12:20:35 INFO     	 * (global step 500: loss: 2.7068461179733276, lr: 1e-05
2023-12-20 12:20:44 INFO     	 * (global step 550: loss: 2.5839768648147583, lr: 1e-05
2023-12-20 12:20:52 INFO     	 * (global step 600: loss: 2.8618381023406982, lr: 1e-05
2023-12-20 12:20:56 INFO     [epoch 5/10] average loss: 2.9, lr: 1e-05
2023-12-20 12:20:56 INFO     saving model related files
2023-12-20 12:20:56 INFO     saving model
2023-12-20 12:20:56 INFO     saving tokenizer
2023-12-20 12:20:57 INFO     saving optimizer
2023-12-20 12:20:58 INFO     remove old optimizer files
2023-12-20 12:21:03 INFO     	 * (global step 650: loss: 2.703231692314148, lr: 1e-05
2023-12-20 12:21:11 INFO     	 * (global step 700: loss: 2.6577662229537964, lr: 1e-05
2023-12-20 12:21:19 INFO     	 * (global step 750: loss: 2.587299108505249, lr: 1e-05
2023-12-20 12:21:28 INFO     	 * (global step 800: loss: 2.6361310482025146, lr: 1e-05
2023-12-20 12:21:36 INFO     	 * (global step 850: loss: 2.8458950519561768, lr: 1e-05
2023-12-20 12:21:45 INFO     	 * (global step 900: loss: 2.726503014564514, lr: 1e-05
2023-12-20 12:21:53 INFO     	 * (global step 950: loss: 2.569584369659424, lr: 1e-05
2023-12-20 12:22:02 INFO     	 * (global step 1000: loss: 2.797834873199463, lr: 1e-05
2023-12-20 12:22:10 INFO     	 * (global step 1050: loss: 2.5796549320220947, lr: 1e-05
2023-12-20 12:22:19 INFO     	 * (global step 1100: loss: 2.6680644750595093, lr: 1e-05
2023-12-20 12:22:27 INFO     	 * (global step 1150: loss: 2.475328803062439, lr: 1e-05
2023-12-20 12:22:36 INFO     	 * (global step 1200: loss: 2.7959383726119995, lr: 1e-05
2023-12-20 12:22:43 INFO     [epoch 6/10] average loss: 2.615, lr: 1e-05
2023-12-20 12:22:43 INFO     saving model related files
2023-12-20 12:22:43 INFO     saving model
2023-12-20 12:22:43 INFO     saving tokenizer
2023-12-20 12:22:44 INFO     saving optimizer
2023-12-20 12:22:44 INFO     remove old optimizer files
2023-12-20 12:22:46 INFO     	 * (global step 1250: loss: 2.6380258798599243, lr: 1e-05
2023-12-20 12:22:54 INFO     	 * (global step 1300: loss: 2.4750324487686157, lr: 1e-05
2023-12-20 12:23:03 INFO     	 * (global step 1350: loss: 2.393694043159485, lr: 1e-05
2023-12-20 12:23:11 INFO     	 * (global step 1400: loss: 2.6383954286575317, lr: 1e-05
2023-12-20 12:23:20 INFO     	 * (global step 1450: loss: 2.602954626083374, lr: 1e-05
2023-12-20 12:23:28 INFO     	 * (global step 1500: loss: 2.444726586341858, lr: 1e-05
2023-12-20 12:23:37 INFO     	 * (global step 1550: loss: 2.4882049560546875, lr: 1e-05
2023-12-20 12:23:45 INFO     	 * (global step 1600: loss: 2.4991941452026367, lr: 1e-05
2023-12-20 12:23:54 INFO     	 * (global step 1650: loss: 2.5083588361740112, lr: 1e-05
2023-12-20 12:24:02 INFO     	 * (global step 1700: loss: 2.508886456489563, lr: 1e-05
2023-12-20 12:24:11 INFO     	 * (global step 1750: loss: 2.466745972633362, lr: 1e-05
2023-12-20 12:24:19 INFO     	 * (global step 1800: loss: 2.5709158182144165, lr: 1e-05
2023-12-20 12:24:28 INFO     	 * (global step 1850: loss: 2.470918893814087, lr: 1e-05
2023-12-20 12:24:30 INFO     [epoch 7/10] average loss: 2.561, lr: 1e-05
2023-12-20 12:24:30 INFO     saving model related files
2023-12-20 12:24:30 INFO     saving model
2023-12-20 12:24:30 INFO     saving tokenizer
2023-12-20 12:24:31 INFO     saving optimizer
2023-12-20 12:24:32 INFO     remove old optimizer files
2023-12-20 12:24:38 INFO     	 * (global step 1900: loss: 2.4201172590255737, lr: 1e-05
2023-12-20 12:24:46 INFO     	 * (global step 1950: loss: 2.470484733581543, lr: 1e-05
2023-12-20 12:24:55 INFO     	 * (global step 2000: loss: 2.5872772932052612, lr: 1e-05
2023-12-20 12:25:03 INFO     	 * (global step 2050: loss: 2.653365731239319, lr: 1e-05
2023-12-20 12:25:12 INFO     	 * (global step 2100: loss: 2.519801139831543, lr: 1e-05
2023-12-20 12:25:20 INFO     	 * (global step 2150: loss: 2.494991660118103, lr: 1e-05
2023-12-20 12:25:29 INFO     	 * (global step 2200: loss: 2.581425189971924, lr: 1e-05
2023-12-20 12:25:37 INFO     	 * (global step 2250: loss: 2.547189235687256, lr: 1e-05
2023-12-20 12:25:46 INFO     	 * (global step 2300: loss: 2.5516327619552612, lr: 1e-05
2023-12-20 12:25:54 INFO     	 * (global step 2350: loss: 2.492852568626404, lr: 1e-05
2023-12-20 12:26:03 INFO     	 * (global step 2400: loss: 2.6132335662841797, lr: 1e-05
2023-12-20 12:26:11 INFO     	 * (global step 2450: loss: 2.4725029468536377, lr: 1e-05
2023-12-20 12:26:17 INFO     [epoch 8/10] average loss: 2.531, lr: 1e-05
2023-12-20 12:26:17 INFO     saving model related files
2023-12-20 12:26:17 INFO     saving model
2023-12-20 12:26:17 INFO     saving tokenizer
2023-12-20 12:26:17 INFO     saving optimizer
2023-12-20 12:26:18 INFO     remove old optimizer files
2023-12-20 12:26:21 INFO     	 * (global step 2500: loss: 2.450691819190979, lr: 1e-05
2023-12-20 12:26:30 INFO     	 * (global step 2550: loss: 2.433626890182495, lr: 1e-05
2023-12-20 12:26:38 INFO     	 * (global step 2600: loss: 2.672733783721924, lr: 1e-05
2023-12-20 12:26:47 INFO     	 * (global step 2650: loss: 2.5503686666488647, lr: 1e-05
2023-12-20 12:26:55 INFO     	 * (global step 2700: loss: 2.4704800844192505, lr: 1e-05
2023-12-20 12:27:04 INFO     	 * (global step 2750: loss: 2.527252674102783, lr: 1e-05
2023-12-20 12:27:12 INFO     	 * (global step 2800: loss: 2.819408416748047, lr: 1e-05
2023-12-20 12:27:20 INFO     	 * (global step 2850: loss: 2.545387864112854, lr: 1e-05
2023-12-20 12:27:29 INFO     	 * (global step 2900: loss: 2.3656184673309326, lr: 1e-05
2023-12-20 12:27:37 INFO     	 * (global step 2950: loss: 2.466547727584839, lr: 1e-05
2023-12-20 12:27:46 INFO     	 * (global step 3000: loss: 2.620983839035034, lr: 1e-05
2023-12-20 12:27:54 INFO     	 * (global step 3050: loss: 2.4525712728500366, lr: 1e-05
2023-12-20 12:28:03 INFO     	 * (global step 3100: loss: 2.449002265930176, lr: 1e-05
2023-12-20 12:28:04 INFO     [epoch 9/10] average loss: 2.508, lr: 1e-05
2023-12-20 12:28:04 INFO     saving model related files
2023-12-20 12:28:04 INFO     saving model
2023-12-20 12:28:04 INFO     saving tokenizer
2023-12-20 12:28:04 INFO     saving optimizer
2023-12-20 12:28:05 INFO     remove old optimizer files
2023-12-20 12:28:05 INFO     complete training: model ckpt was saved at small_finetuned_ckpt/model_eszyci
2023-12-20 12:28:05 INFO     ## 2nd RUN: Configuration 3/4: validation/Bleu_4 = 0.023658109832308528
2023-12-20 12:28:05 INFO     initialize model trainer
2023-12-20 12:28:05 INFO     load config from existing checkpoint at small_finetuned_ckpt/model_mzgdpa
2023-12-20 12:28:05 INFO     hyperparameters
2023-12-20 12:28:05 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-20 12:28:05 INFO     	 * dataset_name: default
2023-12-20 12:28:05 INFO     	 * input_types: ['paragraph']
2023-12-20 12:28:05 INFO     	 * output_types: ['questions_answers']
2023-12-20 12:28:05 INFO     	 * prefix_types: ['qag']
2023-12-20 12:28:05 INFO     	 * model: lmqg/t5-small-squad-qag
2023-12-20 12:28:05 INFO     	 * max_length: 512
2023-12-20 12:28:05 INFO     	 * max_length_output: 512
2023-12-20 12:28:05 INFO     	 * epoch: 10
2023-12-20 12:28:05 INFO     	 * batch: 2
2023-12-20 12:28:05 INFO     	 * lr: 1e-05
2023-12-20 12:28:05 INFO     	 * fp16: False
2023-12-20 12:28:05 INFO     	 * random_seed: 1
2023-12-20 12:28:05 INFO     	 * gradient_accumulation_steps: 2
2023-12-20 12:28:05 INFO     	 * label_smoothing: 0.0
2023-12-20 12:28:05 INFO     load checkpoint from small_finetuned_ckpt/model_mzgdpa/epoch_5
2023-12-20 12:28:06 INFO     use spaCy answer extraction model: positionrank
2023-12-20 12:28:06 INFO     Model `small_finetuned_ckpt/model_mzgdpa/epoch_5`
2023-12-20 12:28:06 INFO     	 * Num of GPU in use: 1
2023-12-20 12:28:06 INFO     	 * Prefix: True
2023-12-20 12:28:06 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 12:28:06 INFO     load optimizer from small_finetuned_ckpt/model_mzgdpa/optimizers/optimizer.5.pt
2023-12-20 12:28:06 INFO     optimizer is loading on cuda
2023-12-20 12:28:11 INFO     dataset preprocessing
2023-12-20 12:28:12 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-20 12:28:12 INFO     start model training
2023-12-20 12:28:21 INFO     	 * (global step 50: loss: 0.3673609346151352, lr: 1e-05
2023-12-20 12:28:29 INFO     	 * (global step 100: loss: 0.2609236389398575, lr: 1e-05
2023-12-20 12:28:37 INFO     	 * (global step 150: loss: 0.3319213092327118, lr: 1e-05
2023-12-20 12:28:45 INFO     	 * (global step 200: loss: 0.3739495873451233, lr: 1e-05
2023-12-20 12:28:53 INFO     	 * (global step 250: loss: 0.39888980984687805, lr: 1e-05
2023-12-20 12:29:02 INFO     	 * (global step 300: loss: 0.3444202393293381, lr: 1e-05
2023-12-20 12:29:10 INFO     	 * (global step 350: loss: 0.5525825470685959, lr: 1e-05
2023-12-20 12:29:18 INFO     	 * (global step 400: loss: 0.33495402336120605, lr: 1e-05
2023-12-20 12:29:26 INFO     	 * (global step 450: loss: 0.27849387377500534, lr: 1e-05
2023-12-20 12:29:34 INFO     	 * (global step 500: loss: 0.40098078548908234, lr: 1e-05
2023-12-20 12:29:43 INFO     	 * (global step 550: loss: 0.29337042570114136, lr: 1e-05
2023-12-20 12:29:51 INFO     	 * (global step 600: loss: 0.5964969992637634, lr: 1e-05
2023-12-20 12:29:54 INFO     [epoch 5/10] average loss: 0.379, lr: 1e-05
2023-12-20 12:29:54 INFO     saving model related files
2023-12-20 12:29:54 INFO     saving model
2023-12-20 12:29:55 INFO     saving tokenizer
2023-12-20 12:29:55 INFO     saving optimizer
2023-12-20 12:29:56 INFO     remove old optimizer files
2023-12-20 12:30:01 INFO     	 * (global step 650: loss: 0.4396572709083557, lr: 1e-05
2023-12-20 12:30:09 INFO     	 * (global step 700: loss: 0.4051142930984497, lr: 1e-05
2023-12-20 12:30:17 INFO     	 * (global step 750: loss: 0.31940512359142303, lr: 1e-05
2023-12-20 12:30:25 INFO     	 * (global step 800: loss: 0.377267062664032, lr: 1e-05
2023-12-20 12:30:34 INFO     	 * (global step 850: loss: 0.6278819143772125, lr: 1e-05
2023-12-20 12:30:42 INFO     	 * (global step 900: loss: 0.4898638501763344, lr: 1e-05
2023-12-20 12:30:50 INFO     	 * (global step 950: loss: 0.3363235890865326, lr: 1e-05
2023-12-20 12:30:58 INFO     	 * (global step 1000: loss: 0.5885159522294998, lr: 1e-05
2023-12-20 12:31:07 INFO     	 * (global step 1050: loss: 0.34119413793087006, lr: 1e-05
2023-12-20 12:31:15 INFO     	 * (global step 1100: loss: 0.4577775001525879, lr: 1e-05
2023-12-20 12:31:23 INFO     	 * (global step 1150: loss: 0.2359614297747612, lr: 1e-05
2023-12-20 12:31:31 INFO     	 * (global step 1200: loss: 0.5828331112861633, lr: 1e-05
2023-12-20 12:31:38 INFO     [epoch 6/10] average loss: 0.375, lr: 1e-05
2023-12-20 12:31:38 INFO     saving model related files
2023-12-20 12:31:38 INFO     saving model
2023-12-20 12:31:39 INFO     saving tokenizer
2023-12-20 12:31:39 INFO     saving optimizer
2023-12-20 12:31:40 INFO     remove old optimizer files
2023-12-20 12:31:41 INFO     	 * (global step 1250: loss: 0.4607953429222107, lr: 1e-05
2023-12-20 12:31:49 INFO     	 * (global step 1300: loss: 0.2702590897679329, lr: 1e-05
2023-12-20 12:31:58 INFO     	 * (global step 1350: loss: 0.1811705306172371, lr: 1e-05
2023-12-20 12:32:06 INFO     	 * (global step 1400: loss: 0.4617185443639755, lr: 1e-05
2023-12-20 12:32:14 INFO     	 * (global step 1450: loss: 0.3964448869228363, lr: 1e-05
2023-12-20 12:32:22 INFO     	 * (global step 1500: loss: 0.24354400485754013, lr: 1e-05
2023-12-20 12:32:31 INFO     	 * (global step 1550: loss: 0.30189353227615356, lr: 1e-05
2023-12-20 12:32:39 INFO     	 * (global step 1600: loss: 0.3249121606349945, lr: 1e-05
2023-12-20 12:32:47 INFO     	 * (global step 1650: loss: 0.3334767520427704, lr: 1e-05
2023-12-20 12:32:55 INFO     	 * (global step 1700: loss: 0.3275139331817627, lr: 1e-05
2023-12-20 12:33:03 INFO     	 * (global step 1750: loss: 0.29005513340234756, lr: 1e-05
2023-12-20 12:33:12 INFO     	 * (global step 1800: loss: 0.38473203778266907, lr: 1e-05
2023-12-20 12:33:20 INFO     	 * (global step 1850: loss: 0.30362361669540405, lr: 1e-05
2023-12-20 12:33:22 INFO     [epoch 7/10] average loss: 0.371, lr: 1e-05
2023-12-20 12:33:22 INFO     saving model related files
2023-12-20 12:33:22 INFO     saving model
2023-12-20 12:33:23 INFO     saving tokenizer
2023-12-20 12:33:23 INFO     saving optimizer
2023-12-20 12:33:24 INFO     remove old optimizer files
2023-12-20 12:33:30 INFO     	 * (global step 1900: loss: 0.23909258842468262, lr: 1e-05
2023-12-20 12:33:38 INFO     	 * (global step 1950: loss: 0.3062393143773079, lr: 1e-05
2023-12-20 12:33:47 INFO     	 * (global step 2000: loss: 0.436226561665535, lr: 1e-05
2023-12-20 12:33:55 INFO     	 * (global step 2050: loss: 0.4825877249240875, lr: 1e-05
2023-12-20 12:34:03 INFO     	 * (global step 2100: loss: 0.34319784492254257, lr: 1e-05
2023-12-20 12:34:11 INFO     	 * (global step 2150: loss: 0.3140881881117821, lr: 1e-05
2023-12-20 12:34:19 INFO     	 * (global step 2200: loss: 0.42314019799232483, lr: 1e-05
2023-12-20 12:34:28 INFO     	 * (global step 2250: loss: 0.38529473543167114, lr: 1e-05
2023-12-20 12:34:36 INFO     	 * (global step 2300: loss: 0.39271144568920135, lr: 1e-05
2023-12-20 12:34:44 INFO     	 * (global step 2350: loss: 0.3594765067100525, lr: 1e-05
2023-12-20 12:34:52 INFO     	 * (global step 2400: loss: 0.4702436476945877, lr: 1e-05
2023-12-20 12:35:01 INFO     	 * (global step 2450: loss: 0.32491547614336014, lr: 1e-05
2023-12-20 12:35:06 INFO     [epoch 8/10] average loss: 0.369, lr: 1e-05
2023-12-20 12:35:06 INFO     saving model related files
2023-12-20 12:35:06 INFO     saving model
2023-12-20 12:35:07 INFO     saving tokenizer
2023-12-20 12:35:07 INFO     saving optimizer
2023-12-20 12:35:08 INFO     remove old optimizer files
2023-12-20 12:35:10 INFO     	 * (global step 2500: loss: 0.30963335931301117, lr: 1e-05
2023-12-20 12:35:19 INFO     	 * (global step 2550: loss: 0.27401742339134216, lr: 1e-05
2023-12-20 12:35:27 INFO     	 * (global step 2600: loss: 0.5425978899002075, lr: 1e-05
2023-12-20 12:35:35 INFO     	 * (global step 2650: loss: 0.43542663753032684, lr: 1e-05
2023-12-20 12:35:43 INFO     	 * (global step 2700: loss: 0.30897289514541626, lr: 1e-05
2023-12-20 12:35:52 INFO     	 * (global step 2750: loss: 0.36020275950431824, lr: 1e-05
2023-12-20 12:36:00 INFO     	 * (global step 2800: loss: 0.6626622825860977, lr: 1e-05
2023-12-20 12:36:08 INFO     	 * (global step 2850: loss: 0.40768739581108093, lr: 1e-05
2023-12-20 12:36:16 INFO     	 * (global step 2900: loss: 0.2412646785378456, lr: 1e-05
2023-12-20 12:36:25 INFO     	 * (global step 2950: loss: 0.333165243268013, lr: 1e-05
2023-12-20 12:36:33 INFO     	 * (global step 3000: loss: 0.4877487123012543, lr: 1e-05
2023-12-20 12:36:41 INFO     	 * (global step 3050: loss: 0.3109453618526459, lr: 1e-05
2023-12-20 12:36:49 INFO     	 * (global step 3100: loss: 0.30687712132930756, lr: 1e-05
2023-12-20 12:36:50 INFO     [epoch 9/10] average loss: 0.366, lr: 1e-05
2023-12-20 12:36:50 INFO     saving model related files
2023-12-20 12:36:50 INFO     saving model
2023-12-20 12:36:51 INFO     saving tokenizer
2023-12-20 12:36:51 INFO     saving optimizer
2023-12-20 12:36:52 INFO     remove old optimizer files
2023-12-20 12:36:52 INFO     complete training: model ckpt was saved at small_finetuned_ckpt/model_mzgdpa
2023-12-20 12:36:52 INFO     ## 2nd RUN (EVAL): Configuration 0/4 ##
2023-12-20 12:36:58 INFO     use spaCy answer extraction model: positionrank
2023-12-20 12:36:58 INFO     Model `small_finetuned_ckpt/model_yhgxcx/epoch_1`
2023-12-20 12:36:58 INFO     	 * Num of GPU in use: 1
2023-12-20 12:36:58 INFO     	 * Prefix: True
2023-12-20 12:36:58 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 12:36:59 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 12:40:27 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 12:44:01 INFO     	Bleu_1: 0.24223396178665949
2023-12-20 12:44:01 INFO     	Bleu_2: 0.1355058557544001
2023-12-20 12:44:01 INFO     	Bleu_3: 0.07589659262353585
2023-12-20 12:44:01 INFO     	Bleu_4: 0.04944739287136915
2023-12-20 12:44:02 INFO     	Bleu_1: 0.22894204595568318
2023-12-20 12:44:02 INFO     	Bleu_2: 0.12601034435510547
2023-12-20 12:44:02 INFO     	Bleu_3: 0.06950011839946535
2023-12-20 12:44:02 INFO     	Bleu_4: 0.044499672352735414
2023-12-20 12:44:08 INFO     use spaCy answer extraction model: positionrank
2023-12-20 12:44:08 INFO     Model `small_finetuned_ckpt/model_yhgxcx/epoch_10`
2023-12-20 12:44:08 INFO     	 * Num of GPU in use: 1
2023-12-20 12:44:08 INFO     	 * Prefix: True
2023-12-20 12:44:08 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 12:44:10 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 12:49:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 12:54:53 INFO     	Bleu_1: 0.11357886049060255
2023-12-20 12:54:53 INFO     	Bleu_2: 0.06161076944841126
2023-12-20 12:54:53 INFO     	Bleu_3: 0.03153732578104941
2023-12-20 12:54:53 INFO     	Bleu_4: 0.01945317642046007
2023-12-20 12:54:54 INFO     	Bleu_1: 0.11163155828862939
2023-12-20 12:54:54 INFO     	Bleu_2: 0.06109528588358488
2023-12-20 12:54:54 INFO     	Bleu_3: 0.032033223391116486
2023-12-20 12:54:54 INFO     	Bleu_4: 0.019906337521064497
2023-12-20 12:54:59 INFO     use spaCy answer extraction model: positionrank
2023-12-20 12:55:00 INFO     Model `small_finetuned_ckpt/model_yhgxcx/epoch_2`
2023-12-20 12:55:00 INFO     	 * Num of GPU in use: 1
2023-12-20 12:55:00 INFO     	 * Prefix: True
2023-12-20 12:55:00 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 12:55:01 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 12:59:24 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 13:03:53 INFO     	Bleu_1: 0.19587115554023266
2023-12-20 13:03:53 INFO     	Bleu_2: 0.10811492606127131
2023-12-20 13:03:53 INFO     	Bleu_3: 0.05870740365931129
2023-12-20 13:03:53 INFO     	Bleu_4: 0.037788504203896324
2023-12-20 13:03:53 INFO     	Bleu_1: 0.19146503856686775
2023-12-20 13:03:53 INFO     	Bleu_2: 0.10495828439586648
2023-12-20 13:03:53 INFO     	Bleu_3: 0.05625734120658811
2023-12-20 13:03:53 INFO     	Bleu_4: 0.03504805328346663
2023-12-20 13:03:58 INFO     use spaCy answer extraction model: positionrank
2023-12-20 13:03:59 INFO     Model `small_finetuned_ckpt/model_yhgxcx/epoch_3`
2023-12-20 13:03:59 INFO     	 * Num of GPU in use: 1
2023-12-20 13:03:59 INFO     	 * Prefix: True
2023-12-20 13:03:59 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 13:04:00 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 13:08:49 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 13:13:41 INFO     	Bleu_1: 0.17153222989645225
2023-12-20 13:13:41 INFO     	Bleu_2: 0.09405903310513439
2023-12-20 13:13:41 INFO     	Bleu_3: 0.05031093682009622
2023-12-20 13:13:41 INFO     	Bleu_4: 0.03215937900705852
2023-12-20 13:13:42 INFO     	Bleu_1: 0.16492805569079833
2023-12-20 13:13:42 INFO     	Bleu_2: 0.09092366873783346
2023-12-20 13:13:42 INFO     	Bleu_3: 0.04993942397913462
2023-12-20 13:13:42 INFO     	Bleu_4: 0.032156891918798496
2023-12-20 13:13:48 INFO     use spaCy answer extraction model: positionrank
2023-12-20 13:13:48 INFO     Model `small_finetuned_ckpt/model_yhgxcx/epoch_4`
2023-12-20 13:13:48 INFO     	 * Num of GPU in use: 1
2023-12-20 13:13:48 INFO     	 * Prefix: True
2023-12-20 13:13:48 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 13:13:49 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 13:18:51 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 13:23:51 INFO     	Bleu_1: 0.15926900942818498
2023-12-20 13:23:51 INFO     	Bleu_2: 0.08703567522624081
2023-12-20 13:23:51 INFO     	Bleu_3: 0.045869894936138605
2023-12-20 13:23:51 INFO     	Bleu_4: 0.028807045432486796
2023-12-20 13:23:51 INFO     	Bleu_1: 0.14870981505184716
2023-12-20 13:23:51 INFO     	Bleu_2: 0.08159382018619163
2023-12-20 13:23:51 INFO     	Bleu_3: 0.04405579378925395
2023-12-20 13:23:51 INFO     	Bleu_4: 0.02815822433009218
2023-12-20 13:23:59 INFO     use spaCy answer extraction model: positionrank
2023-12-20 13:23:59 INFO     Model `small_finetuned_ckpt/model_yhgxcx/epoch_6`
2023-12-20 13:23:59 INFO     	 * Num of GPU in use: 1
2023-12-20 13:23:59 INFO     	 * Prefix: True
2023-12-20 13:23:59 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 13:24:00 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 13:26:37 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 13:29:16 INFO     	Bleu_1: 0.23267469029684246
2023-12-20 13:29:16 INFO     	Bleu_2: 0.12772562053661998
2023-12-20 13:29:16 INFO     	Bleu_3: 0.07011716370613764
2023-12-20 13:29:16 INFO     	Bleu_4: 0.04518239105328662
2023-12-20 13:29:16 INFO     	Bleu_1: 0.23485321405692244
2023-12-20 13:29:16 INFO     	Bleu_2: 0.12838418108581734
2023-12-20 13:29:16 INFO     	Bleu_3: 0.07054597492132514
2023-12-20 13:29:16 INFO     	Bleu_4: 0.044963385864726695
2023-12-20 13:29:23 INFO     use spaCy answer extraction model: positionrank
2023-12-20 13:29:23 INFO     Model `small_finetuned_ckpt/model_yhgxcx/epoch_7`
2023-12-20 13:29:23 INFO     	 * Num of GPU in use: 1
2023-12-20 13:29:23 INFO     	 * Prefix: True
2023-12-20 13:29:23 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 13:29:24 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 13:34:25 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 13:39:21 INFO     	Bleu_1: 0.17060791055598237
2023-12-20 13:39:21 INFO     	Bleu_2: 0.09307941296814311
2023-12-20 13:39:21 INFO     	Bleu_3: 0.049681546678453474
2023-12-20 13:39:21 INFO     	Bleu_4: 0.03149186466847971
2023-12-20 13:39:21 INFO     	Bleu_1: 0.168349860622399
2023-12-20 13:39:21 INFO     	Bleu_2: 0.0917374004410582
2023-12-20 13:39:21 INFO     	Bleu_3: 0.049314261508702315
2023-12-20 13:39:21 INFO     	Bleu_4: 0.030884471359336245
2023-12-20 13:39:27 INFO     use spaCy answer extraction model: positionrank
2023-12-20 13:39:27 INFO     Model `small_finetuned_ckpt/model_yhgxcx/epoch_8`
2023-12-20 13:39:27 INFO     	 * Num of GPU in use: 1
2023-12-20 13:39:27 INFO     	 * Prefix: True
2023-12-20 13:39:27 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 13:39:28 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 13:44:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 13:49:59 INFO     	Bleu_1: 0.133323636861422
2023-12-20 13:49:59 INFO     	Bleu_2: 0.07321294898204538
2023-12-20 13:49:59 INFO     	Bleu_3: 0.03899206057157452
2023-12-20 13:49:59 INFO     	Bleu_4: 0.02481844347430257
2023-12-20 13:50:00 INFO     	Bleu_1: 0.12959301367605758
2023-12-20 13:50:00 INFO     	Bleu_2: 0.07086240095209471
2023-12-20 13:50:00 INFO     	Bleu_3: 0.03750348863217442
2023-12-20 13:50:00 INFO     	Bleu_4: 0.023303698123235395
2023-12-20 13:50:05 INFO     use spaCy answer extraction model: positionrank
2023-12-20 13:50:06 INFO     Model `small_finetuned_ckpt/model_yhgxcx/epoch_9`
2023-12-20 13:50:06 INFO     	 * Num of GPU in use: 1
2023-12-20 13:50:06 INFO     	 * Prefix: True
2023-12-20 13:50:06 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 13:50:07 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 13:55:29 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 14:00:49 INFO     	Bleu_1: 0.12041501828767545
2023-12-20 14:00:49 INFO     	Bleu_2: 0.06561189567298839
2023-12-20 14:00:49 INFO     	Bleu_3: 0.03392819333828164
2023-12-20 14:00:49 INFO     	Bleu_4: 0.02109861282200376
2023-12-20 14:00:50 INFO     	Bleu_1: 0.11549902125376964
2023-12-20 14:00:50 INFO     	Bleu_2: 0.06365533759694067
2023-12-20 14:00:50 INFO     	Bleu_3: 0.03389882567959022
2023-12-20 14:00:50 INFO     	Bleu_4: 0.021346549212025164
2023-12-20 14:00:50 INFO     ## 2nd RUN (EVAL): Configuration 1/4 ##
2023-12-20 14:00:55 INFO     use spaCy answer extraction model: positionrank
2023-12-20 14:00:55 INFO     Model `small_finetuned_ckpt/model_dpyopu/epoch_1`
2023-12-20 14:00:55 INFO     	 * Num of GPU in use: 1
2023-12-20 14:00:55 INFO     	 * Prefix: True
2023-12-20 14:00:55 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 14:00:57 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 14:04:23 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 14:07:57 INFO     	Bleu_1: 0.24223396178665949
2023-12-20 14:07:57 INFO     	Bleu_2: 0.1355058557544001
2023-12-20 14:07:57 INFO     	Bleu_3: 0.07589659262353585
2023-12-20 14:07:57 INFO     	Bleu_4: 0.04944739287136915
2023-12-20 14:07:57 INFO     	Bleu_1: 0.22894204595568318
2023-12-20 14:07:57 INFO     	Bleu_2: 0.12601034435510547
2023-12-20 14:07:57 INFO     	Bleu_3: 0.06950011839946535
2023-12-20 14:07:57 INFO     	Bleu_4: 0.044499672352735414
2023-12-20 14:08:03 INFO     use spaCy answer extraction model: positionrank
2023-12-20 14:08:03 INFO     Model `small_finetuned_ckpt/model_dpyopu/epoch_10`
2023-12-20 14:08:03 INFO     	 * Num of GPU in use: 1
2023-12-20 14:08:03 INFO     	 * Prefix: True
2023-12-20 14:08:03 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 14:08:04 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 14:13:23 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 14:18:40 INFO     	Bleu_1: 0.12269903109476066
2023-12-20 14:18:40 INFO     	Bleu_2: 0.06701842857199289
2023-12-20 14:18:40 INFO     	Bleu_3: 0.03529847711659767
2023-12-20 14:18:40 INFO     	Bleu_4: 0.02239542584950567
2023-12-20 14:18:40 INFO     	Bleu_1: 0.11646446191639388
2023-12-20 14:18:40 INFO     	Bleu_2: 0.06360843444961449
2023-12-20 14:18:40 INFO     	Bleu_3: 0.033679368427429114
2023-12-20 14:18:40 INFO     	Bleu_4: 0.021278636711366268
2023-12-20 14:18:45 INFO     use spaCy answer extraction model: positionrank
2023-12-20 14:18:46 INFO     Model `small_finetuned_ckpt/model_dpyopu/epoch_2`
2023-12-20 14:18:46 INFO     	 * Num of GPU in use: 1
2023-12-20 14:18:46 INFO     	 * Prefix: True
2023-12-20 14:18:46 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 14:18:47 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 14:23:11 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 14:27:40 INFO     	Bleu_1: 0.19587115554023266
2023-12-20 14:27:40 INFO     	Bleu_2: 0.10811492606127131
2023-12-20 14:27:40 INFO     	Bleu_3: 0.05870740365931129
2023-12-20 14:27:40 INFO     	Bleu_4: 0.037788504203896324
2023-12-20 14:27:40 INFO     	Bleu_1: 0.19146503856686775
2023-12-20 14:27:40 INFO     	Bleu_2: 0.10495828439586648
2023-12-20 14:27:40 INFO     	Bleu_3: 0.05625734120658811
2023-12-20 14:27:40 INFO     	Bleu_4: 0.03504805328346663
2023-12-20 14:27:45 INFO     use spaCy answer extraction model: positionrank
2023-12-20 14:27:45 INFO     Model `small_finetuned_ckpt/model_dpyopu/epoch_3`
2023-12-20 14:27:45 INFO     	 * Num of GPU in use: 1
2023-12-20 14:27:45 INFO     	 * Prefix: True
2023-12-20 14:27:45 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 14:27:46 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 14:32:35 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 14:37:28 INFO     	Bleu_1: 0.17153222989645225
2023-12-20 14:37:28 INFO     	Bleu_2: 0.09405903310513439
2023-12-20 14:37:28 INFO     	Bleu_3: 0.05031093682009622
2023-12-20 14:37:28 INFO     	Bleu_4: 0.03215937900705852
2023-12-20 14:37:28 INFO     	Bleu_1: 0.16492805569079833
2023-12-20 14:37:28 INFO     	Bleu_2: 0.09092366873783346
2023-12-20 14:37:28 INFO     	Bleu_3: 0.04993942397913462
2023-12-20 14:37:28 INFO     	Bleu_4: 0.032156891918798496
2023-12-20 14:37:32 INFO     use spaCy answer extraction model: positionrank
2023-12-20 14:37:33 INFO     Model `small_finetuned_ckpt/model_dpyopu/epoch_4`
2023-12-20 14:37:33 INFO     	 * Num of GPU in use: 1
2023-12-20 14:37:33 INFO     	 * Prefix: True
2023-12-20 14:37:33 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 14:37:34 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 14:42:36 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 14:47:36 INFO     	Bleu_1: 0.15926900942818498
2023-12-20 14:47:36 INFO     	Bleu_2: 0.08703567522624081
2023-12-20 14:47:36 INFO     	Bleu_3: 0.045869894936138605
2023-12-20 14:47:36 INFO     	Bleu_4: 0.028807045432486796
2023-12-20 14:47:36 INFO     	Bleu_1: 0.14870981505184716
2023-12-20 14:47:36 INFO     	Bleu_2: 0.08159382018619163
2023-12-20 14:47:36 INFO     	Bleu_3: 0.04405579378925395
2023-12-20 14:47:36 INFO     	Bleu_4: 0.02815822433009218
2023-12-20 14:47:44 INFO     use spaCy answer extraction model: positionrank
2023-12-20 14:47:44 INFO     Model `small_finetuned_ckpt/model_dpyopu/epoch_6`
2023-12-20 14:47:44 INFO     	 * Num of GPU in use: 1
2023-12-20 14:47:44 INFO     	 * Prefix: True
2023-12-20 14:47:44 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 14:47:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 14:52:53 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 14:58:01 INFO     	Bleu_1: 0.14158574641306687
2023-12-20 14:58:01 INFO     	Bleu_2: 0.07707224909012432
2023-12-20 14:58:01 INFO     	Bleu_3: 0.039828568053020375
2023-12-20 14:58:01 INFO     	Bleu_4: 0.02494937767320366
2023-12-20 14:58:01 INFO     	Bleu_1: 0.13899364744137593
2023-12-20 14:58:01 INFO     	Bleu_2: 0.07624084157737951
2023-12-20 14:58:01 INFO     	Bleu_3: 0.04096882730689093
2023-12-20 14:58:01 INFO     	Bleu_4: 0.02609495883250752
2023-12-20 14:58:07 INFO     use spaCy answer extraction model: positionrank
2023-12-20 14:58:07 INFO     Model `small_finetuned_ckpt/model_dpyopu/epoch_7`
2023-12-20 14:58:07 INFO     	 * Num of GPU in use: 1
2023-12-20 14:58:07 INFO     	 * Prefix: True
2023-12-20 14:58:07 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 14:58:08 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 15:03:15 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 15:08:17 INFO     	Bleu_1: 0.14273933977070072
2023-12-20 15:08:17 INFO     	Bleu_2: 0.07797120607633948
2023-12-20 15:08:17 INFO     	Bleu_3: 0.04091866379551828
2023-12-20 15:08:17 INFO     	Bleu_4: 0.025958168767063833
2023-12-20 15:08:18 INFO     	Bleu_1: 0.1369190116052705
2023-12-20 15:08:18 INFO     	Bleu_2: 0.07487103269236074
2023-12-20 15:08:18 INFO     	Bleu_3: 0.040105535393138284
2023-12-20 15:08:18 INFO     	Bleu_4: 0.025548949467305047
2023-12-20 15:08:24 INFO     use spaCy answer extraction model: positionrank
2023-12-20 15:08:24 INFO     Model `small_finetuned_ckpt/model_dpyopu/epoch_8`
2023-12-20 15:08:24 INFO     	 * Num of GPU in use: 1
2023-12-20 15:08:24 INFO     	 * Prefix: True
2023-12-20 15:08:24 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 15:08:25 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 15:13:38 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 15:18:45 INFO     	Bleu_1: 0.13363946684005093
2023-12-20 15:18:45 INFO     	Bleu_2: 0.07258438999486817
2023-12-20 15:18:45 INFO     	Bleu_3: 0.037723410698040055
2023-12-20 15:18:45 INFO     	Bleu_4: 0.023700051286566323
2023-12-20 15:18:45 INFO     	Bleu_1: 0.125704647676161
2023-12-20 15:18:45 INFO     	Bleu_2: 0.0692893602671551
2023-12-20 15:18:45 INFO     	Bleu_3: 0.03755450559708558
2023-12-20 15:18:45 INFO     	Bleu_4: 0.02417381275454169
2023-12-20 15:18:51 INFO     use spaCy answer extraction model: positionrank
2023-12-20 15:18:52 INFO     Model `small_finetuned_ckpt/model_dpyopu/epoch_9`
2023-12-20 15:18:52 INFO     	 * Num of GPU in use: 1
2023-12-20 15:18:52 INFO     	 * Prefix: True
2023-12-20 15:18:52 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 15:18:53 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 15:24:08 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 15:29:21 INFO     	Bleu_1: 0.1311592777624788
2023-12-20 15:29:21 INFO     	Bleu_2: 0.0718986477204443
2023-12-20 15:29:21 INFO     	Bleu_3: 0.03801893126244624
2023-12-20 15:29:21 INFO     	Bleu_4: 0.024133788602323947
2023-12-20 15:29:21 INFO     	Bleu_1: 0.12361385776019833
2023-12-20 15:29:21 INFO     	Bleu_2: 0.06772748925806885
2023-12-20 15:29:21 INFO     	Bleu_3: 0.03643894081236108
2023-12-20 15:29:21 INFO     	Bleu_4: 0.02341136611067053
2023-12-20 15:29:21 INFO     ## 2nd RUN (EVAL): Configuration 2/4 ##
2023-12-20 15:29:26 INFO     use spaCy answer extraction model: positionrank
2023-12-20 15:29:27 INFO     Model `small_finetuned_ckpt/model_eszyci/epoch_1`
2023-12-20 15:29:27 INFO     	 * Num of GPU in use: 1
2023-12-20 15:29:27 INFO     	 * Prefix: True
2023-12-20 15:29:27 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 15:29:28 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 15:33:43 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 15:37:38 INFO     	Bleu_1: 0.22255506607929187
2023-12-20 15:37:38 INFO     	Bleu_2: 0.12402297094649412
2023-12-20 15:37:38 INFO     	Bleu_3: 0.06888878664142187
2023-12-20 15:37:38 INFO     	Bleu_4: 0.044594802740991514
2023-12-20 15:37:38 INFO     	Bleu_1: 0.2063863657645819
2023-12-20 15:37:38 INFO     	Bleu_2: 0.11454784228169868
2023-12-20 15:37:38 INFO     	Bleu_3: 0.06340655854524996
2023-12-20 15:37:38 INFO     	Bleu_4: 0.04084025123415649
2023-12-20 15:37:44 INFO     use spaCy answer extraction model: positionrank
2023-12-20 15:37:44 INFO     Model `small_finetuned_ckpt/model_eszyci/epoch_10`
2023-12-20 15:37:44 INFO     	 * Num of GPU in use: 1
2023-12-20 15:37:44 INFO     	 * Prefix: True
2023-12-20 15:37:44 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 15:37:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 15:43:10 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 15:48:35 INFO     	Bleu_1: 0.10415366059143992
2023-12-20 15:48:35 INFO     	Bleu_2: 0.05649907939212004
2023-12-20 15:48:35 INFO     	Bleu_3: 0.02866579022244986
2023-12-20 15:48:35 INFO     	Bleu_4: 0.01755985879434174
2023-12-20 15:48:35 INFO     	Bleu_1: 0.10262556672230251
2023-12-20 15:48:35 INFO     	Bleu_2: 0.055851221056333555
2023-12-20 15:48:35 INFO     	Bleu_3: 0.028992579080064855
2023-12-20 15:48:35 INFO     	Bleu_4: 0.01785227163873188
2023-12-20 15:48:41 INFO     use spaCy answer extraction model: positionrank
2023-12-20 15:48:41 INFO     Model `small_finetuned_ckpt/model_eszyci/epoch_2`
2023-12-20 15:48:41 INFO     	 * Num of GPU in use: 1
2023-12-20 15:48:41 INFO     	 * Prefix: True
2023-12-20 15:48:41 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 15:48:42 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 15:53:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 15:58:17 INFO     	Bleu_1: 0.1726529965598387
2023-12-20 15:58:17 INFO     	Bleu_2: 0.09496677001536358
2023-12-20 15:58:17 INFO     	Bleu_3: 0.050735296275318004
2023-12-20 15:58:17 INFO     	Bleu_4: 0.03236520527539009
2023-12-20 15:58:17 INFO     	Bleu_1: 0.1717626392929662
2023-12-20 15:58:17 INFO     	Bleu_2: 0.09404879213385099
2023-12-20 15:58:17 INFO     	Bleu_3: 0.050403381920408726
2023-12-20 15:58:17 INFO     	Bleu_4: 0.031837218051344526
2023-12-20 15:58:22 INFO     use spaCy answer extraction model: positionrank
2023-12-20 15:58:23 INFO     Model `small_finetuned_ckpt/model_eszyci/epoch_3`
2023-12-20 15:58:23 INFO     	 * Num of GPU in use: 1
2023-12-20 15:58:23 INFO     	 * Prefix: True
2023-12-20 15:58:23 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 15:58:24 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 16:03:29 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 16:08:28 INFO     	Bleu_1: 0.15514108273259025
2023-12-20 16:08:28 INFO     	Bleu_2: 0.08436074291990228
2023-12-20 16:08:28 INFO     	Bleu_3: 0.04396214939218179
2023-12-20 16:08:28 INFO     	Bleu_4: 0.027421135571338063
2023-12-20 16:08:28 INFO     	Bleu_1: 0.15057509037134265
2023-12-20 16:08:28 INFO     	Bleu_2: 0.08245466292725331
2023-12-20 16:08:28 INFO     	Bleu_3: 0.04443944754335066
2023-12-20 16:08:28 INFO     	Bleu_4: 0.02843003746772814
2023-12-20 16:08:34 INFO     use spaCy answer extraction model: positionrank
2023-12-20 16:08:34 INFO     Model `small_finetuned_ckpt/model_eszyci/epoch_4`
2023-12-20 16:08:34 INFO     	 * Num of GPU in use: 1
2023-12-20 16:08:34 INFO     	 * Prefix: True
2023-12-20 16:08:34 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 16:08:35 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 16:13:48 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 16:18:55 INFO     	Bleu_1: 0.14141547565582524
2023-12-20 16:18:55 INFO     	Bleu_2: 0.07710541502973495
2023-12-20 16:18:55 INFO     	Bleu_3: 0.0401198102501414
2023-12-20 16:18:55 INFO     	Bleu_4: 0.02515104789337203
2023-12-20 16:18:55 INFO     	Bleu_1: 0.13601258110199801
2023-12-20 16:18:55 INFO     	Bleu_2: 0.07441193375926562
2023-12-20 16:18:55 INFO     	Bleu_3: 0.039724710468549224
2023-12-20 16:18:55 INFO     	Bleu_4: 0.02509104675281221
2023-12-20 16:19:03 INFO     use spaCy answer extraction model: positionrank
2023-12-20 16:19:03 INFO     Model `small_finetuned_ckpt/model_eszyci/epoch_6`
2023-12-20 16:19:03 INFO     	 * Num of GPU in use: 1
2023-12-20 16:19:03 INFO     	 * Prefix: True
2023-12-20 16:19:03 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 16:19:04 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 16:23:43 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 16:28:21 INFO     	Bleu_1: 0.18656795206798918
2023-12-20 16:28:21 INFO     	Bleu_2: 0.10281710443752104
2023-12-20 16:28:21 INFO     	Bleu_3: 0.05616933660616506
2023-12-20 16:28:21 INFO     	Bleu_4: 0.03636801664027338
2023-12-20 16:28:21 INFO     	Bleu_1: 0.18558490859385893
2023-12-20 16:28:21 INFO     	Bleu_2: 0.10202676318397194
2023-12-20 16:28:21 INFO     	Bleu_3: 0.05585743948213247
2023-12-20 16:28:21 INFO     	Bleu_4: 0.03549108799012747
2023-12-20 16:28:26 INFO     use spaCy answer extraction model: positionrank
2023-12-20 16:28:27 INFO     Model `small_finetuned_ckpt/model_eszyci/epoch_7`
2023-12-20 16:28:27 INFO     	 * Num of GPU in use: 1
2023-12-20 16:28:27 INFO     	 * Prefix: True
2023-12-20 16:28:27 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 16:28:28 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 16:33:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 16:39:01 INFO     	Bleu_1: 0.12599139086422378
2023-12-20 16:39:01 INFO     	Bleu_2: 0.06935732744955275
2023-12-20 16:39:01 INFO     	Bleu_3: 0.03697637422938765
2023-12-20 16:39:01 INFO     	Bleu_4: 0.02357908334327239
2023-12-20 16:39:02 INFO     	Bleu_1: 0.12329326995852288
2023-12-20 16:39:02 INFO     	Bleu_2: 0.06765689877564467
2023-12-20 16:39:02 INFO     	Bleu_3: 0.03597318506193268
2023-12-20 16:39:02 INFO     	Bleu_4: 0.022477065766698442
2023-12-20 16:39:07 INFO     use spaCy answer extraction model: positionrank
2023-12-20 16:39:08 INFO     Model `small_finetuned_ckpt/model_eszyci/epoch_8`
2023-12-20 16:39:08 INFO     	 * Num of GPU in use: 1
2023-12-20 16:39:08 INFO     	 * Prefix: True
2023-12-20 16:39:08 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 16:39:09 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 16:44:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 16:49:55 INFO     	Bleu_1: 0.11034131026580135
2023-12-20 16:49:55 INFO     	Bleu_2: 0.05996909188653387
2023-12-20 16:49:55 INFO     	Bleu_3: 0.03097371059372292
2023-12-20 16:49:55 INFO     	Bleu_4: 0.01918182537928217
2023-12-20 16:49:56 INFO     	Bleu_1: 0.10948662676730439
2023-12-20 16:49:56 INFO     	Bleu_2: 0.05962871117385873
2023-12-20 16:49:56 INFO     	Bleu_3: 0.031265183009028925
2023-12-20 16:49:56 INFO     	Bleu_4: 0.019381399017316715
2023-12-20 16:50:01 INFO     use spaCy answer extraction model: positionrank
2023-12-20 16:50:02 INFO     Model `small_finetuned_ckpt/model_eszyci/epoch_9`
2023-12-20 16:50:02 INFO     	 * Num of GPU in use: 1
2023-12-20 16:50:02 INFO     	 * Prefix: True
2023-12-20 16:50:02 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 16:50:03 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 16:55:26 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 17:00:50 INFO     	Bleu_1: 0.10785705152793693
2023-12-20 17:00:50 INFO     	Bleu_2: 0.058648193084220084
2023-12-20 17:00:50 INFO     	Bleu_3: 0.030156360294391477
2023-12-20 17:00:50 INFO     	Bleu_4: 0.018701450431947028
2023-12-20 17:00:50 INFO     	Bleu_1: 0.107400693912382
2023-12-20 17:00:50 INFO     	Bleu_2: 0.05856781335581222
2023-12-20 17:00:50 INFO     	Bleu_3: 0.030658078632946076
2023-12-20 17:00:50 INFO     	Bleu_4: 0.01907722352698945
2023-12-20 17:00:50 INFO     ## 2nd RUN (EVAL): Configuration 3/4 ##
2023-12-20 17:00:55 INFO     use spaCy answer extraction model: positionrank
2023-12-20 17:00:55 INFO     Model `small_finetuned_ckpt/model_mzgdpa/epoch_1`
2023-12-20 17:00:55 INFO     	 * Num of GPU in use: 1
2023-12-20 17:00:55 INFO     	 * Prefix: True
2023-12-20 17:00:55 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 17:00:56 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 17:05:12 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 17:09:08 INFO     	Bleu_1: 0.22255506607929187
2023-12-20 17:09:08 INFO     	Bleu_2: 0.12402297094649412
2023-12-20 17:09:08 INFO     	Bleu_3: 0.06888878664142187
2023-12-20 17:09:08 INFO     	Bleu_4: 0.044594802740991514
2023-12-20 17:09:08 INFO     	Bleu_1: 0.2063863657645819
2023-12-20 17:09:08 INFO     	Bleu_2: 0.11454784228169868
2023-12-20 17:09:08 INFO     	Bleu_3: 0.06340655854524996
2023-12-20 17:09:08 INFO     	Bleu_4: 0.04084025123415649
2023-12-20 17:09:14 INFO     use spaCy answer extraction model: positionrank
2023-12-20 17:09:14 INFO     Model `small_finetuned_ckpt/model_mzgdpa/epoch_10`
2023-12-20 17:09:14 INFO     	 * Num of GPU in use: 1
2023-12-20 17:09:14 INFO     	 * Prefix: True
2023-12-20 17:09:14 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 17:09:15 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 17:14:37 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 17:20:01 INFO     	Bleu_1: 0.1161780893147754
2023-12-20 17:20:01 INFO     	Bleu_2: 0.06295397708807494
2023-12-20 17:20:01 INFO     	Bleu_3: 0.032621195848672006
2023-12-20 17:20:01 INFO     	Bleu_4: 0.02044033549625426
2023-12-20 17:20:01 INFO     	Bleu_1: 0.11279196653589264
2023-12-20 17:20:01 INFO     	Bleu_2: 0.06251189453632083
2023-12-20 17:20:01 INFO     	Bleu_3: 0.033747846918212004
2023-12-20 17:20:01 INFO     	Bleu_4: 0.021642707113663687
2023-12-20 17:20:09 INFO     use spaCy answer extraction model: positionrank
2023-12-20 17:20:10 INFO     Model `small_finetuned_ckpt/model_mzgdpa/epoch_2`
2023-12-20 17:20:10 INFO     	 * Num of GPU in use: 1
2023-12-20 17:20:10 INFO     	 * Prefix: True
2023-12-20 17:20:10 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 17:20:11 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 17:25:06 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 17:29:58 INFO     	Bleu_1: 0.1726529965598387
2023-12-20 17:29:58 INFO     	Bleu_2: 0.09496677001536358
2023-12-20 17:29:58 INFO     	Bleu_3: 0.050735296275318004
2023-12-20 17:29:58 INFO     	Bleu_4: 0.03236520527539009
2023-12-20 17:29:58 INFO     	Bleu_1: 0.1717626392929662
2023-12-20 17:29:58 INFO     	Bleu_2: 0.09404879213385099
2023-12-20 17:29:58 INFO     	Bleu_3: 0.050403381920408726
2023-12-20 17:29:58 INFO     	Bleu_4: 0.031837218051344526
2023-12-20 17:30:03 INFO     use spaCy answer extraction model: positionrank
2023-12-20 17:30:04 INFO     Model `small_finetuned_ckpt/model_mzgdpa/epoch_3`
2023-12-20 17:30:04 INFO     	 * Num of GPU in use: 1
2023-12-20 17:30:04 INFO     	 * Prefix: True
2023-12-20 17:30:04 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 17:30:05 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 17:35:13 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 17:40:23 INFO     	Bleu_1: 0.15514108273259025
2023-12-20 17:40:23 INFO     	Bleu_2: 0.08436074291990228
2023-12-20 17:40:23 INFO     	Bleu_3: 0.04396214939218179
2023-12-20 17:40:23 INFO     	Bleu_4: 0.027421135571338063
2023-12-20 17:40:23 INFO     	Bleu_1: 0.15057509037134265
2023-12-20 17:40:23 INFO     	Bleu_2: 0.08245466292725331
2023-12-20 17:40:23 INFO     	Bleu_3: 0.04443944754335066
2023-12-20 17:40:23 INFO     	Bleu_4: 0.02843003746772814
2023-12-20 17:40:29 INFO     use spaCy answer extraction model: positionrank
2023-12-20 17:40:29 INFO     Model `small_finetuned_ckpt/model_mzgdpa/epoch_4`
2023-12-20 17:40:29 INFO     	 * Num of GPU in use: 1
2023-12-20 17:40:29 INFO     	 * Prefix: True
2023-12-20 17:40:29 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 17:40:30 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 17:45:49 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 17:50:59 INFO     	Bleu_1: 0.14141547565582524
2023-12-20 17:50:59 INFO     	Bleu_2: 0.07710541502973495
2023-12-20 17:50:59 INFO     	Bleu_3: 0.0401198102501414
2023-12-20 17:50:59 INFO     	Bleu_4: 0.02515104789337203
2023-12-20 17:51:00 INFO     	Bleu_1: 0.13601258110199801
2023-12-20 17:51:00 INFO     	Bleu_2: 0.07441193375926562
2023-12-20 17:51:00 INFO     	Bleu_3: 0.039724710468549224
2023-12-20 17:51:00 INFO     	Bleu_4: 0.02509104675281221
2023-12-20 17:51:09 INFO     use spaCy answer extraction model: positionrank
2023-12-20 17:51:09 INFO     Model `small_finetuned_ckpt/model_mzgdpa/epoch_6`
2023-12-20 17:51:09 INFO     	 * Num of GPU in use: 1
2023-12-20 17:51:09 INFO     	 * Prefix: True
2023-12-20 17:51:09 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 17:51:10 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 17:56:34 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 18:01:51 INFO     	Bleu_1: 0.1320507179712801
2023-12-20 18:01:51 INFO     	Bleu_2: 0.07181649777381162
2023-12-20 18:01:51 INFO     	Bleu_3: 0.03732760354519628
2023-12-20 18:01:51 INFO     	Bleu_4: 0.023500730245269434
2023-12-20 18:01:51 INFO     	Bleu_1: 0.12516385628314272
2023-12-20 18:01:51 INFO     	Bleu_2: 0.06924164768943125
2023-12-20 18:01:51 INFO     	Bleu_3: 0.03790403461352524
2023-12-20 18:01:51 INFO     	Bleu_4: 0.024700614031061316
2023-12-20 18:01:57 INFO     use spaCy answer extraction model: positionrank
2023-12-20 18:01:58 INFO     Model `small_finetuned_ckpt/model_mzgdpa/epoch_7`
2023-12-20 18:01:58 INFO     	 * Num of GPU in use: 1
2023-12-20 18:01:58 INFO     	 * Prefix: True
2023-12-20 18:01:58 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 18:01:59 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 18:07:14 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 18:12:29 INFO     	Bleu_1: 0.13449018532015317
2023-12-20 18:12:29 INFO     	Bleu_2: 0.07317542908079713
2023-12-20 18:12:29 INFO     	Bleu_3: 0.03785026909450406
2023-12-20 18:12:29 INFO     	Bleu_4: 0.02381374923419926
2023-12-20 18:12:29 INFO     	Bleu_1: 0.12706152629016457
2023-12-20 18:12:29 INFO     	Bleu_2: 0.06963616069578193
2023-12-20 18:12:29 INFO     	Bleu_3: 0.03729527187372878
2023-12-20 18:12:29 INFO     	Bleu_4: 0.02378948332859284
2023-12-20 18:12:35 INFO     use spaCy answer extraction model: positionrank
2023-12-20 18:12:35 INFO     Model `small_finetuned_ckpt/model_mzgdpa/epoch_8`
2023-12-20 18:12:35 INFO     	 * Num of GPU in use: 1
2023-12-20 18:12:35 INFO     	 * Prefix: True
2023-12-20 18:12:35 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 18:12:36 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 18:17:56 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 18:23:19 INFO     	Bleu_1: 0.1252790282031687
2023-12-20 18:23:19 INFO     	Bleu_2: 0.06806297643676855
2023-12-20 18:23:19 INFO     	Bleu_3: 0.03552605591551472
2023-12-20 18:23:19 INFO     	Bleu_4: 0.022426349582976714
2023-12-20 18:23:19 INFO     	Bleu_1: 0.11891284530093835
2023-12-20 18:23:19 INFO     	Bleu_2: 0.06557908515614812
2023-12-20 18:23:19 INFO     	Bleu_3: 0.035401337306576026
2023-12-20 18:23:19 INFO     	Bleu_4: 0.022689306780731947
2023-12-20 18:23:26 INFO     use spaCy answer extraction model: positionrank
2023-12-20 18:23:27 INFO     Model `small_finetuned_ckpt/model_mzgdpa/epoch_9`
2023-12-20 18:23:27 INFO     	 * Num of GPU in use: 1
2023-12-20 18:23:27 INFO     	 * Prefix: True
2023-12-20 18:23:27 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 18:23:28 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-20 18:28:48 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-20 18:34:09 INFO     	Bleu_1: 0.12115751687986012
2023-12-20 18:34:09 INFO     	Bleu_2: 0.06592555081149384
2023-12-20 18:34:09 INFO     	Bleu_3: 0.034441158765187774
2023-12-20 18:34:09 INFO     	Bleu_4: 0.0217380505049867
2023-12-20 18:34:10 INFO     	Bleu_1: 0.11470841769741119
2023-12-20 18:34:10 INFO     	Bleu_2: 0.06256423006232724
2023-12-20 18:34:10 INFO     	Bleu_3: 0.0330773815255583
2023-12-20 18:34:10 INFO     	Bleu_4: 0.02079264884833483
2023-12-20 18:34:10 INFO     2nd RUN RESULTS: 
[('small_finetuned_ckpt/model_yhgxcx/epoch_1', 0.04944739287136915), ('small_finetuned_ckpt/model_dpyopu/epoch_1', 0.04944739287136915), ('small_finetuned_ckpt/model_yhgxcx/epoch_6', 0.04518239105328662), ('small_finetuned_ckpt/model_eszyci/epoch_1', 0.044594802740991514), ('small_finetuned_ckpt/model_mzgdpa/epoch_1', 0.044594802740991514), ('small_finetuned_ckpt/model_yhgxcx/epoch_2', 0.037788504203896324), ('small_finetuned_ckpt/model_dpyopu/epoch_2', 0.037788504203896324), ('small_finetuned_ckpt/model_eszyci/epoch_6', 0.03636801664027338), ('small_finetuned_ckpt/model_eszyci/epoch_2', 0.03236520527539009), ('small_finetuned_ckpt/model_mzgdpa/epoch_2', 0.03236520527539009), ('small_finetuned_ckpt/model_yhgxcx/epoch_3', 0.03215937900705852), ('small_finetuned_ckpt/model_dpyopu/epoch_3', 0.03215937900705852), ('small_finetuned_ckpt/model_yhgxcx/epoch_7', 0.03149186466847971), ('small_finetuned_ckpt/model_yhgxcx/epoch_4', 0.028807045432486796), ('small_finetuned_ckpt/model_dpyopu/epoch_4', 0.028807045432486796), ('small_finetuned_ckpt/model_eszyci/epoch_3', 0.027421135571338063), ('small_finetuned_ckpt/model_mzgdpa/epoch_3', 0.027421135571338063), ('small_finetuned_ckpt/model_dpyopu/epoch_7', 0.025958168767063833), ('small_finetuned_ckpt/model_yhgxcx/epoch_5', 0.025754485750608378), ('small_finetuned_ckpt/model_dpyopu/epoch_5', 0.025754485750608378), ('small_finetuned_ckpt/model_eszyci/epoch_4', 0.02515104789337203), ('small_finetuned_ckpt/model_mzgdpa/epoch_4', 0.02515104789337203), ('small_finetuned_ckpt/model_dpyopu/epoch_6', 0.02494937767320366), ('small_finetuned_ckpt/model_yhgxcx/epoch_8', 0.02481844347430257), ('small_finetuned_ckpt/model_dpyopu/epoch_9', 0.024133788602323947), ('small_finetuned_ckpt/model_mzgdpa/epoch_7', 0.02381374923419926), ('small_finetuned_ckpt/model_dpyopu/epoch_8', 0.023700051286566323), ('small_finetuned_ckpt/model_eszyci/epoch_5', 0.023658109832308528), ('small_finetuned_ckpt/model_mzgdpa/epoch_5', 0.023658109832308528), ('small_finetuned_ckpt/model_eszyci/epoch_7', 0.02357908334327239), ('small_finetuned_ckpt/model_mzgdpa/epoch_6', 0.023500730245269434), ('small_finetuned_ckpt/model_mzgdpa/epoch_8', 0.022426349582976714), ('small_finetuned_ckpt/model_dpyopu/epoch_10', 0.02239542584950567), ('small_finetuned_ckpt/model_mzgdpa/epoch_9', 0.0217380505049867), ('small_finetuned_ckpt/model_yhgxcx/epoch_9', 0.02109861282200376), ('small_finetuned_ckpt/model_mzgdpa/epoch_10', 0.02044033549625426), ('small_finetuned_ckpt/model_yhgxcx/epoch_10', 0.01945317642046007), ('small_finetuned_ckpt/model_eszyci/epoch_8', 0.01918182537928217), ('small_finetuned_ckpt/model_eszyci/epoch_9', 0.018701450431947028), ('small_finetuned_ckpt/model_eszyci/epoch_10', 0.01755985879434174)]
2023-12-20 18:34:10 INFO     	 * rank: 0 | metric: 0.049 | model: small_finetuned_ckpt/model_yhgxcx/epoch_1 |
2023-12-20 18:34:10 INFO     	 * rank: 1 | metric: 0.049 | model: small_finetuned_ckpt/model_dpyopu/epoch_1 |
2023-12-20 18:34:10 INFO     	 * rank: 2 | metric: 0.045 | model: small_finetuned_ckpt/model_yhgxcx/epoch_6 |
2023-12-20 18:34:10 INFO     	 * rank: 3 | metric: 0.045 | model: small_finetuned_ckpt/model_eszyci/epoch_1 |
2023-12-20 18:34:10 INFO     	 * rank: 4 | metric: 0.045 | model: small_finetuned_ckpt/model_mzgdpa/epoch_1 |
2023-12-20 18:34:10 INFO     	 * rank: 5 | metric: 0.038 | model: small_finetuned_ckpt/model_yhgxcx/epoch_2 |
2023-12-20 18:34:10 INFO     	 * rank: 6 | metric: 0.038 | model: small_finetuned_ckpt/model_dpyopu/epoch_2 |
2023-12-20 18:34:10 INFO     	 * rank: 7 | metric: 0.036 | model: small_finetuned_ckpt/model_eszyci/epoch_6 |
2023-12-20 18:34:10 INFO     	 * rank: 8 | metric: 0.032 | model: small_finetuned_ckpt/model_eszyci/epoch_2 |
2023-12-20 18:34:10 INFO     	 * rank: 9 | metric: 0.032 | model: small_finetuned_ckpt/model_mzgdpa/epoch_2 |
2023-12-20 18:34:10 INFO     	 * rank: 10 | metric: 0.032 | model: small_finetuned_ckpt/model_yhgxcx/epoch_3 |
2023-12-20 18:34:10 INFO     	 * rank: 11 | metric: 0.032 | model: small_finetuned_ckpt/model_dpyopu/epoch_3 |
2023-12-20 18:34:10 INFO     	 * rank: 12 | metric: 0.031 | model: small_finetuned_ckpt/model_yhgxcx/epoch_7 |
2023-12-20 18:34:10 INFO     	 * rank: 13 | metric: 0.029 | model: small_finetuned_ckpt/model_yhgxcx/epoch_4 |
2023-12-20 18:34:10 INFO     	 * rank: 14 | metric: 0.029 | model: small_finetuned_ckpt/model_dpyopu/epoch_4 |
2023-12-20 18:34:10 INFO     	 * rank: 15 | metric: 0.027 | model: small_finetuned_ckpt/model_eszyci/epoch_3 |
2023-12-20 18:34:10 INFO     	 * rank: 16 | metric: 0.027 | model: small_finetuned_ckpt/model_mzgdpa/epoch_3 |
2023-12-20 18:34:10 INFO     	 * rank: 17 | metric: 0.026 | model: small_finetuned_ckpt/model_dpyopu/epoch_7 |
2023-12-20 18:34:10 INFO     	 * rank: 18 | metric: 0.026 | model: small_finetuned_ckpt/model_yhgxcx/epoch_5 |
2023-12-20 18:34:10 INFO     	 * rank: 19 | metric: 0.026 | model: small_finetuned_ckpt/model_dpyopu/epoch_5 |
2023-12-20 18:34:10 INFO     	 * rank: 20 | metric: 0.025 | model: small_finetuned_ckpt/model_eszyci/epoch_4 |
2023-12-20 18:34:10 INFO     	 * rank: 21 | metric: 0.025 | model: small_finetuned_ckpt/model_mzgdpa/epoch_4 |
2023-12-20 18:34:10 INFO     	 * rank: 22 | metric: 0.025 | model: small_finetuned_ckpt/model_dpyopu/epoch_6 |
2023-12-20 18:34:10 INFO     	 * rank: 23 | metric: 0.025 | model: small_finetuned_ckpt/model_yhgxcx/epoch_8 |
2023-12-20 18:34:10 INFO     	 * rank: 24 | metric: 0.024 | model: small_finetuned_ckpt/model_dpyopu/epoch_9 |
2023-12-20 18:34:10 INFO     	 * rank: 25 | metric: 0.024 | model: small_finetuned_ckpt/model_mzgdpa/epoch_7 |
2023-12-20 18:34:10 INFO     	 * rank: 26 | metric: 0.024 | model: small_finetuned_ckpt/model_dpyopu/epoch_8 |
2023-12-20 18:34:10 INFO     	 * rank: 27 | metric: 0.024 | model: small_finetuned_ckpt/model_eszyci/epoch_5 |
2023-12-20 18:34:10 INFO     	 * rank: 28 | metric: 0.024 | model: small_finetuned_ckpt/model_mzgdpa/epoch_5 |
2023-12-20 18:34:10 INFO     	 * rank: 29 | metric: 0.024 | model: small_finetuned_ckpt/model_eszyci/epoch_7 |
2023-12-20 18:34:10 INFO     	 * rank: 30 | metric: 0.024 | model: small_finetuned_ckpt/model_mzgdpa/epoch_6 |
2023-12-20 18:34:10 INFO     	 * rank: 31 | metric: 0.022 | model: small_finetuned_ckpt/model_mzgdpa/epoch_8 |
2023-12-20 18:34:10 INFO     	 * rank: 32 | metric: 0.022 | model: small_finetuned_ckpt/model_dpyopu/epoch_10 |
2023-12-20 18:34:10 INFO     	 * rank: 33 | metric: 0.022 | model: small_finetuned_ckpt/model_mzgdpa/epoch_9 |
2023-12-20 18:34:10 INFO     	 * rank: 34 | metric: 0.021 | model: small_finetuned_ckpt/model_yhgxcx/epoch_9 |
2023-12-20 18:34:10 INFO     	 * rank: 35 | metric: 0.02 | model: small_finetuned_ckpt/model_mzgdpa/epoch_10 |
2023-12-20 18:34:10 INFO     	 * rank: 36 | metric: 0.019 | model: small_finetuned_ckpt/model_yhgxcx/epoch_10 |
2023-12-20 18:34:10 INFO     	 * rank: 37 | metric: 0.019 | model: small_finetuned_ckpt/model_eszyci/epoch_8 |
2023-12-20 18:34:10 INFO     	 * rank: 38 | metric: 0.019 | model: small_finetuned_ckpt/model_eszyci/epoch_9 |
2023-12-20 18:34:10 INFO     	 * rank: 39 | metric: 0.018 | model: small_finetuned_ckpt/model_eszyci/epoch_10 |
2023-12-20 18:34:10 INFO     creating small_finetuned_ckpt/best_model
2023-12-20 18:34:10 INFO     copying small_finetuned_ckpt/model_yhgxcx/epoch_1/config.json -> small_finetuned_ckpt/best_model
2023-12-20 18:34:10 INFO     copying small_finetuned_ckpt/model_yhgxcx/epoch_1/generation_config.json -> small_finetuned_ckpt/best_model
2023-12-20 18:34:10 INFO     copying small_finetuned_ckpt/model_yhgxcx/epoch_1/pytorch_model.bin -> small_finetuned_ckpt/best_model
2023-12-20 18:34:10 INFO     copying small_finetuned_ckpt/model_yhgxcx/epoch_1/tokenizer_config.json -> small_finetuned_ckpt/best_model
2023-12-20 18:34:10 INFO     copying small_finetuned_ckpt/model_yhgxcx/epoch_1/special_tokens_map.json -> small_finetuned_ckpt/best_model
2023-12-20 18:34:10 INFO     copying small_finetuned_ckpt/model_yhgxcx/epoch_1/added_tokens.json -> small_finetuned_ckpt/best_model
2023-12-20 18:34:10 INFO     copying small_finetuned_ckpt/model_yhgxcx/epoch_1/spiece.model -> small_finetuned_ckpt/best_model
2023-12-20 18:34:10 INFO     copying small_finetuned_ckpt/model_yhgxcx/epoch_1/tokenizer.json -> small_finetuned_ckpt/best_model
2023-12-20 18:34:10 INFO     copying small_finetuned_ckpt/model_yhgxcx/epoch_1/trainer_config.json -> small_finetuned_ckpt/best_model
2023-12-20 18:34:10 INFO     creating small_finetuned_ckpt/best_model/eval
2023-12-20 18:34:10 INFO     copying small_finetuned_ckpt/model_yhgxcx/epoch_1/eval/samples.test.hyp.paragraph.questions_answers.StellarMilk_newsqa.default.txt -> small_finetuned_ckpt/best_model/eval
2023-12-20 18:34:10 INFO     copying small_finetuned_ckpt/model_yhgxcx/epoch_1/eval/samples.validation.hyp.paragraph.questions_answers.StellarMilk_newsqa.default.txt -> small_finetuned_ckpt/best_model/eval
2023-12-20 18:34:10 INFO     copying small_finetuned_ckpt/model_yhgxcx/epoch_1/eval/metric.first.answer.paragraph.questions_answers.StellarMilk_newsqa.default.json -> small_finetuned_ckpt/best_model/eval
