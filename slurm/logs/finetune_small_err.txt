2023-12-14 22:52:39 INFO     INITIALIZE GRID SEARCHER: 12 configs to try
2023-12-14 22:52:39 INFO     ## 1st RUN: Configuration 0/12 ##
2023-12-14 22:52:39 INFO     initialize model trainer
2023-12-14 22:52:39 INFO     initialize checkpoint at small_finetuned_ckpt/model_dpeblg
2023-12-14 22:52:39 INFO     hyperparameters
2023-12-14 22:52:39 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-14 22:52:39 INFO     	 * dataset_name: default
2023-12-14 22:52:39 INFO     	 * input_types: ['paragraph']
2023-12-14 22:52:39 INFO     	 * output_types: ['questions_answers']
2023-12-14 22:52:39 INFO     	 * prefix_types: ['qag']
2023-12-14 22:52:39 INFO     	 * model: lmqg/t5-small-squad-qag
2023-12-14 22:52:39 INFO     	 * max_length: 512
2023-12-14 22:52:39 INFO     	 * max_length_output: 512
2023-12-14 22:52:39 INFO     	 * epoch: 15
2023-12-14 22:52:39 INFO     	 * batch: 2
2023-12-14 22:52:39 INFO     	 * lr: 0.0001
2023-12-14 22:52:39 INFO     	 * fp16: False
2023-12-14 22:52:39 INFO     	 * random_seed: 1
2023-12-14 22:52:39 INFO     	 * gradient_accumulation_steps: 4
2023-12-14 22:52:39 INFO     	 * label_smoothing: 0.15
2023-12-14 22:52:39 INFO     initialize checkpoint with lmqg/t5-small-squad-qag
2023-12-14 22:52:41 INFO     use spaCy answer extraction model: positionrank
2023-12-14 22:52:43 INFO     Model `lmqg/t5-small-squad-qag`
2023-12-14 22:52:43 INFO     	 * Num of GPU in use: 1
2023-12-14 22:52:43 INFO     	 * Prefix: True
2023-12-14 22:52:43 INFO     	 * Language: en (ignore at the training phase)
2023-12-14 22:52:43 INFO     dataset preprocessing
/home2/g.torresgamez/.local/lib/python3.10/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=False' instead.
  warnings.warn(
2023-12-14 22:52:45 INFO     encode all the data       : 10327

  0%|          | 0/10327 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors

  0%|          | 44/10327 [00:00<00:23, 428.94it/s]
  1%|          | 87/10327 [00:00<00:24, 419.24it/s]
  1%|▏         | 134/10327 [00:00<00:23, 439.24it/s]
  2%|▏         | 187/10327 [00:00<00:21, 473.47it/s]
  2%|▏         | 240/10327 [00:00<00:20, 490.74it/s]
  3%|▎         | 292/10327 [00:00<00:20, 499.00it/s]
  3%|▎         | 342/10327 [00:00<00:20, 489.97it/s]
  4%|▍         | 392/10327 [00:00<00:22, 444.74it/s]
  4%|▍         | 441/10327 [00:00<00:21, 457.59it/s]
  5%|▍         | 495/10327 [00:01<00:20, 479.68it/s]
  5%|▌         | 548/10327 [00:01<00:19, 492.97it/s]
  6%|▌         | 598/10327 [00:01<00:20, 479.77it/s]
  6%|▋         | 647/10327 [00:01<00:20, 469.33it/s]
  7%|▋         | 695/10327 [00:01<00:21, 446.26it/s]
  7%|▋         | 750/10327 [00:01<00:20, 473.05it/s]
  8%|▊         | 807/10327 [00:01<00:19, 500.48it/s]
  8%|▊         | 862/10327 [00:01<00:18, 514.22it/s]
  9%|▉         | 914/10327 [00:01<00:18, 495.45it/s]
  9%|▉         | 968/10327 [00:02<00:18, 507.81it/s]
 10%|▉         | 1020/10327 [00:02<00:19, 480.43it/s]
 10%|█         | 1070/10327 [00:02<00:19, 484.18it/s]
 11%|█         | 1119/10327 [00:02<00:19, 473.12it/s]
 11%|█▏        | 1172/10327 [00:02<00:18, 488.43it/s]
 12%|█▏        | 1222/10327 [00:02<00:20, 439.14it/s]
 12%|█▏        | 1267/10327 [00:02<00:21, 422.99it/s]
 13%|█▎        | 1321/10327 [00:02<00:19, 452.10it/s]
 13%|█▎        | 1377/10327 [00:02<00:18, 481.02it/s]
 14%|█▍        | 1433/10327 [00:03<00:17, 502.14it/s]
 14%|█▍        | 1484/10327 [00:03<00:17, 498.79it/s]
 15%|█▍        | 1535/10327 [00:03<00:17, 497.43it/s]
 15%|█▌        | 1586/10327 [00:03<00:17, 490.49it/s]
 16%|█▌        | 1636/10327 [00:03<00:17, 490.80it/s]
 16%|█▋        | 1687/10327 [00:03<00:17, 494.78it/s]
 17%|█▋        | 1740/10327 [00:03<00:17, 504.12it/s]
 17%|█▋        | 1795/10327 [00:03<00:16, 517.01it/s]
 18%|█▊        | 1847/10327 [00:03<00:16, 517.03it/s]
 18%|█▊        | 1899/10327 [00:03<00:16, 501.15it/s]
 19%|█▉        | 1961/10327 [00:04<00:15, 533.27it/s]
 20%|█▉        | 2015/10327 [00:04<00:16, 504.99it/s]
 20%|██        | 2069/10327 [00:04<00:16, 514.82it/s]
 21%|██        | 2125/10327 [00:04<00:15, 525.15it/s]
 21%|██        | 2178/10327 [00:04<00:16, 496.02it/s]
 22%|██▏       | 2230/10327 [00:04<00:16, 498.45it/s]
 22%|██▏       | 2284/10327 [00:04<00:15, 508.74it/s]
 23%|██▎       | 2336/10327 [00:04<00:17, 466.76it/s]
 23%|██▎       | 2391/10327 [00:04<00:16, 486.51it/s]
 24%|██▎       | 2446/10327 [00:05<00:15, 504.13it/s]
 24%|██▍       | 2498/10327 [00:05<00:15, 496.87it/s]
 25%|██▍       | 2549/10327 [00:05<00:15, 496.78it/s]
 25%|██▌       | 2599/10327 [00:05<00:16, 482.09it/s]
 26%|██▌       | 2653/10327 [00:05<00:15, 497.26it/s]
 26%|██▌       | 2707/10327 [00:05<00:15, 507.65it/s]
 27%|██▋       | 2761/10327 [00:05<00:14, 516.76it/s]
 27%|██▋       | 2813/10327 [00:05<00:14, 514.85it/s]
 28%|██▊       | 2865/10327 [00:05<00:14, 506.14it/s]
 28%|██▊       | 2916/10327 [00:05<00:15, 490.68it/s]
 29%|██▊       | 2966/10327 [00:06<00:15, 489.24it/s]
 29%|██▉       | 3020/10327 [00:06<00:14, 503.58it/s]
 30%|██▉       | 3076/10327 [00:06<00:13, 519.22it/s]
 30%|███       | 3129/10327 [00:06<00:13, 521.79it/s]
 31%|███       | 3182/10327 [00:06<00:14, 490.67it/s]
 31%|███▏      | 3232/10327 [00:06<00:14, 477.56it/s]
 32%|███▏      | 3287/10327 [00:06<00:14, 497.45it/s]
 32%|███▏      | 3340/10327 [00:06<00:13, 506.00it/s]
 33%|███▎      | 3395/10327 [00:06<00:13, 518.50it/s]
 33%|███▎      | 3448/10327 [00:07<00:13, 509.41it/s]
 34%|███▍      | 3500/10327 [00:07<00:13, 493.59it/s]
 34%|███▍      | 3550/10327 [00:07<00:15, 424.91it/s]
 35%|███▍      | 3602/10327 [00:07<00:14, 449.06it/s]
 35%|███▌      | 3654/10327 [00:07<00:14, 466.74it/s]
 36%|███▌      | 3709/10327 [00:07<00:13, 488.36it/s]
 36%|███▋      | 3759/10327 [00:07<00:14, 468.18it/s]
 37%|███▋      | 3807/10327 [00:07<00:14, 453.86it/s]
 37%|███▋      | 3854/10327 [00:07<00:14, 434.36it/s]
 38%|███▊      | 3906/10327 [00:08<00:14, 456.26it/s]
 38%|███▊      | 3959/10327 [00:08<00:13, 475.54it/s]
 39%|███▉      | 4010/10327 [00:08<00:13, 483.61it/s]
 39%|███▉      | 4059/10327 [00:08<00:12, 483.92it/s]
 40%|███▉      | 4108/10327 [00:08<00:13, 474.58it/s]
 40%|████      | 4156/10327 [00:08<00:13, 470.78it/s]
 41%|████      | 4204/10327 [00:08<00:13, 460.06it/s]
 41%|████      | 4257/10327 [00:08<00:12, 478.58it/s]
 42%|████▏     | 4309/10327 [00:08<00:12, 488.88it/s]
 42%|████▏     | 4359/10327 [00:08<00:12, 481.42it/s]
 43%|████▎     | 4408/10327 [00:09<00:12, 472.55it/s]
 43%|████▎     | 4469/10327 [00:09<00:11, 511.24it/s]
 44%|████▍     | 4526/10327 [00:09<00:11, 526.12it/s]
 44%|████▍     | 4583/10327 [00:09<00:10, 535.82it/s]
 45%|████▍     | 4637/10327 [00:09<00:10, 528.68it/s]
 45%|████▌     | 4692/10327 [00:09<00:10, 533.68it/s]
 46%|████▌     | 4750/10327 [00:09<00:10, 544.29it/s]
 47%|████▋     | 4811/10327 [00:09<00:09, 562.56it/s]
 47%|████▋     | 4871/10327 [00:09<00:09, 571.75it/s]
 48%|████▊     | 4929/10327 [00:10<00:10, 533.62it/s]
 48%|████▊     | 4983/10327 [00:10<00:10, 531.16it/s]
 49%|████▉     | 5038/10327 [00:10<00:09, 534.42it/s]
 49%|████▉     | 5093/10327 [00:10<00:09, 537.42it/s]
 50%|████▉     | 5147/10327 [00:10<00:09, 531.70it/s]
 50%|█████     | 5203/10327 [00:10<00:09, 539.69it/s]
 51%|█████     | 5258/10327 [00:10<00:10, 498.50it/s]
 51%|█████▏    | 5314/10327 [00:10<00:09, 514.49it/s]
 52%|█████▏    | 5371/10327 [00:10<00:09, 528.54it/s]
 53%|█████▎    | 5426/10327 [00:10<00:09, 533.68it/s]
 53%|█████▎    | 5480/10327 [00:11<00:09, 526.52it/s]
 54%|█████▎    | 5538/10327 [00:11<00:08, 541.86it/s]
 54%|█████▍    | 5593/10327 [00:11<00:09, 520.37it/s]
 55%|█████▍    | 5648/10327 [00:11<00:08, 527.82it/s]
 55%|█████▌    | 5704/10327 [00:11<00:08, 535.95it/s]
 56%|█████▌    | 5758/10327 [00:11<00:08, 520.54it/s]
 56%|█████▋    | 5811/10327 [00:11<00:08, 513.36it/s]
 57%|█████▋    | 5863/10327 [00:11<00:08, 514.91it/s]
 57%|█████▋    | 5915/10327 [00:11<00:09, 486.15it/s]
 58%|█████▊    | 5968/10327 [00:12<00:08, 497.55it/s]
 58%|█████▊    | 6022/10327 [00:12<00:08, 508.04it/s]
 59%|█████▉    | 6077/10327 [00:12<00:08, 518.36it/s]
 59%|█████▉    | 6130/10327 [00:12<00:08, 490.18it/s]
 60%|█████▉    | 6181/10327 [00:12<00:08, 494.26it/s]
 60%|██████    | 6231/10327 [00:12<00:08, 474.83it/s]
 61%|██████    | 6288/10327 [00:12<00:08, 499.83it/s]
 61%|██████▏   | 6341/10327 [00:12<00:07, 507.75it/s]
 62%|██████▏   | 6393/10327 [00:12<00:08, 483.51it/s]
 62%|██████▏   | 6442/10327 [00:12<00:08, 477.94it/s]
 63%|██████▎   | 6491/10327 [00:13<00:08, 471.26it/s]
 63%|██████▎   | 6539/10327 [00:13<00:08, 437.05it/s]
 64%|██████▍   | 6592/10327 [00:13<00:08, 460.19it/s]
 64%|██████▍   | 6648/10327 [00:13<00:07, 486.98it/s]
 65%|██████▍   | 6701/10327 [00:13<00:07, 498.58it/s]
 65%|██████▌   | 6752/10327 [00:13<00:07, 475.61it/s]
 66%|██████▌   | 6801/10327 [00:13<00:07, 469.55it/s]
 66%|██████▋   | 6849/10327 [00:13<00:07, 448.03it/s]
 67%|██████▋   | 6900/10327 [00:13<00:07, 464.95it/s]
 67%|██████▋   | 6953/10327 [00:14<00:06, 482.60it/s]
 68%|██████▊   | 7007/10327 [00:14<00:06, 498.79it/s]
 68%|██████▊   | 7058/10327 [00:14<00:06, 493.00it/s]
 69%|██████▉   | 7108/10327 [00:14<00:06, 489.56it/s]
 69%|██████▉   | 7158/10327 [00:14<00:06, 479.38it/s]
 70%|██████▉   | 7207/10327 [00:14<00:06, 481.71it/s]
 70%|███████   | 7260/10327 [00:14<00:06, 491.08it/s]
 71%|███████   | 7314/10327 [00:14<00:05, 505.19it/s]
 71%|███████▏  | 7372/10327 [00:14<00:05, 518.14it/s]
 72%|███████▏  | 7426/10327 [00:15<00:05, 524.15it/s]
 73%|███████▎  | 7489/10327 [00:15<00:05, 554.77it/s]
 73%|███████▎  | 7545/10327 [00:15<00:05, 536.59it/s]
 74%|███████▎  | 7599/10327 [00:15<00:05, 535.71it/s]
 74%|███████▍  | 7656/10327 [00:15<00:04, 542.60it/s]
 75%|███████▍  | 7711/10327 [00:15<00:04, 544.53it/s]
 75%|███████▌  | 7766/10327 [00:15<00:04, 529.07it/s]
 76%|███████▌  | 7820/10327 [00:15<00:04, 502.85it/s]
 76%|███████▌  | 7871/10327 [00:15<00:04, 498.67it/s]
 77%|███████▋  | 7925/10327 [00:15<00:04, 509.19it/s]
 77%|███████▋  | 7978/10327 [00:16<00:04, 513.71it/s]
 78%|███████▊  | 8031/10327 [00:16<00:04, 517.73it/s]
 78%|███████▊  | 8088/10327 [00:16<00:04, 532.33it/s]
 79%|███████▉  | 8142/10327 [00:16<00:04, 525.62it/s]
 79%|███████▉  | 8195/10327 [00:16<00:04, 491.71it/s]
 80%|███████▉  | 8245/10327 [00:16<00:04, 488.43it/s]
 80%|████████  | 8298/10327 [00:16<00:04, 499.04it/s]
 81%|████████  | 8350/10327 [00:16<00:03, 503.34it/s]
 81%|████████▏ | 8401/10327 [00:16<00:03, 486.91it/s]
 82%|████████▏ | 8450/10327 [00:17<00:03, 487.14it/s]
 82%|████████▏ | 8499/10327 [00:17<00:03, 485.85it/s]
 83%|████████▎ | 8548/10327 [00:17<00:03, 480.65it/s]
 83%|████████▎ | 8603/10327 [00:17<00:03, 499.51it/s]
 84%|████████▍ | 8659/10327 [00:17<00:03, 515.44it/s]
 84%|████████▍ | 8714/10327 [00:17<00:03, 522.55it/s]
 85%|████████▍ | 8769/10327 [00:17<00:02, 529.45it/s]
 85%|████████▌ | 8822/10327 [00:17<00:02, 527.42it/s]
 86%|████████▌ | 8875/10327 [00:17<00:02, 508.96it/s]
 86%|████████▋ | 8931/10327 [00:17<00:02, 522.25it/s]
 87%|████████▋ | 8989/10327 [00:18<00:02, 538.76it/s]
 88%|████████▊ | 9044/10327 [00:18<00:02, 538.75it/s]
 88%|████████▊ | 9099/10327 [00:18<00:02, 541.28it/s]
 89%|████████▊ | 9154/10327 [00:18<00:02, 532.57it/s]
 89%|████████▉ | 9208/10327 [00:18<00:02, 489.45it/s]
 90%|████████▉ | 9259/10327 [00:18<00:02, 494.73it/s]
 90%|█████████ | 9315/10327 [00:18<00:01, 512.22it/s]
 91%|█████████ | 9368/10327 [00:18<00:01, 514.17it/s]
 91%|█████████ | 9420/10327 [00:18<00:01, 503.82it/s]
 92%|█████████▏| 9471/10327 [00:19<00:01, 497.96it/s]
 92%|█████████▏| 9521/10327 [00:19<00:01, 436.39it/s]
 93%|█████████▎| 9567/10327 [00:19<00:01, 439.76it/s]
 93%|█████████▎| 9613/10327 [00:19<00:01, 443.56it/s]
 94%|█████████▎| 9661/10327 [00:19<00:01, 452.90it/s]
 94%|█████████▍| 9716/10327 [00:19<00:01, 479.39it/s]
 95%|█████████▍| 9765/10327 [00:19<00:01, 462.84it/s]
 95%|█████████▌| 9812/10327 [00:19<00:01, 456.65it/s]
 95%|█████████▌| 9858/10327 [00:19<00:01, 452.84it/s]
 96%|█████████▌| 9904/10327 [00:20<00:01, 414.20it/s]
 96%|█████████▋| 9959/10327 [00:20<00:00, 451.07it/s]
 97%|█████████▋| 10013/10327 [00:20<00:00, 473.77it/s]
 97%|█████████▋| 10065/10327 [00:20<00:00, 486.83it/s]
 98%|█████████▊| 10119/10327 [00:20<00:00, 501.74it/s]
 98%|█████████▊| 10170/10327 [00:20<00:00, 490.49it/s]
 99%|█████████▉| 10233/10327 [00:20<00:00, 528.65it/s]
100%|█████████▉| 10287/10327 [00:20<00:00, 530.58it/s]
100%|██████████| 10327/10327 [00:20<00:00, 496.31it/s]
2023-12-14 22:53:07 INFO     after remove the overflow : 2487
2023-12-14 22:53:07 INFO     preprocessed feature is saved at /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-14 22:53:07 INFO     start model training
2023-12-14 22:53:23 INFO     	 * (global step 50: loss: 0.3651347681879997, lr: 0.0001
2023-12-14 22:53:38 INFO     	 * (global step 100: loss: 0.4017643630504608, lr: 0.0001
2023-12-14 22:53:53 INFO     	 * (global step 150: loss: 0.3726911321282387, lr: 0.0001
2023-12-14 22:54:08 INFO     	 * (global step 200: loss: 0.3527664393186569, lr: 0.0001
2023-12-14 22:54:24 INFO     	 * (global step 250: loss: 0.4465588852763176, lr: 0.0001
2023-12-14 22:54:39 INFO     	 * (global step 300: loss: 0.4871162846684456, lr: 0.0001
2023-12-14 22:54:42 INFO     [epoch 0/15] average loss: 0.405, lr: 0.0001
2023-12-14 22:54:42 INFO     saving model related files
2023-12-14 22:54:42 INFO     saving model
2023-12-14 22:54:43 INFO     saving tokenizer
2023-12-14 22:54:43 INFO     saving optimizer
2023-12-14 22:54:44 INFO     remove old optimizer files
2023-12-14 22:54:56 INFO     	 * (global step 350: loss: 0.3577493913471699, lr: 0.0001
2023-12-14 22:55:12 INFO     	 * (global step 400: loss: 0.349889300763607, lr: 0.0001
2023-12-14 22:55:28 INFO     	 * (global step 450: loss: 0.4609329290688038, lr: 0.0001
2023-12-14 22:55:43 INFO     	 * (global step 500: loss: 0.4614955335855484, lr: 0.0001
2023-12-14 22:55:59 INFO     	 * (global step 550: loss: 0.4844323992729187, lr: 0.0001
2023-12-14 22:56:15 INFO     	 * (global step 600: loss: 0.4174722544848919, lr: 0.0001
2023-12-14 22:56:21 INFO     [epoch 1/15] average loss: 0.377, lr: 0.0001
2023-12-14 22:56:21 INFO     saving model related files
2023-12-14 22:56:21 INFO     saving model
2023-12-14 22:56:22 INFO     saving tokenizer
2023-12-14 22:56:22 INFO     saving optimizer
2023-12-14 22:56:23 INFO     remove old optimizer files
2023-12-14 22:56:32 INFO     	 * (global step 650: loss: 0.34014976769685745, lr: 0.0001
2023-12-14 22:56:48 INFO     	 * (global step 700: loss: 0.3076176233589649, lr: 0.0001
2023-12-14 22:57:04 INFO     	 * (global step 750: loss: 0.3241450637578964, lr: 0.0001
2023-12-14 22:57:19 INFO     	 * (global step 800: loss: 0.3412947431206703, lr: 0.0001
2023-12-14 22:57:35 INFO     	 * (global step 850: loss: 0.48581790924072266, lr: 0.0001
2023-12-14 22:57:51 INFO     	 * (global step 900: loss: 0.48480528965592384, lr: 0.0001
2023-12-14 22:58:00 INFO     [epoch 2/15] average loss: 0.361, lr: 0.0001
2023-12-14 22:58:00 INFO     saving model related files
2023-12-14 22:58:00 INFO     saving model
2023-12-14 22:58:01 INFO     saving tokenizer
2023-12-14 22:58:01 INFO     saving optimizer
2023-12-14 22:58:02 INFO     remove old optimizer files
2023-12-14 22:58:08 INFO     	 * (global step 950: loss: 0.4387468546628952, lr: 0.0001
2023-12-14 22:58:24 INFO     	 * (global step 1000: loss: 0.33793532848358154, lr: 0.0001
2023-12-14 22:58:40 INFO     	 * (global step 1050: loss: 0.3500729501247406, lr: 0.0001
2023-12-14 22:58:55 INFO     	 * (global step 1100: loss: 0.3008650690317154, lr: 0.0001
2023-12-14 22:59:11 INFO     	 * (global step 1150: loss: 0.2612890414893627, lr: 0.0001
2023-12-14 22:59:26 INFO     	 * (global step 1200: loss: 0.3390027806162834, lr: 0.0001
2023-12-14 22:59:39 INFO     [epoch 3/15] average loss: 0.351, lr: 0.0001
2023-12-14 22:59:39 INFO     saving model related files
2023-12-14 22:59:39 INFO     saving model
2023-12-14 22:59:40 INFO     saving tokenizer
2023-12-14 22:59:40 INFO     saving optimizer
2023-12-14 22:59:41 INFO     remove old optimizer files
2023-12-14 22:59:44 INFO     	 * (global step 1250: loss: 0.3592468313872814, lr: 0.0001
2023-12-14 23:00:00 INFO     	 * (global step 1300: loss: 0.242879930883646, lr: 0.0001
2023-12-14 23:00:15 INFO     	 * (global step 1350: loss: 0.3368282578885555, lr: 0.0001
2023-12-14 23:00:31 INFO     	 * (global step 1400: loss: 0.26424307003617287, lr: 0.0001
2023-12-14 23:00:47 INFO     	 * (global step 1450: loss: 0.30083145946264267, lr: 0.0001
2023-12-14 23:01:03 INFO     	 * (global step 1500: loss: 0.43473247438669205, lr: 0.0001
2023-12-14 23:01:18 INFO     	 * (global step 1550: loss: 0.3010720945894718, lr: 0.0001
2023-12-14 23:01:19 INFO     [epoch 4/15] average loss: 0.341, lr: 0.0001
2023-12-14 23:01:19 INFO     saving model related files
2023-12-14 23:01:19 INFO     saving model
2023-12-14 23:01:19 INFO     saving tokenizer
2023-12-14 23:01:19 INFO     saving optimizer
2023-12-14 23:01:20 INFO     remove old optimizer files
2023-12-14 23:01:36 INFO     	 * (global step 1600: loss: 0.2918687015771866, lr: 0.0001
2023-12-14 23:01:52 INFO     	 * (global step 1650: loss: 0.3386906050145626, lr: 0.0001
2023-12-14 23:02:07 INFO     	 * (global step 1700: loss: 0.3547363057732582, lr: 0.0001
2023-12-14 23:02:23 INFO     	 * (global step 1750: loss: 0.24106773361563683, lr: 0.0001
2023-12-14 23:02:38 INFO     	 * (global step 1800: loss: 0.4485040493309498, lr: 0.0001
2023-12-14 23:02:54 INFO     	 * (global step 1850: loss: 0.2980395685881376, lr: 0.0001
2023-12-14 23:02:58 INFO     [epoch 5/15] average loss: 0.334, lr: 0.0001
2023-12-14 23:02:58 INFO     saving model related files
2023-12-14 23:02:58 INFO     saving model
2023-12-14 23:02:58 INFO     saving tokenizer
2023-12-14 23:02:58 INFO     saving optimizer
2023-12-14 23:02:59 INFO     remove old optimizer files
2023-12-14 23:03:12 INFO     	 * (global step 1900: loss: 0.3345091938972473, lr: 0.0001
2023-12-14 23:03:27 INFO     	 * (global step 1950: loss: 0.29924270883202553, lr: 0.0001
2023-12-14 23:03:43 INFO     	 * (global step 2000: loss: 0.3813333511352539, lr: 0.0001
2023-12-14 23:03:59 INFO     	 * (global step 2050: loss: 0.23159443587064743, lr: 0.0001
2023-12-14 23:04:14 INFO     	 * (global step 2100: loss: 0.31289316713809967, lr: 0.0001
2023-12-14 23:04:30 INFO     	 * (global step 2150: loss: 0.33575133979320526, lr: 0.0001
2023-12-14 23:04:37 INFO     [epoch 6/15] average loss: 0.325, lr: 0.0001
2023-12-14 23:04:37 INFO     saving model related files
2023-12-14 23:04:37 INFO     saving model
2023-12-14 23:04:37 INFO     saving tokenizer
2023-12-14 23:04:37 INFO     saving optimizer
2023-12-14 23:04:38 INFO     remove old optimizer files
2023-12-14 23:04:48 INFO     	 * (global step 2200: loss: 0.4448621794581413, lr: 0.0001
2023-12-14 23:05:03 INFO     	 * (global step 2250: loss: 0.46080779284238815, lr: 0.0001
2023-12-14 23:05:19 INFO     	 * (global step 2300: loss: 0.2852686867117882, lr: 0.0001
2023-12-14 23:05:35 INFO     	 * (global step 2350: loss: 0.34238646924495697, lr: 0.0001
2023-12-14 23:05:50 INFO     	 * (global step 2400: loss: 0.42634283378720284, lr: 0.0001
2023-12-14 23:06:06 INFO     	 * (global step 2450: loss: 0.2840861137956381, lr: 0.0001
2023-12-14 23:06:16 INFO     [epoch 7/15] average loss: 0.318, lr: 0.0001
2023-12-14 23:06:16 INFO     saving model related files
2023-12-14 23:06:16 INFO     saving model
2023-12-14 23:06:16 INFO     saving tokenizer
2023-12-14 23:06:16 INFO     saving optimizer
2023-12-14 23:06:17 INFO     remove old optimizer files
2023-12-14 23:06:24 INFO     	 * (global step 2500: loss: 0.4478090777993202, lr: 0.0001
2023-12-14 23:06:39 INFO     	 * (global step 2550: loss: 0.28139806166291237, lr: 0.0001
2023-12-14 23:06:55 INFO     	 * (global step 2600: loss: 0.39776234328746796, lr: 0.0001
2023-12-14 23:07:11 INFO     	 * (global step 2650: loss: 0.315175898373127, lr: 0.0001
2023-12-14 23:07:26 INFO     	 * (global step 2700: loss: 0.33116644993424416, lr: 0.0001
2023-12-14 23:07:42 INFO     	 * (global step 2750: loss: 0.3989001736044884, lr: 0.0001
2023-12-14 23:07:55 INFO     [epoch 8/15] average loss: 0.312, lr: 0.0001
2023-12-14 23:07:55 INFO     saving model related files
2023-12-14 23:07:55 INFO     saving model
2023-12-14 23:07:56 INFO     saving tokenizer
2023-12-14 23:07:56 INFO     saving optimizer
2023-12-14 23:07:57 INFO     remove old optimizer files
2023-12-14 23:08:00 INFO     	 * (global step 2800: loss: 0.40808267146348953, lr: 0.0001
2023-12-14 23:08:16 INFO     	 * (global step 2850: loss: 0.3587266281247139, lr: 0.0001
2023-12-14 23:08:31 INFO     	 * (global step 2900: loss: 0.3176904767751694, lr: 0.0001
2023-12-14 23:08:47 INFO     	 * (global step 2950: loss: 0.33335061371326447, lr: 0.0001
2023-12-14 23:09:03 INFO     	 * (global step 3000: loss: 0.21638232842087746, lr: 0.0001
2023-12-14 23:09:18 INFO     	 * (global step 3050: loss: 0.3437308631837368, lr: 0.0001
2023-12-14 23:09:34 INFO     	 * (global step 3100: loss: 0.31707265973091125, lr: 0.0001
2023-12-14 23:09:34 INFO     [epoch 9/15] average loss: 0.305, lr: 0.0001
2023-12-14 23:09:34 INFO     saving model related files
2023-12-14 23:09:34 INFO     saving model
2023-12-14 23:09:35 INFO     saving tokenizer
2023-12-14 23:09:35 INFO     saving optimizer
2023-12-14 23:09:36 INFO     remove old optimizer files
2023-12-14 23:09:36 INFO     complete training: model ckpt was saved at small_finetuned_ckpt/model_dpeblg
2023-12-14 23:09:36 INFO     ## 1st RUN: Configuration 1/12 ##
2023-12-14 23:09:36 INFO     initialize model trainer
2023-12-14 23:09:36 INFO     initialize checkpoint at small_finetuned_ckpt/model_eszyci
2023-12-14 23:09:36 INFO     hyperparameters
2023-12-14 23:09:36 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-14 23:09:36 INFO     	 * dataset_name: default
2023-12-14 23:09:36 INFO     	 * input_types: ['paragraph']
2023-12-14 23:09:36 INFO     	 * output_types: ['questions_answers']
2023-12-14 23:09:36 INFO     	 * prefix_types: ['qag']
2023-12-14 23:09:36 INFO     	 * model: lmqg/t5-small-squad-qag
2023-12-14 23:09:36 INFO     	 * max_length: 512
2023-12-14 23:09:36 INFO     	 * max_length_output: 512
2023-12-14 23:09:36 INFO     	 * epoch: 15
2023-12-14 23:09:36 INFO     	 * batch: 2
2023-12-14 23:09:36 INFO     	 * lr: 0.0001
2023-12-14 23:09:36 INFO     	 * fp16: False
2023-12-14 23:09:36 INFO     	 * random_seed: 1
2023-12-14 23:09:36 INFO     	 * gradient_accumulation_steps: 2
2023-12-14 23:09:36 INFO     	 * label_smoothing: 0.15
2023-12-14 23:09:36 INFO     initialize checkpoint with lmqg/t5-small-squad-qag
2023-12-14 23:09:38 INFO     use spaCy answer extraction model: positionrank
2023-12-14 23:09:38 INFO     Model `lmqg/t5-small-squad-qag`
2023-12-14 23:09:38 INFO     	 * Num of GPU in use: 1
2023-12-14 23:09:38 INFO     	 * Prefix: True
2023-12-14 23:09:38 INFO     	 * Language: en (ignore at the training phase)
2023-12-14 23:09:38 INFO     dataset preprocessing
2023-12-14 23:09:40 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-14 23:09:41 INFO     start model training
2023-12-14 23:09:49 INFO     	 * (global step 50: loss: 0.4020136296749115, lr: 0.0001
2023-12-14 23:09:57 INFO     	 * (global step 100: loss: 0.2806154638528824, lr: 0.0001
2023-12-14 23:10:05 INFO     	 * (global step 150: loss: 0.3587702661752701, lr: 0.0001
2023-12-14 23:10:13 INFO     	 * (global step 200: loss: 0.3935755640268326, lr: 0.0001
2023-12-14 23:10:21 INFO     	 * (global step 250: loss: 0.42130185663700104, lr: 0.0001
2023-12-14 23:10:29 INFO     	 * (global step 300: loss: 0.36001943051815033, lr: 0.0001
2023-12-14 23:10:38 INFO     	 * (global step 350: loss: 0.5767006874084473, lr: 0.0001
2023-12-14 23:10:46 INFO     	 * (global step 400: loss: 0.3435817211866379, lr: 0.0001
2023-12-14 23:10:54 INFO     	 * (global step 450: loss: 0.2922074869275093, lr: 0.0001
2023-12-14 23:11:02 INFO     	 * (global step 500: loss: 0.4161016643047333, lr: 0.0001
2023-12-14 23:11:10 INFO     	 * (global step 550: loss: 0.29670026898384094, lr: 0.0001
2023-12-14 23:11:18 INFO     	 * (global step 600: loss: 0.6162222474813461, lr: 0.0001
2023-12-14 23:11:22 INFO     [epoch 0/15] average loss: 0.401, lr: 0.0001
2023-12-14 23:11:22 INFO     saving model related files
2023-12-14 23:11:22 INFO     saving model
2023-12-14 23:11:22 INFO     saving tokenizer
2023-12-14 23:11:22 INFO     saving optimizer
2023-12-14 23:11:23 INFO     remove old optimizer files
2023-12-14 23:11:28 INFO     	 * (global step 650: loss: 0.434000626206398, lr: 0.0001
2023-12-14 23:11:36 INFO     	 * (global step 700: loss: 0.40486283600330353, lr: 0.0001
2023-12-14 23:11:44 INFO     	 * (global step 750: loss: 0.3170710653066635, lr: 0.0001
2023-12-14 23:11:52 INFO     	 * (global step 800: loss: 0.37305374443531036, lr: 0.0001
2023-12-14 23:12:01 INFO     	 * (global step 850: loss: 0.6294218003749847, lr: 0.0001
2023-12-14 23:12:09 INFO     	 * (global step 900: loss: 0.4763740822672844, lr: 0.0001
2023-12-14 23:12:17 INFO     	 * (global step 950: loss: 0.3367339074611664, lr: 0.0001
2023-12-14 23:12:25 INFO     	 * (global step 1000: loss: 0.5876975953578949, lr: 0.0001
2023-12-14 23:12:33 INFO     	 * (global step 1050: loss: 0.33479276299476624, lr: 0.0001
2023-12-14 23:12:41 INFO     	 * (global step 1100: loss: 0.4556150436401367, lr: 0.0001
2023-12-14 23:12:49 INFO     	 * (global step 1150: loss: 0.233285591006279, lr: 0.0001
2023-12-14 23:12:57 INFO     	 * (global step 1200: loss: 0.5725034177303314, lr: 0.0001
2023-12-14 23:13:04 INFO     [epoch 1/15] average loss: 0.371, lr: 0.0001
2023-12-14 23:13:04 INFO     saving model related files
2023-12-14 23:13:04 INFO     saving model
2023-12-14 23:13:05 INFO     saving tokenizer
2023-12-14 23:13:05 INFO     saving optimizer
2023-12-14 23:13:06 INFO     remove old optimizer files
2023-12-14 23:13:07 INFO     	 * (global step 1250: loss: 0.44525837898254395, lr: 0.0001
2023-12-14 23:13:15 INFO     	 * (global step 1300: loss: 0.255459189414978, lr: 0.0001
2023-12-14 23:13:23 INFO     	 * (global step 1350: loss: 0.171016126871109, lr: 0.0001
2023-12-14 23:13:31 INFO     	 * (global step 1400: loss: 0.44368670880794525, lr: 0.0001
2023-12-14 23:13:39 INFO     	 * (global step 1450: loss: 0.3812064677476883, lr: 0.0001
2023-12-14 23:13:48 INFO     	 * (global step 1500: loss: 0.23327872157096863, lr: 0.0001
2023-12-14 23:13:56 INFO     	 * (global step 1550: loss: 0.29209259897470474, lr: 0.0001
2023-12-14 23:14:04 INFO     	 * (global step 1600: loss: 0.3087814673781395, lr: 0.0001
2023-12-14 23:14:12 INFO     	 * (global step 1650: loss: 0.31502565741539, lr: 0.0001
2023-12-14 23:14:20 INFO     	 * (global step 1700: loss: 0.3139690011739731, lr: 0.0001
2023-12-14 23:14:28 INFO     	 * (global step 1750: loss: 0.27953629195690155, lr: 0.0001
2023-12-14 23:14:36 INFO     	 * (global step 1800: loss: 0.36808694899082184, lr: 0.0001
2023-12-14 23:14:44 INFO     	 * (global step 1850: loss: 0.2921146899461746, lr: 0.0001
2023-12-14 23:14:47 INFO     [epoch 2/15] average loss: 0.354, lr: 0.0001
2023-12-14 23:14:47 INFO     saving model related files
2023-12-14 23:14:47 INFO     saving model
2023-12-14 23:14:47 INFO     saving tokenizer
2023-12-14 23:14:47 INFO     saving optimizer
2023-12-14 23:14:48 INFO     remove old optimizer files
2023-12-14 23:14:54 INFO     	 * (global step 1900: loss: 0.2202722504734993, lr: 0.0001
2023-12-14 23:15:02 INFO     	 * (global step 1950: loss: 0.287660576403141, lr: 0.0001
2023-12-14 23:15:10 INFO     	 * (global step 2000: loss: 0.39493435621261597, lr: 0.0001
2023-12-14 23:15:19 INFO     	 * (global step 2050: loss: 0.43821926414966583, lr: 0.0001
2023-12-14 23:15:27 INFO     	 * (global step 2100: loss: 0.3237673193216324, lr: 0.0001
2023-12-14 23:15:35 INFO     	 * (global step 2150: loss: 0.30167221277952194, lr: 0.0001
2023-12-14 23:15:43 INFO     	 * (global step 2200: loss: 0.39904356002807617, lr: 0.0001
2023-12-14 23:15:51 INFO     	 * (global step 2250: loss: 0.355005145072937, lr: 0.0001
2023-12-14 23:15:59 INFO     	 * (global step 2300: loss: 0.36594629287719727, lr: 0.0001
2023-12-14 23:16:07 INFO     	 * (global step 2350: loss: 0.33855120092630386, lr: 0.0001
2023-12-14 23:16:16 INFO     	 * (global step 2400: loss: 0.4426475465297699, lr: 0.0001
2023-12-14 23:16:24 INFO     	 * (global step 2450: loss: 0.29116253554821014, lr: 0.0001
2023-12-14 23:16:29 INFO     [epoch 3/15] average loss: 0.341, lr: 0.0001
2023-12-14 23:16:29 INFO     saving model related files
2023-12-14 23:16:29 INFO     saving model
2023-12-14 23:16:30 INFO     saving tokenizer
2023-12-14 23:16:30 INFO     saving optimizer
2023-12-14 23:16:31 INFO     remove old optimizer files
2023-12-14 23:16:33 INFO     	 * (global step 2500: loss: 0.2751796320080757, lr: 0.0001
2023-12-14 23:16:42 INFO     	 * (global step 2550: loss: 0.2529292553663254, lr: 0.0001
2023-12-14 23:16:50 INFO     	 * (global step 2600: loss: 0.48326534032821655, lr: 0.0001
2023-12-14 23:16:58 INFO     	 * (global step 2650: loss: 0.39077845215797424, lr: 0.0001
2023-12-14 23:17:06 INFO     	 * (global step 2700: loss: 0.26996953785419464, lr: 0.0001
2023-12-14 23:17:14 INFO     	 * (global step 2750: loss: 0.3216094672679901, lr: 0.0001
2023-12-14 23:17:22 INFO     	 * (global step 2800: loss: 0.5880442559719086, lr: 0.0001
2023-12-14 23:17:30 INFO     	 * (global step 2850: loss: 0.36046670377254486, lr: 0.0001
2023-12-14 23:17:39 INFO     	 * (global step 2900: loss: 0.21478862687945366, lr: 0.0001
2023-12-14 23:17:47 INFO     	 * (global step 2950: loss: 0.2990403175354004, lr: 0.0001
2023-12-14 23:17:55 INFO     	 * (global step 3000: loss: 0.43146635591983795, lr: 0.0001
2023-12-14 23:18:03 INFO     	 * (global step 3050: loss: 0.2740752696990967, lr: 0.0001
2023-12-14 23:18:11 INFO     	 * (global step 3100: loss: 0.2792280316352844, lr: 0.0001
2023-12-14 23:18:12 INFO     [epoch 4/15] average loss: 0.33, lr: 0.0001
2023-12-14 23:18:12 INFO     saving model related files
2023-12-14 23:18:12 INFO     saving model
2023-12-14 23:18:12 INFO     saving tokenizer
2023-12-14 23:18:12 INFO     saving optimizer
2023-12-14 23:18:13 INFO     remove old optimizer files
2023-12-14 23:18:21 INFO     	 * (global step 3150: loss: 0.34657278656959534, lr: 0.0001
2023-12-14 23:18:29 INFO     	 * (global step 3200: loss: 0.39709968864917755, lr: 0.0001
2023-12-14 23:18:37 INFO     	 * (global step 3250: loss: 0.38360534608364105, lr: 0.0001
2023-12-14 23:18:45 INFO     	 * (global step 3300: loss: 0.3479503244161606, lr: 0.0001
2023-12-14 23:18:53 INFO     	 * (global step 3350: loss: 0.3407905101776123, lr: 0.0001
2023-12-14 23:19:01 INFO     	 * (global step 3400: loss: 0.3220079690217972, lr: 0.0001
2023-12-14 23:19:09 INFO     	 * (global step 3450: loss: 0.30339883267879486, lr: 0.0001
2023-12-14 23:19:18 INFO     	 * (global step 3500: loss: 0.39452098309993744, lr: 0.0001
2023-12-14 23:19:26 INFO     	 * (global step 3550: loss: 0.29361675679683685, lr: 0.0001
2023-12-14 23:19:34 INFO     	 * (global step 3600: loss: 0.34917663037776947, lr: 0.0001
2023-12-14 23:19:42 INFO     	 * (global step 3650: loss: 0.5033886060118675, lr: 0.0001
2023-12-14 23:19:50 INFO     	 * (global step 3700: loss: 0.45834794640541077, lr: 0.0001
2023-12-14 23:19:54 INFO     [epoch 5/15] average loss: 0.321, lr: 0.0001
2023-12-14 23:19:54 INFO     saving model related files
2023-12-14 23:19:54 INFO     saving model
2023-12-14 23:19:56 INFO     saving tokenizer
2023-12-14 23:19:56 INFO     saving optimizer
2023-12-14 23:19:59 INFO     remove old optimizer files
2023-12-14 23:20:03 INFO     	 * (global step 3750: loss: 0.28477975726127625, lr: 0.0001
2023-12-14 23:20:11 INFO     	 * (global step 3800: loss: 0.38383500277996063, lr: 0.0001
2023-12-14 23:20:19 INFO     	 * (global step 3850: loss: 0.31423529982566833, lr: 0.0001
2023-12-14 23:20:27 INFO     	 * (global step 3900: loss: 0.2906855344772339, lr: 0.0001
2023-12-14 23:20:35 INFO     	 * (global step 3950: loss: 0.4681011587381363, lr: 0.0001
2023-12-14 23:20:43 INFO     	 * (global step 4000: loss: 0.35805103182792664, lr: 0.0001
2023-12-14 23:20:51 INFO     	 * (global step 4050: loss: 0.2556639164686203, lr: 0.0001
2023-12-14 23:20:59 INFO     	 * (global step 4100: loss: 0.3240441828966141, lr: 0.0001
2023-12-14 23:21:08 INFO     	 * (global step 4150: loss: 0.22751915454864502, lr: 0.0001
2023-12-14 23:21:16 INFO     	 * (global step 4200: loss: 0.4122035801410675, lr: 0.0001
2023-12-14 23:21:24 INFO     	 * (global step 4250: loss: 0.2710251435637474, lr: 0.0001
2023-12-14 23:21:32 INFO     	 * (global step 4300: loss: 0.3743145763874054, lr: 0.0001
2023-12-14 23:21:40 INFO     [epoch 6/15] average loss: 0.311, lr: 0.0001
2023-12-14 23:21:40 INFO     saving model related files
2023-12-14 23:21:40 INFO     saving model
2023-12-14 23:21:40 INFO     saving tokenizer
2023-12-14 23:21:40 INFO     saving optimizer
2023-12-14 23:21:41 INFO     remove old optimizer files
2023-12-14 23:21:42 INFO     	 * (global step 4350: loss: 0.38796720653772354, lr: 0.0001
2023-12-14 23:21:50 INFO     	 * (global step 4400: loss: 0.2361869066953659, lr: 0.0001
2023-12-14 23:21:58 INFO     	 * (global step 4450: loss: 0.3804842084646225, lr: 0.0001
2023-12-14 23:22:06 INFO     	 * (global step 4500: loss: 0.28088369965553284, lr: 0.0001
2023-12-14 23:22:14 INFO     	 * (global step 4550: loss: 0.30669859051704407, lr: 0.0001
2023-12-14 23:22:22 INFO     	 * (global step 4600: loss: 0.31091390550136566, lr: 0.0001
2023-12-14 23:22:30 INFO     	 * (global step 4650: loss: 0.42142948508262634, lr: 0.0001
2023-12-14 23:22:39 INFO     	 * (global step 4700: loss: 0.259966105222702, lr: 0.0001
2023-12-14 23:22:47 INFO     	 * (global step 4750: loss: 0.2689439579844475, lr: 0.0001
2023-12-14 23:22:55 INFO     	 * (global step 4800: loss: 0.2789633050560951, lr: 0.0001
2023-12-14 23:23:03 INFO     	 * (global step 4850: loss: 0.30454112589359283, lr: 0.0001
2023-12-14 23:23:11 INFO     	 * (global step 4900: loss: 0.36561454832553864, lr: 0.0001
2023-12-14 23:23:19 INFO     	 * (global step 4950: loss: 0.3601957708597183, lr: 0.0001
2023-12-14 23:23:22 INFO     [epoch 7/15] average loss: 0.302, lr: 0.0001
2023-12-14 23:23:22 INFO     saving model related files
2023-12-14 23:23:22 INFO     saving model
2023-12-14 23:23:23 INFO     saving tokenizer
2023-12-14 23:23:23 INFO     saving optimizer
2023-12-14 23:23:24 INFO     remove old optimizer files
2023-12-14 23:23:29 INFO     	 * (global step 5000: loss: 0.20133685320615768, lr: 0.0001
2023-12-14 23:23:37 INFO     	 * (global step 5050: loss: 0.3337125927209854, lr: 0.0001
2023-12-14 23:23:45 INFO     	 * (global step 5100: loss: 0.23069487512111664, lr: 0.0001
2023-12-14 23:23:53 INFO     	 * (global step 5150: loss: 0.18576955050230026, lr: 0.0001
2023-12-14 23:24:01 INFO     	 * (global step 5200: loss: 0.3440094292163849, lr: 0.0001
2023-12-14 23:24:10 INFO     	 * (global step 5250: loss: 0.3268418684601784, lr: 0.0001
2023-12-14 23:24:18 INFO     	 * (global step 5300: loss: 0.24099615961313248, lr: 0.0001
2023-12-14 23:24:26 INFO     	 * (global step 5350: loss: 0.30675798654556274, lr: 0.0001
2023-12-14 23:24:34 INFO     	 * (global step 5400: loss: 0.5169157385826111, lr: 0.0001
2023-12-14 23:24:42 INFO     	 * (global step 5450: loss: 0.22462651878595352, lr: 0.0001
2023-12-14 23:24:50 INFO     	 * (global step 5500: loss: 0.39816661924123764, lr: 0.0001
2023-12-14 23:24:58 INFO     	 * (global step 5550: loss: 0.21279849112033844, lr: 0.0001
2023-12-14 23:25:05 INFO     [epoch 8/15] average loss: 0.295, lr: 0.0001
2023-12-14 23:25:05 INFO     saving model related files
2023-12-14 23:25:05 INFO     saving model
2023-12-14 23:25:05 INFO     saving tokenizer
2023-12-14 23:25:05 INFO     saving optimizer
2023-12-14 23:25:06 INFO     remove old optimizer files
2023-12-14 23:25:08 INFO     	 * (global step 5600: loss: 0.34813883155584335, lr: 0.0001
2023-12-14 23:25:16 INFO     	 * (global step 5650: loss: 0.34224066138267517, lr: 0.0001
2023-12-14 23:25:24 INFO     	 * (global step 5700: loss: 0.2879662811756134, lr: 0.0001
2023-12-14 23:25:32 INFO     	 * (global step 5750: loss: 0.2536086365580559, lr: 0.0001
2023-12-14 23:25:40 INFO     	 * (global step 5800: loss: 0.2214645817875862, lr: 0.0001
2023-12-14 23:25:48 INFO     	 * (global step 5850: loss: 0.44371916353702545, lr: 0.0001
2023-12-14 23:25:57 INFO     	 * (global step 5900: loss: 0.2396007999777794, lr: 0.0001
2023-12-14 23:26:05 INFO     	 * (global step 5950: loss: 0.332764208316803, lr: 0.0001
2023-12-14 23:26:13 INFO     	 * (global step 6000: loss: 0.2563517242670059, lr: 0.0001
2023-12-14 23:26:21 INFO     	 * (global step 6050: loss: 0.25275445729494095, lr: 0.0001
2023-12-14 23:26:29 INFO     	 * (global step 6100: loss: 0.3555210083723068, lr: 0.0001
2023-12-14 23:26:37 INFO     	 * (global step 6150: loss: 0.2735825553536415, lr: 0.0001
2023-12-14 23:26:45 INFO     	 * (global step 6200: loss: 0.41395239531993866, lr: 0.0001
2023-12-14 23:26:47 INFO     [epoch 9/15] average loss: 0.288, lr: 0.0001
2023-12-14 23:26:47 INFO     saving model related files
2023-12-14 23:26:47 INFO     saving model
2023-12-14 23:26:47 INFO     saving tokenizer
2023-12-14 23:26:47 INFO     saving optimizer
2023-12-14 23:26:48 INFO     remove old optimizer files
2023-12-14 23:26:49 INFO     complete training: model ckpt was saved at small_finetuned_ckpt/model_eszyci
2023-12-14 23:26:49 INFO     ## 1st RUN: Configuration 2/12 ##
2023-12-14 23:26:49 INFO     initialize model trainer
2023-12-14 23:26:49 INFO     initialize checkpoint at small_finetuned_ckpt/model_dpyopu
2023-12-14 23:26:49 INFO     hyperparameters
2023-12-14 23:26:49 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-14 23:26:49 INFO     	 * dataset_name: default
2023-12-14 23:26:49 INFO     	 * input_types: ['paragraph']
2023-12-14 23:26:49 INFO     	 * output_types: ['questions_answers']
2023-12-14 23:26:49 INFO     	 * prefix_types: ['qag']
2023-12-14 23:26:49 INFO     	 * model: lmqg/t5-small-squad-qag
2023-12-14 23:26:49 INFO     	 * max_length: 512
2023-12-14 23:26:49 INFO     	 * max_length_output: 512
2023-12-14 23:26:49 INFO     	 * epoch: 15
2023-12-14 23:26:49 INFO     	 * batch: 2
2023-12-14 23:26:49 INFO     	 * lr: 0.0001
2023-12-14 23:26:49 INFO     	 * fp16: False
2023-12-14 23:26:49 INFO     	 * random_seed: 1
2023-12-14 23:26:49 INFO     	 * gradient_accumulation_steps: 4
2023-12-14 23:26:49 INFO     	 * label_smoothing: 0.0
2023-12-14 23:26:49 INFO     initialize checkpoint with lmqg/t5-small-squad-qag
2023-12-14 23:26:50 INFO     use spaCy answer extraction model: positionrank
2023-12-14 23:26:51 INFO     Model `lmqg/t5-small-squad-qag`
2023-12-14 23:26:51 INFO     	 * Num of GPU in use: 1
2023-12-14 23:26:51 INFO     	 * Prefix: True
2023-12-14 23:26:51 INFO     	 * Language: en (ignore at the training phase)
2023-12-14 23:26:51 INFO     dataset preprocessing
2023-12-14 23:26:52 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-14 23:26:52 INFO     start model training
2023-12-14 23:27:08 INFO     	 * (global step 50: loss: 0.3651347681879997, lr: 0.0001
2023-12-14 23:27:24 INFO     	 * (global step 100: loss: 0.4017643630504608, lr: 0.0001
2023-12-14 23:27:39 INFO     	 * (global step 150: loss: 0.3726911321282387, lr: 0.0001
2023-12-14 23:27:55 INFO     	 * (global step 200: loss: 0.3527664393186569, lr: 0.0001
2023-12-14 23:28:11 INFO     	 * (global step 250: loss: 0.4465588852763176, lr: 0.0001
2023-12-14 23:28:26 INFO     	 * (global step 300: loss: 0.4871162846684456, lr: 0.0001
2023-12-14 23:28:30 INFO     [epoch 0/15] average loss: 0.405, lr: 0.0001
2023-12-14 23:28:30 INFO     saving model related files
2023-12-14 23:28:30 INFO     saving model
2023-12-14 23:28:30 INFO     saving tokenizer
2023-12-14 23:28:30 INFO     saving optimizer
2023-12-14 23:28:31 INFO     remove old optimizer files
2023-12-14 23:28:44 INFO     	 * (global step 350: loss: 0.3577493913471699, lr: 0.0001
2023-12-14 23:28:59 INFO     	 * (global step 400: loss: 0.349889300763607, lr: 0.0001
2023-12-14 23:29:15 INFO     	 * (global step 450: loss: 0.4609329290688038, lr: 0.0001
2023-12-14 23:29:31 INFO     	 * (global step 500: loss: 0.4614955335855484, lr: 0.0001
2023-12-14 23:29:47 INFO     	 * (global step 550: loss: 0.4844323992729187, lr: 0.0001
2023-12-14 23:30:02 INFO     	 * (global step 600: loss: 0.4174722544848919, lr: 0.0001
2023-12-14 23:30:09 INFO     [epoch 1/15] average loss: 0.377, lr: 0.0001
2023-12-14 23:30:09 INFO     saving model related files
2023-12-14 23:30:09 INFO     saving model
2023-12-14 23:30:09 INFO     saving tokenizer
2023-12-14 23:30:09 INFO     saving optimizer
2023-12-14 23:30:10 INFO     remove old optimizer files
2023-12-14 23:30:20 INFO     	 * (global step 650: loss: 0.34014976769685745, lr: 0.0001
2023-12-14 23:30:35 INFO     	 * (global step 700: loss: 0.3076176233589649, lr: 0.0001
2023-12-14 23:30:51 INFO     	 * (global step 750: loss: 0.3241450637578964, lr: 0.0001
2023-12-14 23:31:07 INFO     	 * (global step 800: loss: 0.3412947431206703, lr: 0.0001
2023-12-14 23:31:22 INFO     	 * (global step 850: loss: 0.48581790924072266, lr: 0.0001
2023-12-14 23:31:38 INFO     	 * (global step 900: loss: 0.48480528965592384, lr: 0.0001
2023-12-14 23:31:48 INFO     [epoch 2/15] average loss: 0.361, lr: 0.0001
2023-12-14 23:31:48 INFO     saving model related files
2023-12-14 23:31:48 INFO     saving model
2023-12-14 23:31:48 INFO     saving tokenizer
2023-12-14 23:31:48 INFO     saving optimizer
2023-12-14 23:31:49 INFO     remove old optimizer files
2023-12-14 23:31:56 INFO     	 * (global step 950: loss: 0.4387468546628952, lr: 0.0001
2023-12-14 23:32:11 INFO     	 * (global step 1000: loss: 0.33793532848358154, lr: 0.0001
2023-12-14 23:32:27 INFO     	 * (global step 1050: loss: 0.3500729501247406, lr: 0.0001
2023-12-14 23:32:43 INFO     	 * (global step 1100: loss: 0.3008650690317154, lr: 0.0001
2023-12-14 23:32:58 INFO     	 * (global step 1150: loss: 0.2612890414893627, lr: 0.0001
2023-12-14 23:33:14 INFO     	 * (global step 1200: loss: 0.3390027806162834, lr: 0.0001
2023-12-14 23:33:27 INFO     [epoch 3/15] average loss: 0.351, lr: 0.0001
2023-12-14 23:33:27 INFO     saving model related files
2023-12-14 23:33:27 INFO     saving model
2023-12-14 23:33:27 INFO     saving tokenizer
2023-12-14 23:33:27 INFO     saving optimizer
2023-12-14 23:33:28 INFO     remove old optimizer files
2023-12-14 23:33:31 INFO     	 * (global step 1250: loss: 0.3592468313872814, lr: 0.0001
2023-12-14 23:33:47 INFO     	 * (global step 1300: loss: 0.242879930883646, lr: 0.0001
2023-12-14 23:34:03 INFO     	 * (global step 1350: loss: 0.3368282578885555, lr: 0.0001
2023-12-14 23:34:18 INFO     	 * (global step 1400: loss: 0.26424307003617287, lr: 0.0001
2023-12-14 23:34:34 INFO     	 * (global step 1450: loss: 0.30083145946264267, lr: 0.0001
2023-12-14 23:34:50 INFO     	 * (global step 1500: loss: 0.43473247438669205, lr: 0.0001
2023-12-14 23:35:06 INFO     	 * (global step 1550: loss: 0.3010720945894718, lr: 0.0001
2023-12-14 23:35:06 INFO     [epoch 4/15] average loss: 0.341, lr: 0.0001
2023-12-14 23:35:06 INFO     saving model related files
2023-12-14 23:35:06 INFO     saving model
2023-12-14 23:35:06 INFO     saving tokenizer
2023-12-14 23:35:06 INFO     saving optimizer
2023-12-14 23:35:07 INFO     remove old optimizer files
2023-12-14 23:35:23 INFO     	 * (global step 1600: loss: 0.2918687015771866, lr: 0.0001
2023-12-14 23:35:39 INFO     	 * (global step 1650: loss: 0.3386906050145626, lr: 0.0001
2023-12-14 23:35:54 INFO     	 * (global step 1700: loss: 0.3547363057732582, lr: 0.0001
2023-12-14 23:36:10 INFO     	 * (global step 1750: loss: 0.24106773361563683, lr: 0.0001
2023-12-14 23:36:25 INFO     	 * (global step 1800: loss: 0.4485040493309498, lr: 0.0001
2023-12-14 23:36:41 INFO     	 * (global step 1850: loss: 0.2980395685881376, lr: 0.0001
2023-12-14 23:36:44 INFO     [epoch 5/15] average loss: 0.334, lr: 0.0001
2023-12-14 23:36:44 INFO     saving model related files
2023-12-14 23:36:44 INFO     saving model
2023-12-14 23:36:45 INFO     saving tokenizer
2023-12-14 23:36:45 INFO     saving optimizer
2023-12-14 23:36:46 INFO     remove old optimizer files
2023-12-14 23:36:59 INFO     	 * (global step 1900: loss: 0.3345091938972473, lr: 0.0001
2023-12-14 23:37:14 INFO     	 * (global step 1950: loss: 0.29924270883202553, lr: 0.0001
2023-12-14 23:37:30 INFO     	 * (global step 2000: loss: 0.3813333511352539, lr: 0.0001
2023-12-14 23:37:46 INFO     	 * (global step 2050: loss: 0.23159443587064743, lr: 0.0001
2023-12-14 23:38:01 INFO     	 * (global step 2100: loss: 0.31289316713809967, lr: 0.0001
2023-12-14 23:38:17 INFO     	 * (global step 2150: loss: 0.33575133979320526, lr: 0.0001
2023-12-14 23:38:24 INFO     [epoch 6/15] average loss: 0.325, lr: 0.0001
2023-12-14 23:38:24 INFO     saving model related files
2023-12-14 23:38:24 INFO     saving model
2023-12-14 23:38:24 INFO     saving tokenizer
2023-12-14 23:38:24 INFO     saving optimizer
2023-12-14 23:38:25 INFO     remove old optimizer files
2023-12-14 23:38:34 INFO     	 * (global step 2200: loss: 0.4448621794581413, lr: 0.0001
2023-12-14 23:38:50 INFO     	 * (global step 2250: loss: 0.46080779284238815, lr: 0.0001
2023-12-14 23:39:06 INFO     	 * (global step 2300: loss: 0.2852686867117882, lr: 0.0001
2023-12-14 23:39:21 INFO     	 * (global step 2350: loss: 0.34238646924495697, lr: 0.0001
2023-12-14 23:39:37 INFO     	 * (global step 2400: loss: 0.42634283378720284, lr: 0.0001
2023-12-14 23:39:53 INFO     	 * (global step 2450: loss: 0.2840861137956381, lr: 0.0001
2023-12-14 23:40:02 INFO     [epoch 7/15] average loss: 0.318, lr: 0.0001
2023-12-14 23:40:02 INFO     saving model related files
2023-12-14 23:40:02 INFO     saving model
2023-12-14 23:40:03 INFO     saving tokenizer
2023-12-14 23:40:03 INFO     saving optimizer
2023-12-14 23:40:04 INFO     remove old optimizer files
2023-12-14 23:40:10 INFO     	 * (global step 2500: loss: 0.4478090777993202, lr: 0.0001
2023-12-14 23:40:26 INFO     	 * (global step 2550: loss: 0.28139806166291237, lr: 0.0001
2023-12-14 23:40:42 INFO     	 * (global step 2600: loss: 0.39776234328746796, lr: 0.0001
2023-12-14 23:40:57 INFO     	 * (global step 2650: loss: 0.315175898373127, lr: 0.0001
2023-12-14 23:41:13 INFO     	 * (global step 2700: loss: 0.33116644993424416, lr: 0.0001
2023-12-14 23:41:29 INFO     	 * (global step 2750: loss: 0.3989001736044884, lr: 0.0001
2023-12-14 23:41:41 INFO     [epoch 8/15] average loss: 0.312, lr: 0.0001
2023-12-14 23:41:41 INFO     saving model related files
2023-12-14 23:41:41 INFO     saving model
2023-12-14 23:41:42 INFO     saving tokenizer
2023-12-14 23:41:42 INFO     saving optimizer
2023-12-14 23:41:43 INFO     remove old optimizer files
2023-12-14 23:41:46 INFO     	 * (global step 2800: loss: 0.40808267146348953, lr: 0.0001
2023-12-14 23:42:02 INFO     	 * (global step 2850: loss: 0.3587266281247139, lr: 0.0001
2023-12-14 23:42:17 INFO     	 * (global step 2900: loss: 0.3176904767751694, lr: 0.0001
2023-12-14 23:42:33 INFO     	 * (global step 2950: loss: 0.33335061371326447, lr: 0.0001
2023-12-14 23:42:48 INFO     	 * (global step 3000: loss: 0.21638232842087746, lr: 0.0001
2023-12-14 23:43:04 INFO     	 * (global step 3050: loss: 0.3437308631837368, lr: 0.0001
2023-12-14 23:43:20 INFO     	 * (global step 3100: loss: 0.31707265973091125, lr: 0.0001
2023-12-14 23:43:20 INFO     [epoch 9/15] average loss: 0.305, lr: 0.0001
2023-12-14 23:43:20 INFO     saving model related files
2023-12-14 23:43:20 INFO     saving model
2023-12-14 23:43:21 INFO     saving tokenizer
2023-12-14 23:43:21 INFO     saving optimizer
2023-12-14 23:43:22 INFO     remove old optimizer files
2023-12-14 23:43:22 INFO     complete training: model ckpt was saved at small_finetuned_ckpt/model_dpyopu
2023-12-14 23:43:22 INFO     ## 1st RUN: Configuration 3/12 ##
2023-12-14 23:43:22 INFO     initialize model trainer
2023-12-14 23:43:22 INFO     initialize checkpoint at small_finetuned_ckpt/model_mzgdpa
2023-12-14 23:43:22 INFO     hyperparameters
2023-12-14 23:43:22 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-14 23:43:22 INFO     	 * dataset_name: default
2023-12-14 23:43:22 INFO     	 * input_types: ['paragraph']
2023-12-14 23:43:22 INFO     	 * output_types: ['questions_answers']
2023-12-14 23:43:22 INFO     	 * prefix_types: ['qag']
2023-12-14 23:43:22 INFO     	 * model: lmqg/t5-small-squad-qag
2023-12-14 23:43:22 INFO     	 * max_length: 512
2023-12-14 23:43:22 INFO     	 * max_length_output: 512
2023-12-14 23:43:22 INFO     	 * epoch: 15
2023-12-14 23:43:22 INFO     	 * batch: 2
2023-12-14 23:43:22 INFO     	 * lr: 0.0001
2023-12-14 23:43:22 INFO     	 * fp16: False
2023-12-14 23:43:22 INFO     	 * random_seed: 1
2023-12-14 23:43:22 INFO     	 * gradient_accumulation_steps: 2
2023-12-14 23:43:22 INFO     	 * label_smoothing: 0.0
2023-12-14 23:43:22 INFO     initialize checkpoint with lmqg/t5-small-squad-qag
2023-12-14 23:43:23 INFO     use spaCy answer extraction model: positionrank
2023-12-14 23:43:24 INFO     Model `lmqg/t5-small-squad-qag`
2023-12-14 23:43:24 INFO     	 * Num of GPU in use: 1
2023-12-14 23:43:24 INFO     	 * Prefix: True
2023-12-14 23:43:24 INFO     	 * Language: en (ignore at the training phase)
2023-12-14 23:43:24 INFO     dataset preprocessing
2023-12-14 23:43:25 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-14 23:43:25 INFO     start model training
2023-12-14 23:43:33 INFO     	 * (global step 50: loss: 0.4020136296749115, lr: 0.0001
2023-12-14 23:43:41 INFO     	 * (global step 100: loss: 0.2806154638528824, lr: 0.0001
2023-12-14 23:43:50 INFO     	 * (global step 150: loss: 0.3587702661752701, lr: 0.0001
2023-12-14 23:43:58 INFO     	 * (global step 200: loss: 0.3935755640268326, lr: 0.0001
2023-12-14 23:44:06 INFO     	 * (global step 250: loss: 0.42130185663700104, lr: 0.0001
2023-12-14 23:44:14 INFO     	 * (global step 300: loss: 0.36001943051815033, lr: 0.0001
2023-12-14 23:44:22 INFO     	 * (global step 350: loss: 0.5767006874084473, lr: 0.0001
2023-12-14 23:44:30 INFO     	 * (global step 400: loss: 0.3435817211866379, lr: 0.0001
2023-12-14 23:44:38 INFO     	 * (global step 450: loss: 0.2922074869275093, lr: 0.0001
2023-12-14 23:44:46 INFO     	 * (global step 500: loss: 0.4161016643047333, lr: 0.0001
2023-12-14 23:44:54 INFO     	 * (global step 550: loss: 0.29670026898384094, lr: 0.0001
2023-12-14 23:45:03 INFO     	 * (global step 600: loss: 0.6162222474813461, lr: 0.0001
2023-12-14 23:45:06 INFO     [epoch 0/15] average loss: 0.401, lr: 0.0001
2023-12-14 23:45:06 INFO     saving model related files
2023-12-14 23:45:06 INFO     saving model
2023-12-14 23:45:06 INFO     saving tokenizer
2023-12-14 23:45:07 INFO     saving optimizer
2023-12-14 23:45:07 INFO     remove old optimizer files
2023-12-14 23:45:12 INFO     	 * (global step 650: loss: 0.434000626206398, lr: 0.0001
2023-12-14 23:45:20 INFO     	 * (global step 700: loss: 0.40486283600330353, lr: 0.0001
2023-12-14 23:45:28 INFO     	 * (global step 750: loss: 0.3170710653066635, lr: 0.0001
2023-12-14 23:45:37 INFO     	 * (global step 800: loss: 0.37305374443531036, lr: 0.0001
2023-12-14 23:45:45 INFO     	 * (global step 850: loss: 0.6294218003749847, lr: 0.0001
2023-12-14 23:45:53 INFO     	 * (global step 900: loss: 0.4763740822672844, lr: 0.0001
2023-12-14 23:46:01 INFO     	 * (global step 950: loss: 0.3367339074611664, lr: 0.0001
2023-12-14 23:46:09 INFO     	 * (global step 1000: loss: 0.5876975953578949, lr: 0.0001
2023-12-14 23:46:17 INFO     	 * (global step 1050: loss: 0.33479276299476624, lr: 0.0001
2023-12-14 23:46:25 INFO     	 * (global step 1100: loss: 0.4556150436401367, lr: 0.0001
2023-12-14 23:46:34 INFO     	 * (global step 1150: loss: 0.233285591006279, lr: 0.0001
2023-12-14 23:46:42 INFO     	 * (global step 1200: loss: 0.5725034177303314, lr: 0.0001
2023-12-14 23:46:49 INFO     [epoch 1/15] average loss: 0.371, lr: 0.0001
2023-12-14 23:46:49 INFO     saving model related files
2023-12-14 23:46:49 INFO     saving model
2023-12-14 23:46:49 INFO     saving tokenizer
2023-12-14 23:46:49 INFO     saving optimizer
2023-12-14 23:46:50 INFO     remove old optimizer files
2023-12-14 23:46:51 INFO     	 * (global step 1250: loss: 0.44525837898254395, lr: 0.0001
2023-12-14 23:47:00 INFO     	 * (global step 1300: loss: 0.255459189414978, lr: 0.0001
2023-12-14 23:47:08 INFO     	 * (global step 1350: loss: 0.171016126871109, lr: 0.0001
2023-12-14 23:47:16 INFO     	 * (global step 1400: loss: 0.44368670880794525, lr: 0.0001
2023-12-14 23:47:24 INFO     	 * (global step 1450: loss: 0.3812064677476883, lr: 0.0001
2023-12-14 23:47:32 INFO     	 * (global step 1500: loss: 0.23327872157096863, lr: 0.0001
2023-12-14 23:47:40 INFO     	 * (global step 1550: loss: 0.29209259897470474, lr: 0.0001
2023-12-14 23:47:48 INFO     	 * (global step 1600: loss: 0.3087814673781395, lr: 0.0001
2023-12-14 23:47:56 INFO     	 * (global step 1650: loss: 0.31502565741539, lr: 0.0001
2023-12-14 23:48:04 INFO     	 * (global step 1700: loss: 0.3139690011739731, lr: 0.0001
2023-12-14 23:48:13 INFO     	 * (global step 1750: loss: 0.27953629195690155, lr: 0.0001
2023-12-14 23:48:21 INFO     	 * (global step 1800: loss: 0.36808694899082184, lr: 0.0001
2023-12-14 23:48:29 INFO     	 * (global step 1850: loss: 0.2921146899461746, lr: 0.0001
2023-12-14 23:48:31 INFO     [epoch 2/15] average loss: 0.354, lr: 0.0001
2023-12-14 23:48:31 INFO     saving model related files
2023-12-14 23:48:31 INFO     saving model
2023-12-14 23:48:31 INFO     saving tokenizer
2023-12-14 23:48:32 INFO     saving optimizer
2023-12-14 23:48:33 INFO     remove old optimizer files
2023-12-14 23:48:39 INFO     	 * (global step 1900: loss: 0.2202722504734993, lr: 0.0001
2023-12-14 23:48:47 INFO     	 * (global step 1950: loss: 0.287660576403141, lr: 0.0001
2023-12-14 23:48:55 INFO     	 * (global step 2000: loss: 0.39493435621261597, lr: 0.0001
2023-12-14 23:49:03 INFO     	 * (global step 2050: loss: 0.43821926414966583, lr: 0.0001
2023-12-14 23:49:11 INFO     	 * (global step 2100: loss: 0.3237673193216324, lr: 0.0001
2023-12-14 23:49:19 INFO     	 * (global step 2150: loss: 0.30167221277952194, lr: 0.0001
2023-12-14 23:49:27 INFO     	 * (global step 2200: loss: 0.39904356002807617, lr: 0.0001
2023-12-14 23:49:35 INFO     	 * (global step 2250: loss: 0.355005145072937, lr: 0.0001
2023-12-14 23:49:44 INFO     	 * (global step 2300: loss: 0.36594629287719727, lr: 0.0001
2023-12-14 23:49:52 INFO     	 * (global step 2350: loss: 0.33855120092630386, lr: 0.0001
2023-12-14 23:50:00 INFO     	 * (global step 2400: loss: 0.4426475465297699, lr: 0.0001
2023-12-14 23:50:08 INFO     	 * (global step 2450: loss: 0.29116253554821014, lr: 0.0001
2023-12-14 23:50:13 INFO     [epoch 3/15] average loss: 0.341, lr: 0.0001
2023-12-14 23:50:13 INFO     saving model related files
2023-12-14 23:50:13 INFO     saving model
2023-12-14 23:50:14 INFO     saving tokenizer
2023-12-14 23:50:14 INFO     saving optimizer
2023-12-14 23:50:15 INFO     remove old optimizer files
2023-12-14 23:50:18 INFO     	 * (global step 2500: loss: 0.2751796320080757, lr: 0.0001
2023-12-14 23:50:26 INFO     	 * (global step 2550: loss: 0.2529292553663254, lr: 0.0001
2023-12-14 23:50:34 INFO     	 * (global step 2600: loss: 0.48326534032821655, lr: 0.0001
2023-12-14 23:50:42 INFO     	 * (global step 2650: loss: 0.39077845215797424, lr: 0.0001
2023-12-14 23:50:50 INFO     	 * (global step 2700: loss: 0.26996953785419464, lr: 0.0001
2023-12-14 23:50:58 INFO     	 * (global step 2750: loss: 0.3216094672679901, lr: 0.0001
2023-12-14 23:51:06 INFO     	 * (global step 2800: loss: 0.5880442559719086, lr: 0.0001
2023-12-14 23:51:14 INFO     	 * (global step 2850: loss: 0.36046670377254486, lr: 0.0001
2023-12-14 23:51:23 INFO     	 * (global step 2900: loss: 0.21478862687945366, lr: 0.0001
2023-12-14 23:51:31 INFO     	 * (global step 2950: loss: 0.2990403175354004, lr: 0.0001
2023-12-14 23:51:39 INFO     	 * (global step 3000: loss: 0.43146635591983795, lr: 0.0001
2023-12-14 23:51:47 INFO     	 * (global step 3050: loss: 0.2740752696990967, lr: 0.0001
2023-12-14 23:51:55 INFO     	 * (global step 3100: loss: 0.2792280316352844, lr: 0.0001
2023-12-14 23:51:56 INFO     [epoch 4/15] average loss: 0.33, lr: 0.0001
2023-12-14 23:51:56 INFO     saving model related files
2023-12-14 23:51:56 INFO     saving model
2023-12-14 23:51:56 INFO     saving tokenizer
2023-12-14 23:51:57 INFO     saving optimizer
2023-12-14 23:51:57 INFO     remove old optimizer files
2023-12-14 23:52:05 INFO     	 * (global step 3150: loss: 0.34657278656959534, lr: 0.0001
2023-12-14 23:52:13 INFO     	 * (global step 3200: loss: 0.39709968864917755, lr: 0.0001
2023-12-14 23:52:21 INFO     	 * (global step 3250: loss: 0.38360534608364105, lr: 0.0001
2023-12-14 23:52:29 INFO     	 * (global step 3300: loss: 0.3479503244161606, lr: 0.0001
2023-12-14 23:52:37 INFO     	 * (global step 3350: loss: 0.3407905101776123, lr: 0.0001
2023-12-14 23:52:45 INFO     	 * (global step 3400: loss: 0.3220079690217972, lr: 0.0001
2023-12-14 23:52:54 INFO     	 * (global step 3450: loss: 0.30339883267879486, lr: 0.0001
2023-12-14 23:53:02 INFO     	 * (global step 3500: loss: 0.39452098309993744, lr: 0.0001
2023-12-14 23:53:10 INFO     	 * (global step 3550: loss: 0.29361675679683685, lr: 0.0001
2023-12-14 23:53:18 INFO     	 * (global step 3600: loss: 0.34917663037776947, lr: 0.0001
2023-12-14 23:53:26 INFO     	 * (global step 3650: loss: 0.5033886060118675, lr: 0.0001
2023-12-14 23:53:34 INFO     	 * (global step 3700: loss: 0.45834794640541077, lr: 0.0001
2023-12-14 23:53:38 INFO     [epoch 5/15] average loss: 0.321, lr: 0.0001
2023-12-14 23:53:38 INFO     saving model related files
2023-12-14 23:53:38 INFO     saving model
2023-12-14 23:53:39 INFO     saving tokenizer
2023-12-14 23:53:39 INFO     saving optimizer
2023-12-14 23:53:40 INFO     remove old optimizer files
2023-12-14 23:53:44 INFO     	 * (global step 3750: loss: 0.28477975726127625, lr: 0.0001
2023-12-14 23:53:52 INFO     	 * (global step 3800: loss: 0.38383500277996063, lr: 0.0001
2023-12-14 23:54:00 INFO     	 * (global step 3850: loss: 0.31423529982566833, lr: 0.0001
2023-12-14 23:54:08 INFO     	 * (global step 3900: loss: 0.2906855344772339, lr: 0.0001
2023-12-14 23:54:16 INFO     	 * (global step 3950: loss: 0.4681011587381363, lr: 0.0001
2023-12-14 23:54:24 INFO     	 * (global step 4000: loss: 0.35805103182792664, lr: 0.0001
2023-12-14 23:54:32 INFO     	 * (global step 4050: loss: 0.2556639164686203, lr: 0.0001
2023-12-14 23:54:41 INFO     	 * (global step 4100: loss: 0.3240441828966141, lr: 0.0001
2023-12-14 23:54:49 INFO     	 * (global step 4150: loss: 0.22751915454864502, lr: 0.0001
2023-12-14 23:54:57 INFO     	 * (global step 4200: loss: 0.4122035801410675, lr: 0.0001
2023-12-14 23:55:05 INFO     	 * (global step 4250: loss: 0.2710251435637474, lr: 0.0001
2023-12-14 23:55:13 INFO     	 * (global step 4300: loss: 0.3743145763874054, lr: 0.0001
2023-12-14 23:55:21 INFO     [epoch 6/15] average loss: 0.311, lr: 0.0001
2023-12-14 23:55:21 INFO     saving model related files
2023-12-14 23:55:21 INFO     saving model
2023-12-14 23:55:21 INFO     saving tokenizer
2023-12-14 23:55:21 INFO     saving optimizer
2023-12-14 23:55:22 INFO     remove old optimizer files
2023-12-14 23:55:23 INFO     	 * (global step 4350: loss: 0.38796720653772354, lr: 0.0001
2023-12-14 23:55:31 INFO     	 * (global step 4400: loss: 0.2361869066953659, lr: 0.0001
2023-12-14 23:55:39 INFO     	 * (global step 4450: loss: 0.3804842084646225, lr: 0.0001
2023-12-14 23:55:47 INFO     	 * (global step 4500: loss: 0.28088369965553284, lr: 0.0001
2023-12-14 23:55:55 INFO     	 * (global step 4550: loss: 0.30669859051704407, lr: 0.0001
2023-12-14 23:56:04 INFO     	 * (global step 4600: loss: 0.31091390550136566, lr: 0.0001
2023-12-14 23:56:12 INFO     	 * (global step 4650: loss: 0.42142948508262634, lr: 0.0001
2023-12-14 23:56:20 INFO     	 * (global step 4700: loss: 0.259966105222702, lr: 0.0001
2023-12-14 23:56:28 INFO     	 * (global step 4750: loss: 0.2689439579844475, lr: 0.0001
2023-12-14 23:56:36 INFO     	 * (global step 4800: loss: 0.2789633050560951, lr: 0.0001
2023-12-14 23:56:44 INFO     	 * (global step 4850: loss: 0.30454112589359283, lr: 0.0001
2023-12-14 23:56:52 INFO     	 * (global step 4900: loss: 0.36561454832553864, lr: 0.0001
2023-12-14 23:57:00 INFO     	 * (global step 4950: loss: 0.3601957708597183, lr: 0.0001
2023-12-14 23:57:03 INFO     [epoch 7/15] average loss: 0.302, lr: 0.0001
2023-12-14 23:57:03 INFO     saving model related files
2023-12-14 23:57:03 INFO     saving model
2023-12-14 23:57:04 INFO     saving tokenizer
2023-12-14 23:57:04 INFO     saving optimizer
2023-12-14 23:57:05 INFO     remove old optimizer files
2023-12-14 23:57:10 INFO     	 * (global step 5000: loss: 0.20133685320615768, lr: 0.0001
2023-12-14 23:57:18 INFO     	 * (global step 5050: loss: 0.3337125927209854, lr: 0.0001
2023-12-14 23:57:26 INFO     	 * (global step 5100: loss: 0.23069487512111664, lr: 0.0001
2023-12-14 23:57:35 INFO     	 * (global step 5150: loss: 0.18576955050230026, lr: 0.0001
2023-12-14 23:57:43 INFO     	 * (global step 5200: loss: 0.3440094292163849, lr: 0.0001
2023-12-14 23:57:51 INFO     	 * (global step 5250: loss: 0.3268418684601784, lr: 0.0001
2023-12-14 23:57:59 INFO     	 * (global step 5300: loss: 0.24099615961313248, lr: 0.0001
2023-12-14 23:58:07 INFO     	 * (global step 5350: loss: 0.30675798654556274, lr: 0.0001
2023-12-14 23:58:15 INFO     	 * (global step 5400: loss: 0.5169157385826111, lr: 0.0001
2023-12-14 23:58:23 INFO     	 * (global step 5450: loss: 0.22462651878595352, lr: 0.0001
2023-12-14 23:58:31 INFO     	 * (global step 5500: loss: 0.39816661924123764, lr: 0.0001
2023-12-14 23:58:39 INFO     	 * (global step 5550: loss: 0.21279849112033844, lr: 0.0001
2023-12-14 23:58:46 INFO     [epoch 8/15] average loss: 0.295, lr: 0.0001
2023-12-14 23:58:46 INFO     saving model related files
2023-12-14 23:58:46 INFO     saving model
2023-12-14 23:58:46 INFO     saving tokenizer
2023-12-14 23:58:46 INFO     saving optimizer
2023-12-14 23:58:47 INFO     remove old optimizer files
2023-12-14 23:58:49 INFO     	 * (global step 5600: loss: 0.34813883155584335, lr: 0.0001
2023-12-14 23:58:57 INFO     	 * (global step 5650: loss: 0.34224066138267517, lr: 0.0001
2023-12-14 23:59:06 INFO     	 * (global step 5700: loss: 0.2879662811756134, lr: 0.0001
2023-12-14 23:59:14 INFO     	 * (global step 5750: loss: 0.2536086365580559, lr: 0.0001
2023-12-14 23:59:22 INFO     	 * (global step 5800: loss: 0.2214645817875862, lr: 0.0001
2023-12-14 23:59:30 INFO     	 * (global step 5850: loss: 0.44371916353702545, lr: 0.0001
2023-12-14 23:59:38 INFO     	 * (global step 5900: loss: 0.2396007999777794, lr: 0.0001
2023-12-14 23:59:46 INFO     	 * (global step 5950: loss: 0.332764208316803, lr: 0.0001
2023-12-14 23:59:54 INFO     	 * (global step 6000: loss: 0.2563517242670059, lr: 0.0001
2023-12-15 00:00:03 INFO     	 * (global step 6050: loss: 0.25275445729494095, lr: 0.0001
2023-12-15 00:00:11 INFO     	 * (global step 6100: loss: 0.3555210083723068, lr: 0.0001
2023-12-15 00:00:19 INFO     	 * (global step 6150: loss: 0.2735825553536415, lr: 0.0001
2023-12-15 00:00:27 INFO     	 * (global step 6200: loss: 0.41395239531993866, lr: 0.0001
2023-12-15 00:00:29 INFO     [epoch 9/15] average loss: 0.288, lr: 0.0001
2023-12-15 00:00:29 INFO     saving model related files
2023-12-15 00:00:29 INFO     saving model
2023-12-15 00:00:29 INFO     saving tokenizer
2023-12-15 00:00:29 INFO     saving optimizer
2023-12-15 00:00:30 INFO     remove old optimizer files
2023-12-15 00:00:31 INFO     complete training: model ckpt was saved at small_finetuned_ckpt/model_mzgdpa
2023-12-15 00:00:31 INFO     ## 1st RUN: Configuration 4/12 ##
2023-12-15 00:00:31 INFO     initialize model trainer
2023-12-15 00:00:31 INFO     initialize checkpoint at small_finetuned_ckpt/model_mntyya
2023-12-15 00:00:31 INFO     hyperparameters
2023-12-15 00:00:31 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 00:00:31 INFO     	 * dataset_name: default
2023-12-15 00:00:31 INFO     	 * input_types: ['paragraph']
2023-12-15 00:00:31 INFO     	 * output_types: ['questions_answers']
2023-12-15 00:00:31 INFO     	 * prefix_types: ['qag']
2023-12-15 00:00:31 INFO     	 * model: lmqg/t5-small-squad-qag
2023-12-15 00:00:31 INFO     	 * max_length: 512
2023-12-15 00:00:31 INFO     	 * max_length_output: 512
2023-12-15 00:00:31 INFO     	 * epoch: 15
2023-12-15 00:00:31 INFO     	 * batch: 2
2023-12-15 00:00:31 INFO     	 * lr: 5e-05
2023-12-15 00:00:31 INFO     	 * fp16: False
2023-12-15 00:00:31 INFO     	 * random_seed: 1
2023-12-15 00:00:31 INFO     	 * gradient_accumulation_steps: 4
2023-12-15 00:00:31 INFO     	 * label_smoothing: 0.15
2023-12-15 00:00:31 INFO     initialize checkpoint with lmqg/t5-small-squad-qag
2023-12-15 00:00:32 INFO     use spaCy answer extraction model: positionrank
2023-12-15 00:00:33 INFO     Model `lmqg/t5-small-squad-qag`
2023-12-15 00:00:33 INFO     	 * Num of GPU in use: 1
2023-12-15 00:00:33 INFO     	 * Prefix: True
2023-12-15 00:00:33 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 00:00:33 INFO     dataset preprocessing
2023-12-15 00:00:34 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 00:00:34 INFO     start model training
2023-12-15 00:00:50 INFO     	 * (global step 50: loss: 0.3780455216765404, lr: 5e-05
2023-12-15 00:01:06 INFO     	 * (global step 100: loss: 0.4107731580734253, lr: 5e-05
2023-12-15 00:01:21 INFO     	 * (global step 150: loss: 0.3773842975497246, lr: 5e-05
2023-12-15 00:01:37 INFO     	 * (global step 200: loss: 0.3609277978539467, lr: 5e-05
2023-12-15 00:01:52 INFO     	 * (global step 250: loss: 0.45456424355506897, lr: 5e-05
2023-12-15 00:02:08 INFO     	 * (global step 300: loss: 0.49351224303245544, lr: 5e-05
2023-12-15 00:02:11 INFO     [epoch 0/15] average loss: 0.415, lr: 5e-05
2023-12-15 00:02:11 INFO     saving model related files
2023-12-15 00:02:11 INFO     saving model
2023-12-15 00:02:12 INFO     saving tokenizer
2023-12-15 00:02:12 INFO     saving optimizer
2023-12-15 00:02:13 INFO     remove old optimizer files
2023-12-15 00:02:26 INFO     	 * (global step 350: loss: 0.36770598217844963, lr: 5e-05
2023-12-15 00:02:42 INFO     	 * (global step 400: loss: 0.35949231684207916, lr: 5e-05
2023-12-15 00:02:57 INFO     	 * (global step 450: loss: 0.4779057167470455, lr: 5e-05
2023-12-15 00:03:13 INFO     	 * (global step 500: loss: 0.4729894623160362, lr: 5e-05
2023-12-15 00:03:29 INFO     	 * (global step 550: loss: 0.4967128336429596, lr: 5e-05
2023-12-15 00:03:44 INFO     	 * (global step 600: loss: 0.4300440363585949, lr: 5e-05
2023-12-15 00:03:51 INFO     [epoch 1/15] average loss: 0.388, lr: 5e-05
2023-12-15 00:03:51 INFO     saving model related files
2023-12-15 00:03:51 INFO     saving model
2023-12-15 00:03:51 INFO     saving tokenizer
2023-12-15 00:03:51 INFO     saving optimizer
2023-12-15 00:03:52 INFO     remove old optimizer files
2023-12-15 00:04:02 INFO     	 * (global step 650: loss: 0.35603807494044304, lr: 5e-05
2023-12-15 00:04:17 INFO     	 * (global step 700: loss: 0.31962211430072784, lr: 5e-05
2023-12-15 00:04:33 INFO     	 * (global step 750: loss: 0.3377528116106987, lr: 5e-05
2023-12-15 00:04:49 INFO     	 * (global step 800: loss: 0.3568972498178482, lr: 5e-05
2023-12-15 00:05:05 INFO     	 * (global step 850: loss: 0.5064076408743858, lr: 5e-05
2023-12-15 00:05:20 INFO     	 * (global step 900: loss: 0.502933606505394, lr: 5e-05
2023-12-15 00:05:30 INFO     [epoch 2/15] average loss: 0.375, lr: 5e-05
2023-12-15 00:05:30 INFO     saving model related files
2023-12-15 00:05:30 INFO     saving model
2023-12-15 00:05:30 INFO     saving tokenizer
2023-12-15 00:05:30 INFO     saving optimizer
2023-12-15 00:05:31 INFO     remove old optimizer files
2023-12-15 00:05:38 INFO     	 * (global step 950: loss: 0.46155864745378494, lr: 5e-05
2023-12-15 00:05:53 INFO     	 * (global step 1000: loss: 0.35418394953012466, lr: 5e-05
2023-12-15 00:06:09 INFO     	 * (global step 1050: loss: 0.3593408092856407, lr: 5e-05
2023-12-15 00:06:25 INFO     	 * (global step 1100: loss: 0.31431515514850616, lr: 5e-05
2023-12-15 00:06:41 INFO     	 * (global step 1150: loss: 0.2748367562890053, lr: 5e-05
2023-12-15 00:06:56 INFO     	 * (global step 1200: loss: 0.3507280871272087, lr: 5e-05
2023-12-15 00:07:09 INFO     [epoch 3/15] average loss: 0.367, lr: 5e-05
2023-12-15 00:07:09 INFO     saving model related files
2023-12-15 00:07:09 INFO     saving model
2023-12-15 00:07:10 INFO     saving tokenizer
2023-12-15 00:07:10 INFO     saving optimizer
2023-12-15 00:07:11 INFO     remove old optimizer files
2023-12-15 00:07:14 INFO     	 * (global step 1250: loss: 0.3761179968714714, lr: 5e-05
2023-12-15 00:07:29 INFO     	 * (global step 1300: loss: 0.25526250526309013, lr: 5e-05
2023-12-15 00:07:45 INFO     	 * (global step 1350: loss: 0.3541148081421852, lr: 5e-05
2023-12-15 00:08:01 INFO     	 * (global step 1400: loss: 0.27936220169067383, lr: 5e-05
2023-12-15 00:08:17 INFO     	 * (global step 1450: loss: 0.31512853503227234, lr: 5e-05
2023-12-15 00:08:32 INFO     	 * (global step 1500: loss: 0.45651693269610405, lr: 5e-05
2023-12-15 00:08:48 INFO     	 * (global step 1550: loss: 0.320077084004879, lr: 5e-05
2023-12-15 00:08:48 INFO     [epoch 4/15] average loss: 0.36, lr: 5e-05
2023-12-15 00:08:48 INFO     saving model related files
2023-12-15 00:08:48 INFO     saving model
2023-12-15 00:08:49 INFO     saving tokenizer
2023-12-15 00:08:49 INFO     saving optimizer
2023-12-15 00:08:50 INFO     remove old optimizer files
2023-12-15 00:09:06 INFO     	 * (global step 1600: loss: 0.3088424578309059, lr: 5e-05
2023-12-15 00:09:21 INFO     	 * (global step 1650: loss: 0.3577736206352711, lr: 5e-05
2023-12-15 00:09:37 INFO     	 * (global step 1700: loss: 0.37511271238327026, lr: 5e-05
2023-12-15 00:09:52 INFO     	 * (global step 1750: loss: 0.2559734918177128, lr: 5e-05
2023-12-15 00:10:08 INFO     	 * (global step 1800: loss: 0.47647181153297424, lr: 5e-05
2023-12-15 00:10:24 INFO     	 * (global step 1850: loss: 0.32012697122991085, lr: 5e-05
2023-12-15 00:10:27 INFO     [epoch 5/15] average loss: 0.354, lr: 5e-05
2023-12-15 00:10:27 INFO     saving model related files
2023-12-15 00:10:27 INFO     saving model
2023-12-15 00:10:28 INFO     saving tokenizer
2023-12-15 00:10:28 INFO     saving optimizer
2023-12-15 00:10:29 INFO     remove old optimizer files
2023-12-15 00:10:41 INFO     	 * (global step 1900: loss: 0.35779913514852524, lr: 5e-05
2023-12-15 00:10:57 INFO     	 * (global step 1950: loss: 0.31898563727736473, lr: 5e-05
2023-12-15 00:11:13 INFO     	 * (global step 2000: loss: 0.4044246971607208, lr: 5e-05
2023-12-15 00:11:28 INFO     	 * (global step 2050: loss: 0.2467988096177578, lr: 5e-05
2023-12-15 00:11:44 INFO     	 * (global step 2100: loss: 0.32863330841064453, lr: 5e-05
2023-12-15 00:12:00 INFO     	 * (global step 2150: loss: 0.3660738095641136, lr: 5e-05
2023-12-15 00:12:06 INFO     [epoch 6/15] average loss: 0.348, lr: 5e-05
2023-12-15 00:12:06 INFO     saving model related files
2023-12-15 00:12:06 INFO     saving model
2023-12-15 00:12:07 INFO     saving tokenizer
2023-12-15 00:12:07 INFO     saving optimizer
2023-12-15 00:12:08 INFO     remove old optimizer files
2023-12-15 00:12:17 INFO     	 * (global step 2200: loss: 0.4806148111820221, lr: 5e-05
2023-12-15 00:12:33 INFO     	 * (global step 2250: loss: 0.503802478313446, lr: 5e-05
2023-12-15 00:12:48 INFO     	 * (global step 2300: loss: 0.304580919444561, lr: 5e-05
2023-12-15 00:13:04 INFO     	 * (global step 2350: loss: 0.3749650716781616, lr: 5e-05
2023-12-15 00:13:20 INFO     	 * (global step 2400: loss: 0.46525727212429047, lr: 5e-05
2023-12-15 00:13:36 INFO     	 * (global step 2450: loss: 0.30373828392475843, lr: 5e-05
2023-12-15 00:13:45 INFO     [epoch 7/15] average loss: 0.343, lr: 5e-05
2023-12-15 00:13:45 INFO     saving model related files
2023-12-15 00:13:45 INFO     saving model
2023-12-15 00:13:46 INFO     saving tokenizer
2023-12-15 00:13:46 INFO     saving optimizer
2023-12-15 00:13:47 INFO     remove old optimizer files
2023-12-15 00:13:53 INFO     	 * (global step 2500: loss: 0.4904075562953949, lr: 5e-05
2023-12-15 00:14:09 INFO     	 * (global step 2550: loss: 0.3049139752984047, lr: 5e-05
2023-12-15 00:14:24 INFO     	 * (global step 2600: loss: 0.43762294203042984, lr: 5e-05
2023-12-15 00:14:40 INFO     	 * (global step 2650: loss: 0.3380793258547783, lr: 5e-05
2023-12-15 00:14:56 INFO     	 * (global step 2700: loss: 0.36257416754961014, lr: 5e-05
2023-12-15 00:15:11 INFO     	 * (global step 2750: loss: 0.43518567830324173, lr: 5e-05
2023-12-15 00:15:24 INFO     [epoch 8/15] average loss: 0.338, lr: 5e-05
2023-12-15 00:15:24 INFO     saving model related files
2023-12-15 00:15:24 INFO     saving model
2023-12-15 00:15:25 INFO     saving tokenizer
2023-12-15 00:15:25 INFO     saving optimizer
2023-12-15 00:15:26 INFO     remove old optimizer files
2023-12-15 00:15:29 INFO     	 * (global step 2800: loss: 0.44579746574163437, lr: 5e-05
2023-12-15 00:15:45 INFO     	 * (global step 2850: loss: 0.3942920118570328, lr: 5e-05
2023-12-15 00:16:00 INFO     	 * (global step 2900: loss: 0.34988250210881233, lr: 5e-05
2023-12-15 00:16:16 INFO     	 * (global step 2950: loss: 0.36229629814624786, lr: 5e-05
2023-12-15 00:16:32 INFO     	 * (global step 3000: loss: 0.23466076329350471, lr: 5e-05
2023-12-15 00:16:47 INFO     	 * (global step 3050: loss: 0.37257495149970055, lr: 5e-05
2023-12-15 00:17:03 INFO     	 * (global step 3100: loss: 0.3467252403497696, lr: 5e-05
2023-12-15 00:17:03 INFO     [epoch 9/15] average loss: 0.333, lr: 5e-05
2023-12-15 00:17:03 INFO     saving model related files
2023-12-15 00:17:03 INFO     saving model
2023-12-15 00:17:04 INFO     saving tokenizer
2023-12-15 00:17:04 INFO     saving optimizer
2023-12-15 00:17:05 INFO     remove old optimizer files
2023-12-15 00:17:05 INFO     complete training: model ckpt was saved at small_finetuned_ckpt/model_mntyya
2023-12-15 00:17:05 INFO     ## 1st RUN: Configuration 5/12 ##
2023-12-15 00:17:05 INFO     initialize model trainer
2023-12-15 00:17:05 INFO     initialize checkpoint at small_finetuned_ckpt/model_woixzh
2023-12-15 00:17:05 INFO     hyperparameters
2023-12-15 00:17:05 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 00:17:05 INFO     	 * dataset_name: default
2023-12-15 00:17:05 INFO     	 * input_types: ['paragraph']
2023-12-15 00:17:05 INFO     	 * output_types: ['questions_answers']
2023-12-15 00:17:05 INFO     	 * prefix_types: ['qag']
2023-12-15 00:17:05 INFO     	 * model: lmqg/t5-small-squad-qag
2023-12-15 00:17:05 INFO     	 * max_length: 512
2023-12-15 00:17:05 INFO     	 * max_length_output: 512
2023-12-15 00:17:05 INFO     	 * epoch: 15
2023-12-15 00:17:05 INFO     	 * batch: 2
2023-12-15 00:17:05 INFO     	 * lr: 5e-05
2023-12-15 00:17:05 INFO     	 * fp16: False
2023-12-15 00:17:05 INFO     	 * random_seed: 1
2023-12-15 00:17:05 INFO     	 * gradient_accumulation_steps: 2
2023-12-15 00:17:05 INFO     	 * label_smoothing: 0.15
2023-12-15 00:17:05 INFO     initialize checkpoint with lmqg/t5-small-squad-qag
2023-12-15 00:17:06 INFO     use spaCy answer extraction model: positionrank
2023-12-15 00:17:07 INFO     Model `lmqg/t5-small-squad-qag`
2023-12-15 00:17:07 INFO     	 * Num of GPU in use: 1
2023-12-15 00:17:07 INFO     	 * Prefix: True
2023-12-15 00:17:07 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 00:17:07 INFO     dataset preprocessing
2023-12-15 00:17:08 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 00:17:08 INFO     start model training
2023-12-15 00:17:16 INFO     	 * (global step 50: loss: 0.41809624433517456, lr: 5e-05
2023-12-15 00:17:24 INFO     	 * (global step 100: loss: 0.286358967423439, lr: 5e-05
2023-12-15 00:17:32 INFO     	 * (global step 150: loss: 0.3721126765012741, lr: 5e-05
2023-12-15 00:17:40 INFO     	 * (global step 200: loss: 0.40180808305740356, lr: 5e-05
2023-12-15 00:17:49 INFO     	 * (global step 250: loss: 0.42627498507499695, lr: 5e-05
2023-12-15 00:17:57 INFO     	 * (global step 300: loss: 0.3639523983001709, lr: 5e-05
2023-12-15 00:18:05 INFO     	 * (global step 350: loss: 0.5841566026210785, lr: 5e-05
2023-12-15 00:18:13 INFO     	 * (global step 400: loss: 0.34957315027713776, lr: 5e-05
2023-12-15 00:18:21 INFO     	 * (global step 450: loss: 0.29910197854042053, lr: 5e-05
2023-12-15 00:18:29 INFO     	 * (global step 500: loss: 0.4229959100484848, lr: 5e-05
2023-12-15 00:18:37 INFO     	 * (global step 550: loss: 0.3039970248937607, lr: 5e-05
2023-12-15 00:18:45 INFO     	 * (global step 600: loss: 0.6186522096395493, lr: 5e-05
2023-12-15 00:18:49 INFO     [epoch 0/15] average loss: 0.409, lr: 5e-05
2023-12-15 00:18:49 INFO     saving model related files
2023-12-15 00:18:49 INFO     saving model
2023-12-15 00:18:49 INFO     saving tokenizer
2023-12-15 00:18:50 INFO     saving optimizer
2023-12-15 00:18:51 INFO     remove old optimizer files
2023-12-15 00:18:55 INFO     	 * (global step 650: loss: 0.44917337596416473, lr: 5e-05
2023-12-15 00:19:03 INFO     	 * (global step 700: loss: 0.4115673750638962, lr: 5e-05
2023-12-15 00:19:11 INFO     	 * (global step 750: loss: 0.325979083776474, lr: 5e-05
2023-12-15 00:19:20 INFO     	 * (global step 800: loss: 0.38475118577480316, lr: 5e-05
2023-12-15 00:19:28 INFO     	 * (global step 850: loss: 0.6454284638166428, lr: 5e-05
2023-12-15 00:19:36 INFO     	 * (global step 900: loss: 0.49665357917547226, lr: 5e-05
2023-12-15 00:19:44 INFO     	 * (global step 950: loss: 0.3418770581483841, lr: 5e-05
2023-12-15 00:19:52 INFO     	 * (global step 1000: loss: 0.6004842519760132, lr: 5e-05
2023-12-15 00:20:00 INFO     	 * (global step 1050: loss: 0.34437231719493866, lr: 5e-05
2023-12-15 00:20:08 INFO     	 * (global step 1100: loss: 0.4652298390865326, lr: 5e-05
2023-12-15 00:20:16 INFO     	 * (global step 1150: loss: 0.23874156922101974, lr: 5e-05
2023-12-15 00:20:24 INFO     	 * (global step 1200: loss: 0.5884462594985962, lr: 5e-05
2023-12-15 00:20:31 INFO     [epoch 1/15] average loss: 0.382, lr: 5e-05
2023-12-15 00:20:31 INFO     saving model related files
2023-12-15 00:20:31 INFO     saving model
2023-12-15 00:20:32 INFO     saving tokenizer
2023-12-15 00:20:32 INFO     saving optimizer
2023-12-15 00:20:33 INFO     remove old optimizer files
2023-12-15 00:20:34 INFO     	 * (global step 1250: loss: 0.4601747542619705, lr: 5e-05
2023-12-15 00:20:42 INFO     	 * (global step 1300: loss: 0.2657741904258728, lr: 5e-05
2023-12-15 00:20:51 INFO     	 * (global step 1350: loss: 0.17852959036827087, lr: 5e-05
2023-12-15 00:20:59 INFO     	 * (global step 1400: loss: 0.46040020883083344, lr: 5e-05
2023-12-15 00:21:07 INFO     	 * (global step 1450: loss: 0.3924756199121475, lr: 5e-05
2023-12-15 00:21:15 INFO     	 * (global step 1500: loss: 0.23990752547979355, lr: 5e-05
2023-12-15 00:21:23 INFO     	 * (global step 1550: loss: 0.30334699898958206, lr: 5e-05
2023-12-15 00:21:31 INFO     	 * (global step 1600: loss: 0.3210824429988861, lr: 5e-05
2023-12-15 00:21:39 INFO     	 * (global step 1650: loss: 0.32976357638835907, lr: 5e-05
2023-12-15 00:21:47 INFO     	 * (global step 1700: loss: 0.3249228000640869, lr: 5e-05
2023-12-15 00:21:56 INFO     	 * (global step 1750: loss: 0.2876528948545456, lr: 5e-05
2023-12-15 00:22:04 INFO     	 * (global step 1800: loss: 0.38115377724170685, lr: 5e-05
2023-12-15 00:22:12 INFO     	 * (global step 1850: loss: 0.2995053753256798, lr: 5e-05
2023-12-15 00:22:14 INFO     [epoch 2/15] average loss: 0.368, lr: 5e-05
2023-12-15 00:22:14 INFO     saving model related files
2023-12-15 00:22:14 INFO     saving model
2023-12-15 00:22:14 INFO     saving tokenizer
2023-12-15 00:22:15 INFO     saving optimizer
2023-12-15 00:22:16 INFO     remove old optimizer files
2023-12-15 00:22:22 INFO     	 * (global step 1900: loss: 0.23196668177843094, lr: 5e-05
2023-12-15 00:22:30 INFO     	 * (global step 1950: loss: 0.301974892616272, lr: 5e-05
2023-12-15 00:22:38 INFO     	 * (global step 2000: loss: 0.42038482427597046, lr: 5e-05
2023-12-15 00:22:46 INFO     	 * (global step 2050: loss: 0.46615874767303467, lr: 5e-05
2023-12-15 00:22:54 INFO     	 * (global step 2100: loss: 0.33964142203330994, lr: 5e-05
2023-12-15 00:23:02 INFO     	 * (global step 2150: loss: 0.31045982986688614, lr: 5e-05
2023-12-15 00:23:10 INFO     	 * (global step 2200: loss: 0.4163629412651062, lr: 5e-05
2023-12-15 00:23:18 INFO     	 * (global step 2250: loss: 0.37272024154663086, lr: 5e-05
2023-12-15 00:23:27 INFO     	 * (global step 2300: loss: 0.3831852823495865, lr: 5e-05
2023-12-15 00:23:35 INFO     	 * (global step 2350: loss: 0.35348306596279144, lr: 5e-05
2023-12-15 00:23:43 INFO     	 * (global step 2400: loss: 0.4610773175954819, lr: 5e-05
2023-12-15 00:23:51 INFO     	 * (global step 2450: loss: 0.31234708428382874, lr: 5e-05
2023-12-15 00:23:56 INFO     [epoch 3/15] average loss: 0.359, lr: 5e-05
2023-12-15 00:23:56 INFO     saving model related files
2023-12-15 00:23:56 INFO     saving model
2023-12-15 00:23:57 INFO     saving tokenizer
2023-12-15 00:23:57 INFO     saving optimizer
2023-12-15 00:23:58 INFO     remove old optimizer files
2023-12-15 00:24:01 INFO     	 * (global step 2500: loss: 0.2958718091249466, lr: 5e-05
2023-12-15 00:24:09 INFO     	 * (global step 2550: loss: 0.2659468948841095, lr: 5e-05
2023-12-15 00:24:17 INFO     	 * (global step 2600: loss: 0.5164542347192764, lr: 5e-05
2023-12-15 00:24:25 INFO     	 * (global step 2650: loss: 0.4137146472930908, lr: 5e-05
2023-12-15 00:24:33 INFO     	 * (global step 2700: loss: 0.29279883205890656, lr: 5e-05
2023-12-15 00:24:41 INFO     	 * (global step 2750: loss: 0.34371304512023926, lr: 5e-05
2023-12-15 00:24:49 INFO     	 * (global step 2800: loss: 0.6325331926345825, lr: 5e-05
2023-12-15 00:24:58 INFO     	 * (global step 2850: loss: 0.3882659524679184, lr: 5e-05
2023-12-15 00:25:06 INFO     	 * (global step 2900: loss: 0.22958653420209885, lr: 5e-05
2023-12-15 00:25:14 INFO     	 * (global step 2950: loss: 0.32017071545124054, lr: 5e-05
2023-12-15 00:25:22 INFO     	 * (global step 3000: loss: 0.4632372111082077, lr: 5e-05
2023-12-15 00:25:30 INFO     	 * (global step 3050: loss: 0.2926095426082611, lr: 5e-05
2023-12-15 00:25:38 INFO     	 * (global step 3100: loss: 0.29308387637138367, lr: 5e-05
2023-12-15 00:25:39 INFO     [epoch 4/15] average loss: 0.35, lr: 5e-05
2023-12-15 00:25:39 INFO     saving model related files
2023-12-15 00:25:39 INFO     saving model
2023-12-15 00:25:39 INFO     saving tokenizer
2023-12-15 00:25:39 INFO     saving optimizer
2023-12-15 00:25:40 INFO     remove old optimizer files
2023-12-15 00:25:48 INFO     	 * (global step 3150: loss: 0.3738354817032814, lr: 5e-05
2023-12-15 00:25:56 INFO     	 * (global step 3200: loss: 0.4306213855743408, lr: 5e-05
2023-12-15 00:26:04 INFO     	 * (global step 3250: loss: 0.40026482194662094, lr: 5e-05
2023-12-15 00:26:12 INFO     	 * (global step 3300: loss: 0.3683840185403824, lr: 5e-05
2023-12-15 00:26:20 INFO     	 * (global step 3350: loss: 0.36822739243507385, lr: 5e-05
2023-12-15 00:26:29 INFO     	 * (global step 3400: loss: 0.3489926606416702, lr: 5e-05
2023-12-15 00:26:37 INFO     	 * (global step 3450: loss: 0.3220568150281906, lr: 5e-05
2023-12-15 00:26:45 INFO     	 * (global step 3500: loss: 0.4208225756883621, lr: 5e-05
2023-12-15 00:26:53 INFO     	 * (global step 3550: loss: 0.31956426054239273, lr: 5e-05
2023-12-15 00:27:01 INFO     	 * (global step 3600: loss: 0.3720497339963913, lr: 5e-05
2023-12-15 00:27:09 INFO     	 * (global step 3650: loss: 0.5271793976426125, lr: 5e-05
2023-12-15 00:27:17 INFO     	 * (global step 3700: loss: 0.5054533332586288, lr: 5e-05
2023-12-15 00:27:22 INFO     [epoch 5/15] average loss: 0.344, lr: 5e-05
2023-12-15 00:27:22 INFO     saving model related files
2023-12-15 00:27:22 INFO     saving model
2023-12-15 00:27:22 INFO     saving tokenizer
2023-12-15 00:27:22 INFO     saving optimizer
2023-12-15 00:27:23 INFO     remove old optimizer files
2023-12-15 00:27:27 INFO     	 * (global step 3750: loss: 0.30836255848407745, lr: 5e-05
2023-12-15 00:27:35 INFO     	 * (global step 3800: loss: 0.4156266003847122, lr: 5e-05
2023-12-15 00:27:43 INFO     	 * (global step 3850: loss: 0.3440245985984802, lr: 5e-05
2023-12-15 00:27:51 INFO     	 * (global step 3900: loss: 0.31694532185792923, lr: 5e-05
2023-12-15 00:27:59 INFO     	 * (global step 3950: loss: 0.504056915640831, lr: 5e-05
2023-12-15 00:28:08 INFO     	 * (global step 4000: loss: 0.389311283826828, lr: 5e-05
2023-12-15 00:28:16 INFO     	 * (global step 4050: loss: 0.2757980301976204, lr: 5e-05
2023-12-15 00:28:24 INFO     	 * (global step 4100: loss: 0.3595430552959442, lr: 5e-05
2023-12-15 00:28:32 INFO     	 * (global step 4150: loss: 0.24142248183488846, lr: 5e-05
2023-12-15 00:28:40 INFO     	 * (global step 4200: loss: 0.443401575088501, lr: 5e-05
2023-12-15 00:28:48 INFO     	 * (global step 4250: loss: 0.28903647512197495, lr: 5e-05
2023-12-15 00:28:56 INFO     	 * (global step 4300: loss: 0.39994125068187714, lr: 5e-05
2023-12-15 00:29:04 INFO     [epoch 6/15] average loss: 0.336, lr: 5e-05
2023-12-15 00:29:04 INFO     saving model related files
2023-12-15 00:29:04 INFO     saving model
2023-12-15 00:29:04 INFO     saving tokenizer
2023-12-15 00:29:05 INFO     saving optimizer
2023-12-15 00:29:06 INFO     remove old optimizer files
2023-12-15 00:29:06 INFO     	 * (global step 4350: loss: 0.42879149317741394, lr: 5e-05
2023-12-15 00:29:14 INFO     	 * (global step 4400: loss: 0.26480528712272644, lr: 5e-05
2023-12-15 00:29:22 INFO     	 * (global step 4450: loss: 0.41867993772029877, lr: 5e-05
2023-12-15 00:29:30 INFO     	 * (global step 4500: loss: 0.3098124563694, lr: 5e-05
2023-12-15 00:29:39 INFO     	 * (global step 4550: loss: 0.33236537873744965, lr: 5e-05
2023-12-15 00:29:47 INFO     	 * (global step 4600: loss: 0.3419768214225769, lr: 5e-05
2023-12-15 00:29:55 INFO     	 * (global step 4650: loss: 0.45855462551116943, lr: 5e-05
2023-12-15 00:30:03 INFO     	 * (global step 4700: loss: 0.2821419835090637, lr: 5e-05
2023-12-15 00:30:11 INFO     	 * (global step 4750: loss: 0.3003136143088341, lr: 5e-05
2023-12-15 00:30:19 INFO     	 * (global step 4800: loss: 0.3169853463768959, lr: 5e-05
2023-12-15 00:30:27 INFO     	 * (global step 4850: loss: 0.32572511583566666, lr: 5e-05
2023-12-15 00:30:35 INFO     	 * (global step 4900: loss: 0.4023936241865158, lr: 5e-05
2023-12-15 00:30:43 INFO     	 * (global step 4950: loss: 0.40535736083984375, lr: 5e-05
2023-12-15 00:30:46 INFO     [epoch 7/15] average loss: 0.33, lr: 5e-05
2023-12-15 00:30:46 INFO     saving model related files
2023-12-15 00:30:46 INFO     saving model
2023-12-15 00:30:47 INFO     saving tokenizer
2023-12-15 00:30:47 INFO     saving optimizer
2023-12-15 00:30:48 INFO     remove old optimizer files
2023-12-15 00:30:53 INFO     	 * (global step 5000: loss: 0.22356399148702621, lr: 5e-05
2023-12-15 00:31:02 INFO     	 * (global step 5050: loss: 0.36298857629299164, lr: 5e-05
2023-12-15 00:31:10 INFO     	 * (global step 5100: loss: 0.2519294396042824, lr: 5e-05
2023-12-15 00:31:18 INFO     	 * (global step 5150: loss: 0.19899775087833405, lr: 5e-05
2023-12-15 00:31:26 INFO     	 * (global step 5200: loss: 0.3797121047973633, lr: 5e-05
2023-12-15 00:31:34 INFO     	 * (global step 5250: loss: 0.3632555678486824, lr: 5e-05
2023-12-15 00:31:42 INFO     	 * (global step 5300: loss: 0.25891710072755814, lr: 5e-05
2023-12-15 00:31:50 INFO     	 * (global step 5350: loss: 0.3383786529302597, lr: 5e-05
2023-12-15 00:31:58 INFO     	 * (global step 5400: loss: 0.5770666301250458, lr: 5e-05
2023-12-15 00:32:07 INFO     	 * (global step 5450: loss: 0.2457612007856369, lr: 5e-05
2023-12-15 00:32:15 INFO     	 * (global step 5500: loss: 0.41919924318790436, lr: 5e-05
2023-12-15 00:32:23 INFO     	 * (global step 5550: loss: 0.23570477217435837, lr: 5e-05
2023-12-15 00:32:29 INFO     [epoch 8/15] average loss: 0.325, lr: 5e-05
2023-12-15 00:32:29 INFO     saving model related files
2023-12-15 00:32:29 INFO     saving model
2023-12-15 00:32:30 INFO     saving tokenizer
2023-12-15 00:32:30 INFO     saving optimizer
2023-12-15 00:32:31 INFO     remove old optimizer files
2023-12-15 00:32:33 INFO     	 * (global step 5600: loss: 0.3780443146824837, lr: 5e-05
2023-12-15 00:32:42 INFO     	 * (global step 5650: loss: 0.3909812569618225, lr: 5e-05
2023-12-15 00:32:50 INFO     	 * (global step 5700: loss: 0.3272462785243988, lr: 5e-05
2023-12-15 00:32:58 INFO     	 * (global step 5750: loss: 0.2900751382112503, lr: 5e-05
2023-12-15 00:33:06 INFO     	 * (global step 5800: loss: 0.25181426852941513, lr: 5e-05
2023-12-15 00:33:14 INFO     	 * (global step 5850: loss: 0.4997807741165161, lr: 5e-05
2023-12-15 00:33:22 INFO     	 * (global step 5900: loss: 0.26398860663175583, lr: 5e-05
2023-12-15 00:33:30 INFO     	 * (global step 5950: loss: 0.3629453480243683, lr: 5e-05
2023-12-15 00:33:38 INFO     	 * (global step 6000: loss: 0.286574587225914, lr: 5e-05
2023-12-15 00:33:46 INFO     	 * (global step 6050: loss: 0.28656964749097824, lr: 5e-05
2023-12-15 00:33:55 INFO     	 * (global step 6100: loss: 0.39327672123908997, lr: 5e-05
2023-12-15 00:34:03 INFO     	 * (global step 6150: loss: 0.29808489978313446, lr: 5e-05
2023-12-15 00:34:11 INFO     	 * (global step 6200: loss: 0.4662894755601883, lr: 5e-05
2023-12-15 00:34:12 INFO     [epoch 9/15] average loss: 0.319, lr: 5e-05
2023-12-15 00:34:12 INFO     saving model related files
2023-12-15 00:34:12 INFO     saving model
2023-12-15 00:34:13 INFO     saving tokenizer
2023-12-15 00:34:13 INFO     saving optimizer
2023-12-15 00:34:14 INFO     remove old optimizer files
2023-12-15 00:34:14 INFO     complete training: model ckpt was saved at small_finetuned_ckpt/model_woixzh
2023-12-15 00:34:14 INFO     ## 1st RUN: Configuration 6/12 ##
2023-12-15 00:34:14 INFO     initialize model trainer
2023-12-15 00:34:14 INFO     initialize checkpoint at small_finetuned_ckpt/model_sdkaaa
2023-12-15 00:34:14 INFO     hyperparameters
2023-12-15 00:34:14 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 00:34:14 INFO     	 * dataset_name: default
2023-12-15 00:34:14 INFO     	 * input_types: ['paragraph']
2023-12-15 00:34:14 INFO     	 * output_types: ['questions_answers']
2023-12-15 00:34:14 INFO     	 * prefix_types: ['qag']
2023-12-15 00:34:14 INFO     	 * model: lmqg/t5-small-squad-qag
2023-12-15 00:34:14 INFO     	 * max_length: 512
2023-12-15 00:34:14 INFO     	 * max_length_output: 512
2023-12-15 00:34:14 INFO     	 * epoch: 15
2023-12-15 00:34:14 INFO     	 * batch: 2
2023-12-15 00:34:14 INFO     	 * lr: 5e-05
2023-12-15 00:34:14 INFO     	 * fp16: False
2023-12-15 00:34:14 INFO     	 * random_seed: 1
2023-12-15 00:34:14 INFO     	 * gradient_accumulation_steps: 4
2023-12-15 00:34:14 INFO     	 * label_smoothing: 0.0
2023-12-15 00:34:14 INFO     initialize checkpoint with lmqg/t5-small-squad-qag
2023-12-15 00:34:16 INFO     use spaCy answer extraction model: positionrank
2023-12-15 00:34:16 INFO     Model `lmqg/t5-small-squad-qag`
2023-12-15 00:34:16 INFO     	 * Num of GPU in use: 1
2023-12-15 00:34:16 INFO     	 * Prefix: True
2023-12-15 00:34:16 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 00:34:16 INFO     dataset preprocessing
2023-12-15 00:34:17 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 00:34:18 INFO     start model training
2023-12-15 00:34:33 INFO     	 * (global step 50: loss: 0.3780455216765404, lr: 5e-05
2023-12-15 00:34:49 INFO     	 * (global step 100: loss: 0.4107731580734253, lr: 5e-05
2023-12-15 00:35:05 INFO     	 * (global step 150: loss: 0.3773842975497246, lr: 5e-05
2023-12-15 00:35:20 INFO     	 * (global step 200: loss: 0.3609277978539467, lr: 5e-05
2023-12-15 00:35:36 INFO     	 * (global step 250: loss: 0.45456424355506897, lr: 5e-05
2023-12-15 00:35:52 INFO     	 * (global step 300: loss: 0.49351224303245544, lr: 5e-05
2023-12-15 00:35:55 INFO     [epoch 0/15] average loss: 0.415, lr: 5e-05
2023-12-15 00:35:55 INFO     saving model related files
2023-12-15 00:35:55 INFO     saving model
2023-12-15 00:35:56 INFO     saving tokenizer
2023-12-15 00:35:56 INFO     saving optimizer
2023-12-15 00:35:57 INFO     remove old optimizer files
2023-12-15 00:36:10 INFO     	 * (global step 350: loss: 0.36770598217844963, lr: 5e-05
2023-12-15 00:36:25 INFO     	 * (global step 400: loss: 0.35949231684207916, lr: 5e-05
2023-12-15 00:36:41 INFO     	 * (global step 450: loss: 0.4779057167470455, lr: 5e-05
2023-12-15 00:36:57 INFO     	 * (global step 500: loss: 0.4729894623160362, lr: 5e-05
2023-12-15 00:37:12 INFO     	 * (global step 550: loss: 0.4967128336429596, lr: 5e-05
2023-12-15 00:37:28 INFO     	 * (global step 600: loss: 0.4300440363585949, lr: 5e-05
2023-12-15 00:37:34 INFO     [epoch 1/15] average loss: 0.388, lr: 5e-05
2023-12-15 00:37:34 INFO     saving model related files
2023-12-15 00:37:34 INFO     saving model
2023-12-15 00:37:35 INFO     saving tokenizer
2023-12-15 00:37:35 INFO     saving optimizer
2023-12-15 00:37:36 INFO     remove old optimizer files
2023-12-15 00:37:46 INFO     	 * (global step 650: loss: 0.35603807494044304, lr: 5e-05
2023-12-15 00:38:01 INFO     	 * (global step 700: loss: 0.31962211430072784, lr: 5e-05
2023-12-15 00:38:17 INFO     	 * (global step 750: loss: 0.3377528116106987, lr: 5e-05
2023-12-15 00:38:32 INFO     	 * (global step 800: loss: 0.3568972498178482, lr: 5e-05
2023-12-15 00:38:48 INFO     	 * (global step 850: loss: 0.5064076408743858, lr: 5e-05
2023-12-15 00:39:04 INFO     	 * (global step 900: loss: 0.502933606505394, lr: 5e-05
2023-12-15 00:39:13 INFO     [epoch 2/15] average loss: 0.375, lr: 5e-05
2023-12-15 00:39:13 INFO     saving model related files
2023-12-15 00:39:13 INFO     saving model
2023-12-15 00:39:14 INFO     saving tokenizer
2023-12-15 00:39:14 INFO     saving optimizer
2023-12-15 00:39:15 INFO     remove old optimizer files
2023-12-15 00:39:21 INFO     	 * (global step 950: loss: 0.46155864745378494, lr: 5e-05
2023-12-15 00:39:37 INFO     	 * (global step 1000: loss: 0.35418394953012466, lr: 5e-05
2023-12-15 00:39:53 INFO     	 * (global step 1050: loss: 0.3593408092856407, lr: 5e-05
2023-12-15 00:40:09 INFO     	 * (global step 1100: loss: 0.31431515514850616, lr: 5e-05
2023-12-15 00:40:24 INFO     	 * (global step 1150: loss: 0.2748367562890053, lr: 5e-05
2023-12-15 00:40:40 INFO     	 * (global step 1200: loss: 0.3507280871272087, lr: 5e-05
2023-12-15 00:40:53 INFO     [epoch 3/15] average loss: 0.367, lr: 5e-05
2023-12-15 00:40:53 INFO     saving model related files
2023-12-15 00:40:53 INFO     saving model
2023-12-15 00:40:53 INFO     saving tokenizer
2023-12-15 00:40:53 INFO     saving optimizer
2023-12-15 00:40:54 INFO     remove old optimizer files
2023-12-15 00:40:57 INFO     	 * (global step 1250: loss: 0.3761179968714714, lr: 5e-05
2023-12-15 00:41:13 INFO     	 * (global step 1300: loss: 0.25526250526309013, lr: 5e-05
2023-12-15 00:41:29 INFO     	 * (global step 1350: loss: 0.3541148081421852, lr: 5e-05
2023-12-15 00:41:45 INFO     	 * (global step 1400: loss: 0.27936220169067383, lr: 5e-05
2023-12-15 00:42:00 INFO     	 * (global step 1450: loss: 0.31512853503227234, lr: 5e-05
2023-12-15 00:42:16 INFO     	 * (global step 1500: loss: 0.45651693269610405, lr: 5e-05
2023-12-15 00:42:32 INFO     	 * (global step 1550: loss: 0.320077084004879, lr: 5e-05
2023-12-15 00:42:32 INFO     [epoch 4/15] average loss: 0.36, lr: 5e-05
2023-12-15 00:42:32 INFO     saving model related files
2023-12-15 00:42:32 INFO     saving model
2023-12-15 00:42:33 INFO     saving tokenizer
2023-12-15 00:42:33 INFO     saving optimizer
2023-12-15 00:42:34 INFO     remove old optimizer files
2023-12-15 00:42:49 INFO     	 * (global step 1600: loss: 0.3088424578309059, lr: 5e-05
2023-12-15 00:43:05 INFO     	 * (global step 1650: loss: 0.3577736206352711, lr: 5e-05
2023-12-15 00:43:21 INFO     	 * (global step 1700: loss: 0.37511271238327026, lr: 5e-05
2023-12-15 00:43:36 INFO     	 * (global step 1750: loss: 0.2559734918177128, lr: 5e-05
2023-12-15 00:43:52 INFO     	 * (global step 1800: loss: 0.47647181153297424, lr: 5e-05
2023-12-15 00:44:08 INFO     	 * (global step 1850: loss: 0.32012697122991085, lr: 5e-05
2023-12-15 00:44:11 INFO     [epoch 5/15] average loss: 0.354, lr: 5e-05
2023-12-15 00:44:11 INFO     saving model related files
2023-12-15 00:44:11 INFO     saving model
2023-12-15 00:44:12 INFO     saving tokenizer
2023-12-15 00:44:12 INFO     saving optimizer
2023-12-15 00:44:13 INFO     remove old optimizer files
2023-12-15 00:44:25 INFO     	 * (global step 1900: loss: 0.35779913514852524, lr: 5e-05
2023-12-15 00:44:41 INFO     	 * (global step 1950: loss: 0.31898563727736473, lr: 5e-05
2023-12-15 00:44:57 INFO     	 * (global step 2000: loss: 0.4044246971607208, lr: 5e-05
2023-12-15 00:45:12 INFO     	 * (global step 2050: loss: 0.2467988096177578, lr: 5e-05
2023-12-15 00:45:28 INFO     	 * (global step 2100: loss: 0.32863330841064453, lr: 5e-05
2023-12-15 00:45:44 INFO     	 * (global step 2150: loss: 0.3660738095641136, lr: 5e-05
2023-12-15 00:45:50 INFO     [epoch 6/15] average loss: 0.348, lr: 5e-05
2023-12-15 00:45:50 INFO     saving model related files
2023-12-15 00:45:50 INFO     saving model
2023-12-15 00:45:51 INFO     saving tokenizer
2023-12-15 00:45:51 INFO     saving optimizer
2023-12-15 00:45:52 INFO     remove old optimizer files
2023-12-15 00:46:02 INFO     	 * (global step 2200: loss: 0.4806148111820221, lr: 5e-05
2023-12-15 00:46:17 INFO     	 * (global step 2250: loss: 0.503802478313446, lr: 5e-05
2023-12-15 00:46:33 INFO     	 * (global step 2300: loss: 0.304580919444561, lr: 5e-05
2023-12-15 00:46:49 INFO     	 * (global step 2350: loss: 0.3749650716781616, lr: 5e-05
2023-12-15 00:47:04 INFO     	 * (global step 2400: loss: 0.46525727212429047, lr: 5e-05
2023-12-15 00:47:20 INFO     	 * (global step 2450: loss: 0.30373828392475843, lr: 5e-05
2023-12-15 00:47:30 INFO     [epoch 7/15] average loss: 0.343, lr: 5e-05
2023-12-15 00:47:30 INFO     saving model related files
2023-12-15 00:47:30 INFO     saving model
2023-12-15 00:47:30 INFO     saving tokenizer
2023-12-15 00:47:30 INFO     saving optimizer
2023-12-15 00:47:31 INFO     remove old optimizer files
2023-12-15 00:47:38 INFO     	 * (global step 2500: loss: 0.4904075562953949, lr: 5e-05
2023-12-15 00:47:53 INFO     	 * (global step 2550: loss: 0.3049139752984047, lr: 5e-05
2023-12-15 00:48:09 INFO     	 * (global step 2600: loss: 0.43762294203042984, lr: 5e-05
2023-12-15 00:48:25 INFO     	 * (global step 2650: loss: 0.3380793258547783, lr: 5e-05
2023-12-15 00:48:40 INFO     	 * (global step 2700: loss: 0.36257416754961014, lr: 5e-05
2023-12-15 00:48:56 INFO     	 * (global step 2750: loss: 0.43518567830324173, lr: 5e-05
2023-12-15 00:49:09 INFO     [epoch 8/15] average loss: 0.338, lr: 5e-05
2023-12-15 00:49:09 INFO     saving model related files
2023-12-15 00:49:09 INFO     saving model
2023-12-15 00:49:09 INFO     saving tokenizer
2023-12-15 00:49:09 INFO     saving optimizer
2023-12-15 00:49:10 INFO     remove old optimizer files
2023-12-15 00:49:14 INFO     	 * (global step 2800: loss: 0.44579746574163437, lr: 5e-05
2023-12-15 00:49:29 INFO     	 * (global step 2850: loss: 0.3942920118570328, lr: 5e-05
2023-12-15 00:49:45 INFO     	 * (global step 2900: loss: 0.34988250210881233, lr: 5e-05
2023-12-15 00:50:01 INFO     	 * (global step 2950: loss: 0.36229629814624786, lr: 5e-05
2023-12-15 00:50:16 INFO     	 * (global step 3000: loss: 0.23466076329350471, lr: 5e-05
2023-12-15 00:50:32 INFO     	 * (global step 3050: loss: 0.37257495149970055, lr: 5e-05
2023-12-15 00:50:48 INFO     	 * (global step 3100: loss: 0.3467252403497696, lr: 5e-05
2023-12-15 00:50:48 INFO     [epoch 9/15] average loss: 0.333, lr: 5e-05
2023-12-15 00:50:48 INFO     saving model related files
2023-12-15 00:50:48 INFO     saving model
2023-12-15 00:50:49 INFO     saving tokenizer
2023-12-15 00:50:49 INFO     saving optimizer
2023-12-15 00:50:50 INFO     remove old optimizer files
2023-12-15 00:50:50 INFO     complete training: model ckpt was saved at small_finetuned_ckpt/model_sdkaaa
2023-12-15 00:50:50 INFO     ## 1st RUN: Configuration 7/12 ##
2023-12-15 00:50:50 INFO     initialize model trainer
2023-12-15 00:50:50 INFO     initialize checkpoint at small_finetuned_ckpt/model_uramvg
2023-12-15 00:50:50 INFO     hyperparameters
2023-12-15 00:50:50 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 00:50:50 INFO     	 * dataset_name: default
2023-12-15 00:50:50 INFO     	 * input_types: ['paragraph']
2023-12-15 00:50:50 INFO     	 * output_types: ['questions_answers']
2023-12-15 00:50:50 INFO     	 * prefix_types: ['qag']
2023-12-15 00:50:50 INFO     	 * model: lmqg/t5-small-squad-qag
2023-12-15 00:50:50 INFO     	 * max_length: 512
2023-12-15 00:50:50 INFO     	 * max_length_output: 512
2023-12-15 00:50:50 INFO     	 * epoch: 15
2023-12-15 00:50:50 INFO     	 * batch: 2
2023-12-15 00:50:50 INFO     	 * lr: 5e-05
2023-12-15 00:50:50 INFO     	 * fp16: False
2023-12-15 00:50:50 INFO     	 * random_seed: 1
2023-12-15 00:50:50 INFO     	 * gradient_accumulation_steps: 2
2023-12-15 00:50:50 INFO     	 * label_smoothing: 0.0
2023-12-15 00:50:50 INFO     initialize checkpoint with lmqg/t5-small-squad-qag
2023-12-15 00:50:51 INFO     use spaCy answer extraction model: positionrank
2023-12-15 00:50:52 INFO     Model `lmqg/t5-small-squad-qag`
2023-12-15 00:50:52 INFO     	 * Num of GPU in use: 1
2023-12-15 00:50:52 INFO     	 * Prefix: True
2023-12-15 00:50:52 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 00:50:52 INFO     dataset preprocessing
2023-12-15 00:50:53 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 00:50:53 INFO     start model training
2023-12-15 00:51:02 INFO     	 * (global step 50: loss: 0.41809624433517456, lr: 5e-05
2023-12-15 00:51:10 INFO     	 * (global step 100: loss: 0.286358967423439, lr: 5e-05
2023-12-15 00:51:18 INFO     	 * (global step 150: loss: 0.3721126765012741, lr: 5e-05
2023-12-15 00:51:26 INFO     	 * (global step 200: loss: 0.40180808305740356, lr: 5e-05
2023-12-15 00:51:34 INFO     	 * (global step 250: loss: 0.42627498507499695, lr: 5e-05
2023-12-15 00:51:42 INFO     	 * (global step 300: loss: 0.3639523983001709, lr: 5e-05
2023-12-15 00:51:51 INFO     	 * (global step 350: loss: 0.5841566026210785, lr: 5e-05
2023-12-15 00:51:59 INFO     	 * (global step 400: loss: 0.34957315027713776, lr: 5e-05
2023-12-15 00:52:07 INFO     	 * (global step 450: loss: 0.29910197854042053, lr: 5e-05
2023-12-15 00:52:15 INFO     	 * (global step 500: loss: 0.4229959100484848, lr: 5e-05
2023-12-15 00:52:23 INFO     	 * (global step 550: loss: 0.3039970248937607, lr: 5e-05
2023-12-15 00:52:31 INFO     	 * (global step 600: loss: 0.6186522096395493, lr: 5e-05
2023-12-15 00:52:35 INFO     [epoch 0/15] average loss: 0.409, lr: 5e-05
2023-12-15 00:52:35 INFO     saving model related files
2023-12-15 00:52:35 INFO     saving model
2023-12-15 00:52:35 INFO     saving tokenizer
2023-12-15 00:52:35 INFO     saving optimizer
2023-12-15 00:52:36 INFO     remove old optimizer files
2023-12-15 00:52:41 INFO     	 * (global step 650: loss: 0.44917337596416473, lr: 5e-05
2023-12-15 00:52:49 INFO     	 * (global step 700: loss: 0.4115673750638962, lr: 5e-05
2023-12-15 00:52:57 INFO     	 * (global step 750: loss: 0.325979083776474, lr: 5e-05
2023-12-15 00:53:05 INFO     	 * (global step 800: loss: 0.38475118577480316, lr: 5e-05
2023-12-15 00:53:13 INFO     	 * (global step 850: loss: 0.6454284638166428, lr: 5e-05
2023-12-15 00:53:21 INFO     	 * (global step 900: loss: 0.49665357917547226, lr: 5e-05
2023-12-15 00:53:29 INFO     	 * (global step 950: loss: 0.3418770581483841, lr: 5e-05
2023-12-15 00:53:37 INFO     	 * (global step 1000: loss: 0.6004842519760132, lr: 5e-05
2023-12-15 00:53:46 INFO     	 * (global step 1050: loss: 0.34437231719493866, lr: 5e-05
2023-12-15 00:53:54 INFO     	 * (global step 1100: loss: 0.4652298390865326, lr: 5e-05
2023-12-15 00:54:02 INFO     	 * (global step 1150: loss: 0.23874156922101974, lr: 5e-05
2023-12-15 00:54:10 INFO     	 * (global step 1200: loss: 0.5884462594985962, lr: 5e-05
2023-12-15 00:54:17 INFO     [epoch 1/15] average loss: 0.382, lr: 5e-05
2023-12-15 00:54:17 INFO     saving model related files
2023-12-15 00:54:17 INFO     saving model
2023-12-15 00:54:17 INFO     saving tokenizer
2023-12-15 00:54:17 INFO     saving optimizer
2023-12-15 00:54:19 INFO     remove old optimizer files
2023-12-15 00:54:20 INFO     	 * (global step 1250: loss: 0.4601747542619705, lr: 5e-05
2023-12-15 00:54:28 INFO     	 * (global step 1300: loss: 0.2657741904258728, lr: 5e-05
2023-12-15 00:54:36 INFO     	 * (global step 1350: loss: 0.17852959036827087, lr: 5e-05
2023-12-15 00:54:44 INFO     	 * (global step 1400: loss: 0.46040020883083344, lr: 5e-05
2023-12-15 00:54:52 INFO     	 * (global step 1450: loss: 0.3924756199121475, lr: 5e-05
2023-12-15 00:55:01 INFO     	 * (global step 1500: loss: 0.23990752547979355, lr: 5e-05
2023-12-15 00:55:09 INFO     	 * (global step 1550: loss: 0.30334699898958206, lr: 5e-05
2023-12-15 00:55:17 INFO     	 * (global step 1600: loss: 0.3210824429988861, lr: 5e-05
2023-12-15 00:55:25 INFO     	 * (global step 1650: loss: 0.32976357638835907, lr: 5e-05
2023-12-15 00:55:33 INFO     	 * (global step 1700: loss: 0.3249228000640869, lr: 5e-05
2023-12-15 00:55:41 INFO     	 * (global step 1750: loss: 0.2876528948545456, lr: 5e-05
2023-12-15 00:55:49 INFO     	 * (global step 1800: loss: 0.38115377724170685, lr: 5e-05
2023-12-15 00:55:57 INFO     	 * (global step 1850: loss: 0.2995053753256798, lr: 5e-05
2023-12-15 00:55:59 INFO     [epoch 2/15] average loss: 0.368, lr: 5e-05
2023-12-15 00:55:59 INFO     saving model related files
2023-12-15 00:55:59 INFO     saving model
2023-12-15 00:56:00 INFO     saving tokenizer
2023-12-15 00:56:00 INFO     saving optimizer
2023-12-15 00:56:01 INFO     remove old optimizer files
2023-12-15 00:56:07 INFO     	 * (global step 1900: loss: 0.23196668177843094, lr: 5e-05
2023-12-15 00:56:15 INFO     	 * (global step 1950: loss: 0.301974892616272, lr: 5e-05
2023-12-15 00:56:23 INFO     	 * (global step 2000: loss: 0.42038482427597046, lr: 5e-05
2023-12-15 00:56:32 INFO     	 * (global step 2050: loss: 0.46615874767303467, lr: 5e-05
2023-12-15 00:56:40 INFO     	 * (global step 2100: loss: 0.33964142203330994, lr: 5e-05
2023-12-15 00:56:48 INFO     	 * (global step 2150: loss: 0.31045982986688614, lr: 5e-05
2023-12-15 00:56:56 INFO     	 * (global step 2200: loss: 0.4163629412651062, lr: 5e-05
2023-12-15 00:57:04 INFO     	 * (global step 2250: loss: 0.37272024154663086, lr: 5e-05
2023-12-15 00:57:12 INFO     	 * (global step 2300: loss: 0.3831852823495865, lr: 5e-05
2023-12-15 00:57:20 INFO     	 * (global step 2350: loss: 0.35348306596279144, lr: 5e-05
2023-12-15 00:57:29 INFO     	 * (global step 2400: loss: 0.4610773175954819, lr: 5e-05
2023-12-15 00:57:37 INFO     	 * (global step 2450: loss: 0.31234708428382874, lr: 5e-05
2023-12-15 00:57:42 INFO     [epoch 3/15] average loss: 0.359, lr: 5e-05
2023-12-15 00:57:42 INFO     saving model related files
2023-12-15 00:57:42 INFO     saving model
2023-12-15 00:57:43 INFO     saving tokenizer
2023-12-15 00:57:43 INFO     saving optimizer
2023-12-15 00:57:44 INFO     remove old optimizer files
2023-12-15 00:57:46 INFO     	 * (global step 2500: loss: 0.2958718091249466, lr: 5e-05
2023-12-15 00:57:55 INFO     	 * (global step 2550: loss: 0.2659468948841095, lr: 5e-05
2023-12-15 00:58:03 INFO     	 * (global step 2600: loss: 0.5164542347192764, lr: 5e-05
2023-12-15 00:58:11 INFO     	 * (global step 2650: loss: 0.4137146472930908, lr: 5e-05
2023-12-15 00:58:19 INFO     	 * (global step 2700: loss: 0.29279883205890656, lr: 5e-05
2023-12-15 00:58:27 INFO     	 * (global step 2750: loss: 0.34371304512023926, lr: 5e-05
2023-12-15 00:58:35 INFO     	 * (global step 2800: loss: 0.6325331926345825, lr: 5e-05
2023-12-15 00:58:43 INFO     	 * (global step 2850: loss: 0.3882659524679184, lr: 5e-05
2023-12-15 00:58:51 INFO     	 * (global step 2900: loss: 0.22958653420209885, lr: 5e-05
2023-12-15 00:59:00 INFO     	 * (global step 2950: loss: 0.32017071545124054, lr: 5e-05
2023-12-15 00:59:08 INFO     	 * (global step 3000: loss: 0.4632372111082077, lr: 5e-05
2023-12-15 00:59:16 INFO     	 * (global step 3050: loss: 0.2926095426082611, lr: 5e-05
2023-12-15 00:59:24 INFO     	 * (global step 3100: loss: 0.29308387637138367, lr: 5e-05
2023-12-15 00:59:25 INFO     [epoch 4/15] average loss: 0.35, lr: 5e-05
2023-12-15 00:59:25 INFO     saving model related files
2023-12-15 00:59:25 INFO     saving model
2023-12-15 00:59:25 INFO     saving tokenizer
2023-12-15 00:59:25 INFO     saving optimizer
2023-12-15 00:59:26 INFO     remove old optimizer files
2023-12-15 00:59:34 INFO     	 * (global step 3150: loss: 0.3738354817032814, lr: 5e-05
2023-12-15 00:59:42 INFO     	 * (global step 3200: loss: 0.4306213855743408, lr: 5e-05
2023-12-15 00:59:50 INFO     	 * (global step 3250: loss: 0.40026482194662094, lr: 5e-05
2023-12-15 00:59:58 INFO     	 * (global step 3300: loss: 0.3683840185403824, lr: 5e-05
2023-12-15 01:00:06 INFO     	 * (global step 3350: loss: 0.36822739243507385, lr: 5e-05
2023-12-15 01:00:14 INFO     	 * (global step 3400: loss: 0.3489926606416702, lr: 5e-05
2023-12-15 01:00:22 INFO     	 * (global step 3450: loss: 0.3220568150281906, lr: 5e-05
2023-12-15 01:00:30 INFO     	 * (global step 3500: loss: 0.4208225756883621, lr: 5e-05
2023-12-15 01:00:38 INFO     	 * (global step 3550: loss: 0.31956426054239273, lr: 5e-05
2023-12-15 01:00:46 INFO     	 * (global step 3600: loss: 0.3720497339963913, lr: 5e-05
2023-12-15 01:00:55 INFO     	 * (global step 3650: loss: 0.5271793976426125, lr: 5e-05
2023-12-15 01:01:03 INFO     	 * (global step 3700: loss: 0.5054533332586288, lr: 5e-05
2023-12-15 01:01:07 INFO     [epoch 5/15] average loss: 0.344, lr: 5e-05
2023-12-15 01:01:07 INFO     saving model related files
2023-12-15 01:01:07 INFO     saving model
2023-12-15 01:01:07 INFO     saving tokenizer
2023-12-15 01:01:08 INFO     saving optimizer
2023-12-15 01:01:09 INFO     remove old optimizer files
2023-12-15 01:01:13 INFO     	 * (global step 3750: loss: 0.30836255848407745, lr: 5e-05
2023-12-15 01:01:21 INFO     	 * (global step 3800: loss: 0.4156266003847122, lr: 5e-05
2023-12-15 01:01:29 INFO     	 * (global step 3850: loss: 0.3440245985984802, lr: 5e-05
2023-12-15 01:01:37 INFO     	 * (global step 3900: loss: 0.31694532185792923, lr: 5e-05
2023-12-15 01:01:45 INFO     	 * (global step 3950: loss: 0.504056915640831, lr: 5e-05
2023-12-15 01:01:53 INFO     	 * (global step 4000: loss: 0.389311283826828, lr: 5e-05
2023-12-15 01:02:01 INFO     	 * (global step 4050: loss: 0.2757980301976204, lr: 5e-05
2023-12-15 01:02:09 INFO     	 * (global step 4100: loss: 0.3595430552959442, lr: 5e-05
2023-12-15 01:02:18 INFO     	 * (global step 4150: loss: 0.24142248183488846, lr: 5e-05
2023-12-15 01:02:26 INFO     	 * (global step 4200: loss: 0.443401575088501, lr: 5e-05
2023-12-15 01:02:34 INFO     	 * (global step 4250: loss: 0.28903647512197495, lr: 5e-05
2023-12-15 01:02:42 INFO     	 * (global step 4300: loss: 0.39994125068187714, lr: 5e-05
2023-12-15 01:02:50 INFO     [epoch 6/15] average loss: 0.336, lr: 5e-05
2023-12-15 01:02:50 INFO     saving model related files
2023-12-15 01:02:50 INFO     saving model
2023-12-15 01:02:50 INFO     saving tokenizer
2023-12-15 01:02:50 INFO     saving optimizer
2023-12-15 01:02:51 INFO     remove old optimizer files
2023-12-15 01:02:52 INFO     	 * (global step 4350: loss: 0.42879149317741394, lr: 5e-05
2023-12-15 01:03:00 INFO     	 * (global step 4400: loss: 0.26480528712272644, lr: 5e-05
2023-12-15 01:03:08 INFO     	 * (global step 4450: loss: 0.41867993772029877, lr: 5e-05
2023-12-15 01:03:16 INFO     	 * (global step 4500: loss: 0.3098124563694, lr: 5e-05
2023-12-15 01:03:24 INFO     	 * (global step 4550: loss: 0.33236537873744965, lr: 5e-05
2023-12-15 01:03:32 INFO     	 * (global step 4600: loss: 0.3419768214225769, lr: 5e-05
2023-12-15 01:03:40 INFO     	 * (global step 4650: loss: 0.45855462551116943, lr: 5e-05
2023-12-15 01:03:49 INFO     	 * (global step 4700: loss: 0.2821419835090637, lr: 5e-05
2023-12-15 01:03:57 INFO     	 * (global step 4750: loss: 0.3003136143088341, lr: 5e-05
2023-12-15 01:04:05 INFO     	 * (global step 4800: loss: 0.3169853463768959, lr: 5e-05
2023-12-15 01:04:13 INFO     	 * (global step 4850: loss: 0.32572511583566666, lr: 5e-05
2023-12-15 01:04:21 INFO     	 * (global step 4900: loss: 0.4023936241865158, lr: 5e-05
2023-12-15 01:04:29 INFO     	 * (global step 4950: loss: 0.40535736083984375, lr: 5e-05
2023-12-15 01:04:32 INFO     [epoch 7/15] average loss: 0.33, lr: 5e-05
2023-12-15 01:04:32 INFO     saving model related files
2023-12-15 01:04:32 INFO     saving model
2023-12-15 01:04:33 INFO     saving tokenizer
2023-12-15 01:04:33 INFO     saving optimizer
2023-12-15 01:04:34 INFO     remove old optimizer files
2023-12-15 01:04:39 INFO     	 * (global step 5000: loss: 0.22356399148702621, lr: 5e-05
2023-12-15 01:04:47 INFO     	 * (global step 5050: loss: 0.36298857629299164, lr: 5e-05
2023-12-15 01:04:55 INFO     	 * (global step 5100: loss: 0.2519294396042824, lr: 5e-05
2023-12-15 01:05:03 INFO     	 * (global step 5150: loss: 0.19899775087833405, lr: 5e-05
2023-12-15 01:05:11 INFO     	 * (global step 5200: loss: 0.3797121047973633, lr: 5e-05
2023-12-15 01:05:19 INFO     	 * (global step 5250: loss: 0.3632555678486824, lr: 5e-05
2023-12-15 01:05:28 INFO     	 * (global step 5300: loss: 0.25891710072755814, lr: 5e-05
2023-12-15 01:05:36 INFO     	 * (global step 5350: loss: 0.3383786529302597, lr: 5e-05
2023-12-15 01:05:44 INFO     	 * (global step 5400: loss: 0.5770666301250458, lr: 5e-05
2023-12-15 01:05:52 INFO     	 * (global step 5450: loss: 0.2457612007856369, lr: 5e-05
2023-12-15 01:06:00 INFO     	 * (global step 5500: loss: 0.41919924318790436, lr: 5e-05
2023-12-15 01:06:08 INFO     	 * (global step 5550: loss: 0.23570477217435837, lr: 5e-05
2023-12-15 01:06:15 INFO     [epoch 8/15] average loss: 0.325, lr: 5e-05
2023-12-15 01:06:15 INFO     saving model related files
2023-12-15 01:06:15 INFO     saving model
2023-12-15 01:06:15 INFO     saving tokenizer
2023-12-15 01:06:15 INFO     saving optimizer
2023-12-15 01:06:16 INFO     remove old optimizer files
2023-12-15 01:06:18 INFO     	 * (global step 5600: loss: 0.3780443146824837, lr: 5e-05
2023-12-15 01:06:26 INFO     	 * (global step 5650: loss: 0.3909812569618225, lr: 5e-05
2023-12-15 01:06:34 INFO     	 * (global step 5700: loss: 0.3272462785243988, lr: 5e-05
2023-12-15 01:06:42 INFO     	 * (global step 5750: loss: 0.2900751382112503, lr: 5e-05
2023-12-15 01:06:51 INFO     	 * (global step 5800: loss: 0.25181426852941513, lr: 5e-05
2023-12-15 01:06:59 INFO     	 * (global step 5850: loss: 0.4997807741165161, lr: 5e-05
2023-12-15 01:07:07 INFO     	 * (global step 5900: loss: 0.26398860663175583, lr: 5e-05
2023-12-15 01:07:15 INFO     	 * (global step 5950: loss: 0.3629453480243683, lr: 5e-05
2023-12-15 01:07:23 INFO     	 * (global step 6000: loss: 0.286574587225914, lr: 5e-05
2023-12-15 01:07:31 INFO     	 * (global step 6050: loss: 0.28656964749097824, lr: 5e-05
2023-12-15 01:07:39 INFO     	 * (global step 6100: loss: 0.39327672123908997, lr: 5e-05
2023-12-15 01:07:47 INFO     	 * (global step 6150: loss: 0.29808489978313446, lr: 5e-05
2023-12-15 01:07:55 INFO     	 * (global step 6200: loss: 0.4662894755601883, lr: 5e-05
2023-12-15 01:07:57 INFO     [epoch 9/15] average loss: 0.319, lr: 5e-05
2023-12-15 01:07:57 INFO     saving model related files
2023-12-15 01:07:57 INFO     saving model
2023-12-15 01:07:58 INFO     saving tokenizer
2023-12-15 01:07:58 INFO     saving optimizer
2023-12-15 01:07:59 INFO     remove old optimizer files
2023-12-15 01:07:59 INFO     complete training: model ckpt was saved at small_finetuned_ckpt/model_uramvg
2023-12-15 01:07:59 INFO     ## 1st RUN: Configuration 8/12 ##
2023-12-15 01:07:59 INFO     initialize model trainer
2023-12-15 01:07:59 INFO     initialize checkpoint at small_finetuned_ckpt/model_nxaqhy
2023-12-15 01:07:59 INFO     hyperparameters
2023-12-15 01:07:59 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 01:07:59 INFO     	 * dataset_name: default
2023-12-15 01:07:59 INFO     	 * input_types: ['paragraph']
2023-12-15 01:07:59 INFO     	 * output_types: ['questions_answers']
2023-12-15 01:07:59 INFO     	 * prefix_types: ['qag']
2023-12-15 01:07:59 INFO     	 * model: lmqg/t5-small-squad-qag
2023-12-15 01:07:59 INFO     	 * max_length: 512
2023-12-15 01:07:59 INFO     	 * max_length_output: 512
2023-12-15 01:07:59 INFO     	 * epoch: 15
2023-12-15 01:07:59 INFO     	 * batch: 2
2023-12-15 01:07:59 INFO     	 * lr: 1e-05
2023-12-15 01:07:59 INFO     	 * fp16: False
2023-12-15 01:07:59 INFO     	 * random_seed: 1
2023-12-15 01:07:59 INFO     	 * gradient_accumulation_steps: 4
2023-12-15 01:07:59 INFO     	 * label_smoothing: 0.15
2023-12-15 01:07:59 INFO     initialize checkpoint with lmqg/t5-small-squad-qag
2023-12-15 01:08:00 INFO     use spaCy answer extraction model: positionrank
2023-12-15 01:08:01 INFO     Model `lmqg/t5-small-squad-qag`
2023-12-15 01:08:01 INFO     	 * Num of GPU in use: 1
2023-12-15 01:08:01 INFO     	 * Prefix: True
2023-12-15 01:08:01 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 01:08:01 INFO     dataset preprocessing
2023-12-15 01:08:02 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 01:08:02 INFO     start model training
2023-12-15 01:08:18 INFO     	 * (global step 50: loss: 0.4174450859427452, lr: 1e-05
2023-12-15 01:08:34 INFO     	 * (global step 100: loss: 0.44526612013578415, lr: 1e-05
2023-12-15 01:08:49 INFO     	 * (global step 150: loss: 0.4049782007932663, lr: 1e-05
2023-12-15 01:09:05 INFO     	 * (global step 200: loss: 0.3935903534293175, lr: 1e-05
2023-12-15 01:09:21 INFO     	 * (global step 250: loss: 0.48380047082901, lr: 1e-05
2023-12-15 01:09:36 INFO     	 * (global step 300: loss: 0.5217370912432671, lr: 1e-05
2023-12-15 01:09:40 INFO     [epoch 0/15] average loss: 0.447, lr: 1e-05
2023-12-15 01:09:40 INFO     saving model related files
2023-12-15 01:09:40 INFO     saving model
2023-12-15 01:09:40 INFO     saving tokenizer
2023-12-15 01:09:40 INFO     saving optimizer
2023-12-15 01:09:41 INFO     remove old optimizer files
2023-12-15 01:09:54 INFO     	 * (global step 350: loss: 0.38673805817961693, lr: 1e-05
2023-12-15 01:10:10 INFO     	 * (global step 400: loss: 0.3854989558458328, lr: 1e-05
2023-12-15 01:10:25 INFO     	 * (global step 450: loss: 0.5139632523059845, lr: 1e-05
2023-12-15 01:10:41 INFO     	 * (global step 500: loss: 0.5006912723183632, lr: 1e-05
2023-12-15 01:10:57 INFO     	 * (global step 550: loss: 0.5277958959341049, lr: 1e-05
2023-12-15 01:11:12 INFO     	 * (global step 600: loss: 0.45590126514434814, lr: 1e-05
2023-12-15 01:11:19 INFO     [epoch 1/15] average loss: 0.415, lr: 1e-05
2023-12-15 01:11:19 INFO     saving model related files
2023-12-15 01:11:19 INFO     saving model
2023-12-15 01:11:19 INFO     saving tokenizer
2023-12-15 01:11:19 INFO     saving optimizer
2023-12-15 01:11:20 INFO     remove old optimizer files
2023-12-15 01:11:30 INFO     	 * (global step 650: loss: 0.3872021660208702, lr: 1e-05
2023-12-15 01:11:46 INFO     	 * (global step 700: loss: 0.3514437712728977, lr: 1e-05
2023-12-15 01:12:01 INFO     	 * (global step 750: loss: 0.36614473164081573, lr: 1e-05
2023-12-15 01:12:17 INFO     	 * (global step 800: loss: 0.38412441313266754, lr: 1e-05
2023-12-15 01:12:33 INFO     	 * (global step 850: loss: 0.5429331362247467, lr: 1e-05
2023-12-15 01:12:48 INFO     	 * (global step 900: loss: 0.541653111577034, lr: 1e-05
2023-12-15 01:12:58 INFO     [epoch 2/15] average loss: 0.405, lr: 1e-05
2023-12-15 01:12:58 INFO     saving model related files
2023-12-15 01:12:58 INFO     saving model
2023-12-15 01:12:58 INFO     saving tokenizer
2023-12-15 01:12:58 INFO     saving optimizer
2023-12-15 01:12:59 INFO     remove old optimizer files
2023-12-15 01:13:06 INFO     	 * (global step 950: loss: 0.5032149404287338, lr: 1e-05
2023-12-15 01:13:21 INFO     	 * (global step 1000: loss: 0.3875398226082325, lr: 1e-05
2023-12-15 01:13:37 INFO     	 * (global step 1050: loss: 0.3773440942168236, lr: 1e-05
2023-12-15 01:13:53 INFO     	 * (global step 1100: loss: 0.33245617151260376, lr: 1e-05
2023-12-15 01:14:09 INFO     	 * (global step 1150: loss: 0.3028443232178688, lr: 1e-05
2023-12-15 01:14:24 INFO     	 * (global step 1200: loss: 0.37584253400564194, lr: 1e-05
2023-12-15 01:14:37 INFO     [epoch 3/15] average loss: 0.399, lr: 1e-05
2023-12-15 01:14:37 INFO     saving model related files
2023-12-15 01:14:37 INFO     saving model
2023-12-15 01:14:38 INFO     saving tokenizer
2023-12-15 01:14:38 INFO     saving optimizer
2023-12-15 01:14:39 INFO     remove old optimizer files
2023-12-15 01:14:42 INFO     	 * (global step 1250: loss: 0.40696991980075836, lr: 1e-05
2023-12-15 01:14:57 INFO     	 * (global step 1300: loss: 0.2805713154375553, lr: 1e-05
2023-12-15 01:15:13 INFO     	 * (global step 1350: loss: 0.3866093009710312, lr: 1e-05
2023-12-15 01:15:29 INFO     	 * (global step 1400: loss: 0.30510955303907394, lr: 1e-05
2023-12-15 01:15:45 INFO     	 * (global step 1450: loss: 0.34071657061576843, lr: 1e-05
2023-12-15 01:16:00 INFO     	 * (global step 1500: loss: 0.5018950253725052, lr: 1e-05
2023-12-15 01:16:16 INFO     	 * (global step 1550: loss: 0.3489021733403206, lr: 1e-05
2023-12-15 01:16:16 INFO     [epoch 4/15] average loss: 0.394, lr: 1e-05
2023-12-15 01:16:16 INFO     saving model related files
2023-12-15 01:16:16 INFO     saving model
2023-12-15 01:16:17 INFO     saving tokenizer
2023-12-15 01:16:17 INFO     saving optimizer
2023-12-15 01:16:18 INFO     remove old optimizer files
2023-12-15 01:16:33 INFO     	 * (global step 1600: loss: 0.33800070360302925, lr: 1e-05
2023-12-15 01:16:49 INFO     	 * (global step 1650: loss: 0.3891017585992813, lr: 1e-05
2023-12-15 01:17:05 INFO     	 * (global step 1700: loss: 0.4171433225274086, lr: 1e-05
2023-12-15 01:17:20 INFO     	 * (global step 1750: loss: 0.28159792721271515, lr: 1e-05
2023-12-15 01:17:36 INFO     	 * (global step 1800: loss: 0.5299264900386333, lr: 1e-05
2023-12-15 01:17:52 INFO     	 * (global step 1850: loss: 0.3575959224253893, lr: 1e-05
2023-12-15 01:17:55 INFO     [epoch 5/15] average loss: 0.391, lr: 1e-05
2023-12-15 01:17:55 INFO     saving model related files
2023-12-15 01:17:55 INFO     saving model
2023-12-15 01:17:56 INFO     saving tokenizer
2023-12-15 01:17:56 INFO     saving optimizer
2023-12-15 01:17:57 INFO     remove old optimizer files
2023-12-15 01:18:09 INFO     	 * (global step 1900: loss: 0.39507728070020676, lr: 1e-05
2023-12-15 01:18:25 INFO     	 * (global step 1950: loss: 0.3534868694841862, lr: 1e-05
2023-12-15 01:18:41 INFO     	 * (global step 2000: loss: 0.444861464202404, lr: 1e-05
2023-12-15 01:18:56 INFO     	 * (global step 2050: loss: 0.2787151299417019, lr: 1e-05
2023-12-15 01:19:12 INFO     	 * (global step 2100: loss: 0.3575834184885025, lr: 1e-05
2023-12-15 01:19:28 INFO     	 * (global step 2150: loss: 0.4207726866006851, lr: 1e-05
2023-12-15 01:19:34 INFO     [epoch 6/15] average loss: 0.387, lr: 1e-05
2023-12-15 01:19:34 INFO     saving model related files
2023-12-15 01:19:34 INFO     saving model
2023-12-15 01:19:35 INFO     saving tokenizer
2023-12-15 01:19:35 INFO     saving optimizer
2023-12-15 01:19:36 INFO     remove old optimizer files
2023-12-15 01:19:45 INFO     	 * (global step 2200: loss: 0.5402817651629448, lr: 1e-05
2023-12-15 01:20:01 INFO     	 * (global step 2250: loss: 0.5675267577171326, lr: 1e-05
2023-12-15 01:20:16 INFO     	 * (global step 2300: loss: 0.3388836458325386, lr: 1e-05
2023-12-15 01:20:32 INFO     	 * (global step 2350: loss: 0.4156746566295624, lr: 1e-05
2023-12-15 01:20:48 INFO     	 * (global step 2400: loss: 0.5255554914474487, lr: 1e-05
2023-12-15 01:21:03 INFO     	 * (global step 2450: loss: 0.32990221958607435, lr: 1e-05
2023-12-15 01:21:13 INFO     [epoch 7/15] average loss: 0.384, lr: 1e-05
2023-12-15 01:21:13 INFO     saving model related files
2023-12-15 01:21:13 INFO     saving model
2023-12-15 01:21:13 INFO     saving tokenizer
2023-12-15 01:21:14 INFO     saving optimizer
2023-12-15 01:21:15 INFO     remove old optimizer files
2023-12-15 01:21:21 INFO     	 * (global step 2500: loss: 0.56254892796278, lr: 1e-05
2023-12-15 01:21:37 INFO     	 * (global step 2550: loss: 0.3410922884941101, lr: 1e-05
2023-12-15 01:21:52 INFO     	 * (global step 2600: loss: 0.499522402882576, lr: 1e-05
2023-12-15 01:22:08 INFO     	 * (global step 2650: loss: 0.3756989538669586, lr: 1e-05
2023-12-15 01:22:24 INFO     	 * (global step 2700: loss: 0.4060283377766609, lr: 1e-05
2023-12-15 01:22:39 INFO     	 * (global step 2750: loss: 0.48508820682764053, lr: 1e-05
2023-12-15 01:22:52 INFO     [epoch 8/15] average loss: 0.381, lr: 1e-05
2023-12-15 01:22:52 INFO     saving model related files
2023-12-15 01:22:52 INFO     saving model
2023-12-15 01:22:53 INFO     saving tokenizer
2023-12-15 01:22:53 INFO     saving optimizer
2023-12-15 01:22:54 INFO     remove old optimizer files
2023-12-15 01:22:57 INFO     	 * (global step 2800: loss: 0.496693879365921, lr: 1e-05
2023-12-15 01:23:13 INFO     	 * (global step 2850: loss: 0.4493448734283447, lr: 1e-05
2023-12-15 01:23:28 INFO     	 * (global step 2900: loss: 0.4004187509417534, lr: 1e-05
2023-12-15 01:23:44 INFO     	 * (global step 2950: loss: 0.4111032411456108, lr: 1e-05
2023-12-15 01:23:59 INFO     	 * (global step 3000: loss: 0.26008398085832596, lr: 1e-05
2023-12-15 01:24:15 INFO     	 * (global step 3050: loss: 0.41575707495212555, lr: 1e-05
2023-12-15 01:24:31 INFO     	 * (global step 3100: loss: 0.40677905082702637, lr: 1e-05
2023-12-15 01:24:31 INFO     [epoch 9/15] average loss: 0.378, lr: 1e-05
2023-12-15 01:24:31 INFO     saving model related files
2023-12-15 01:24:31 INFO     saving model
2023-12-15 01:24:32 INFO     saving tokenizer
2023-12-15 01:24:32 INFO     saving optimizer
2023-12-15 01:24:33 INFO     remove old optimizer files
2023-12-15 01:24:33 INFO     complete training: model ckpt was saved at small_finetuned_ckpt/model_nxaqhy
2023-12-15 01:24:33 INFO     ## 1st RUN: Configuration 9/12 ##
2023-12-15 01:24:33 INFO     initialize model trainer
2023-12-15 01:24:33 INFO     initialize checkpoint at small_finetuned_ckpt/model_oprhlh
2023-12-15 01:24:33 INFO     hyperparameters
2023-12-15 01:24:33 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 01:24:33 INFO     	 * dataset_name: default
2023-12-15 01:24:33 INFO     	 * input_types: ['paragraph']
2023-12-15 01:24:33 INFO     	 * output_types: ['questions_answers']
2023-12-15 01:24:33 INFO     	 * prefix_types: ['qag']
2023-12-15 01:24:33 INFO     	 * model: lmqg/t5-small-squad-qag
2023-12-15 01:24:33 INFO     	 * max_length: 512
2023-12-15 01:24:33 INFO     	 * max_length_output: 512
2023-12-15 01:24:33 INFO     	 * epoch: 15
2023-12-15 01:24:33 INFO     	 * batch: 2
2023-12-15 01:24:33 INFO     	 * lr: 1e-05
2023-12-15 01:24:33 INFO     	 * fp16: False
2023-12-15 01:24:33 INFO     	 * random_seed: 1
2023-12-15 01:24:33 INFO     	 * gradient_accumulation_steps: 2
2023-12-15 01:24:33 INFO     	 * label_smoothing: 0.15
2023-12-15 01:24:33 INFO     initialize checkpoint with lmqg/t5-small-squad-qag
2023-12-15 01:24:34 INFO     use spaCy answer extraction model: positionrank
2023-12-15 01:24:35 INFO     Model `lmqg/t5-small-squad-qag`
2023-12-15 01:24:35 INFO     	 * Num of GPU in use: 1
2023-12-15 01:24:35 INFO     	 * Prefix: True
2023-12-15 01:24:35 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 01:24:35 INFO     dataset preprocessing
2023-12-15 01:24:36 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 01:24:37 INFO     start model training
2023-12-15 01:24:45 INFO     	 * (global step 50: loss: 0.4581184983253479, lr: 1e-05
2023-12-15 01:24:53 INFO     	 * (global step 100: loss: 0.31322166323661804, lr: 1e-05
2023-12-15 01:25:01 INFO     	 * (global step 150: loss: 0.4114130139350891, lr: 1e-05
2023-12-15 01:25:09 INFO     	 * (global step 200: loss: 0.4368223398923874, lr: 1e-05
2023-12-15 01:25:17 INFO     	 * (global step 250: loss: 0.45259372889995575, lr: 1e-05
2023-12-15 01:25:25 INFO     	 * (global step 300: loss: 0.3870916813611984, lr: 1e-05
2023-12-15 01:25:33 INFO     	 * (global step 350: loss: 0.6222658753395081, lr: 1e-05
2023-12-15 01:25:42 INFO     	 * (global step 400: loss: 0.3737747520208359, lr: 1e-05
2023-12-15 01:25:50 INFO     	 * (global step 450: loss: 0.3162582144141197, lr: 1e-05
2023-12-15 01:25:58 INFO     	 * (global step 500: loss: 0.44897033274173737, lr: 1e-05
2023-12-15 01:26:06 INFO     	 * (global step 550: loss: 0.3220909982919693, lr: 1e-05
2023-12-15 01:26:14 INFO     	 * (global step 600: loss: 0.6511140465736389, lr: 1e-05
2023-12-15 01:26:17 INFO     [epoch 0/15] average loss: 0.437, lr: 1e-05
2023-12-15 01:26:17 INFO     saving model related files
2023-12-15 01:26:17 INFO     saving model
2023-12-15 01:26:18 INFO     saving tokenizer
2023-12-15 01:26:18 INFO     saving optimizer
2023-12-15 01:26:19 INFO     remove old optimizer files
2023-12-15 01:26:24 INFO     	 * (global step 650: loss: 0.4795816093683243, lr: 1e-05
2023-12-15 01:26:32 INFO     	 * (global step 700: loss: 0.43261465430259705, lr: 1e-05
2023-12-15 01:26:40 INFO     	 * (global step 750: loss: 0.350529745221138, lr: 1e-05
2023-12-15 01:26:48 INFO     	 * (global step 800: loss: 0.4104063808917999, lr: 1e-05
2023-12-15 01:26:56 INFO     	 * (global step 850: loss: 0.6829293370246887, lr: 1e-05
2023-12-15 01:27:04 INFO     	 * (global step 900: loss: 0.5406392365694046, lr: 1e-05
2023-12-15 01:27:13 INFO     	 * (global step 950: loss: 0.3603086471557617, lr: 1e-05
2023-12-15 01:27:21 INFO     	 * (global step 1000: loss: 0.6313239485025406, lr: 1e-05
2023-12-15 01:27:29 INFO     	 * (global step 1050: loss: 0.3617406040430069, lr: 1e-05
2023-12-15 01:27:37 INFO     	 * (global step 1100: loss: 0.4810079038143158, lr: 1e-05
2023-12-15 01:27:45 INFO     	 * (global step 1150: loss: 0.25516095757484436, lr: 1e-05
2023-12-15 01:27:53 INFO     	 * (global step 1200: loss: 0.6278806626796722, lr: 1e-05
2023-12-15 01:28:00 INFO     [epoch 1/15] average loss: 0.408, lr: 1e-05
2023-12-15 01:28:00 INFO     saving model related files
2023-12-15 01:28:00 INFO     saving model
2023-12-15 01:28:01 INFO     saving tokenizer
2023-12-15 01:28:01 INFO     saving optimizer
2023-12-15 01:28:02 INFO     remove old optimizer files
2023-12-15 01:28:03 INFO     	 * (global step 1250: loss: 0.4877290427684784, lr: 1e-05
2023-12-15 01:28:11 INFO     	 * (global step 1300: loss: 0.29171570390462875, lr: 1e-05
2023-12-15 01:28:19 INFO     	 * (global step 1350: loss: 0.2023996114730835, lr: 1e-05
2023-12-15 01:28:27 INFO     	 * (global step 1400: loss: 0.489560142159462, lr: 1e-05
2023-12-15 01:28:36 INFO     	 * (global step 1450: loss: 0.42874380946159363, lr: 1e-05
2023-12-15 01:28:44 INFO     	 * (global step 1500: loss: 0.2634795233607292, lr: 1e-05
2023-12-15 01:28:52 INFO     	 * (global step 1550: loss: 0.31569740921258926, lr: 1e-05
2023-12-15 01:29:00 INFO     	 * (global step 1600: loss: 0.3451329618692398, lr: 1e-05
2023-12-15 01:29:08 INFO     	 * (global step 1650: loss: 0.3560454994440079, lr: 1e-05
2023-12-15 01:29:16 INFO     	 * (global step 1700: loss: 0.34665510058403015, lr: 1e-05
2023-12-15 01:29:24 INFO     	 * (global step 1750: loss: 0.3040071651339531, lr: 1e-05
2023-12-15 01:29:33 INFO     	 * (global step 1800: loss: 0.4170057624578476, lr: 1e-05
2023-12-15 01:29:41 INFO     	 * (global step 1850: loss: 0.3199593350291252, lr: 1e-05
2023-12-15 01:29:43 INFO     [epoch 2/15] average loss: 0.398, lr: 1e-05
2023-12-15 01:29:43 INFO     saving model related files
2023-12-15 01:29:43 INFO     saving model
2023-12-15 01:29:43 INFO     saving tokenizer
2023-12-15 01:29:43 INFO     saving optimizer
2023-12-15 01:29:44 INFO     remove old optimizer files
2023-12-15 01:29:50 INFO     	 * (global step 1900: loss: 0.2564830780029297, lr: 1e-05
2023-12-15 01:29:59 INFO     	 * (global step 1950: loss: 0.3277565538883209, lr: 1e-05
2023-12-15 01:30:07 INFO     	 * (global step 2000: loss: 0.46879102289676666, lr: 1e-05
2023-12-15 01:30:15 INFO     	 * (global step 2050: loss: 0.523597776889801, lr: 1e-05
2023-12-15 01:30:23 INFO     	 * (global step 2100: loss: 0.3621535748243332, lr: 1e-05
2023-12-15 01:30:31 INFO     	 * (global step 2150: loss: 0.33858779072761536, lr: 1e-05
2023-12-15 01:30:39 INFO     	 * (global step 2200: loss: 0.4529307335615158, lr: 1e-05
2023-12-15 01:30:47 INFO     	 * (global step 2250: loss: 0.4112921953201294, lr: 1e-05
2023-12-15 01:30:55 INFO     	 * (global step 2300: loss: 0.41829873621463776, lr: 1e-05
2023-12-15 01:31:03 INFO     	 * (global step 2350: loss: 0.37792278826236725, lr: 1e-05
2023-12-15 01:31:12 INFO     	 * (global step 2400: loss: 0.4924331605434418, lr: 1e-05
2023-12-15 01:31:20 INFO     	 * (global step 2450: loss: 0.34562264382839203, lr: 1e-05
2023-12-15 01:31:25 INFO     [epoch 3/15] average loss: 0.392, lr: 1e-05
2023-12-15 01:31:25 INFO     saving model related files
2023-12-15 01:31:25 INFO     saving model
2023-12-15 01:31:26 INFO     saving tokenizer
2023-12-15 01:31:26 INFO     saving optimizer
2023-12-15 01:31:27 INFO     remove old optimizer files
2023-12-15 01:31:30 INFO     	 * (global step 2500: loss: 0.32126055657863617, lr: 1e-05
2023-12-15 01:31:38 INFO     	 * (global step 2550: loss: 0.28897348791360855, lr: 1e-05
2023-12-15 01:31:46 INFO     	 * (global step 2600: loss: 0.5761749446392059, lr: 1e-05
2023-12-15 01:31:54 INFO     	 * (global step 2650: loss: 0.4577275365591049, lr: 1e-05
2023-12-15 01:32:02 INFO     	 * (global step 2700: loss: 0.3312246948480606, lr: 1e-05
2023-12-15 01:32:10 INFO     	 * (global step 2750: loss: 0.3803910166025162, lr: 1e-05
2023-12-15 01:32:18 INFO     	 * (global step 2800: loss: 0.7083421945571899, lr: 1e-05
2023-12-15 01:32:26 INFO     	 * (global step 2850: loss: 0.43387313187122345, lr: 1e-05
2023-12-15 01:32:35 INFO     	 * (global step 2900: loss: 0.25521960854530334, lr: 1e-05
2023-12-15 01:32:43 INFO     	 * (global step 2950: loss: 0.35302241146564484, lr: 1e-05
2023-12-15 01:32:51 INFO     	 * (global step 3000: loss: 0.5151858180761337, lr: 1e-05
2023-12-15 01:32:59 INFO     	 * (global step 3050: loss: 0.3347122222185135, lr: 1e-05
2023-12-15 01:33:07 INFO     	 * (global step 3100: loss: 0.323006272315979, lr: 1e-05
2023-12-15 01:33:08 INFO     [epoch 4/15] average loss: 0.387, lr: 1e-05
2023-12-15 01:33:08 INFO     saving model related files
2023-12-15 01:33:08 INFO     saving model
2023-12-15 01:33:08 INFO     saving tokenizer
2023-12-15 01:33:08 INFO     saving optimizer
2023-12-15 01:33:09 INFO     remove old optimizer files
2023-12-15 01:33:17 INFO     	 * (global step 3150: loss: 0.4170493856072426, lr: 1e-05
2023-12-15 01:33:25 INFO     	 * (global step 3200: loss: 0.49944036453962326, lr: 1e-05
2023-12-15 01:33:33 INFO     	 * (global step 3250: loss: 0.4490365833044052, lr: 1e-05
2023-12-15 01:33:41 INFO     	 * (global step 3300: loss: 0.4029781073331833, lr: 1e-05
2023-12-15 01:33:49 INFO     	 * (global step 3350: loss: 0.41198575496673584, lr: 1e-05
2023-12-15 01:33:57 INFO     	 * (global step 3400: loss: 0.3948093503713608, lr: 1e-05
2023-12-15 01:34:05 INFO     	 * (global step 3450: loss: 0.35012832283973694, lr: 1e-05
2023-12-15 01:34:13 INFO     	 * (global step 3500: loss: 0.4698253870010376, lr: 1e-05
2023-12-15 01:34:22 INFO     	 * (global step 3550: loss: 0.3531917929649353, lr: 1e-05
2023-12-15 01:34:30 INFO     	 * (global step 3600: loss: 0.4123632609844208, lr: 1e-05
2023-12-15 01:34:38 INFO     	 * (global step 3650: loss: 0.5724200010299683, lr: 1e-05
2023-12-15 01:34:46 INFO     	 * (global step 3700: loss: 0.5786304920911789, lr: 1e-05
2023-12-15 01:34:50 INFO     [epoch 5/15] average loss: 0.383, lr: 1e-05
2023-12-15 01:34:50 INFO     saving model related files
2023-12-15 01:34:50 INFO     saving model
2023-12-15 01:34:51 INFO     saving tokenizer
2023-12-15 01:34:51 INFO     saving optimizer
2023-12-15 01:34:52 INFO     remove old optimizer files
2023-12-15 01:34:56 INFO     	 * (global step 3750: loss: 0.3441110849380493, lr: 1e-05
2023-12-15 01:35:04 INFO     	 * (global step 3800: loss: 0.4683651030063629, lr: 1e-05
2023-12-15 01:35:12 INFO     	 * (global step 3850: loss: 0.3949882239103317, lr: 1e-05
2023-12-15 01:35:20 INFO     	 * (global step 3900: loss: 0.3407428115606308, lr: 1e-05
2023-12-15 01:35:28 INFO     	 * (global step 3950: loss: 0.5801534801721573, lr: 1e-05
2023-12-15 01:35:36 INFO     	 * (global step 4000: loss: 0.4359287917613983, lr: 1e-05
2023-12-15 01:35:45 INFO     	 * (global step 4050: loss: 0.303151898086071, lr: 1e-05
2023-12-15 01:35:53 INFO     	 * (global step 4100: loss: 0.399493008852005, lr: 1e-05
2023-12-15 01:36:01 INFO     	 * (global step 4150: loss: 0.2666035071015358, lr: 1e-05
2023-12-15 01:36:09 INFO     	 * (global step 4200: loss: 0.49240463972091675, lr: 1e-05
2023-12-15 01:36:17 INFO     	 * (global step 4250: loss: 0.33000557124614716, lr: 1e-05
2023-12-15 01:36:25 INFO     	 * (global step 4300: loss: 0.4331340342760086, lr: 1e-05
2023-12-15 01:36:33 INFO     [epoch 6/15] average loss: 0.379, lr: 1e-05
2023-12-15 01:36:33 INFO     saving model related files
2023-12-15 01:36:33 INFO     saving model
2023-12-15 01:36:33 INFO     saving tokenizer
2023-12-15 01:36:33 INFO     saving optimizer
2023-12-15 01:36:34 INFO     remove old optimizer files
2023-12-15 01:36:35 INFO     	 * (global step 4350: loss: 0.49141959846019745, lr: 1e-05
2023-12-15 01:36:43 INFO     	 * (global step 4400: loss: 0.30000388622283936, lr: 1e-05
2023-12-15 01:36:51 INFO     	 * (global step 4450: loss: 0.4884933829307556, lr: 1e-05
2023-12-15 01:36:59 INFO     	 * (global step 4500: loss: 0.3597251623868942, lr: 1e-05
2023-12-15 01:37:07 INFO     	 * (global step 4550: loss: 0.3748200386762619, lr: 1e-05
2023-12-15 01:37:15 INFO     	 * (global step 4600: loss: 0.38733260333538055, lr: 1e-05
2023-12-15 01:37:24 INFO     	 * (global step 4650: loss: 0.49317651987075806, lr: 1e-05
2023-12-15 01:37:32 INFO     	 * (global step 4700: loss: 0.3213815838098526, lr: 1e-05
2023-12-15 01:37:40 INFO     	 * (global step 4750: loss: 0.3519253432750702, lr: 1e-05
2023-12-15 01:37:48 INFO     	 * (global step 4800: loss: 0.36682435870170593, lr: 1e-05
2023-12-15 01:37:56 INFO     	 * (global step 4850: loss: 0.367228165268898, lr: 1e-05
2023-12-15 01:38:04 INFO     	 * (global step 4900: loss: 0.4576527625322342, lr: 1e-05
2023-12-15 01:38:12 INFO     	 * (global step 4950: loss: 0.46731723845005035, lr: 1e-05
2023-12-15 01:38:15 INFO     [epoch 7/15] average loss: 0.376, lr: 1e-05
2023-12-15 01:38:15 INFO     saving model related files
2023-12-15 01:38:15 INFO     saving model
2023-12-15 01:38:16 INFO     saving tokenizer
2023-12-15 01:38:16 INFO     saving optimizer
2023-12-15 01:38:17 INFO     remove old optimizer files
2023-12-15 01:38:22 INFO     	 * (global step 5000: loss: 0.26044320315122604, lr: 1e-05
2023-12-15 01:38:30 INFO     	 * (global step 5050: loss: 0.41404326260089874, lr: 1e-05
2023-12-15 01:38:38 INFO     	 * (global step 5100: loss: 0.29476113617420197, lr: 1e-05
2023-12-15 01:38:47 INFO     	 * (global step 5150: loss: 0.22244969010353088, lr: 1e-05
2023-12-15 01:38:55 INFO     	 * (global step 5200: loss: 0.4361014664173126, lr: 1e-05
2023-12-15 01:39:03 INFO     	 * (global step 5250: loss: 0.43066152930259705, lr: 1e-05
2023-12-15 01:39:11 INFO     	 * (global step 5300: loss: 0.28480781614780426, lr: 1e-05
2023-12-15 01:39:19 INFO     	 * (global step 5350: loss: 0.3870171159505844, lr: 1e-05
2023-12-15 01:39:27 INFO     	 * (global step 5400: loss: 0.661286473274231, lr: 1e-05
2023-12-15 01:39:35 INFO     	 * (global step 5450: loss: 0.28123705089092255, lr: 1e-05
2023-12-15 01:39:43 INFO     	 * (global step 5500: loss: 0.46645908057689667, lr: 1e-05
2023-12-15 01:39:51 INFO     	 * (global step 5550: loss: 0.270759217441082, lr: 1e-05
2023-12-15 01:39:58 INFO     [epoch 8/15] average loss: 0.373, lr: 1e-05
2023-12-15 01:39:58 INFO     saving model related files
2023-12-15 01:39:58 INFO     saving model
2023-12-15 01:39:58 INFO     saving tokenizer
2023-12-15 01:39:59 INFO     saving optimizer
2023-12-15 01:40:00 INFO     remove old optimizer files
2023-12-15 01:40:01 INFO     	 * (global step 5600: loss: 0.4309171438217163, lr: 1e-05
2023-12-15 01:40:10 INFO     	 * (global step 5650: loss: 0.45694538950920105, lr: 1e-05
2023-12-15 01:40:18 INFO     	 * (global step 5700: loss: 0.3849133253097534, lr: 1e-05
2023-12-15 01:40:26 INFO     	 * (global step 5750: loss: 0.32703931629657745, lr: 1e-05
2023-12-15 01:40:34 INFO     	 * (global step 5800: loss: 0.29240772128105164, lr: 1e-05
2023-12-15 01:40:42 INFO     	 * (global step 5850: loss: 0.5860472172498703, lr: 1e-05
2023-12-15 01:40:50 INFO     	 * (global step 5900: loss: 0.3107245862483978, lr: 1e-05
2023-12-15 01:40:58 INFO     	 * (global step 5950: loss: 0.41678206622600555, lr: 1e-05
2023-12-15 01:41:07 INFO     	 * (global step 6000: loss: 0.32310283184051514, lr: 1e-05
2023-12-15 01:41:15 INFO     	 * (global step 6050: loss: 0.3339453488588333, lr: 1e-05
2023-12-15 01:41:23 INFO     	 * (global step 6100: loss: 0.4438970685005188, lr: 1e-05
2023-12-15 01:41:31 INFO     	 * (global step 6150: loss: 0.3327438235282898, lr: 1e-05
2023-12-15 01:41:39 INFO     	 * (global step 6200: loss: 0.5663698464632034, lr: 1e-05
2023-12-15 01:41:41 INFO     [epoch 9/15] average loss: 0.37, lr: 1e-05
2023-12-15 01:41:41 INFO     saving model related files
2023-12-15 01:41:41 INFO     saving model
2023-12-15 01:41:41 INFO     saving tokenizer
2023-12-15 01:41:41 INFO     saving optimizer
2023-12-15 01:41:42 INFO     remove old optimizer files
2023-12-15 01:41:42 INFO     complete training: model ckpt was saved at small_finetuned_ckpt/model_oprhlh
2023-12-15 01:41:42 INFO     ## 1st RUN: Configuration 10/12 ##
2023-12-15 01:41:42 INFO     initialize model trainer
2023-12-15 01:41:42 INFO     initialize checkpoint at small_finetuned_ckpt/model_vhyoja
2023-12-15 01:41:42 INFO     hyperparameters
2023-12-15 01:41:42 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 01:41:42 INFO     	 * dataset_name: default
2023-12-15 01:41:42 INFO     	 * input_types: ['paragraph']
2023-12-15 01:41:42 INFO     	 * output_types: ['questions_answers']
2023-12-15 01:41:42 INFO     	 * prefix_types: ['qag']
2023-12-15 01:41:42 INFO     	 * model: lmqg/t5-small-squad-qag
2023-12-15 01:41:42 INFO     	 * max_length: 512
2023-12-15 01:41:42 INFO     	 * max_length_output: 512
2023-12-15 01:41:42 INFO     	 * epoch: 15
2023-12-15 01:41:42 INFO     	 * batch: 2
2023-12-15 01:41:42 INFO     	 * lr: 1e-05
2023-12-15 01:41:42 INFO     	 * fp16: False
2023-12-15 01:41:42 INFO     	 * random_seed: 1
2023-12-15 01:41:42 INFO     	 * gradient_accumulation_steps: 4
2023-12-15 01:41:42 INFO     	 * label_smoothing: 0.0
2023-12-15 01:41:42 INFO     initialize checkpoint with lmqg/t5-small-squad-qag
2023-12-15 01:41:44 INFO     use spaCy answer extraction model: positionrank
2023-12-15 01:41:44 INFO     Model `lmqg/t5-small-squad-qag`
2023-12-15 01:41:44 INFO     	 * Num of GPU in use: 1
2023-12-15 01:41:44 INFO     	 * Prefix: True
2023-12-15 01:41:44 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 01:41:44 INFO     dataset preprocessing
2023-12-15 01:41:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 01:41:46 INFO     start model training
2023-12-15 01:42:01 INFO     	 * (global step 50: loss: 0.4174450859427452, lr: 1e-05
2023-12-15 01:42:17 INFO     	 * (global step 100: loss: 0.44526612013578415, lr: 1e-05
2023-12-15 01:42:33 INFO     	 * (global step 150: loss: 0.4049782007932663, lr: 1e-05
2023-12-15 01:42:48 INFO     	 * (global step 200: loss: 0.3935903534293175, lr: 1e-05
2023-12-15 01:43:04 INFO     	 * (global step 250: loss: 0.48380047082901, lr: 1e-05
2023-12-15 01:43:20 INFO     	 * (global step 300: loss: 0.5217370912432671, lr: 1e-05
2023-12-15 01:43:23 INFO     [epoch 0/15] average loss: 0.447, lr: 1e-05
2023-12-15 01:43:23 INFO     saving model related files
2023-12-15 01:43:23 INFO     saving model
2023-12-15 01:43:24 INFO     saving tokenizer
2023-12-15 01:43:24 INFO     saving optimizer
2023-12-15 01:43:25 INFO     remove old optimizer files
2023-12-15 01:43:37 INFO     	 * (global step 350: loss: 0.38673805817961693, lr: 1e-05
2023-12-15 01:43:53 INFO     	 * (global step 400: loss: 0.3854989558458328, lr: 1e-05
2023-12-15 01:44:09 INFO     	 * (global step 450: loss: 0.5139632523059845, lr: 1e-05
2023-12-15 01:44:24 INFO     	 * (global step 500: loss: 0.5006912723183632, lr: 1e-05
2023-12-15 01:44:40 INFO     	 * (global step 550: loss: 0.5277958959341049, lr: 1e-05
2023-12-15 01:44:56 INFO     	 * (global step 600: loss: 0.45590126514434814, lr: 1e-05
2023-12-15 01:45:02 INFO     [epoch 1/15] average loss: 0.415, lr: 1e-05
2023-12-15 01:45:02 INFO     saving model related files
2023-12-15 01:45:02 INFO     saving model
2023-12-15 01:45:03 INFO     saving tokenizer
2023-12-15 01:45:03 INFO     saving optimizer
2023-12-15 01:45:04 INFO     remove old optimizer files
2023-12-15 01:45:13 INFO     	 * (global step 650: loss: 0.3872021660208702, lr: 1e-05
2023-12-15 01:45:29 INFO     	 * (global step 700: loss: 0.3514437712728977, lr: 1e-05
2023-12-15 01:45:44 INFO     	 * (global step 750: loss: 0.36614473164081573, lr: 1e-05
2023-12-15 01:46:00 INFO     	 * (global step 800: loss: 0.38412441313266754, lr: 1e-05
2023-12-15 01:46:16 INFO     	 * (global step 850: loss: 0.5429331362247467, lr: 1e-05
2023-12-15 01:46:32 INFO     	 * (global step 900: loss: 0.541653111577034, lr: 1e-05
2023-12-15 01:46:41 INFO     [epoch 2/15] average loss: 0.405, lr: 1e-05
2023-12-15 01:46:41 INFO     saving model related files
2023-12-15 01:46:41 INFO     saving model
2023-12-15 01:46:42 INFO     saving tokenizer
2023-12-15 01:46:42 INFO     saving optimizer
2023-12-15 01:46:43 INFO     remove old optimizer files
2023-12-15 01:46:49 INFO     	 * (global step 950: loss: 0.5032149404287338, lr: 1e-05
2023-12-15 01:47:05 INFO     	 * (global step 1000: loss: 0.3875398226082325, lr: 1e-05
2023-12-15 01:47:21 INFO     	 * (global step 1050: loss: 0.3773440942168236, lr: 1e-05
2023-12-15 01:47:36 INFO     	 * (global step 1100: loss: 0.33245617151260376, lr: 1e-05
2023-12-15 01:47:52 INFO     	 * (global step 1150: loss: 0.3028443232178688, lr: 1e-05
2023-12-15 01:48:08 INFO     	 * (global step 1200: loss: 0.37584253400564194, lr: 1e-05
2023-12-15 01:48:20 INFO     [epoch 3/15] average loss: 0.399, lr: 1e-05
2023-12-15 01:48:20 INFO     saving model related files
2023-12-15 01:48:20 INFO     saving model
2023-12-15 01:48:21 INFO     saving tokenizer
2023-12-15 01:48:21 INFO     saving optimizer
2023-12-15 01:48:22 INFO     remove old optimizer files
2023-12-15 01:48:25 INFO     	 * (global step 1250: loss: 0.40696991980075836, lr: 1e-05
2023-12-15 01:48:41 INFO     	 * (global step 1300: loss: 0.2805713154375553, lr: 1e-05
2023-12-15 01:48:57 INFO     	 * (global step 1350: loss: 0.3866093009710312, lr: 1e-05
2023-12-15 01:49:12 INFO     	 * (global step 1400: loss: 0.30510955303907394, lr: 1e-05
2023-12-15 01:49:28 INFO     	 * (global step 1450: loss: 0.34071657061576843, lr: 1e-05
2023-12-15 01:49:44 INFO     	 * (global step 1500: loss: 0.5018950253725052, lr: 1e-05
2023-12-15 01:49:59 INFO     	 * (global step 1550: loss: 0.3489021733403206, lr: 1e-05
2023-12-15 01:50:00 INFO     [epoch 4/15] average loss: 0.394, lr: 1e-05
2023-12-15 01:50:00 INFO     saving model related files
2023-12-15 01:50:00 INFO     saving model
2023-12-15 01:50:00 INFO     saving tokenizer
2023-12-15 01:50:00 INFO     saving optimizer
2023-12-15 01:50:01 INFO     remove old optimizer files
2023-12-15 01:50:17 INFO     	 * (global step 1600: loss: 0.33800070360302925, lr: 1e-05
2023-12-15 01:50:33 INFO     	 * (global step 1650: loss: 0.3891017585992813, lr: 1e-05
2023-12-15 01:50:48 INFO     	 * (global step 1700: loss: 0.4171433225274086, lr: 1e-05
2023-12-15 01:51:04 INFO     	 * (global step 1750: loss: 0.28159792721271515, lr: 1e-05
2023-12-15 01:51:20 INFO     	 * (global step 1800: loss: 0.5299264900386333, lr: 1e-05
2023-12-15 01:51:36 INFO     	 * (global step 1850: loss: 0.3575959224253893, lr: 1e-05
2023-12-15 01:51:39 INFO     [epoch 5/15] average loss: 0.391, lr: 1e-05
2023-12-15 01:51:39 INFO     saving model related files
2023-12-15 01:51:39 INFO     saving model
2023-12-15 01:51:39 INFO     saving tokenizer
2023-12-15 01:51:39 INFO     saving optimizer
2023-12-15 01:51:40 INFO     remove old optimizer files
2023-12-15 01:51:53 INFO     	 * (global step 1900: loss: 0.39507728070020676, lr: 1e-05
2023-12-15 01:52:09 INFO     	 * (global step 1950: loss: 0.3534868694841862, lr: 1e-05
2023-12-15 01:52:24 INFO     	 * (global step 2000: loss: 0.444861464202404, lr: 1e-05
2023-12-15 01:52:40 INFO     	 * (global step 2050: loss: 0.2787151299417019, lr: 1e-05
2023-12-15 01:52:56 INFO     	 * (global step 2100: loss: 0.3575834184885025, lr: 1e-05
2023-12-15 01:53:11 INFO     	 * (global step 2150: loss: 0.4207726866006851, lr: 1e-05
2023-12-15 01:53:18 INFO     [epoch 6/15] average loss: 0.387, lr: 1e-05
2023-12-15 01:53:18 INFO     saving model related files
2023-12-15 01:53:18 INFO     saving model
2023-12-15 01:53:18 INFO     saving tokenizer
2023-12-15 01:53:18 INFO     saving optimizer
2023-12-15 01:53:20 INFO     remove old optimizer files
2023-12-15 01:53:29 INFO     	 * (global step 2200: loss: 0.5402817651629448, lr: 1e-05
2023-12-15 01:53:45 INFO     	 * (global step 2250: loss: 0.5675267577171326, lr: 1e-05
2023-12-15 01:54:00 INFO     	 * (global step 2300: loss: 0.3388836458325386, lr: 1e-05
2023-12-15 01:54:16 INFO     	 * (global step 2350: loss: 0.4156746566295624, lr: 1e-05
2023-12-15 01:54:32 INFO     	 * (global step 2400: loss: 0.5255554914474487, lr: 1e-05
2023-12-15 01:54:47 INFO     	 * (global step 2450: loss: 0.32990221958607435, lr: 1e-05
2023-12-15 01:54:57 INFO     [epoch 7/15] average loss: 0.384, lr: 1e-05
2023-12-15 01:54:57 INFO     saving model related files
2023-12-15 01:54:57 INFO     saving model
2023-12-15 01:54:58 INFO     saving tokenizer
2023-12-15 01:54:58 INFO     saving optimizer
2023-12-15 01:54:59 INFO     remove old optimizer files
2023-12-15 01:55:05 INFO     	 * (global step 2500: loss: 0.56254892796278, lr: 1e-05
2023-12-15 01:55:21 INFO     	 * (global step 2550: loss: 0.3410922884941101, lr: 1e-05
2023-12-15 01:55:36 INFO     	 * (global step 2600: loss: 0.499522402882576, lr: 1e-05
2023-12-15 01:55:52 INFO     	 * (global step 2650: loss: 0.3756989538669586, lr: 1e-05
2023-12-15 01:56:08 INFO     	 * (global step 2700: loss: 0.4060283377766609, lr: 1e-05
2023-12-15 01:56:23 INFO     	 * (global step 2750: loss: 0.48508820682764053, lr: 1e-05
2023-12-15 01:56:36 INFO     [epoch 8/15] average loss: 0.381, lr: 1e-05
2023-12-15 01:56:36 INFO     saving model related files
2023-12-15 01:56:36 INFO     saving model
2023-12-15 01:56:37 INFO     saving tokenizer
2023-12-15 01:56:37 INFO     saving optimizer
2023-12-15 01:56:38 INFO     remove old optimizer files
2023-12-15 01:56:41 INFO     	 * (global step 2800: loss: 0.496693879365921, lr: 1e-05
2023-12-15 01:56:56 INFO     	 * (global step 2850: loss: 0.4493448734283447, lr: 1e-05
2023-12-15 01:57:12 INFO     	 * (global step 2900: loss: 0.4004187509417534, lr: 1e-05
2023-12-15 01:57:28 INFO     	 * (global step 2950: loss: 0.4111032411456108, lr: 1e-05
2023-12-15 01:57:43 INFO     	 * (global step 3000: loss: 0.26008398085832596, lr: 1e-05
2023-12-15 01:57:59 INFO     	 * (global step 3050: loss: 0.41575707495212555, lr: 1e-05
2023-12-15 01:58:15 INFO     	 * (global step 3100: loss: 0.40677905082702637, lr: 1e-05
2023-12-15 01:58:15 INFO     [epoch 9/15] average loss: 0.378, lr: 1e-05
2023-12-15 01:58:15 INFO     saving model related files
2023-12-15 01:58:15 INFO     saving model
2023-12-15 01:58:16 INFO     saving tokenizer
2023-12-15 01:58:16 INFO     saving optimizer
2023-12-15 01:58:17 INFO     remove old optimizer files
2023-12-15 01:58:17 INFO     complete training: model ckpt was saved at small_finetuned_ckpt/model_vhyoja
2023-12-15 01:58:17 INFO     ## 1st RUN: Configuration 11/12 ##
2023-12-15 01:58:17 INFO     initialize model trainer
2023-12-15 01:58:17 INFO     initialize checkpoint at small_finetuned_ckpt/model_nrudfu
2023-12-15 01:58:17 INFO     hyperparameters
2023-12-15 01:58:17 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 01:58:17 INFO     	 * dataset_name: default
2023-12-15 01:58:17 INFO     	 * input_types: ['paragraph']
2023-12-15 01:58:17 INFO     	 * output_types: ['questions_answers']
2023-12-15 01:58:17 INFO     	 * prefix_types: ['qag']
2023-12-15 01:58:17 INFO     	 * model: lmqg/t5-small-squad-qag
2023-12-15 01:58:17 INFO     	 * max_length: 512
2023-12-15 01:58:17 INFO     	 * max_length_output: 512
2023-12-15 01:58:17 INFO     	 * epoch: 15
2023-12-15 01:58:17 INFO     	 * batch: 2
2023-12-15 01:58:17 INFO     	 * lr: 1e-05
2023-12-15 01:58:17 INFO     	 * fp16: False
2023-12-15 01:58:17 INFO     	 * random_seed: 1
2023-12-15 01:58:17 INFO     	 * gradient_accumulation_steps: 2
2023-12-15 01:58:17 INFO     	 * label_smoothing: 0.0
2023-12-15 01:58:17 INFO     initialize checkpoint with lmqg/t5-small-squad-qag
2023-12-15 01:58:18 INFO     use spaCy answer extraction model: positionrank
2023-12-15 01:58:19 INFO     Model `lmqg/t5-small-squad-qag`
2023-12-15 01:58:19 INFO     	 * Num of GPU in use: 1
2023-12-15 01:58:19 INFO     	 * Prefix: True
2023-12-15 01:58:19 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 01:58:19 INFO     dataset preprocessing
2023-12-15 01:58:20 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 01:58:20 INFO     start model training
2023-12-15 01:58:28 INFO     	 * (global step 50: loss: 0.4581184983253479, lr: 1e-05
2023-12-15 01:58:36 INFO     	 * (global step 100: loss: 0.31322166323661804, lr: 1e-05
2023-12-15 01:58:45 INFO     	 * (global step 150: loss: 0.4114130139350891, lr: 1e-05
2023-12-15 01:58:53 INFO     	 * (global step 200: loss: 0.4368223398923874, lr: 1e-05
2023-12-15 01:59:01 INFO     	 * (global step 250: loss: 0.45259372889995575, lr: 1e-05
2023-12-15 01:59:09 INFO     	 * (global step 300: loss: 0.3870916813611984, lr: 1e-05
2023-12-15 01:59:17 INFO     	 * (global step 350: loss: 0.6222658753395081, lr: 1e-05
2023-12-15 01:59:25 INFO     	 * (global step 400: loss: 0.3737747520208359, lr: 1e-05
2023-12-15 01:59:33 INFO     	 * (global step 450: loss: 0.3162582144141197, lr: 1e-05
2023-12-15 01:59:41 INFO     	 * (global step 500: loss: 0.44897033274173737, lr: 1e-05
2023-12-15 01:59:50 INFO     	 * (global step 550: loss: 0.3220909982919693, lr: 1e-05
2023-12-15 01:59:58 INFO     	 * (global step 600: loss: 0.6511140465736389, lr: 1e-05
2023-12-15 02:00:01 INFO     [epoch 0/15] average loss: 0.437, lr: 1e-05
2023-12-15 02:00:01 INFO     saving model related files
2023-12-15 02:00:01 INFO     saving model
2023-12-15 02:00:02 INFO     saving tokenizer
2023-12-15 02:00:02 INFO     saving optimizer
2023-12-15 02:00:03 INFO     remove old optimizer files
2023-12-15 02:00:08 INFO     	 * (global step 650: loss: 0.4795816093683243, lr: 1e-05
2023-12-15 02:00:16 INFO     	 * (global step 700: loss: 0.43261465430259705, lr: 1e-05
2023-12-15 02:00:24 INFO     	 * (global step 750: loss: 0.350529745221138, lr: 1e-05
2023-12-15 02:00:32 INFO     	 * (global step 800: loss: 0.4104063808917999, lr: 1e-05
2023-12-15 02:00:40 INFO     	 * (global step 850: loss: 0.6829293370246887, lr: 1e-05
2023-12-15 02:00:48 INFO     	 * (global step 900: loss: 0.5406392365694046, lr: 1e-05
2023-12-15 02:00:56 INFO     	 * (global step 950: loss: 0.3603086471557617, lr: 1e-05
2023-12-15 02:01:04 INFO     	 * (global step 1000: loss: 0.6313239485025406, lr: 1e-05
2023-12-15 02:01:12 INFO     	 * (global step 1050: loss: 0.3617406040430069, lr: 1e-05
2023-12-15 02:01:20 INFO     	 * (global step 1100: loss: 0.4810079038143158, lr: 1e-05
2023-12-15 02:01:28 INFO     	 * (global step 1150: loss: 0.25516095757484436, lr: 1e-05
2023-12-15 02:01:37 INFO     	 * (global step 1200: loss: 0.6278806626796722, lr: 1e-05
2023-12-15 02:01:44 INFO     [epoch 1/15] average loss: 0.408, lr: 1e-05
2023-12-15 02:01:44 INFO     saving model related files
2023-12-15 02:01:44 INFO     saving model
2023-12-15 02:01:44 INFO     saving tokenizer
2023-12-15 02:01:44 INFO     saving optimizer
2023-12-15 02:01:45 INFO     remove old optimizer files
2023-12-15 02:01:47 INFO     	 * (global step 1250: loss: 0.4877290427684784, lr: 1e-05
2023-12-15 02:01:55 INFO     	 * (global step 1300: loss: 0.29171570390462875, lr: 1e-05
2023-12-15 02:02:03 INFO     	 * (global step 1350: loss: 0.2023996114730835, lr: 1e-05
2023-12-15 02:02:11 INFO     	 * (global step 1400: loss: 0.489560142159462, lr: 1e-05
2023-12-15 02:02:19 INFO     	 * (global step 1450: loss: 0.42874380946159363, lr: 1e-05
2023-12-15 02:02:27 INFO     	 * (global step 1500: loss: 0.2634795233607292, lr: 1e-05
2023-12-15 02:02:35 INFO     	 * (global step 1550: loss: 0.31569740921258926, lr: 1e-05
2023-12-15 02:02:43 INFO     	 * (global step 1600: loss: 0.3451329618692398, lr: 1e-05
2023-12-15 02:02:52 INFO     	 * (global step 1650: loss: 0.3560454994440079, lr: 1e-05
2023-12-15 02:03:00 INFO     	 * (global step 1700: loss: 0.34665510058403015, lr: 1e-05
2023-12-15 02:03:08 INFO     	 * (global step 1750: loss: 0.3040071651339531, lr: 1e-05
2023-12-15 02:03:16 INFO     	 * (global step 1800: loss: 0.4170057624578476, lr: 1e-05
2023-12-15 02:03:24 INFO     	 * (global step 1850: loss: 0.3199593350291252, lr: 1e-05
2023-12-15 02:03:26 INFO     [epoch 2/15] average loss: 0.398, lr: 1e-05
2023-12-15 02:03:26 INFO     saving model related files
2023-12-15 02:03:26 INFO     saving model
2023-12-15 02:03:27 INFO     saving tokenizer
2023-12-15 02:03:27 INFO     saving optimizer
2023-12-15 02:03:28 INFO     remove old optimizer files
2023-12-15 02:03:34 INFO     	 * (global step 1900: loss: 0.2564830780029297, lr: 1e-05
2023-12-15 02:03:42 INFO     	 * (global step 1950: loss: 0.3277565538883209, lr: 1e-05
2023-12-15 02:03:50 INFO     	 * (global step 2000: loss: 0.46879102289676666, lr: 1e-05
2023-12-15 02:03:58 INFO     	 * (global step 2050: loss: 0.523597776889801, lr: 1e-05
2023-12-15 02:04:06 INFO     	 * (global step 2100: loss: 0.3621535748243332, lr: 1e-05
2023-12-15 02:04:15 INFO     	 * (global step 2150: loss: 0.33858779072761536, lr: 1e-05
2023-12-15 02:04:23 INFO     	 * (global step 2200: loss: 0.4529307335615158, lr: 1e-05
2023-12-15 02:04:31 INFO     	 * (global step 2250: loss: 0.4112921953201294, lr: 1e-05
2023-12-15 02:04:39 INFO     	 * (global step 2300: loss: 0.41829873621463776, lr: 1e-05
2023-12-15 02:04:47 INFO     	 * (global step 2350: loss: 0.37792278826236725, lr: 1e-05
2023-12-15 02:04:55 INFO     	 * (global step 2400: loss: 0.4924331605434418, lr: 1e-05
2023-12-15 02:05:03 INFO     	 * (global step 2450: loss: 0.34562264382839203, lr: 1e-05
2023-12-15 02:05:09 INFO     [epoch 3/15] average loss: 0.392, lr: 1e-05
2023-12-15 02:05:09 INFO     saving model related files
2023-12-15 02:05:09 INFO     saving model
2023-12-15 02:05:09 INFO     saving tokenizer
2023-12-15 02:05:09 INFO     saving optimizer
2023-12-15 02:05:11 INFO     remove old optimizer files
2023-12-15 02:05:13 INFO     	 * (global step 2500: loss: 0.32126055657863617, lr: 1e-05
2023-12-15 02:05:21 INFO     	 * (global step 2550: loss: 0.28897348791360855, lr: 1e-05
2023-12-15 02:05:29 INFO     	 * (global step 2600: loss: 0.5761749446392059, lr: 1e-05
2023-12-15 02:05:38 INFO     	 * (global step 2650: loss: 0.4577275365591049, lr: 1e-05
2023-12-15 02:05:46 INFO     	 * (global step 2700: loss: 0.3312246948480606, lr: 1e-05
2023-12-15 02:05:54 INFO     	 * (global step 2750: loss: 0.3803910166025162, lr: 1e-05
2023-12-15 02:06:02 INFO     	 * (global step 2800: loss: 0.7083421945571899, lr: 1e-05
2023-12-15 02:06:10 INFO     	 * (global step 2850: loss: 0.43387313187122345, lr: 1e-05
2023-12-15 02:06:18 INFO     	 * (global step 2900: loss: 0.25521960854530334, lr: 1e-05
2023-12-15 02:06:26 INFO     	 * (global step 2950: loss: 0.35302241146564484, lr: 1e-05
2023-12-15 02:06:34 INFO     	 * (global step 3000: loss: 0.5151858180761337, lr: 1e-05
2023-12-15 02:06:42 INFO     	 * (global step 3050: loss: 0.3347122222185135, lr: 1e-05
2023-12-15 02:06:51 INFO     	 * (global step 3100: loss: 0.323006272315979, lr: 1e-05
2023-12-15 02:06:51 INFO     [epoch 4/15] average loss: 0.387, lr: 1e-05
2023-12-15 02:06:51 INFO     saving model related files
2023-12-15 02:06:51 INFO     saving model
2023-12-15 02:06:52 INFO     saving tokenizer
2023-12-15 02:06:52 INFO     saving optimizer
2023-12-15 02:06:53 INFO     remove old optimizer files
2023-12-15 02:07:00 INFO     	 * (global step 3150: loss: 0.4170493856072426, lr: 1e-05
2023-12-15 02:07:08 INFO     	 * (global step 3200: loss: 0.49944036453962326, lr: 1e-05
2023-12-15 02:07:17 INFO     	 * (global step 3250: loss: 0.4490365833044052, lr: 1e-05
2023-12-15 02:07:25 INFO     	 * (global step 3300: loss: 0.4029781073331833, lr: 1e-05
2023-12-15 02:07:33 INFO     	 * (global step 3350: loss: 0.41198575496673584, lr: 1e-05
2023-12-15 02:07:41 INFO     	 * (global step 3400: loss: 0.3948093503713608, lr: 1e-05
2023-12-15 02:07:49 INFO     	 * (global step 3450: loss: 0.35012832283973694, lr: 1e-05
2023-12-15 02:07:57 INFO     	 * (global step 3500: loss: 0.4698253870010376, lr: 1e-05
2023-12-15 02:08:05 INFO     	 * (global step 3550: loss: 0.3531917929649353, lr: 1e-05
2023-12-15 02:08:13 INFO     	 * (global step 3600: loss: 0.4123632609844208, lr: 1e-05
2023-12-15 02:08:22 INFO     	 * (global step 3650: loss: 0.5724200010299683, lr: 1e-05
2023-12-15 02:08:30 INFO     	 * (global step 3700: loss: 0.5786304920911789, lr: 1e-05
2023-12-15 02:08:34 INFO     [epoch 5/15] average loss: 0.383, lr: 1e-05
2023-12-15 02:08:34 INFO     saving model related files
2023-12-15 02:08:34 INFO     saving model
2023-12-15 02:08:34 INFO     saving tokenizer
2023-12-15 02:08:35 INFO     saving optimizer
2023-12-15 02:08:36 INFO     remove old optimizer files
2023-12-15 02:08:40 INFO     	 * (global step 3750: loss: 0.3441110849380493, lr: 1e-05
2023-12-15 02:08:48 INFO     	 * (global step 3800: loss: 0.4683651030063629, lr: 1e-05
2023-12-15 02:08:56 INFO     	 * (global step 3850: loss: 0.3949882239103317, lr: 1e-05
2023-12-15 02:09:04 INFO     	 * (global step 3900: loss: 0.3407428115606308, lr: 1e-05
2023-12-15 02:09:12 INFO     	 * (global step 3950: loss: 0.5801534801721573, lr: 1e-05
2023-12-15 02:09:20 INFO     	 * (global step 4000: loss: 0.4359287917613983, lr: 1e-05
2023-12-15 02:09:28 INFO     	 * (global step 4050: loss: 0.303151898086071, lr: 1e-05
2023-12-15 02:09:36 INFO     	 * (global step 4100: loss: 0.399493008852005, lr: 1e-05
2023-12-15 02:09:45 INFO     	 * (global step 4150: loss: 0.2666035071015358, lr: 1e-05
2023-12-15 02:09:53 INFO     	 * (global step 4200: loss: 0.49240463972091675, lr: 1e-05
2023-12-15 02:10:01 INFO     	 * (global step 4250: loss: 0.33000557124614716, lr: 1e-05
2023-12-15 02:10:09 INFO     	 * (global step 4300: loss: 0.4331340342760086, lr: 1e-05
2023-12-15 02:10:17 INFO     [epoch 6/15] average loss: 0.379, lr: 1e-05
2023-12-15 02:10:17 INFO     saving model related files
2023-12-15 02:10:17 INFO     saving model
2023-12-15 02:10:17 INFO     saving tokenizer
2023-12-15 02:10:17 INFO     saving optimizer
2023-12-15 02:10:18 INFO     remove old optimizer files
2023-12-15 02:10:19 INFO     	 * (global step 4350: loss: 0.49141959846019745, lr: 1e-05
2023-12-15 02:10:27 INFO     	 * (global step 4400: loss: 0.30000388622283936, lr: 1e-05
2023-12-15 02:10:35 INFO     	 * (global step 4450: loss: 0.4884933829307556, lr: 1e-05
2023-12-15 02:10:43 INFO     	 * (global step 4500: loss: 0.3597251623868942, lr: 1e-05
2023-12-15 02:10:51 INFO     	 * (global step 4550: loss: 0.3748200386762619, lr: 1e-05
2023-12-15 02:11:00 INFO     	 * (global step 4600: loss: 0.38733260333538055, lr: 1e-05
2023-12-15 02:11:08 INFO     	 * (global step 4650: loss: 0.49317651987075806, lr: 1e-05
2023-12-15 02:11:16 INFO     	 * (global step 4700: loss: 0.3213815838098526, lr: 1e-05
2023-12-15 02:11:24 INFO     	 * (global step 4750: loss: 0.3519253432750702, lr: 1e-05
2023-12-15 02:11:32 INFO     	 * (global step 4800: loss: 0.36682435870170593, lr: 1e-05
2023-12-15 02:11:40 INFO     	 * (global step 4850: loss: 0.367228165268898, lr: 1e-05
2023-12-15 02:11:48 INFO     	 * (global step 4900: loss: 0.4576527625322342, lr: 1e-05
2023-12-15 02:11:56 INFO     	 * (global step 4950: loss: 0.46731723845005035, lr: 1e-05
2023-12-15 02:11:59 INFO     [epoch 7/15] average loss: 0.376, lr: 1e-05
2023-12-15 02:11:59 INFO     saving model related files
2023-12-15 02:11:59 INFO     saving model
2023-12-15 02:12:00 INFO     saving tokenizer
2023-12-15 02:12:00 INFO     saving optimizer
2023-12-15 02:12:01 INFO     remove old optimizer files
2023-12-15 02:12:06 INFO     	 * (global step 5000: loss: 0.26044320315122604, lr: 1e-05
2023-12-15 02:12:14 INFO     	 * (global step 5050: loss: 0.41404326260089874, lr: 1e-05
2023-12-15 02:12:23 INFO     	 * (global step 5100: loss: 0.29476113617420197, lr: 1e-05
2023-12-15 02:12:31 INFO     	 * (global step 5150: loss: 0.22244969010353088, lr: 1e-05
2023-12-15 02:12:39 INFO     	 * (global step 5200: loss: 0.4361014664173126, lr: 1e-05
2023-12-15 02:12:47 INFO     	 * (global step 5250: loss: 0.43066152930259705, lr: 1e-05
2023-12-15 02:12:55 INFO     	 * (global step 5300: loss: 0.28480781614780426, lr: 1e-05
2023-12-15 02:13:03 INFO     	 * (global step 5350: loss: 0.3870171159505844, lr: 1e-05
2023-12-15 02:13:11 INFO     	 * (global step 5400: loss: 0.661286473274231, lr: 1e-05
2023-12-15 02:13:19 INFO     	 * (global step 5450: loss: 0.28123705089092255, lr: 1e-05
2023-12-15 02:13:27 INFO     	 * (global step 5500: loss: 0.46645908057689667, lr: 1e-05
2023-12-15 02:13:36 INFO     	 * (global step 5550: loss: 0.270759217441082, lr: 1e-05
2023-12-15 02:13:42 INFO     [epoch 8/15] average loss: 0.373, lr: 1e-05
2023-12-15 02:13:42 INFO     saving model related files
2023-12-15 02:13:42 INFO     saving model
2023-12-15 02:13:42 INFO     saving tokenizer
2023-12-15 02:13:43 INFO     saving optimizer
2023-12-15 02:13:44 INFO     remove old optimizer files
2023-12-15 02:13:45 INFO     	 * (global step 5600: loss: 0.4309171438217163, lr: 1e-05
2023-12-15 02:13:53 INFO     	 * (global step 5650: loss: 0.45694538950920105, lr: 1e-05
2023-12-15 02:14:02 INFO     	 * (global step 5700: loss: 0.3849133253097534, lr: 1e-05
2023-12-15 02:14:10 INFO     	 * (global step 5750: loss: 0.32703931629657745, lr: 1e-05
2023-12-15 02:14:18 INFO     	 * (global step 5800: loss: 0.29240772128105164, lr: 1e-05
2023-12-15 02:14:26 INFO     	 * (global step 5850: loss: 0.5860472172498703, lr: 1e-05
2023-12-15 02:14:34 INFO     	 * (global step 5900: loss: 0.3107245862483978, lr: 1e-05
2023-12-15 02:14:42 INFO     	 * (global step 5950: loss: 0.41678206622600555, lr: 1e-05
2023-12-15 02:14:50 INFO     	 * (global step 6000: loss: 0.32310283184051514, lr: 1e-05
2023-12-15 02:14:58 INFO     	 * (global step 6050: loss: 0.3339453488588333, lr: 1e-05
2023-12-15 02:15:06 INFO     	 * (global step 6100: loss: 0.4438970685005188, lr: 1e-05
2023-12-15 02:15:14 INFO     	 * (global step 6150: loss: 0.3327438235282898, lr: 1e-05
2023-12-15 02:15:23 INFO     	 * (global step 6200: loss: 0.5663698464632034, lr: 1e-05
2023-12-15 02:15:24 INFO     [epoch 9/15] average loss: 0.37, lr: 1e-05
2023-12-15 02:15:24 INFO     saving model related files
2023-12-15 02:15:24 INFO     saving model
2023-12-15 02:15:25 INFO     saving tokenizer
2023-12-15 02:15:25 INFO     saving optimizer
2023-12-15 02:15:26 INFO     remove old optimizer files
2023-12-15 02:15:26 INFO     complete training: model ckpt was saved at small_finetuned_ckpt/model_nrudfu
2023-12-15 02:15:26 INFO     ## 1st RUN (EVAL): Configuration 0/12 ##
2023-12-15 02:15:52 INFO     use spaCy answer extraction model: positionrank
2023-12-15 02:15:53 INFO     Model `small_finetuned_ckpt/model_dpeblg/epoch_10`
2023-12-15 02:15:53 INFO     	 * Num of GPU in use: 1
2023-12-15 02:15:53 INFO     	 * Prefix: True
2023-12-15 02:15:53 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 02:15:54 INFO     encode all the data       : 574

  0%|          | 0/574 [00:00<?, ?it/s]
  8%|▊         | 45/574 [00:00<00:01, 448.59it/s]
 18%|█▊        | 101/574 [00:00<00:00, 510.33it/s]
 27%|██▋       | 155/574 [00:00<00:00, 518.57it/s]
 37%|███▋      | 215/574 [00:00<00:00, 548.82it/s]
 47%|████▋     | 270/574 [00:00<00:00, 512.77it/s]
 58%|█████▊    | 332/574 [00:00<00:00, 546.33it/s]
 68%|██████▊   | 388/574 [00:00<00:00, 518.93it/s]
 78%|███████▊  | 447/574 [00:00<00:00, 538.31it/s]
 88%|████████▊ | 507/574 [00:00<00:00, 554.21it/s]
 98%|█████████▊| 563/574 [00:01<00:00, 535.49it/s]
100%|██████████| 574/574 [00:01<00:00, 530.61it/s]
2023-12-15 02:15:55 INFO     after remove the overflow : 574
2023-12-15 02:15:56 INFO     preprocessed feature is saved at /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 02:22:29 INFO     encode all the data       : 574

  0%|          | 0/574 [00:00<?, ?it/s]
  9%|▉         | 51/574 [00:00<00:01, 509.46it/s]
 18%|█▊        | 105/574 [00:00<00:00, 524.94it/s]
 28%|██▊       | 161/574 [00:00<00:00, 535.48it/s]
 37%|███▋      | 215/574 [00:00<00:00, 532.19it/s]
 49%|████▉     | 280/574 [00:00<00:00, 571.16it/s]
 59%|█████▉    | 340/574 [00:00<00:00, 580.84it/s]
 70%|██████▉   | 399/574 [00:00<00:00, 580.12it/s]
 80%|███████▉  | 458/574 [00:00<00:00, 579.79it/s]
 90%|████████▉ | 516/574 [00:00<00:00, 544.48it/s]
100%|██████████| 574/574 [00:01<00:00, 557.74it/s]
2023-12-15 02:22:31 INFO     after remove the overflow : 574
2023-12-15 02:22:31 INFO     preprocessed feature is saved at /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 02:29:04 INFO     	Bleu_1: 0.10367105098781014
2023-12-15 02:29:04 INFO     	Bleu_2: 0.05665615027618275
2023-12-15 02:29:04 INFO     	Bleu_3: 0.029178023554439148
2023-12-15 02:29:04 INFO     	Bleu_4: 0.018219341283034124
2023-12-15 02:29:05 INFO     	Bleu_1: 0.1030275143862259
2023-12-15 02:29:05 INFO     	Bleu_2: 0.05619549610313485
2023-12-15 02:29:05 INFO     	Bleu_3: 0.0300470195575287
2023-12-15 02:29:05 INFO     	Bleu_4: 0.01925904720319145
2023-12-15 02:29:05 INFO     ## 1st RUN (EVAL): Configuration 1/12 ##
2023-12-15 02:29:25 INFO     use spaCy answer extraction model: positionrank
2023-12-15 02:29:25 INFO     Model `small_finetuned_ckpt/model_eszyci/epoch_10`
2023-12-15 02:29:25 INFO     	 * Num of GPU in use: 1
2023-12-15 02:29:25 INFO     	 * Prefix: True
2023-12-15 02:29:25 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 02:29:26 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 02:36:03 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 02:42:36 INFO     	Bleu_1: 0.10289481722251939
2023-12-15 02:42:36 INFO     	Bleu_2: 0.05631302541011595
2023-12-15 02:42:36 INFO     	Bleu_3: 0.029111356688588302
2023-12-15 02:42:36 INFO     	Bleu_4: 0.018169988321308995
2023-12-15 02:42:37 INFO     	Bleu_1: 0.10326825225059227
2023-12-15 02:42:37 INFO     	Bleu_2: 0.056432951054666
2023-12-15 02:42:37 INFO     	Bleu_3: 0.030182167176590703
2023-12-15 02:42:37 INFO     	Bleu_4: 0.01934970238576287
2023-12-15 02:42:37 INFO     ## 1st RUN (EVAL): Configuration 2/12 ##
2023-12-15 02:43:00 INFO     use spaCy answer extraction model: positionrank
2023-12-15 02:43:00 INFO     Model `small_finetuned_ckpt/model_dpyopu/epoch_10`
2023-12-15 02:43:00 INFO     	 * Num of GPU in use: 1
2023-12-15 02:43:00 INFO     	 * Prefix: True
2023-12-15 02:43:00 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 02:43:01 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 02:48:25 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 02:53:43 INFO     	Bleu_1: 0.10367105098781014
2023-12-15 02:53:43 INFO     	Bleu_2: 0.05665615027618275
2023-12-15 02:53:43 INFO     	Bleu_3: 0.029178023554439148
2023-12-15 02:53:43 INFO     	Bleu_4: 0.018219341283034124
2023-12-15 02:53:43 INFO     	Bleu_1: 0.1030275143862259
2023-12-15 02:53:43 INFO     	Bleu_2: 0.05619549610313485
2023-12-15 02:53:43 INFO     	Bleu_3: 0.0300470195575287
2023-12-15 02:53:43 INFO     	Bleu_4: 0.01925904720319145
2023-12-15 02:53:43 INFO     ## 1st RUN (EVAL): Configuration 3/12 ##
2023-12-15 02:53:56 INFO     use spaCy answer extraction model: positionrank
2023-12-15 02:53:56 INFO     Model `small_finetuned_ckpt/model_mzgdpa/epoch_10`
2023-12-15 02:53:56 INFO     	 * Num of GPU in use: 1
2023-12-15 02:53:56 INFO     	 * Prefix: True
2023-12-15 02:53:56 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 02:53:57 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 02:59:18 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 03:04:38 INFO     	Bleu_1: 0.10289481722251939
2023-12-15 03:04:38 INFO     	Bleu_2: 0.05631302541011595
2023-12-15 03:04:38 INFO     	Bleu_3: 0.029111356688588302
2023-12-15 03:04:38 INFO     	Bleu_4: 0.018169988321308995
2023-12-15 03:04:39 INFO     	Bleu_1: 0.10326825225059227
2023-12-15 03:04:39 INFO     	Bleu_2: 0.056432951054666
2023-12-15 03:04:39 INFO     	Bleu_3: 0.030182167176590703
2023-12-15 03:04:39 INFO     	Bleu_4: 0.01934970238576287
2023-12-15 03:04:39 INFO     ## 1st RUN (EVAL): Configuration 4/12 ##
2023-12-15 03:04:52 INFO     use spaCy answer extraction model: positionrank
2023-12-15 03:04:53 INFO     Model `small_finetuned_ckpt/model_mntyya/epoch_10`
2023-12-15 03:04:53 INFO     	 * Num of GPU in use: 1
2023-12-15 03:04:53 INFO     	 * Prefix: True
2023-12-15 03:04:53 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 03:04:54 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 03:10:08 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 03:15:27 INFO     	Bleu_1: 0.10459418160102514
2023-12-15 03:15:27 INFO     	Bleu_2: 0.05708181880812579
2023-12-15 03:15:27 INFO     	Bleu_3: 0.029612054804052455
2023-12-15 03:15:27 INFO     	Bleu_4: 0.018639460712103928
2023-12-15 03:15:28 INFO     	Bleu_1: 0.10319170801890888
2023-12-15 03:15:28 INFO     	Bleu_2: 0.05611105407947135
2023-12-15 03:15:28 INFO     	Bleu_3: 0.029377293326951562
2023-12-15 03:15:28 INFO     	Bleu_4: 0.018554524371950046
2023-12-15 03:15:28 INFO     ## 1st RUN (EVAL): Configuration 5/12 ##
2023-12-15 03:15:50 INFO     use spaCy answer extraction model: positionrank
2023-12-15 03:15:50 INFO     Model `small_finetuned_ckpt/model_woixzh/epoch_10`
2023-12-15 03:15:50 INFO     	 * Num of GPU in use: 1
2023-12-15 03:15:50 INFO     	 * Prefix: True
2023-12-15 03:15:50 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 03:15:51 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 03:21:07 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 03:26:25 INFO     	Bleu_1: 0.10484365781710853
2023-12-15 03:26:25 INFO     	Bleu_2: 0.05752649358736148
2023-12-15 03:26:25 INFO     	Bleu_3: 0.030162521761009208
2023-12-15 03:26:25 INFO     	Bleu_4: 0.019148507855686385
2023-12-15 03:26:26 INFO     	Bleu_1: 0.10649993959163885
2023-12-15 03:26:26 INFO     	Bleu_2: 0.058229065113155565
2023-12-15 03:26:26 INFO     	Bleu_3: 0.030939821124331714
2023-12-15 03:26:26 INFO     	Bleu_4: 0.019851632874695446
2023-12-15 03:26:26 INFO     ## 1st RUN (EVAL): Configuration 6/12 ##
2023-12-15 03:26:46 INFO     use spaCy answer extraction model: positionrank
2023-12-15 03:26:47 INFO     Model `small_finetuned_ckpt/model_sdkaaa/epoch_10`
2023-12-15 03:26:47 INFO     	 * Num of GPU in use: 1
2023-12-15 03:26:47 INFO     	 * Prefix: True
2023-12-15 03:26:47 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 03:26:48 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 03:32:01 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 03:37:20 INFO     	Bleu_1: 0.10459418160102514
2023-12-15 03:37:20 INFO     	Bleu_2: 0.05708181880812579
2023-12-15 03:37:20 INFO     	Bleu_3: 0.029612054804052455
2023-12-15 03:37:20 INFO     	Bleu_4: 0.018639460712103928
2023-12-15 03:37:20 INFO     	Bleu_1: 0.10319170801890888
2023-12-15 03:37:20 INFO     	Bleu_2: 0.05611105407947135
2023-12-15 03:37:20 INFO     	Bleu_3: 0.029377293326951562
2023-12-15 03:37:20 INFO     	Bleu_4: 0.018554524371950046
2023-12-15 03:37:20 INFO     ## 1st RUN (EVAL): Configuration 7/12 ##
2023-12-15 03:37:37 INFO     use spaCy answer extraction model: positionrank
2023-12-15 03:37:37 INFO     Model `small_finetuned_ckpt/model_uramvg/epoch_10`
2023-12-15 03:37:37 INFO     	 * Num of GPU in use: 1
2023-12-15 03:37:37 INFO     	 * Prefix: True
2023-12-15 03:37:37 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 03:37:39 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 03:42:54 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 03:48:12 INFO     	Bleu_1: 0.10484365781710853
2023-12-15 03:48:12 INFO     	Bleu_2: 0.05752649358736148
2023-12-15 03:48:12 INFO     	Bleu_3: 0.030162521761009208
2023-12-15 03:48:12 INFO     	Bleu_4: 0.019148507855686385
2023-12-15 03:48:13 INFO     	Bleu_1: 0.10649993959163885
2023-12-15 03:48:13 INFO     	Bleu_2: 0.058229065113155565
2023-12-15 03:48:13 INFO     	Bleu_3: 0.030939821124331714
2023-12-15 03:48:13 INFO     	Bleu_4: 0.019851632874695446
2023-12-15 03:48:13 INFO     ## 1st RUN (EVAL): Configuration 8/12 ##
2023-12-15 03:48:22 INFO     use spaCy answer extraction model: positionrank
2023-12-15 03:48:22 INFO     Model `small_finetuned_ckpt/model_nxaqhy/epoch_10`
2023-12-15 03:48:22 INFO     	 * Num of GPU in use: 1
2023-12-15 03:48:22 INFO     	 * Prefix: True
2023-12-15 03:48:22 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 03:48:23 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 03:53:34 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 03:58:43 INFO     	Bleu_1: 0.12626106611414484
2023-12-15 03:58:43 INFO     	Bleu_2: 0.06877743296771668
2023-12-15 03:58:43 INFO     	Bleu_3: 0.03609440575543837
2023-12-15 03:58:43 INFO     	Bleu_4: 0.02285404140578058
2023-12-15 03:58:43 INFO     	Bleu_1: 0.11935594333754888
2023-12-15 03:58:43 INFO     	Bleu_2: 0.06550737580285725
2023-12-15 03:58:43 INFO     	Bleu_3: 0.03522704379844863
2023-12-15 03:58:43 INFO     	Bleu_4: 0.02251476005553795
2023-12-15 03:58:43 INFO     ## 1st RUN (EVAL): Configuration 9/12 ##
2023-12-15 03:58:58 INFO     use spaCy answer extraction model: positionrank
2023-12-15 03:58:58 INFO     Model `small_finetuned_ckpt/model_oprhlh/epoch_10`
2023-12-15 03:58:58 INFO     	 * Num of GPU in use: 1
2023-12-15 03:58:58 INFO     	 * Prefix: True
2023-12-15 03:58:58 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 03:58:59 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 04:04:12 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 04:09:26 INFO     	Bleu_1: 0.11910189744825843
2023-12-15 04:09:26 INFO     	Bleu_2: 0.06472536809023023
2023-12-15 04:09:26 INFO     	Bleu_3: 0.03376843562195795
2023-12-15 04:09:26 INFO     	Bleu_4: 0.021321283395881168
2023-12-15 04:09:26 INFO     	Bleu_1: 0.1138881578947361
2023-12-15 04:09:26 INFO     	Bleu_2: 0.06254354388790281
2023-12-15 04:09:26 INFO     	Bleu_3: 0.03358572325631241
2023-12-15 04:09:26 INFO     	Bleu_4: 0.021426130540208536
2023-12-15 04:09:26 INFO     ## 1st RUN (EVAL): Configuration 10/12 ##
2023-12-15 04:09:42 INFO     use spaCy answer extraction model: positionrank
2023-12-15 04:09:43 INFO     Model `small_finetuned_ckpt/model_vhyoja/epoch_10`
2023-12-15 04:09:43 INFO     	 * Num of GPU in use: 1
2023-12-15 04:09:43 INFO     	 * Prefix: True
2023-12-15 04:09:43 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 04:09:44 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 04:14:54 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 04:20:02 INFO     	Bleu_1: 0.12626106611414484
2023-12-15 04:20:02 INFO     	Bleu_2: 0.06877743296771668
2023-12-15 04:20:02 INFO     	Bleu_3: 0.03609440575543837
2023-12-15 04:20:02 INFO     	Bleu_4: 0.02285404140578058
2023-12-15 04:20:03 INFO     	Bleu_1: 0.11935594333754888
2023-12-15 04:20:03 INFO     	Bleu_2: 0.06550737580285725
2023-12-15 04:20:03 INFO     	Bleu_3: 0.03522704379844863
2023-12-15 04:20:03 INFO     	Bleu_4: 0.02251476005553795
2023-12-15 04:20:03 INFO     ## 1st RUN (EVAL): Configuration 11/12 ##
2023-12-15 04:20:20 INFO     use spaCy answer extraction model: positionrank
2023-12-15 04:20:21 INFO     Model `small_finetuned_ckpt/model_nrudfu/epoch_10`
2023-12-15 04:20:21 INFO     	 * Num of GPU in use: 1
2023-12-15 04:20:21 INFO     	 * Prefix: True
2023-12-15 04:20:21 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 04:20:22 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 04:25:35 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 04:30:48 INFO     	Bleu_1: 0.11910189744825843
2023-12-15 04:30:48 INFO     	Bleu_2: 0.06472536809023023
2023-12-15 04:30:48 INFO     	Bleu_3: 0.03376843562195795
2023-12-15 04:30:48 INFO     	Bleu_4: 0.021321283395881168
2023-12-15 04:30:48 INFO     	Bleu_1: 0.1138881578947361
2023-12-15 04:30:48 INFO     	Bleu_2: 0.06254354388790281
2023-12-15 04:30:48 INFO     	Bleu_3: 0.03358572325631241
2023-12-15 04:30:48 INFO     	Bleu_4: 0.021426130540208536
2023-12-15 04:30:49 INFO     1st RUN RESULTS (validation/Bleu_4)
2023-12-15 04:30:49 INFO     	 * rank: 0 | metric: 0.023 | model: small_finetuned_ckpt/model_nxaqhy/epoch_10 |
2023-12-15 04:30:49 INFO     	 * rank: 1 | metric: 0.023 | model: small_finetuned_ckpt/model_vhyoja/epoch_10 |
2023-12-15 04:30:49 INFO     	 * rank: 2 | metric: 0.021 | model: small_finetuned_ckpt/model_oprhlh/epoch_10 |
2023-12-15 04:30:49 INFO     	 * rank: 3 | metric: 0.021 | model: small_finetuned_ckpt/model_nrudfu/epoch_10 |
2023-12-15 04:30:49 INFO     	 * rank: 4 | metric: 0.019 | model: small_finetuned_ckpt/model_woixzh/epoch_10 |
2023-12-15 04:30:49 INFO     	 * rank: 5 | metric: 0.019 | model: small_finetuned_ckpt/model_uramvg/epoch_10 |
2023-12-15 04:30:49 INFO     	 * rank: 6 | metric: 0.019 | model: small_finetuned_ckpt/model_mntyya/epoch_10 |
2023-12-15 04:30:49 INFO     	 * rank: 7 | metric: 0.019 | model: small_finetuned_ckpt/model_sdkaaa/epoch_10 |
2023-12-15 04:30:49 INFO     	 * rank: 8 | metric: 0.018 | model: small_finetuned_ckpt/model_dpeblg/epoch_10 |
2023-12-15 04:30:49 INFO     	 * rank: 9 | metric: 0.018 | model: small_finetuned_ckpt/model_dpyopu/epoch_10 |
2023-12-15 04:30:49 INFO     	 * rank: 10 | metric: 0.018 | model: small_finetuned_ckpt/model_eszyci/epoch_10 |
2023-12-15 04:30:49 INFO     	 * rank: 11 | metric: 0.018 | model: small_finetuned_ckpt/model_mzgdpa/epoch_10 |
2023-12-15 04:30:49 INFO     ## 2nd RUN: Configuration 0/5: validation/Bleu_4 = 0.02285404140578058
2023-12-15 04:30:49 INFO     initialize model trainer
2023-12-15 04:30:49 INFO     load config from existing checkpoint at small_finetuned_ckpt/model_nxaqhy
2023-12-15 04:30:49 INFO     hyperparameters
2023-12-15 04:30:49 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 04:30:49 INFO     	 * dataset_name: default
2023-12-15 04:30:49 INFO     	 * input_types: ['paragraph']
2023-12-15 04:30:49 INFO     	 * output_types: ['questions_answers']
2023-12-15 04:30:49 INFO     	 * prefix_types: ['qag']
2023-12-15 04:30:49 INFO     	 * model: lmqg/t5-small-squad-qag
2023-12-15 04:30:49 INFO     	 * max_length: 512
2023-12-15 04:30:49 INFO     	 * max_length_output: 512
2023-12-15 04:30:49 INFO     	 * epoch: 15
2023-12-15 04:30:49 INFO     	 * batch: 2
2023-12-15 04:30:49 INFO     	 * lr: 1e-05
2023-12-15 04:30:49 INFO     	 * fp16: False
2023-12-15 04:30:49 INFO     	 * random_seed: 1
2023-12-15 04:30:49 INFO     	 * gradient_accumulation_steps: 4
2023-12-15 04:30:49 INFO     	 * label_smoothing: 0.15
2023-12-15 04:30:49 INFO     load checkpoint from small_finetuned_ckpt/model_nxaqhy/epoch_10
2023-12-15 04:30:49 INFO     use spaCy answer extraction model: positionrank
2023-12-15 04:30:49 INFO     Model `small_finetuned_ckpt/model_nxaqhy/epoch_10`
2023-12-15 04:30:49 INFO     	 * Num of GPU in use: 1
2023-12-15 04:30:49 INFO     	 * Prefix: True
2023-12-15 04:30:49 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 04:30:49 INFO     load optimizer from small_finetuned_ckpt/model_nxaqhy/optimizers/optimizer.10.pt
2023-12-15 04:30:49 INFO     optimizer is loading on cuda
2023-12-15 04:31:22 INFO     dataset preprocessing
2023-12-15 04:31:23 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 04:31:23 INFO     start model training
2023-12-15 04:31:39 INFO     	 * (global step 50: loss: 3.1458043456077576, lr: 1e-05
2023-12-15 04:31:55 INFO     	 * (global step 100: loss: 2.9806167483329773, lr: 1e-05
2023-12-15 04:32:11 INFO     	 * (global step 150: loss: 2.8740312457084656, lr: 1e-05
2023-12-15 04:32:27 INFO     	 * (global step 200: loss: 2.776536226272583, lr: 1e-05
2023-12-15 04:32:43 INFO     	 * (global step 250: loss: 2.8019737601280212, lr: 1e-05
2023-12-15 04:32:59 INFO     	 * (global step 300: loss: 2.8206962943077087, lr: 1e-05
2023-12-15 04:33:02 INFO     [epoch 10/15] average loss: 3.051, lr: 1e-05
2023-12-15 04:33:02 INFO     saving model related files
2023-12-15 04:33:02 INFO     saving model
2023-12-15 04:33:03 INFO     saving tokenizer
2023-12-15 04:33:03 INFO     saving optimizer
2023-12-15 04:33:04 INFO     remove old optimizer files
2023-12-15 04:33:17 INFO     	 * (global step 350: loss: 2.6824073791503906, lr: 1e-05
2023-12-15 04:33:33 INFO     	 * (global step 400: loss: 2.6597554087638855, lr: 1e-05
2023-12-15 04:33:49 INFO     	 * (global step 450: loss: 2.7495182156562805, lr: 1e-05
2023-12-15 04:34:05 INFO     	 * (global step 500: loss: 2.7201836109161377, lr: 1e-05
2023-12-15 04:34:21 INFO     	 * (global step 550: loss: 2.7414172887802124, lr: 1e-05
2023-12-15 04:34:37 INFO     	 * (global step 600: loss: 2.6799137592315674, lr: 1e-05
2023-12-15 04:34:44 INFO     [epoch 11/15] average loss: 2.672, lr: 1e-05
2023-12-15 04:34:44 INFO     saving model related files
2023-12-15 04:34:44 INFO     saving model
2023-12-15 04:34:44 INFO     saving tokenizer
2023-12-15 04:34:44 INFO     saving optimizer
2023-12-15 04:34:45 INFO     remove old optimizer files
2023-12-15 04:34:55 INFO     	 * (global step 650: loss: 2.585816979408264, lr: 1e-05
2023-12-15 04:35:11 INFO     	 * (global step 700: loss: 2.566296875476837, lr: 1e-05
2023-12-15 04:35:27 INFO     	 * (global step 750: loss: 2.5787587761878967, lr: 1e-05
2023-12-15 04:35:43 INFO     	 * (global step 800: loss: 2.5794875025749207, lr: 1e-05
2023-12-15 04:35:59 INFO     	 * (global step 850: loss: 2.724430561065674, lr: 1e-05
2023-12-15 04:36:15 INFO     	 * (global step 900: loss: 2.7012665271759033, lr: 1e-05
2023-12-15 04:36:25 INFO     [epoch 12/15] average loss: 2.607, lr: 1e-05
2023-12-15 04:36:25 INFO     saving model related files
2023-12-15 04:36:25 INFO     saving model
2023-12-15 04:36:26 INFO     saving tokenizer
2023-12-15 04:36:26 INFO     saving optimizer
2023-12-15 04:36:27 INFO     remove old optimizer files
2023-12-15 04:36:33 INFO     	 * (global step 950: loss: 2.6724765300750732, lr: 1e-05
2023-12-15 04:36:49 INFO     	 * (global step 1000: loss: 2.559168577194214, lr: 1e-05
2023-12-15 04:37:05 INFO     	 * (global step 1050: loss: 2.5668275356292725, lr: 1e-05
2023-12-15 04:37:21 INFO     	 * (global step 1100: loss: 2.510456144809723, lr: 1e-05
2023-12-15 04:37:38 INFO     	 * (global step 1150: loss: 2.4787718057632446, lr: 1e-05
2023-12-15 04:37:54 INFO     	 * (global step 1200: loss: 2.5537657141685486, lr: 1e-05
2023-12-15 04:38:07 INFO     [epoch 13/15] average loss: 2.573, lr: 1e-05
2023-12-15 04:38:07 INFO     saving model related files
2023-12-15 04:38:07 INFO     saving model
2023-12-15 04:38:07 INFO     saving tokenizer
2023-12-15 04:38:07 INFO     saving optimizer
2023-12-15 04:38:08 INFO     remove old optimizer files
2023-12-15 04:38:11 INFO     	 * (global step 1250: loss: 2.5673214197158813, lr: 1e-05
2023-12-15 04:38:27 INFO     	 * (global step 1300: loss: 2.4585354328155518, lr: 1e-05
2023-12-15 04:38:44 INFO     	 * (global step 1350: loss: 2.5627121329307556, lr: 1e-05
2023-12-15 04:39:00 INFO     	 * (global step 1400: loss: 2.4786198139190674, lr: 1e-05
2023-12-15 04:39:16 INFO     	 * (global step 1450: loss: 2.5034343600273132, lr: 1e-05
2023-12-15 04:39:32 INFO     	 * (global step 1500: loss: 2.6516091227531433, lr: 1e-05
2023-12-15 04:39:48 INFO     	 * (global step 1550: loss: 2.4951162338256836, lr: 1e-05
2023-12-15 04:39:48 INFO     [epoch 14/15] average loss: 2.549, lr: 1e-05
2023-12-15 04:39:48 INFO     saving model related files
2023-12-15 04:39:48 INFO     saving model
2023-12-15 04:39:48 INFO     saving tokenizer
2023-12-15 04:39:48 INFO     saving optimizer
2023-12-15 04:39:49 INFO     remove old optimizer files
2023-12-15 04:39:49 INFO     complete training: model ckpt was saved at small_finetuned_ckpt/model_nxaqhy
2023-12-15 04:39:49 INFO     ## 2nd RUN: Configuration 1/5: validation/Bleu_4 = 0.02285404140578058
2023-12-15 04:39:49 INFO     initialize model trainer
2023-12-15 04:39:49 INFO     load config from existing checkpoint at small_finetuned_ckpt/model_vhyoja
2023-12-15 04:39:49 INFO     hyperparameters
2023-12-15 04:39:49 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 04:39:49 INFO     	 * dataset_name: default
2023-12-15 04:39:49 INFO     	 * input_types: ['paragraph']
2023-12-15 04:39:49 INFO     	 * output_types: ['questions_answers']
2023-12-15 04:39:49 INFO     	 * prefix_types: ['qag']
2023-12-15 04:39:49 INFO     	 * model: lmqg/t5-small-squad-qag
2023-12-15 04:39:49 INFO     	 * max_length: 512
2023-12-15 04:39:49 INFO     	 * max_length_output: 512
2023-12-15 04:39:49 INFO     	 * epoch: 15
2023-12-15 04:39:49 INFO     	 * batch: 2
2023-12-15 04:39:49 INFO     	 * lr: 1e-05
2023-12-15 04:39:49 INFO     	 * fp16: False
2023-12-15 04:39:49 INFO     	 * random_seed: 1
2023-12-15 04:39:49 INFO     	 * gradient_accumulation_steps: 4
2023-12-15 04:39:49 INFO     	 * label_smoothing: 0.0
2023-12-15 04:39:49 INFO     load checkpoint from small_finetuned_ckpt/model_vhyoja/epoch_10
2023-12-15 04:39:50 INFO     use spaCy answer extraction model: positionrank
2023-12-15 04:39:50 INFO     Model `small_finetuned_ckpt/model_vhyoja/epoch_10`
2023-12-15 04:39:50 INFO     	 * Num of GPU in use: 1
2023-12-15 04:39:50 INFO     	 * Prefix: True
2023-12-15 04:39:50 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 04:39:50 INFO     load optimizer from small_finetuned_ckpt/model_vhyoja/optimizers/optimizer.10.pt
2023-12-15 04:39:50 INFO     optimizer is loading on cuda
2023-12-15 04:40:18 INFO     dataset preprocessing
2023-12-15 04:40:19 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 04:40:19 INFO     start model training
2023-12-15 04:40:35 INFO     	 * (global step 50: loss: 0.332665279507637, lr: 1e-05
2023-12-15 04:40:50 INFO     	 * (global step 100: loss: 0.37437763810157776, lr: 1e-05
2023-12-15 04:41:06 INFO     	 * (global step 150: loss: 0.34931397438049316, lr: 1e-05
2023-12-15 04:41:22 INFO     	 * (global step 200: loss: 0.33288733661174774, lr: 1e-05
2023-12-15 04:41:37 INFO     	 * (global step 250: loss: 0.4256419539451599, lr: 1e-05
2023-12-15 04:41:53 INFO     	 * (global step 300: loss: 0.4674191288650036, lr: 1e-05
2023-12-15 04:41:56 INFO     [epoch 10/15] average loss: 0.374, lr: 1e-05
2023-12-15 04:41:56 INFO     saving model related files
2023-12-15 04:41:56 INFO     saving model
2023-12-15 04:41:57 INFO     saving tokenizer
2023-12-15 04:41:57 INFO     saving optimizer
2023-12-15 04:41:58 INFO     remove old optimizer files
2023-12-15 04:42:11 INFO     	 * (global step 350: loss: 0.3537619225680828, lr: 1e-05
2023-12-15 04:42:26 INFO     	 * (global step 400: loss: 0.3432767167687416, lr: 1e-05
2023-12-15 04:42:42 INFO     	 * (global step 450: loss: 0.459067914634943, lr: 1e-05
2023-12-15 04:42:58 INFO     	 * (global step 500: loss: 0.4543481469154358, lr: 1e-05
2023-12-15 04:43:13 INFO     	 * (global step 550: loss: 0.47778042405843735, lr: 1e-05
2023-12-15 04:43:29 INFO     	 * (global step 600: loss: 0.4151357002556324, lr: 1e-05
2023-12-15 04:43:36 INFO     [epoch 11/15] average loss: 0.372, lr: 1e-05
2023-12-15 04:43:36 INFO     saving model related files
2023-12-15 04:43:36 INFO     saving model
2023-12-15 04:43:36 INFO     saving tokenizer
2023-12-15 04:43:36 INFO     saving optimizer
2023-12-15 04:43:37 INFO     remove old optimizer files
2023-12-15 04:43:47 INFO     	 * (global step 650: loss: 0.3500543124973774, lr: 1e-05
2023-12-15 04:44:02 INFO     	 * (global step 700: loss: 0.3152810111641884, lr: 1e-05
2023-12-15 04:44:18 INFO     	 * (global step 750: loss: 0.33080900460481644, lr: 1e-05
2023-12-15 04:44:34 INFO     	 * (global step 800: loss: 0.34881430864334106, lr: 1e-05
2023-12-15 04:44:50 INFO     	 * (global step 850: loss: 0.4979715868830681, lr: 1e-05
2023-12-15 04:45:05 INFO     	 * (global step 900: loss: 0.4927961602807045, lr: 1e-05
2023-12-15 04:45:15 INFO     [epoch 12/15] average loss: 0.369, lr: 1e-05
2023-12-15 04:45:15 INFO     saving model related files
2023-12-15 04:45:15 INFO     saving model
2023-12-15 04:45:15 INFO     saving tokenizer
2023-12-15 04:45:15 INFO     saving optimizer
2023-12-15 04:45:16 INFO     remove old optimizer files
2023-12-15 04:45:23 INFO     	 * (global step 950: loss: 0.46154269576072693, lr: 1e-05
2023-12-15 04:45:38 INFO     	 * (global step 1000: loss: 0.35401754453778267, lr: 1e-05
2023-12-15 04:45:54 INFO     	 * (global step 1050: loss: 0.3572983071208, lr: 1e-05
2023-12-15 04:46:10 INFO     	 * (global step 1100: loss: 0.3119014985859394, lr: 1e-05
2023-12-15 04:46:25 INFO     	 * (global step 1150: loss: 0.2767697535455227, lr: 1e-05
2023-12-15 04:46:41 INFO     	 * (global step 1200: loss: 0.34935374557971954, lr: 1e-05
2023-12-15 04:46:54 INFO     [epoch 13/15] average loss: 0.367, lr: 1e-05
2023-12-15 04:46:54 INFO     saving model related files
2023-12-15 04:46:54 INFO     saving model
2023-12-15 04:46:54 INFO     saving tokenizer
2023-12-15 04:46:54 INFO     saving optimizer
2023-12-15 04:46:55 INFO     remove old optimizer files
2023-12-15 04:46:58 INFO     	 * (global step 1250: loss: 0.37900837883353233, lr: 1e-05
2023-12-15 04:47:14 INFO     	 * (global step 1300: loss: 0.2579158656299114, lr: 1e-05
2023-12-15 04:47:30 INFO     	 * (global step 1350: loss: 0.359480082988739, lr: 1e-05
2023-12-15 04:47:45 INFO     	 * (global step 1400: loss: 0.28256410732865334, lr: 1e-05
2023-12-15 04:48:01 INFO     	 * (global step 1450: loss: 0.3198712542653084, lr: 1e-05
2023-12-15 04:48:17 INFO     	 * (global step 1500: loss: 0.46492644399404526, lr: 1e-05
2023-12-15 04:48:32 INFO     	 * (global step 1550: loss: 0.32480981945991516, lr: 1e-05
2023-12-15 04:48:33 INFO     [epoch 14/15] average loss: 0.365, lr: 1e-05
2023-12-15 04:48:33 INFO     saving model related files
2023-12-15 04:48:33 INFO     saving model
2023-12-15 04:48:33 INFO     saving tokenizer
2023-12-15 04:48:33 INFO     saving optimizer
2023-12-15 04:48:34 INFO     remove old optimizer files
2023-12-15 04:48:34 INFO     complete training: model ckpt was saved at small_finetuned_ckpt/model_vhyoja
2023-12-15 04:48:34 INFO     ## 2nd RUN: Configuration 2/5: validation/Bleu_4 = 0.021321283395881168
2023-12-15 04:48:34 INFO     initialize model trainer
2023-12-15 04:48:34 INFO     load config from existing checkpoint at small_finetuned_ckpt/model_oprhlh
2023-12-15 04:48:34 INFO     hyperparameters
2023-12-15 04:48:34 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 04:48:34 INFO     	 * dataset_name: default
2023-12-15 04:48:34 INFO     	 * input_types: ['paragraph']
2023-12-15 04:48:34 INFO     	 * output_types: ['questions_answers']
2023-12-15 04:48:34 INFO     	 * prefix_types: ['qag']
2023-12-15 04:48:34 INFO     	 * model: lmqg/t5-small-squad-qag
2023-12-15 04:48:34 INFO     	 * max_length: 512
2023-12-15 04:48:34 INFO     	 * max_length_output: 512
2023-12-15 04:48:34 INFO     	 * epoch: 15
2023-12-15 04:48:34 INFO     	 * batch: 2
2023-12-15 04:48:34 INFO     	 * lr: 1e-05
2023-12-15 04:48:34 INFO     	 * fp16: False
2023-12-15 04:48:34 INFO     	 * random_seed: 1
2023-12-15 04:48:34 INFO     	 * gradient_accumulation_steps: 2
2023-12-15 04:48:34 INFO     	 * label_smoothing: 0.15
2023-12-15 04:48:34 INFO     load checkpoint from small_finetuned_ckpt/model_oprhlh/epoch_10
2023-12-15 04:48:35 INFO     use spaCy answer extraction model: positionrank
2023-12-15 04:48:35 INFO     Model `small_finetuned_ckpt/model_oprhlh/epoch_10`
2023-12-15 04:48:35 INFO     	 * Num of GPU in use: 1
2023-12-15 04:48:35 INFO     	 * Prefix: True
2023-12-15 04:48:35 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 04:48:35 INFO     load optimizer from small_finetuned_ckpt/model_oprhlh/optimizers/optimizer.10.pt
2023-12-15 04:48:35 INFO     optimizer is loading on cuda
2023-12-15 04:48:52 INFO     dataset preprocessing
2023-12-15 04:48:53 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 04:48:53 INFO     start model training
2023-12-15 04:49:01 INFO     	 * (global step 50: loss: 3.206494450569153, lr: 1e-05
2023-12-15 04:49:10 INFO     	 * (global step 100: loss: 2.916473388671875, lr: 1e-05
2023-12-15 04:49:18 INFO     	 * (global step 150: loss: 2.828893542289734, lr: 1e-05
2023-12-15 04:49:26 INFO     	 * (global step 200: loss: 2.862015128135681, lr: 1e-05
2023-12-15 04:49:35 INFO     	 * (global step 250: loss: 2.811436414718628, lr: 1e-05
2023-12-15 04:49:43 INFO     	 * (global step 300: loss: 2.7259175777435303, lr: 1e-05
2023-12-15 04:49:51 INFO     	 * (global step 350: loss: 2.9049792289733887, lr: 1e-05
2023-12-15 04:50:00 INFO     	 * (global step 400: loss: 2.6662466526031494, lr: 1e-05
2023-12-15 04:50:08 INFO     	 * (global step 450: loss: 2.588676929473877, lr: 1e-05
2023-12-15 04:50:16 INFO     	 * (global step 500: loss: 2.689422369003296, lr: 1e-05
2023-12-15 04:50:25 INFO     	 * (global step 550: loss: 2.5778533220291138, lr: 1e-05
2023-12-15 04:50:33 INFO     	 * (global step 600: loss: 2.849149227142334, lr: 1e-05
2023-12-15 04:50:37 INFO     [epoch 10/15] average loss: 2.89, lr: 1e-05
2023-12-15 04:50:37 INFO     saving model related files
2023-12-15 04:50:37 INFO     saving model
2023-12-15 04:50:37 INFO     saving tokenizer
2023-12-15 04:50:37 INFO     saving optimizer
2023-12-15 04:50:38 INFO     remove old optimizer files
2023-12-15 04:50:43 INFO     	 * (global step 650: loss: 2.689265727996826, lr: 1e-05
2023-12-15 04:50:52 INFO     	 * (global step 700: loss: 2.6492207050323486, lr: 1e-05
2023-12-15 04:51:00 INFO     	 * (global step 750: loss: 2.5776811838150024, lr: 1e-05
2023-12-15 04:51:08 INFO     	 * (global step 800: loss: 2.6235318183898926, lr: 1e-05
2023-12-15 04:51:17 INFO     	 * (global step 850: loss: 2.8220850229263306, lr: 1e-05
2023-12-15 04:51:25 INFO     	 * (global step 900: loss: 2.7060728073120117, lr: 1e-05
2023-12-15 04:51:33 INFO     	 * (global step 950: loss: 2.5611761808395386, lr: 1e-05
2023-12-15 04:51:42 INFO     	 * (global step 1000: loss: 2.7800629138946533, lr: 1e-05
2023-12-15 04:51:50 INFO     	 * (global step 1050: loss: 2.5684584379196167, lr: 1e-05
2023-12-15 04:51:59 INFO     	 * (global step 1100: loss: 2.6533617973327637, lr: 1e-05
2023-12-15 04:52:07 INFO     	 * (global step 1150: loss: 2.4679194688796997, lr: 1e-05
2023-12-15 04:52:15 INFO     	 * (global step 1200: loss: 2.7770981788635254, lr: 1e-05
2023-12-15 04:52:22 INFO     [epoch 11/15] average loss: 2.603, lr: 1e-05
2023-12-15 04:52:22 INFO     saving model related files
2023-12-15 04:52:22 INFO     saving model
2023-12-15 04:52:23 INFO     saving tokenizer
2023-12-15 04:52:23 INFO     saving optimizer
2023-12-15 04:52:24 INFO     remove old optimizer files
2023-12-15 04:52:25 INFO     	 * (global step 1250: loss: 2.624125123023987, lr: 1e-05
2023-12-15 04:52:34 INFO     	 * (global step 1300: loss: 2.464623808860779, lr: 1e-05
2023-12-15 04:52:42 INFO     	 * (global step 1350: loss: 2.388208508491516, lr: 1e-05
2023-12-15 04:52:50 INFO     	 * (global step 1400: loss: 2.6279836893081665, lr: 1e-05
2023-12-15 04:52:59 INFO     	 * (global step 1450: loss: 2.592760443687439, lr: 1e-05
2023-12-15 04:53:07 INFO     	 * (global step 1500: loss: 2.4400339126586914, lr: 1e-05
2023-12-15 04:53:16 INFO     	 * (global step 1550: loss: 2.4808493852615356, lr: 1e-05
2023-12-15 04:53:24 INFO     	 * (global step 1600: loss: 2.4897741079330444, lr: 1e-05
2023-12-15 04:53:32 INFO     	 * (global step 1650: loss: 2.4977376461029053, lr: 1e-05
2023-12-15 04:53:41 INFO     	 * (global step 1700: loss: 2.500727891921997, lr: 1e-05
2023-12-15 04:53:49 INFO     	 * (global step 1750: loss: 2.459255576133728, lr: 1e-05
2023-12-15 04:53:57 INFO     	 * (global step 1800: loss: 2.558256149291992, lr: 1e-05
2023-12-15 04:54:06 INFO     	 * (global step 1850: loss: 2.4622830152511597, lr: 1e-05
2023-12-15 04:54:08 INFO     [epoch 12/15] average loss: 2.55, lr: 1e-05
2023-12-15 04:54:08 INFO     saving model related files
2023-12-15 04:54:08 INFO     saving model
2023-12-15 04:54:08 INFO     saving tokenizer
2023-12-15 04:54:08 INFO     saving optimizer
2023-12-15 04:54:09 INFO     remove old optimizer files
2023-12-15 04:54:16 INFO     	 * (global step 1900: loss: 2.411982297897339, lr: 1e-05
2023-12-15 04:54:24 INFO     	 * (global step 1950: loss: 2.4608370065689087, lr: 1e-05
2023-12-15 04:54:32 INFO     	 * (global step 2000: loss: 2.5747344493865967, lr: 1e-05
2023-12-15 04:54:41 INFO     	 * (global step 2050: loss: 2.639338970184326, lr: 1e-05
2023-12-15 04:54:49 INFO     	 * (global step 2100: loss: 2.5133835077285767, lr: 1e-05
2023-12-15 04:54:58 INFO     	 * (global step 2150: loss: 2.4844599962234497, lr: 1e-05
2023-12-15 04:55:06 INFO     	 * (global step 2200: loss: 2.570660948753357, lr: 1e-05
2023-12-15 04:55:14 INFO     	 * (global step 2250: loss: 2.536175489425659, lr: 1e-05
2023-12-15 04:55:23 INFO     	 * (global step 2300: loss: 2.5402621030807495, lr: 1e-05
2023-12-15 04:55:31 INFO     	 * (global step 2350: loss: 2.4841043949127197, lr: 1e-05
2023-12-15 04:55:39 INFO     	 * (global step 2400: loss: 2.6036497354507446, lr: 1e-05
2023-12-15 04:55:48 INFO     	 * (global step 2450: loss: 2.4647504091262817, lr: 1e-05
2023-12-15 04:55:54 INFO     [epoch 13/15] average loss: 2.52, lr: 1e-05
2023-12-15 04:55:54 INFO     saving model related files
2023-12-15 04:55:54 INFO     saving model
2023-12-15 04:55:54 INFO     saving tokenizer
2023-12-15 04:55:54 INFO     saving optimizer
2023-12-15 04:55:55 INFO     remove old optimizer files
2023-12-15 04:55:58 INFO     	 * (global step 2500: loss: 2.443997859954834, lr: 1e-05
2023-12-15 04:56:06 INFO     	 * (global step 2550: loss: 2.426958918571472, lr: 1e-05
2023-12-15 04:56:14 INFO     	 * (global step 2600: loss: 2.6565377712249756, lr: 1e-05
2023-12-15 04:56:23 INFO     	 * (global step 2650: loss: 2.5403170585632324, lr: 1e-05
2023-12-15 04:56:31 INFO     	 * (global step 2700: loss: 2.463324546813965, lr: 1e-05
2023-12-15 04:56:40 INFO     	 * (global step 2750: loss: 2.520508050918579, lr: 1e-05
2023-12-15 04:56:48 INFO     	 * (global step 2800: loss: 2.798349380493164, lr: 1e-05
2023-12-15 04:56:56 INFO     	 * (global step 2850: loss: 2.5339690446853638, lr: 1e-05
2023-12-15 04:57:05 INFO     	 * (global step 2900: loss: 2.3595664501190186, lr: 1e-05
2023-12-15 04:57:13 INFO     	 * (global step 2950: loss: 2.459113597869873, lr: 1e-05
2023-12-15 04:57:21 INFO     	 * (global step 3000: loss: 2.609388828277588, lr: 1e-05
2023-12-15 04:57:30 INFO     	 * (global step 3050: loss: 2.4404200315475464, lr: 1e-05
2023-12-15 04:57:38 INFO     	 * (global step 3100: loss: 2.4407613277435303, lr: 1e-05
2023-12-15 04:57:39 INFO     [epoch 14/15] average loss: 2.499, lr: 1e-05
2023-12-15 04:57:39 INFO     saving model related files
2023-12-15 04:57:39 INFO     saving model
2023-12-15 04:57:40 INFO     saving tokenizer
2023-12-15 04:57:40 INFO     saving optimizer
2023-12-15 04:57:41 INFO     remove old optimizer files
2023-12-15 04:57:41 INFO     complete training: model ckpt was saved at small_finetuned_ckpt/model_oprhlh
2023-12-15 04:57:41 INFO     ## 2nd RUN: Configuration 3/5: validation/Bleu_4 = 0.021321283395881168
2023-12-15 04:57:41 INFO     initialize model trainer
2023-12-15 04:57:41 INFO     load config from existing checkpoint at small_finetuned_ckpt/model_nrudfu
2023-12-15 04:57:41 INFO     hyperparameters
2023-12-15 04:57:41 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 04:57:41 INFO     	 * dataset_name: default
2023-12-15 04:57:41 INFO     	 * input_types: ['paragraph']
2023-12-15 04:57:41 INFO     	 * output_types: ['questions_answers']
2023-12-15 04:57:41 INFO     	 * prefix_types: ['qag']
2023-12-15 04:57:41 INFO     	 * model: lmqg/t5-small-squad-qag
2023-12-15 04:57:41 INFO     	 * max_length: 512
2023-12-15 04:57:41 INFO     	 * max_length_output: 512
2023-12-15 04:57:41 INFO     	 * epoch: 15
2023-12-15 04:57:41 INFO     	 * batch: 2
2023-12-15 04:57:41 INFO     	 * lr: 1e-05
2023-12-15 04:57:41 INFO     	 * fp16: False
2023-12-15 04:57:41 INFO     	 * random_seed: 1
2023-12-15 04:57:41 INFO     	 * gradient_accumulation_steps: 2
2023-12-15 04:57:41 INFO     	 * label_smoothing: 0.0
2023-12-15 04:57:41 INFO     load checkpoint from small_finetuned_ckpt/model_nrudfu/epoch_10
2023-12-15 04:57:41 INFO     use spaCy answer extraction model: positionrank
2023-12-15 04:57:42 INFO     Model `small_finetuned_ckpt/model_nrudfu/epoch_10`
2023-12-15 04:57:42 INFO     	 * Num of GPU in use: 1
2023-12-15 04:57:42 INFO     	 * Prefix: True
2023-12-15 04:57:42 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 04:57:42 INFO     load optimizer from small_finetuned_ckpt/model_nrudfu/optimizers/optimizer.10.pt
2023-12-15 04:57:42 INFO     optimizer is loading on cuda
2023-12-15 04:58:20 INFO     dataset preprocessing
2023-12-15 04:58:21 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 04:58:21 INFO     start model training
2023-12-15 04:58:29 INFO     	 * (global step 50: loss: 0.35736532509326935, lr: 1e-05
2023-12-15 04:58:37 INFO     	 * (global step 100: loss: 0.24923700094223022, lr: 1e-05
2023-12-15 04:58:45 INFO     	 * (global step 150: loss: 0.3166409283876419, lr: 1e-05
2023-12-15 04:58:53 INFO     	 * (global step 200: loss: 0.36053672432899475, lr: 1e-05
2023-12-15 04:59:01 INFO     	 * (global step 250: loss: 0.38474369049072266, lr: 1e-05
2023-12-15 04:59:09 INFO     	 * (global step 300: loss: 0.33076658844947815, lr: 1e-05
2023-12-15 04:59:17 INFO     	 * (global step 350: loss: 0.5328958630561829, lr: 1e-05
2023-12-15 04:59:26 INFO     	 * (global step 400: loss: 0.3208368718624115, lr: 1e-05
2023-12-15 04:59:34 INFO     	 * (global step 450: loss: 0.26406625658273697, lr: 1e-05
2023-12-15 04:59:42 INFO     	 * (global step 500: loss: 0.38541126251220703, lr: 1e-05
2023-12-15 04:59:50 INFO     	 * (global step 550: loss: 0.28179794549942017, lr: 1e-05
2023-12-15 04:59:58 INFO     	 * (global step 600: loss: 0.5773507803678513, lr: 1e-05
2023-12-15 05:00:02 INFO     [epoch 10/15] average loss: 0.364, lr: 1e-05
2023-12-15 05:00:02 INFO     saving model related files
2023-12-15 05:00:02 INFO     saving model
2023-12-15 05:00:02 INFO     saving tokenizer
2023-12-15 05:00:02 INFO     saving optimizer
2023-12-15 05:00:03 INFO     remove old optimizer files
2023-12-15 05:00:08 INFO     	 * (global step 650: loss: 0.42384617030620575, lr: 1e-05
2023-12-15 05:00:16 INFO     	 * (global step 700: loss: 0.395551472902298, lr: 1e-05
2023-12-15 05:00:25 INFO     	 * (global step 750: loss: 0.3122842609882355, lr: 1e-05
2023-12-15 05:00:33 INFO     	 * (global step 800: loss: 0.36451417207717896, lr: 1e-05
2023-12-15 05:00:41 INFO     	 * (global step 850: loss: 0.6035177856683731, lr: 1e-05
2023-12-15 05:00:49 INFO     	 * (global step 900: loss: 0.4660636857151985, lr: 1e-05
2023-12-15 05:00:57 INFO     	 * (global step 950: loss: 0.3268593028187752, lr: 1e-05
2023-12-15 05:01:05 INFO     	 * (global step 1000: loss: 0.5694844722747803, lr: 1e-05
2023-12-15 05:01:13 INFO     	 * (global step 1050: loss: 0.32989028096199036, lr: 1e-05
2023-12-15 05:01:21 INFO     	 * (global step 1100: loss: 0.4453105628490448, lr: 1e-05
2023-12-15 05:01:30 INFO     	 * (global step 1150: loss: 0.22770127654075623, lr: 1e-05
2023-12-15 05:01:38 INFO     	 * (global step 1200: loss: 0.562985748052597, lr: 1e-05
2023-12-15 05:01:45 INFO     [epoch 11/15] average loss: 0.362, lr: 1e-05
2023-12-15 05:01:45 INFO     saving model related files
2023-12-15 05:01:45 INFO     saving model
2023-12-15 05:01:45 INFO     saving tokenizer
2023-12-15 05:01:45 INFO     saving optimizer
2023-12-15 05:01:46 INFO     remove old optimizer files
2023-12-15 05:01:47 INFO     	 * (global step 1250: loss: 0.4469117671251297, lr: 1e-05
2023-12-15 05:01:55 INFO     	 * (global step 1300: loss: 0.25959355384111404, lr: 1e-05
2023-12-15 05:02:04 INFO     	 * (global step 1350: loss: 0.17312312126159668, lr: 1e-05
2023-12-15 05:02:12 INFO     	 * (global step 1400: loss: 0.4477340877056122, lr: 1e-05
2023-12-15 05:02:20 INFO     	 * (global step 1450: loss: 0.3827575743198395, lr: 1e-05
2023-12-15 05:02:28 INFO     	 * (global step 1500: loss: 0.236397884786129, lr: 1e-05
2023-12-15 05:02:36 INFO     	 * (global step 1550: loss: 0.29372885078191757, lr: 1e-05
2023-12-15 05:02:44 INFO     	 * (global step 1600: loss: 0.3152942955493927, lr: 1e-05
2023-12-15 05:02:52 INFO     	 * (global step 1650: loss: 0.32152949273586273, lr: 1e-05
2023-12-15 05:03:01 INFO     	 * (global step 1700: loss: 0.3173605650663376, lr: 1e-05
2023-12-15 05:03:09 INFO     	 * (global step 1750: loss: 0.2832137644290924, lr: 1e-05
2023-12-15 05:03:17 INFO     	 * (global step 1800: loss: 0.36927665770053864, lr: 1e-05
2023-12-15 05:03:25 INFO     	 * (global step 1850: loss: 0.2959485873579979, lr: 1e-05
2023-12-15 05:03:27 INFO     [epoch 12/15] average loss: 0.358, lr: 1e-05
2023-12-15 05:03:27 INFO     saving model related files
2023-12-15 05:03:27 INFO     saving model
2023-12-15 05:03:28 INFO     saving tokenizer
2023-12-15 05:03:28 INFO     saving optimizer
2023-12-15 05:03:29 INFO     remove old optimizer files
2023-12-15 05:03:35 INFO     	 * (global step 1900: loss: 0.23095703125, lr: 1e-05
2023-12-15 05:03:43 INFO     	 * (global step 1950: loss: 0.29580946266651154, lr: 1e-05
2023-12-15 05:03:51 INFO     	 * (global step 2000: loss: 0.41814637184143066, lr: 1e-05
2023-12-15 05:03:59 INFO     	 * (global step 2050: loss: 0.46490880846977234, lr: 1e-05
2023-12-15 05:04:07 INFO     	 * (global step 2100: loss: 0.3371478468179703, lr: 1e-05
2023-12-15 05:04:15 INFO     	 * (global step 2150: loss: 0.3026130199432373, lr: 1e-05
2023-12-15 05:04:24 INFO     	 * (global step 2200: loss: 0.40900103747844696, lr: 1e-05
2023-12-15 05:04:32 INFO     	 * (global step 2250: loss: 0.3722284287214279, lr: 1e-05
2023-12-15 05:04:40 INFO     	 * (global step 2300: loss: 0.3800511062145233, lr: 1e-05
2023-12-15 05:04:48 INFO     	 * (global step 2350: loss: 0.3493987023830414, lr: 1e-05
2023-12-15 05:04:56 INFO     	 * (global step 2400: loss: 0.45722542703151703, lr: 1e-05
2023-12-15 05:05:04 INFO     	 * (global step 2450: loss: 0.31452464312314987, lr: 1e-05
2023-12-15 05:05:10 INFO     [epoch 13/15] average loss: 0.357, lr: 1e-05
2023-12-15 05:05:10 INFO     saving model related files
2023-12-15 05:05:10 INFO     saving model
2023-12-15 05:05:10 INFO     saving tokenizer
2023-12-15 05:05:10 INFO     saving optimizer
2023-12-15 05:05:11 INFO     remove old optimizer files
2023-12-15 05:05:14 INFO     	 * (global step 2500: loss: 0.303411602973938, lr: 1e-05
2023-12-15 05:05:22 INFO     	 * (global step 2550: loss: 0.26616159081459045, lr: 1e-05
2023-12-15 05:05:30 INFO     	 * (global step 2600: loss: 0.5239793881773949, lr: 1e-05
2023-12-15 05:05:38 INFO     	 * (global step 2650: loss: 0.4241113215684891, lr: 1e-05
2023-12-15 05:05:46 INFO     	 * (global step 2700: loss: 0.29750169813632965, lr: 1e-05
2023-12-15 05:05:54 INFO     	 * (global step 2750: loss: 0.34871064871549606, lr: 1e-05
2023-12-15 05:06:02 INFO     	 * (global step 2800: loss: 0.6391788870096207, lr: 1e-05
2023-12-15 05:06:11 INFO     	 * (global step 2850: loss: 0.39464332163333893, lr: 1e-05
2023-12-15 05:06:19 INFO     	 * (global step 2900: loss: 0.23492520302534103, lr: 1e-05
2023-12-15 05:06:27 INFO     	 * (global step 2950: loss: 0.32261544466018677, lr: 1e-05
2023-12-15 05:06:35 INFO     	 * (global step 3000: loss: 0.4709360599517822, lr: 1e-05
2023-12-15 05:06:43 INFO     	 * (global step 3050: loss: 0.29788102209568024, lr: 1e-05
2023-12-15 05:06:51 INFO     	 * (global step 3100: loss: 0.2977372854948044, lr: 1e-05
2023-12-15 05:06:52 INFO     [epoch 14/15] average loss: 0.354, lr: 1e-05
2023-12-15 05:06:52 INFO     saving model related files
2023-12-15 05:06:52 INFO     saving model
2023-12-15 05:06:53 INFO     saving tokenizer
2023-12-15 05:06:53 INFO     saving optimizer
2023-12-15 05:06:54 INFO     remove old optimizer files
2023-12-15 05:06:54 INFO     complete training: model ckpt was saved at small_finetuned_ckpt/model_nrudfu
2023-12-15 05:06:54 INFO     ## 2nd RUN: Configuration 4/5: validation/Bleu_4 = 0.019148507855686385
2023-12-15 05:06:54 INFO     initialize model trainer
2023-12-15 05:06:54 INFO     load config from existing checkpoint at small_finetuned_ckpt/model_woixzh
2023-12-15 05:06:54 INFO     hyperparameters
2023-12-15 05:06:54 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 05:06:54 INFO     	 * dataset_name: default
2023-12-15 05:06:54 INFO     	 * input_types: ['paragraph']
2023-12-15 05:06:54 INFO     	 * output_types: ['questions_answers']
2023-12-15 05:06:54 INFO     	 * prefix_types: ['qag']
2023-12-15 05:06:54 INFO     	 * model: lmqg/t5-small-squad-qag
2023-12-15 05:06:54 INFO     	 * max_length: 512
2023-12-15 05:06:54 INFO     	 * max_length_output: 512
2023-12-15 05:06:54 INFO     	 * epoch: 15
2023-12-15 05:06:54 INFO     	 * batch: 2
2023-12-15 05:06:54 INFO     	 * lr: 5e-05
2023-12-15 05:06:54 INFO     	 * fp16: False
2023-12-15 05:06:54 INFO     	 * random_seed: 1
2023-12-15 05:06:54 INFO     	 * gradient_accumulation_steps: 2
2023-12-15 05:06:54 INFO     	 * label_smoothing: 0.15
2023-12-15 05:06:54 INFO     load checkpoint from small_finetuned_ckpt/model_woixzh/epoch_10
2023-12-15 05:06:54 INFO     use spaCy answer extraction model: positionrank
2023-12-15 05:06:55 INFO     Model `small_finetuned_ckpt/model_woixzh/epoch_10`
2023-12-15 05:06:55 INFO     	 * Num of GPU in use: 1
2023-12-15 05:06:55 INFO     	 * Prefix: True
2023-12-15 05:06:55 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 05:06:55 INFO     load optimizer from small_finetuned_ckpt/model_woixzh/optimizers/optimizer.10.pt
2023-12-15 05:06:55 INFO     optimizer is loading on cuda
2023-12-15 05:07:34 INFO     dataset preprocessing
2023-12-15 05:07:35 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/lmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 05:07:36 INFO     start model training
2023-12-15 05:07:44 INFO     	 * (global step 50: loss: 2.7473093271255493, lr: 5e-05
2023-12-15 05:07:52 INFO     	 * (global step 100: loss: 2.504344344139099, lr: 5e-05
2023-12-15 05:08:00 INFO     	 * (global step 150: loss: 2.505995273590088, lr: 5e-05
2023-12-15 05:08:09 INFO     	 * (global step 200: loss: 2.5458078384399414, lr: 5e-05
2023-12-15 05:08:17 INFO     	 * (global step 250: loss: 2.5216230154037476, lr: 5e-05
2023-12-15 05:08:25 INFO     	 * (global step 300: loss: 2.4590744972229004, lr: 5e-05
2023-12-15 05:08:34 INFO     	 * (global step 350: loss: 2.6382426023483276, lr: 5e-05
2023-12-15 05:08:42 INFO     	 * (global step 400: loss: 2.429666519165039, lr: 5e-05
2023-12-15 05:08:50 INFO     	 * (global step 450: loss: 2.3711048364639282, lr: 5e-05
2023-12-15 05:08:59 INFO     	 * (global step 500: loss: 2.4742895364761353, lr: 5e-05
2023-12-15 05:09:07 INFO     	 * (global step 550: loss: 2.3845595121383667, lr: 5e-05
2023-12-15 05:09:15 INFO     	 * (global step 600: loss: 2.6130309104919434, lr: 5e-05
2023-12-15 05:09:19 INFO     [epoch 10/15] average loss: 2.57, lr: 5e-05
2023-12-15 05:09:19 INFO     saving model related files
2023-12-15 05:09:19 INFO     saving model
2023-12-15 05:09:20 INFO     saving tokenizer
2023-12-15 05:09:20 INFO     saving optimizer
2023-12-15 05:09:21 INFO     remove old optimizer files
2023-12-15 05:09:26 INFO     	 * (global step 650: loss: 2.485754370689392, lr: 5e-05
2023-12-15 05:09:34 INFO     	 * (global step 700: loss: 2.460529327392578, lr: 5e-05
2023-12-15 05:09:42 INFO     	 * (global step 750: loss: 2.4024722576141357, lr: 5e-05
2023-12-15 05:09:51 INFO     	 * (global step 800: loss: 2.4349751472473145, lr: 5e-05
2023-12-15 05:09:59 INFO     	 * (global step 850: loss: 2.6048238277435303, lr: 5e-05
2023-12-15 05:10:08 INFO     	 * (global step 900: loss: 2.4966180324554443, lr: 5e-05
2023-12-15 05:10:16 INFO     	 * (global step 950: loss: 2.389469265937805, lr: 5e-05
2023-12-15 05:10:24 INFO     	 * (global step 1000: loss: 2.57901668548584, lr: 5e-05
2023-12-15 05:10:33 INFO     	 * (global step 1050: loss: 2.3792093992233276, lr: 5e-05
2023-12-15 05:10:41 INFO     	 * (global step 1100: loss: 2.4707099199295044, lr: 5e-05
2023-12-15 05:10:49 INFO     	 * (global step 1150: loss: 2.3045852184295654, lr: 5e-05
2023-12-15 05:10:58 INFO     	 * (global step 1200: loss: 2.57932710647583, lr: 5e-05
2023-12-15 05:11:05 INFO     [epoch 11/15] average loss: 2.42, lr: 5e-05
2023-12-15 05:11:05 INFO     saving model related files
2023-12-15 05:11:05 INFO     saving model
2023-12-15 05:11:05 INFO     saving tokenizer
2023-12-15 05:11:05 INFO     saving optimizer
2023-12-15 05:11:06 INFO     remove old optimizer files
2023-12-15 05:11:08 INFO     	 * (global step 1250: loss: 2.4650341272354126, lr: 5e-05
2023-12-15 05:11:16 INFO     	 * (global step 1300: loss: 2.3126286268234253, lr: 5e-05
2023-12-15 05:11:25 INFO     	 * (global step 1350: loss: 2.242606043815613, lr: 5e-05
2023-12-15 05:11:33 INFO     	 * (global step 1400: loss: 2.463226079940796, lr: 5e-05
2023-12-15 05:11:42 INFO     	 * (global step 1450: loss: 2.43615186214447, lr: 5e-05
2023-12-15 05:11:50 INFO     	 * (global step 1500: loss: 2.2976233959198, lr: 5e-05
2023-12-15 05:11:58 INFO     	 * (global step 1550: loss: 2.341778039932251, lr: 5e-05
2023-12-15 05:12:07 INFO     	 * (global step 1600: loss: 2.351510167121887, lr: 5e-05
2023-12-15 05:12:15 INFO     	 * (global step 1650: loss: 2.352262496948242, lr: 5e-05
2023-12-15 05:12:24 INFO     	 * (global step 1700: loss: 2.3586655855178833, lr: 5e-05
2023-12-15 05:12:32 INFO     	 * (global step 1750: loss: 2.3253889083862305, lr: 5e-05
2023-12-15 05:12:40 INFO     	 * (global step 1800: loss: 2.3960647583007812, lr: 5e-05
2023-12-15 05:12:49 INFO     	 * (global step 1850: loss: 2.3313772678375244, lr: 5e-05
2023-12-15 05:12:51 INFO     [epoch 12/15] average loss: 2.391, lr: 5e-05
2023-12-15 05:12:51 INFO     saving model related files
2023-12-15 05:12:51 INFO     saving model
2023-12-15 05:12:51 INFO     saving tokenizer
2023-12-15 05:12:52 INFO     saving optimizer
2023-12-15 05:12:52 INFO     remove old optimizer files
2023-12-15 05:12:59 INFO     	 * (global step 1900: loss: 2.2814825773239136, lr: 5e-05
2023-12-15 05:13:07 INFO     	 * (global step 1950: loss: 2.328150749206543, lr: 5e-05
2023-12-15 05:13:15 INFO     	 * (global step 2000: loss: 2.414975881576538, lr: 5e-05
2023-12-15 05:13:24 INFO     	 * (global step 2050: loss: 2.476427435874939, lr: 5e-05
2023-12-15 05:13:32 INFO     	 * (global step 2100: loss: 2.3691779375076294, lr: 5e-05
2023-12-15 05:13:41 INFO     	 * (global step 2150: loss: 2.341399669647217, lr: 5e-05
2023-12-15 05:13:49 INFO     	 * (global step 2200: loss: 2.421017050743103, lr: 5e-05
2023-12-15 05:13:57 INFO     	 * (global step 2250: loss: 2.3912287950515747, lr: 5e-05
2023-12-15 05:14:06 INFO     	 * (global step 2300: loss: 2.39387845993042, lr: 5e-05
2023-12-15 05:14:14 INFO     	 * (global step 2350: loss: 2.354680299758911, lr: 5e-05
2023-12-15 05:14:22 INFO     	 * (global step 2400: loss: 2.45258367061615, lr: 5e-05
2023-12-15 05:14:31 INFO     	 * (global step 2450: loss: 2.3370174169540405, lr: 5e-05
2023-12-15 05:14:36 INFO     [epoch 13/15] average loss: 2.375, lr: 5e-05
2023-12-15 05:14:36 INFO     saving model related files
2023-12-15 05:14:36 INFO     saving model
2023-12-15 05:14:37 INFO     saving tokenizer
2023-12-15 05:14:37 INFO     saving optimizer
2023-12-15 05:14:38 INFO     remove old optimizer files
2023-12-15 05:14:41 INFO     	 * (global step 2500: loss: 2.313032031059265, lr: 5e-05
2023-12-15 05:14:49 INFO     	 * (global step 2550: loss: 2.3015663623809814, lr: 5e-05
2023-12-15 05:14:58 INFO     	 * (global step 2600: loss: 2.4813625812530518, lr: 5e-05
2023-12-15 05:15:06 INFO     	 * (global step 2650: loss: 2.394994616508484, lr: 5e-05
2023-12-15 05:15:14 INFO     	 * (global step 2700: loss: 2.323728919029236, lr: 5e-05
2023-12-15 05:15:23 INFO     	 * (global step 2750: loss: 2.3688987493515015, lr: 5e-05
2023-12-15 05:15:31 INFO     	 * (global step 2800: loss: 2.594163656234741, lr: 5e-05
2023-12-15 05:15:39 INFO     	 * (global step 2850: loss: 2.3840131759643555, lr: 5e-05
2023-12-15 05:15:48 INFO     	 * (global step 2900: loss: 2.2535942792892456, lr: 5e-05
2023-12-15 05:15:56 INFO     	 * (global step 2950: loss: 2.3259414434432983, lr: 5e-05
2023-12-15 05:16:04 INFO     	 * (global step 3000: loss: 2.451346755027771, lr: 5e-05
2023-12-15 05:16:13 INFO     	 * (global step 3050: loss: 2.319766640663147, lr: 5e-05
2023-12-15 05:16:21 INFO     	 * (global step 3100: loss: 2.32508647441864, lr: 5e-05
2023-12-15 05:16:22 INFO     [epoch 14/15] average loss: 2.363, lr: 5e-05
2023-12-15 05:16:22 INFO     saving model related files
2023-12-15 05:16:22 INFO     saving model
2023-12-15 05:16:23 INFO     saving tokenizer
2023-12-15 05:16:23 INFO     saving optimizer
2023-12-15 05:16:24 INFO     remove old optimizer files
2023-12-15 05:16:24 INFO     complete training: model ckpt was saved at small_finetuned_ckpt/model_woixzh
2023-12-15 05:16:24 INFO     ## 2nd RUN (EVAL): Configuration 0/5 ##
2023-12-15 05:16:45 INFO     use spaCy answer extraction model: positionrank
2023-12-15 05:16:45 INFO     Model `small_finetuned_ckpt/model_nxaqhy/epoch_1`
2023-12-15 05:16:45 INFO     	 * Num of GPU in use: 1
2023-12-15 05:16:45 INFO     	 * Prefix: True
2023-12-15 05:16:45 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 05:16:46 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 05:20:11 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 05:23:43 INFO     	Bleu_1: 0.24223396178665949
2023-12-15 05:23:43 INFO     	Bleu_2: 0.1355058557544001
2023-12-15 05:23:43 INFO     	Bleu_3: 0.07589659262353585
2023-12-15 05:23:43 INFO     	Bleu_4: 0.04944739287136915
2023-12-15 05:23:43 INFO     	Bleu_1: 0.22894204595568318
2023-12-15 05:23:43 INFO     	Bleu_2: 0.12601034435510547
2023-12-15 05:23:43 INFO     	Bleu_3: 0.06950011839946535
2023-12-15 05:23:43 INFO     	Bleu_4: 0.044499672352735414
2023-12-15 05:24:03 INFO     use spaCy answer extraction model: positionrank
2023-12-15 05:24:03 INFO     Model `small_finetuned_ckpt/model_nxaqhy/epoch_11`
2023-12-15 05:24:03 INFO     	 * Num of GPU in use: 1
2023-12-15 05:24:03 INFO     	 * Prefix: True
2023-12-15 05:24:03 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 05:24:04 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 05:26:49 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 05:29:23 INFO     	Bleu_1: 0.2375807020929466
2023-12-15 05:29:23 INFO     	Bleu_2: 0.13030341006589785
2023-12-15 05:29:23 INFO     	Bleu_3: 0.07243042157287824
2023-12-15 05:29:23 INFO     	Bleu_4: 0.047256764247969874
2023-12-15 05:29:24 INFO     	Bleu_1: 0.23140422979908928
2023-12-15 05:29:24 INFO     	Bleu_2: 0.127526117490355
2023-12-15 05:29:24 INFO     	Bleu_3: 0.07224753862936549
2023-12-15 05:29:24 INFO     	Bleu_4: 0.047578860106066524
2023-12-15 05:29:37 INFO     use spaCy answer extraction model: positionrank
2023-12-15 05:29:37 INFO     Model `small_finetuned_ckpt/model_nxaqhy/epoch_12`
2023-12-15 05:29:37 INFO     	 * Num of GPU in use: 1
2023-12-15 05:29:37 INFO     	 * Prefix: True
2023-12-15 05:29:37 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 05:29:38 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 05:34:42 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 05:39:46 INFO     	Bleu_1: 0.14746503411424478
2023-12-15 05:39:46 INFO     	Bleu_2: 0.08098491260114007
2023-12-15 05:39:46 INFO     	Bleu_3: 0.044005126867293395
2023-12-15 05:39:46 INFO     	Bleu_4: 0.028535151242880794
2023-12-15 05:39:46 INFO     	Bleu_1: 0.15012985979122773
2023-12-15 05:39:46 INFO     	Bleu_2: 0.08202600207320947
2023-12-15 05:39:46 INFO     	Bleu_3: 0.04425771776753733
2023-12-15 05:39:46 INFO     	Bleu_4: 0.027896304262716515
2023-12-15 05:39:58 INFO     use spaCy answer extraction model: positionrank
2023-12-15 05:39:58 INFO     Model `small_finetuned_ckpt/model_nxaqhy/epoch_13`
2023-12-15 05:39:58 INFO     	 * Num of GPU in use: 1
2023-12-15 05:39:58 INFO     	 * Prefix: True
2023-12-15 05:39:58 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 05:39:59 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 05:45:16 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 05:50:30 INFO     	Bleu_1: 0.12184328579127453
2023-12-15 05:50:30 INFO     	Bleu_2: 0.06662055862962064
2023-12-15 05:50:30 INFO     	Bleu_3: 0.035496207941785436
2023-12-15 05:50:30 INFO     	Bleu_4: 0.022671142120146877
2023-12-15 05:50:31 INFO     	Bleu_1: 0.12140556063953961
2023-12-15 05:50:31 INFO     	Bleu_2: 0.06645908000871208
2023-12-15 05:50:31 INFO     	Bleu_3: 0.035236533038990145
2023-12-15 05:50:31 INFO     	Bleu_4: 0.021918573930996138
2023-12-15 05:50:53 INFO     use spaCy answer extraction model: positionrank
2023-12-15 05:50:54 INFO     Model `small_finetuned_ckpt/model_nxaqhy/epoch_14`
2023-12-15 05:50:54 INFO     	 * Num of GPU in use: 1
2023-12-15 05:50:54 INFO     	 * Prefix: True
2023-12-15 05:50:54 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 05:50:55 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 05:56:11 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 06:01:28 INFO     	Bleu_1: 0.11514320557251147
2023-12-15 06:01:28 INFO     	Bleu_2: 0.0626943136245854
2023-12-15 06:01:28 INFO     	Bleu_3: 0.03272602938155689
2023-12-15 06:01:28 INFO     	Bleu_4: 0.020516478342669786
2023-12-15 06:01:28 INFO     	Bleu_1: 0.11269297771370143
2023-12-15 06:01:28 INFO     	Bleu_2: 0.06172574429101968
2023-12-15 06:01:28 INFO     	Bleu_3: 0.03266464008738211
2023-12-15 06:01:28 INFO     	Bleu_4: 0.02039345199921608
2023-12-15 06:01:46 INFO     use spaCy answer extraction model: positionrank
2023-12-15 06:01:46 INFO     Model `small_finetuned_ckpt/model_nxaqhy/epoch_15`
2023-12-15 06:01:46 INFO     	 * Num of GPU in use: 1
2023-12-15 06:01:46 INFO     	 * Prefix: True
2023-12-15 06:01:46 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 06:01:47 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 06:07:09 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 06:12:27 INFO     	Bleu_1: 0.10880422009911545
2023-12-15 06:12:27 INFO     	Bleu_2: 0.059004337716255184
2023-12-15 06:12:27 INFO     	Bleu_3: 0.030153141249440258
2023-12-15 06:12:27 INFO     	Bleu_4: 0.018661040203592764
2023-12-15 06:12:27 INFO     	Bleu_1: 0.10534008488362073
2023-12-15 06:12:27 INFO     	Bleu_2: 0.05754927055749819
2023-12-15 06:12:27 INFO     	Bleu_3: 0.030077603400660367
2023-12-15 06:12:27 INFO     	Bleu_4: 0.018582409869541426
2023-12-15 06:12:40 INFO     use spaCy answer extraction model: positionrank
2023-12-15 06:12:40 INFO     Model `small_finetuned_ckpt/model_nxaqhy/epoch_2`
2023-12-15 06:12:40 INFO     	 * Num of GPU in use: 1
2023-12-15 06:12:40 INFO     	 * Prefix: True
2023-12-15 06:12:40 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 06:12:42 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 06:17:00 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 06:21:25 INFO     	Bleu_1: 0.19587115554023266
2023-12-15 06:21:25 INFO     	Bleu_2: 0.10811492606127131
2023-12-15 06:21:25 INFO     	Bleu_3: 0.05870740365931129
2023-12-15 06:21:25 INFO     	Bleu_4: 0.037788504203896324
2023-12-15 06:21:26 INFO     	Bleu_1: 0.19146503856686775
2023-12-15 06:21:26 INFO     	Bleu_2: 0.10495828439586648
2023-12-15 06:21:26 INFO     	Bleu_3: 0.05625734120658811
2023-12-15 06:21:26 INFO     	Bleu_4: 0.03504805328346663
2023-12-15 06:21:42 INFO     use spaCy answer extraction model: positionrank
2023-12-15 06:21:42 INFO     Model `small_finetuned_ckpt/model_nxaqhy/epoch_3`
2023-12-15 06:21:42 INFO     	 * Num of GPU in use: 1
2023-12-15 06:21:42 INFO     	 * Prefix: True
2023-12-15 06:21:42 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 06:21:43 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 06:26:29 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 06:31:17 INFO     	Bleu_1: 0.17153222989645225
2023-12-15 06:31:17 INFO     	Bleu_2: 0.09405903310513439
2023-12-15 06:31:17 INFO     	Bleu_3: 0.05031093682009622
2023-12-15 06:31:17 INFO     	Bleu_4: 0.03215937900705852
2023-12-15 06:31:18 INFO     	Bleu_1: 0.16492805569079833
2023-12-15 06:31:18 INFO     	Bleu_2: 0.09092366873783346
2023-12-15 06:31:18 INFO     	Bleu_3: 0.04993942397913462
2023-12-15 06:31:18 INFO     	Bleu_4: 0.032156891918798496
2023-12-15 06:31:35 INFO     use spaCy answer extraction model: positionrank
2023-12-15 06:31:35 INFO     Model `small_finetuned_ckpt/model_nxaqhy/epoch_4`
2023-12-15 06:31:35 INFO     	 * Num of GPU in use: 1
2023-12-15 06:31:35 INFO     	 * Prefix: True
2023-12-15 06:31:35 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 06:31:36 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 06:36:36 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 06:41:32 INFO     	Bleu_1: 0.15926900942818498
2023-12-15 06:41:32 INFO     	Bleu_2: 0.08703567522624081
2023-12-15 06:41:32 INFO     	Bleu_3: 0.045869894936138605
2023-12-15 06:41:32 INFO     	Bleu_4: 0.028807045432486796
2023-12-15 06:41:33 INFO     	Bleu_1: 0.14870981505184716
2023-12-15 06:41:33 INFO     	Bleu_2: 0.08159382018619163
2023-12-15 06:41:33 INFO     	Bleu_3: 0.04405579378925395
2023-12-15 06:41:33 INFO     	Bleu_4: 0.02815822433009218
2023-12-15 06:41:45 INFO     use spaCy answer extraction model: positionrank
2023-12-15 06:41:46 INFO     Model `small_finetuned_ckpt/model_nxaqhy/epoch_5`
2023-12-15 06:41:46 INFO     	 * Num of GPU in use: 1
2023-12-15 06:41:46 INFO     	 * Prefix: True
2023-12-15 06:41:46 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 06:41:47 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 06:46:57 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 06:52:02 INFO     	Bleu_1: 0.14497009145288134
2023-12-15 06:52:02 INFO     	Bleu_2: 0.07890852120871021
2023-12-15 06:52:02 INFO     	Bleu_3: 0.04102389534299622
2023-12-15 06:52:02 INFO     	Bleu_4: 0.025754485750608378
2023-12-15 06:52:03 INFO     	Bleu_1: 0.13601018187789846
2023-12-15 06:52:03 INFO     	Bleu_2: 0.07467052278879864
2023-12-15 06:52:03 INFO     	Bleu_3: 0.0399351841085161
2023-12-15 06:52:03 INFO     	Bleu_4: 0.025435115922939844
2023-12-15 06:52:16 INFO     use spaCy answer extraction model: positionrank
2023-12-15 06:52:17 INFO     Model `small_finetuned_ckpt/model_nxaqhy/epoch_6`
2023-12-15 06:52:17 INFO     	 * Num of GPU in use: 1
2023-12-15 06:52:17 INFO     	 * Prefix: True
2023-12-15 06:52:17 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 06:52:18 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 06:57:20 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 07:02:19 INFO     	Bleu_1: 0.1445555883325722
2023-12-15 07:02:19 INFO     	Bleu_2: 0.07898412502167217
2023-12-15 07:02:19 INFO     	Bleu_3: 0.04144005020904818
2023-12-15 07:02:19 INFO     	Bleu_4: 0.026206240536265735
2023-12-15 07:02:20 INFO     	Bleu_1: 0.14204187322192005
2023-12-15 07:02:20 INFO     	Bleu_2: 0.0780621045570315
2023-12-15 07:02:20 INFO     	Bleu_3: 0.04220273394320809
2023-12-15 07:02:20 INFO     	Bleu_4: 0.02714828744651992
2023-12-15 07:02:36 INFO     use spaCy answer extraction model: positionrank
2023-12-15 07:02:36 INFO     Model `small_finetuned_ckpt/model_nxaqhy/epoch_7`
2023-12-15 07:02:36 INFO     	 * Num of GPU in use: 1
2023-12-15 07:02:36 INFO     	 * Prefix: True
2023-12-15 07:02:36 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 07:02:37 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 07:07:48 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 07:12:52 INFO     	Bleu_1: 0.13670186023127084
2023-12-15 07:12:52 INFO     	Bleu_2: 0.07427537648415039
2023-12-15 07:12:52 INFO     	Bleu_3: 0.03831447651999451
2023-12-15 07:12:52 INFO     	Bleu_4: 0.02398447675020153
2023-12-15 07:12:53 INFO     	Bleu_1: 0.12448817906513455
2023-12-15 07:12:53 INFO     	Bleu_2: 0.06839872113095392
2023-12-15 07:12:53 INFO     	Bleu_3: 0.03706083921988285
2023-12-15 07:12:53 INFO     	Bleu_4: 0.024013470740575013
2023-12-15 07:13:09 INFO     use spaCy answer extraction model: positionrank
2023-12-15 07:13:10 INFO     Model `small_finetuned_ckpt/model_nxaqhy/epoch_8`
2023-12-15 07:13:10 INFO     	 * Num of GPU in use: 1
2023-12-15 07:13:10 INFO     	 * Prefix: True
2023-12-15 07:13:10 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 07:13:11 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 07:18:19 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 07:23:25 INFO     	Bleu_1: 0.1302215980767847
2023-12-15 07:23:25 INFO     	Bleu_2: 0.07103750377489501
2023-12-15 07:23:25 INFO     	Bleu_3: 0.0371605945678779
2023-12-15 07:23:25 INFO     	Bleu_4: 0.0233810495239621
2023-12-15 07:23:26 INFO     	Bleu_1: 0.12432372505543146
2023-12-15 07:23:26 INFO     	Bleu_2: 0.06850771417622724
2023-12-15 07:23:26 INFO     	Bleu_3: 0.03716241870096637
2023-12-15 07:23:26 INFO     	Bleu_4: 0.024051866447040347
2023-12-15 07:23:41 INFO     use spaCy answer extraction model: positionrank
2023-12-15 07:23:41 INFO     Model `small_finetuned_ckpt/model_nxaqhy/epoch_9`
2023-12-15 07:23:41 INFO     	 * Num of GPU in use: 1
2023-12-15 07:23:41 INFO     	 * Prefix: True
2023-12-15 07:23:41 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 07:23:42 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 07:28:54 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 07:34:00 INFO     	Bleu_1: 0.12954077069642547
2023-12-15 07:34:00 INFO     	Bleu_2: 0.07052471734698325
2023-12-15 07:34:00 INFO     	Bleu_3: 0.03689678895041669
2023-12-15 07:34:00 INFO     	Bleu_4: 0.023249936720927768
2023-12-15 07:34:00 INFO     	Bleu_1: 0.1240661015193872
2023-12-15 07:34:00 INFO     	Bleu_2: 0.06810123623480707
2023-12-15 07:34:00 INFO     	Bleu_3: 0.036814234206644994
2023-12-15 07:34:00 INFO     	Bleu_4: 0.023714467913567794
2023-12-15 07:34:00 INFO     ## 2nd RUN (EVAL): Configuration 1/5 ##
2023-12-15 07:34:14 INFO     use spaCy answer extraction model: positionrank
2023-12-15 07:34:14 INFO     Model `small_finetuned_ckpt/model_vhyoja/epoch_1`
2023-12-15 07:34:14 INFO     	 * Num of GPU in use: 1
2023-12-15 07:34:14 INFO     	 * Prefix: True
2023-12-15 07:34:14 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 07:34:15 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 07:37:39 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 07:41:11 INFO     	Bleu_1: 0.24223396178665949
2023-12-15 07:41:11 INFO     	Bleu_2: 0.1355058557544001
2023-12-15 07:41:11 INFO     	Bleu_3: 0.07589659262353585
2023-12-15 07:41:11 INFO     	Bleu_4: 0.04944739287136915
2023-12-15 07:41:11 INFO     	Bleu_1: 0.22894204595568318
2023-12-15 07:41:11 INFO     	Bleu_2: 0.12601034435510547
2023-12-15 07:41:11 INFO     	Bleu_3: 0.06950011839946535
2023-12-15 07:41:11 INFO     	Bleu_4: 0.044499672352735414
2023-12-15 07:41:29 INFO     use spaCy answer extraction model: positionrank
2023-12-15 07:41:30 INFO     Model `small_finetuned_ckpt/model_vhyoja/epoch_11`
2023-12-15 07:41:30 INFO     	 * Num of GPU in use: 1
2023-12-15 07:41:30 INFO     	 * Prefix: True
2023-12-15 07:41:30 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 07:41:31 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 07:46:43 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 07:51:53 INFO     	Bleu_1: 0.12406291883293723
2023-12-15 07:51:53 INFO     	Bleu_2: 0.06751360913726977
2023-12-15 07:51:53 INFO     	Bleu_3: 0.03509619467900257
2023-12-15 07:51:53 INFO     	Bleu_4: 0.022041354071268265
2023-12-15 07:51:54 INFO     	Bleu_1: 0.11632747456059127
2023-12-15 07:51:54 INFO     	Bleu_2: 0.06381130479565325
2023-12-15 07:51:54 INFO     	Bleu_3: 0.034223930651290875
2023-12-15 07:51:54 INFO     	Bleu_4: 0.021947471528135643
2023-12-15 07:52:09 INFO     use spaCy answer extraction model: positionrank
2023-12-15 07:52:10 INFO     Model `small_finetuned_ckpt/model_vhyoja/epoch_12`
2023-12-15 07:52:10 INFO     	 * Num of GPU in use: 1
2023-12-15 07:52:10 INFO     	 * Prefix: True
2023-12-15 07:52:10 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 07:52:11 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 07:57:18 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 08:02:26 INFO     	Bleu_1: 0.127456421094242
2023-12-15 08:02:26 INFO     	Bleu_2: 0.0691464655850669
2023-12-15 08:02:26 INFO     	Bleu_3: 0.03586354573186174
2023-12-15 08:02:26 INFO     	Bleu_4: 0.022497667675167216
2023-12-15 08:02:27 INFO     	Bleu_1: 0.12242549680855595
2023-12-15 08:02:27 INFO     	Bleu_2: 0.06724057621900786
2023-12-15 08:02:27 INFO     	Bleu_3: 0.03622703852673368
2023-12-15 08:02:27 INFO     	Bleu_4: 0.023210501988953736
2023-12-15 08:02:40 INFO     use spaCy answer extraction model: positionrank
2023-12-15 08:02:40 INFO     Model `small_finetuned_ckpt/model_vhyoja/epoch_13`
2023-12-15 08:02:40 INFO     	 * Num of GPU in use: 1
2023-12-15 08:02:40 INFO     	 * Prefix: True
2023-12-15 08:02:40 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 08:02:41 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 08:07:54 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 08:13:08 INFO     	Bleu_1: 0.12163532088954339
2023-12-15 08:13:08 INFO     	Bleu_2: 0.06613684208076283
2023-12-15 08:13:08 INFO     	Bleu_3: 0.03457430283387025
2023-12-15 08:13:08 INFO     	Bleu_4: 0.021839653431317774
2023-12-15 08:13:09 INFO     	Bleu_1: 0.11429459123574787
2023-12-15 08:13:09 INFO     	Bleu_2: 0.06272590151892476
2023-12-15 08:13:09 INFO     	Bleu_3: 0.03354974034668639
2023-12-15 08:13:09 INFO     	Bleu_4: 0.02140194274547005
2023-12-15 08:13:22 INFO     use spaCy answer extraction model: positionrank
2023-12-15 08:13:22 INFO     Model `small_finetuned_ckpt/model_vhyoja/epoch_14`
2023-12-15 08:13:22 INFO     	 * Num of GPU in use: 1
2023-12-15 08:13:22 INFO     	 * Prefix: True
2023-12-15 08:13:22 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 08:13:23 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 08:18:38 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 08:23:58 INFO     	Bleu_1: 0.12173611700472346
2023-12-15 08:23:58 INFO     	Bleu_2: 0.06648976995352565
2023-12-15 08:23:58 INFO     	Bleu_3: 0.035152325997415926
2023-12-15 08:23:58 INFO     	Bleu_4: 0.02242380427780753
2023-12-15 08:23:58 INFO     	Bleu_1: 0.11459006455309066
2023-12-15 08:23:58 INFO     	Bleu_2: 0.06291253969729195
2023-12-15 08:23:58 INFO     	Bleu_3: 0.033540026394661006
2023-12-15 08:23:58 INFO     	Bleu_4: 0.021237142596716822
2023-12-15 08:24:09 INFO     use spaCy answer extraction model: positionrank
2023-12-15 08:24:09 INFO     Model `small_finetuned_ckpt/model_vhyoja/epoch_15`
2023-12-15 08:24:09 INFO     	 * Num of GPU in use: 1
2023-12-15 08:24:09 INFO     	 * Prefix: True
2023-12-15 08:24:09 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 08:24:10 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 08:29:23 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 08:34:39 INFO     	Bleu_1: 0.11480209150671489
2023-12-15 08:34:39 INFO     	Bleu_2: 0.06248250047422273
2023-12-15 08:34:39 INFO     	Bleu_3: 0.03265925454345977
2023-12-15 08:34:39 INFO     	Bleu_4: 0.020658798699533916
2023-12-15 08:34:40 INFO     	Bleu_1: 0.10980876788421533
2023-12-15 08:34:40 INFO     	Bleu_2: 0.06022936143261007
2023-12-15 08:34:40 INFO     	Bleu_3: 0.032160948054042085
2023-12-15 08:34:40 INFO     	Bleu_4: 0.020541492364199415
2023-12-15 08:34:55 INFO     use spaCy answer extraction model: positionrank
2023-12-15 08:34:55 INFO     Model `small_finetuned_ckpt/model_vhyoja/epoch_2`
2023-12-15 08:34:55 INFO     	 * Num of GPU in use: 1
2023-12-15 08:34:55 INFO     	 * Prefix: True
2023-12-15 08:34:55 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 08:34:56 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 08:39:17 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 08:43:41 INFO     	Bleu_1: 0.19587115554023266
2023-12-15 08:43:41 INFO     	Bleu_2: 0.10811492606127131
2023-12-15 08:43:41 INFO     	Bleu_3: 0.05870740365931129
2023-12-15 08:43:41 INFO     	Bleu_4: 0.037788504203896324
2023-12-15 08:43:42 INFO     	Bleu_1: 0.19146503856686775
2023-12-15 08:43:42 INFO     	Bleu_2: 0.10495828439586648
2023-12-15 08:43:42 INFO     	Bleu_3: 0.05625734120658811
2023-12-15 08:43:42 INFO     	Bleu_4: 0.03504805328346663
2023-12-15 08:43:56 INFO     use spaCy answer extraction model: positionrank
2023-12-15 08:43:56 INFO     Model `small_finetuned_ckpt/model_vhyoja/epoch_3`
2023-12-15 08:43:56 INFO     	 * Num of GPU in use: 1
2023-12-15 08:43:56 INFO     	 * Prefix: True
2023-12-15 08:43:56 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 08:43:58 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 08:48:43 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 08:53:31 INFO     	Bleu_1: 0.17153222989645225
2023-12-15 08:53:31 INFO     	Bleu_2: 0.09405903310513439
2023-12-15 08:53:31 INFO     	Bleu_3: 0.05031093682009622
2023-12-15 08:53:31 INFO     	Bleu_4: 0.03215937900705852
2023-12-15 08:53:32 INFO     	Bleu_1: 0.16492805569079833
2023-12-15 08:53:32 INFO     	Bleu_2: 0.09092366873783346
2023-12-15 08:53:32 INFO     	Bleu_3: 0.04993942397913462
2023-12-15 08:53:32 INFO     	Bleu_4: 0.032156891918798496
2023-12-15 08:53:45 INFO     use spaCy answer extraction model: positionrank
2023-12-15 08:53:45 INFO     Model `small_finetuned_ckpt/model_vhyoja/epoch_4`
2023-12-15 08:53:45 INFO     	 * Num of GPU in use: 1
2023-12-15 08:53:45 INFO     	 * Prefix: True
2023-12-15 08:53:45 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 08:53:46 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 08:58:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 09:03:39 INFO     	Bleu_1: 0.15926900942818498
2023-12-15 09:03:39 INFO     	Bleu_2: 0.08703567522624081
2023-12-15 09:03:39 INFO     	Bleu_3: 0.045869894936138605
2023-12-15 09:03:39 INFO     	Bleu_4: 0.028807045432486796
2023-12-15 09:03:40 INFO     	Bleu_1: 0.14870981505184716
2023-12-15 09:03:40 INFO     	Bleu_2: 0.08159382018619163
2023-12-15 09:03:40 INFO     	Bleu_3: 0.04405579378925395
2023-12-15 09:03:40 INFO     	Bleu_4: 0.02815822433009218
2023-12-15 09:03:54 INFO     use spaCy answer extraction model: positionrank
2023-12-15 09:03:54 INFO     Model `small_finetuned_ckpt/model_vhyoja/epoch_5`
2023-12-15 09:03:54 INFO     	 * Num of GPU in use: 1
2023-12-15 09:03:54 INFO     	 * Prefix: True
2023-12-15 09:03:54 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 09:03:55 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 09:09:07 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 09:14:12 INFO     	Bleu_1: 0.14497009145288134
2023-12-15 09:14:12 INFO     	Bleu_2: 0.07890852120871021
2023-12-15 09:14:12 INFO     	Bleu_3: 0.04102389534299622
2023-12-15 09:14:12 INFO     	Bleu_4: 0.025754485750608378
2023-12-15 09:14:12 INFO     	Bleu_1: 0.13601018187789846
2023-12-15 09:14:12 INFO     	Bleu_2: 0.07467052278879864
2023-12-15 09:14:12 INFO     	Bleu_3: 0.0399351841085161
2023-12-15 09:14:12 INFO     	Bleu_4: 0.025435115922939844
2023-12-15 09:14:24 INFO     use spaCy answer extraction model: positionrank
2023-12-15 09:14:25 INFO     Model `small_finetuned_ckpt/model_vhyoja/epoch_6`
2023-12-15 09:14:25 INFO     	 * Num of GPU in use: 1
2023-12-15 09:14:25 INFO     	 * Prefix: True
2023-12-15 09:14:25 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 09:14:26 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 09:19:30 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 09:24:30 INFO     	Bleu_1: 0.1445555883325722
2023-12-15 09:24:30 INFO     	Bleu_2: 0.07898412502167217
2023-12-15 09:24:30 INFO     	Bleu_3: 0.04144005020904818
2023-12-15 09:24:30 INFO     	Bleu_4: 0.026206240536265735
2023-12-15 09:24:30 INFO     	Bleu_1: 0.14204187322192005
2023-12-15 09:24:30 INFO     	Bleu_2: 0.0780621045570315
2023-12-15 09:24:30 INFO     	Bleu_3: 0.04220273394320809
2023-12-15 09:24:30 INFO     	Bleu_4: 0.02714828744651992
2023-12-15 09:24:48 INFO     use spaCy answer extraction model: positionrank
2023-12-15 09:24:48 INFO     Model `small_finetuned_ckpt/model_vhyoja/epoch_7`
2023-12-15 09:24:48 INFO     	 * Num of GPU in use: 1
2023-12-15 09:24:48 INFO     	 * Prefix: True
2023-12-15 09:24:48 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 09:24:49 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 09:30:00 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 09:35:04 INFO     	Bleu_1: 0.13670186023127084
2023-12-15 09:35:04 INFO     	Bleu_2: 0.07427537648415039
2023-12-15 09:35:04 INFO     	Bleu_3: 0.03831447651999451
2023-12-15 09:35:04 INFO     	Bleu_4: 0.02398447675020153
2023-12-15 09:35:05 INFO     	Bleu_1: 0.12448817906513455
2023-12-15 09:35:05 INFO     	Bleu_2: 0.06839872113095392
2023-12-15 09:35:05 INFO     	Bleu_3: 0.03706083921988285
2023-12-15 09:35:05 INFO     	Bleu_4: 0.024013470740575013
2023-12-15 09:35:18 INFO     use spaCy answer extraction model: positionrank
2023-12-15 09:35:19 INFO     Model `small_finetuned_ckpt/model_vhyoja/epoch_8`
2023-12-15 09:35:19 INFO     	 * Num of GPU in use: 1
2023-12-15 09:35:19 INFO     	 * Prefix: True
2023-12-15 09:35:19 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 09:35:20 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 09:40:30 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 09:45:35 INFO     	Bleu_1: 0.1302215980767847
2023-12-15 09:45:35 INFO     	Bleu_2: 0.07103750377489501
2023-12-15 09:45:35 INFO     	Bleu_3: 0.0371605945678779
2023-12-15 09:45:35 INFO     	Bleu_4: 0.0233810495239621
2023-12-15 09:45:36 INFO     	Bleu_1: 0.12432372505543146
2023-12-15 09:45:36 INFO     	Bleu_2: 0.06850771417622724
2023-12-15 09:45:36 INFO     	Bleu_3: 0.03716241870096637
2023-12-15 09:45:36 INFO     	Bleu_4: 0.024051866447040347
2023-12-15 09:45:50 INFO     use spaCy answer extraction model: positionrank
2023-12-15 09:45:51 INFO     Model `small_finetuned_ckpt/model_vhyoja/epoch_9`
2023-12-15 09:45:51 INFO     	 * Num of GPU in use: 1
2023-12-15 09:45:51 INFO     	 * Prefix: True
2023-12-15 09:45:51 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 09:45:52 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 09:51:02 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 09:56:08 INFO     	Bleu_1: 0.12954077069642547
2023-12-15 09:56:08 INFO     	Bleu_2: 0.07052471734698325
2023-12-15 09:56:08 INFO     	Bleu_3: 0.03689678895041669
2023-12-15 09:56:08 INFO     	Bleu_4: 0.023249936720927768
2023-12-15 09:56:09 INFO     	Bleu_1: 0.1240661015193872
2023-12-15 09:56:09 INFO     	Bleu_2: 0.06810123623480707
2023-12-15 09:56:09 INFO     	Bleu_3: 0.036814234206644994
2023-12-15 09:56:09 INFO     	Bleu_4: 0.023714467913567794
2023-12-15 09:56:09 INFO     ## 2nd RUN (EVAL): Configuration 2/5 ##
2023-12-15 09:56:20 INFO     use spaCy answer extraction model: positionrank
2023-12-15 09:56:20 INFO     Model `small_finetuned_ckpt/model_oprhlh/epoch_1`
2023-12-15 09:56:20 INFO     	 * Num of GPU in use: 1
2023-12-15 09:56:20 INFO     	 * Prefix: True
2023-12-15 09:56:20 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 09:56:21 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 10:00:33 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 10:04:26 INFO     	Bleu_1: 0.22255506607929187
2023-12-15 10:04:26 INFO     	Bleu_2: 0.12402297094649412
2023-12-15 10:04:26 INFO     	Bleu_3: 0.06888878664142187
2023-12-15 10:04:26 INFO     	Bleu_4: 0.044594802740991514
2023-12-15 10:04:26 INFO     	Bleu_1: 0.2063863657645819
2023-12-15 10:04:26 INFO     	Bleu_2: 0.11454784228169868
2023-12-15 10:04:26 INFO     	Bleu_3: 0.06340655854524996
2023-12-15 10:04:26 INFO     	Bleu_4: 0.04084025123415649
2023-12-15 10:04:44 INFO     use spaCy answer extraction model: positionrank
2023-12-15 10:04:44 INFO     Model `small_finetuned_ckpt/model_oprhlh/epoch_11`
2023-12-15 10:04:44 INFO     	 * Num of GPU in use: 1
2023-12-15 10:04:44 INFO     	 * Prefix: True
2023-12-15 10:04:44 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 10:04:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 10:09:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 10:14:22 INFO     	Bleu_1: 0.16701841792662622
2023-12-15 10:14:22 INFO     	Bleu_2: 0.09222032903434224
2023-12-15 10:14:22 INFO     	Bleu_3: 0.05092927431334679
2023-12-15 10:14:22 INFO     	Bleu_4: 0.03332006101802874
2023-12-15 10:14:22 INFO     	Bleu_1: 0.16835777329904592
2023-12-15 10:14:22 INFO     	Bleu_2: 0.09226114437805295
2023-12-15 10:14:22 INFO     	Bleu_3: 0.05039717498043845
2023-12-15 10:14:22 INFO     	Bleu_4: 0.032067562383647615
2023-12-15 10:14:38 INFO     use spaCy answer extraction model: positionrank
2023-12-15 10:14:39 INFO     Model `small_finetuned_ckpt/model_oprhlh/epoch_12`
2023-12-15 10:14:39 INFO     	 * Num of GPU in use: 1
2023-12-15 10:14:39 INFO     	 * Prefix: True
2023-12-15 10:14:39 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 10:14:40 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 10:19:57 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 10:25:12 INFO     	Bleu_1: 0.11713936936486664
2023-12-15 10:25:12 INFO     	Bleu_2: 0.06390069413247369
2023-12-15 10:25:12 INFO     	Bleu_3: 0.03371535807347547
2023-12-15 10:25:12 INFO     	Bleu_4: 0.021356445068363164
2023-12-15 10:25:13 INFO     	Bleu_1: 0.1165458001830591
2023-12-15 10:25:13 INFO     	Bleu_2: 0.06359015805237311
2023-12-15 10:25:13 INFO     	Bleu_3: 0.03343323709937953
2023-12-15 10:25:13 INFO     	Bleu_4: 0.02071250968486003
2023-12-15 10:25:46 INFO     use spaCy answer extraction model: positionrank
2023-12-15 10:25:46 INFO     Model `small_finetuned_ckpt/model_oprhlh/epoch_13`
2023-12-15 10:25:46 INFO     	 * Num of GPU in use: 1
2023-12-15 10:25:46 INFO     	 * Prefix: True
2023-12-15 10:25:46 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 10:25:47 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 10:31:09 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 10:36:29 INFO     	Bleu_1: 0.1074299083053463
2023-12-15 10:36:29 INFO     	Bleu_2: 0.05856976613216461
2023-12-15 10:36:29 INFO     	Bleu_3: 0.030401357015869927
2023-12-15 10:36:29 INFO     	Bleu_4: 0.019035768980693615
2023-12-15 10:36:30 INFO     	Bleu_1: 0.09989384104000917
2023-12-15 10:36:30 INFO     	Bleu_2: 0.05495904495557264
2023-12-15 10:36:30 INFO     	Bleu_3: 0.029051387030475002
2023-12-15 10:36:30 INFO     	Bleu_4: 0.018272880453034657
2023-12-15 10:36:54 INFO     use spaCy answer extraction model: positionrank
2023-12-15 10:36:55 INFO     Model `small_finetuned_ckpt/model_oprhlh/epoch_14`
2023-12-15 10:36:55 INFO     	 * Num of GPU in use: 1
2023-12-15 10:36:55 INFO     	 * Prefix: True
2023-12-15 10:36:55 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 10:36:56 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 10:42:17 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 10:47:35 INFO     	Bleu_1: 0.10696653867251717
2023-12-15 10:47:35 INFO     	Bleu_2: 0.058068396049603475
2023-12-15 10:47:35 INFO     	Bleu_3: 0.02975999750625118
2023-12-15 10:47:35 INFO     	Bleu_4: 0.018471772553295678
2023-12-15 10:47:36 INFO     	Bleu_1: 0.10125556669542249
2023-12-15 10:47:36 INFO     	Bleu_2: 0.05564191229655197
2023-12-15 10:47:36 INFO     	Bleu_3: 0.02925416978731543
2023-12-15 10:47:36 INFO     	Bleu_4: 0.018283190107163283
2023-12-15 10:47:57 INFO     use spaCy answer extraction model: positionrank
2023-12-15 10:47:57 INFO     Model `small_finetuned_ckpt/model_oprhlh/epoch_15`
2023-12-15 10:47:57 INFO     	 * Num of GPU in use: 1
2023-12-15 10:47:57 INFO     	 * Prefix: True
2023-12-15 10:47:57 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 10:47:59 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 10:53:22 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 10:58:42 INFO     	Bleu_1: 0.10290321816760933
2023-12-15 10:58:42 INFO     	Bleu_2: 0.05606402494366416
2023-12-15 10:58:42 INFO     	Bleu_3: 0.02900170080935564
2023-12-15 10:58:42 INFO     	Bleu_4: 0.018089911570606795
2023-12-15 10:58:42 INFO     	Bleu_1: 0.09676434676434624
2023-12-15 10:58:42 INFO     	Bleu_2: 0.053035258431832454
2023-12-15 10:58:42 INFO     	Bleu_3: 0.02786936719831118
2023-12-15 10:58:42 INFO     	Bleu_4: 0.017458620914314375
2023-12-15 10:58:57 INFO     use spaCy answer extraction model: positionrank
2023-12-15 10:58:58 INFO     Model `small_finetuned_ckpt/model_oprhlh/epoch_2`
2023-12-15 10:58:58 INFO     	 * Num of GPU in use: 1
2023-12-15 10:58:58 INFO     	 * Prefix: True
2023-12-15 10:58:58 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 10:58:59 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 11:03:44 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 11:08:25 INFO     	Bleu_1: 0.1726529965598387
2023-12-15 11:08:25 INFO     	Bleu_2: 0.09496677001536358
2023-12-15 11:08:25 INFO     	Bleu_3: 0.050735296275318004
2023-12-15 11:08:25 INFO     	Bleu_4: 0.03236520527539009
2023-12-15 11:08:26 INFO     	Bleu_1: 0.1717626392929662
2023-12-15 11:08:26 INFO     	Bleu_2: 0.09404879213385099
2023-12-15 11:08:26 INFO     	Bleu_3: 0.050403381920408726
2023-12-15 11:08:26 INFO     	Bleu_4: 0.031837218051344526
2023-12-15 11:08:46 INFO     use spaCy answer extraction model: positionrank
2023-12-15 11:08:46 INFO     Model `small_finetuned_ckpt/model_oprhlh/epoch_3`
2023-12-15 11:08:46 INFO     	 * Num of GPU in use: 1
2023-12-15 11:08:46 INFO     	 * Prefix: True
2023-12-15 11:08:46 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 11:08:47 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 11:13:47 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 11:18:41 INFO     	Bleu_1: 0.15514108273259025
2023-12-15 11:18:41 INFO     	Bleu_2: 0.08436074291990228
2023-12-15 11:18:41 INFO     	Bleu_3: 0.04396214939218179
2023-12-15 11:18:41 INFO     	Bleu_4: 0.027421135571338063
2023-12-15 11:18:42 INFO     	Bleu_1: 0.15057509037134265
2023-12-15 11:18:42 INFO     	Bleu_2: 0.08245466292725331
2023-12-15 11:18:42 INFO     	Bleu_3: 0.04443944754335066
2023-12-15 11:18:42 INFO     	Bleu_4: 0.02843003746772814
2023-12-15 11:19:03 INFO     use spaCy answer extraction model: positionrank
2023-12-15 11:19:04 INFO     Model `small_finetuned_ckpt/model_oprhlh/epoch_4`
2023-12-15 11:19:04 INFO     	 * Num of GPU in use: 1
2023-12-15 11:19:04 INFO     	 * Prefix: True
2023-12-15 11:19:04 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 11:19:05 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 11:24:12 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 11:29:14 INFO     	Bleu_1: 0.14141547565582524
2023-12-15 11:29:14 INFO     	Bleu_2: 0.07710541502973495
2023-12-15 11:29:14 INFO     	Bleu_3: 0.0401198102501414
2023-12-15 11:29:14 INFO     	Bleu_4: 0.02515104789337203
2023-12-15 11:29:14 INFO     	Bleu_1: 0.13601258110199801
2023-12-15 11:29:14 INFO     	Bleu_2: 0.07441193375926562
2023-12-15 11:29:14 INFO     	Bleu_3: 0.039724710468549224
2023-12-15 11:29:14 INFO     	Bleu_4: 0.02509104675281221
2023-12-15 11:29:27 INFO     use spaCy answer extraction model: positionrank
2023-12-15 11:29:28 INFO     Model `small_finetuned_ckpt/model_oprhlh/epoch_5`
2023-12-15 11:29:28 INFO     	 * Num of GPU in use: 1
2023-12-15 11:29:28 INFO     	 * Prefix: True
2023-12-15 11:29:28 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 11:29:29 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 11:34:38 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 11:39:44 INFO     	Bleu_1: 0.1352631360676802
2023-12-15 11:39:44 INFO     	Bleu_2: 0.07350537613758125
2023-12-15 11:39:44 INFO     	Bleu_3: 0.03789767436150744
2023-12-15 11:39:44 INFO     	Bleu_4: 0.023658109832308528
2023-12-15 11:39:45 INFO     	Bleu_1: 0.1256035036845479
2023-12-15 11:39:45 INFO     	Bleu_2: 0.06876728634280045
2023-12-15 11:39:45 INFO     	Bleu_3: 0.03660614135969604
2023-12-15 11:39:45 INFO     	Bleu_4: 0.023295631944150054
2023-12-15 11:40:00 INFO     use spaCy answer extraction model: positionrank
2023-12-15 11:40:00 INFO     Model `small_finetuned_ckpt/model_oprhlh/epoch_6`
2023-12-15 11:40:00 INFO     	 * Num of GPU in use: 1
2023-12-15 11:40:00 INFO     	 * Prefix: True
2023-12-15 11:40:00 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 11:40:02 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 11:45:12 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 11:50:16 INFO     	Bleu_1: 0.1318453732246825
2023-12-15 11:50:16 INFO     	Bleu_2: 0.07198130930177589
2023-12-15 11:50:16 INFO     	Bleu_3: 0.03790970707532742
2023-12-15 11:50:16 INFO     	Bleu_4: 0.024094792515923766
2023-12-15 11:50:16 INFO     	Bleu_1: 0.12681850255979008
2023-12-15 11:50:16 INFO     	Bleu_2: 0.0697678921828895
2023-12-15 11:50:16 INFO     	Bleu_3: 0.03784811759519804
2023-12-15 11:50:16 INFO     	Bleu_4: 0.024515141656330047
2023-12-15 11:50:30 INFO     use spaCy answer extraction model: positionrank
2023-12-15 11:50:31 INFO     Model `small_finetuned_ckpt/model_oprhlh/epoch_7`
2023-12-15 11:50:31 INFO     	 * Num of GPU in use: 1
2023-12-15 11:50:31 INFO     	 * Prefix: True
2023-12-15 11:50:31 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 11:50:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 11:55:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 12:00:59 INFO     	Bleu_1: 0.11633712190775389
2023-12-15 12:00:59 INFO     	Bleu_2: 0.06287858188040109
2023-12-15 12:00:59 INFO     	Bleu_3: 0.031964840103718105
2023-12-15 12:00:59 INFO     	Bleu_4: 0.019736252334527745
2023-12-15 12:01:00 INFO     	Bleu_1: 0.11383206675805453
2023-12-15 12:01:00 INFO     	Bleu_2: 0.06216396978992693
2023-12-15 12:01:00 INFO     	Bleu_3: 0.03250224279830606
2023-12-15 12:01:00 INFO     	Bleu_4: 0.02027424620393611
2023-12-15 12:01:14 INFO     use spaCy answer extraction model: positionrank
2023-12-15 12:01:14 INFO     Model `small_finetuned_ckpt/model_oprhlh/epoch_8`
2023-12-15 12:01:14 INFO     	 * Num of GPU in use: 1
2023-12-15 12:01:14 INFO     	 * Prefix: True
2023-12-15 12:01:14 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 12:01:16 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 12:06:27 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 12:11:39 INFO     	Bleu_1: 0.1197319709232499
2023-12-15 12:11:39 INFO     	Bleu_2: 0.06466570509071846
2023-12-15 12:11:39 INFO     	Bleu_3: 0.033058839557138786
2023-12-15 12:11:39 INFO     	Bleu_4: 0.020481046375409782
2023-12-15 12:11:39 INFO     	Bleu_1: 0.11805598403981739
2023-12-15 12:11:39 INFO     	Bleu_2: 0.0648146092346074
2023-12-15 12:11:39 INFO     	Bleu_3: 0.034729580066933455
2023-12-15 12:11:39 INFO     	Bleu_4: 0.022141352378445243
2023-12-15 12:11:51 INFO     use spaCy answer extraction model: positionrank
2023-12-15 12:11:52 INFO     Model `small_finetuned_ckpt/model_oprhlh/epoch_9`
2023-12-15 12:11:52 INFO     	 * Num of GPU in use: 1
2023-12-15 12:11:52 INFO     	 * Prefix: True
2023-12-15 12:11:52 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 12:11:53 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 12:17:05 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 12:22:17 INFO     	Bleu_1: 0.12368000115805156
2023-12-15 12:22:17 INFO     	Bleu_2: 0.06742242363167357
2023-12-15 12:22:17 INFO     	Bleu_3: 0.035469437114870654
2023-12-15 12:22:17 INFO     	Bleu_4: 0.022495463165880095
2023-12-15 12:22:17 INFO     	Bleu_1: 0.11738234283751209
2023-12-15 12:22:17 INFO     	Bleu_2: 0.06425965923703256
2023-12-15 12:22:17 INFO     	Bleu_3: 0.03431483618532636
2023-12-15 12:22:17 INFO     	Bleu_4: 0.021840965952374444
2023-12-15 12:22:17 INFO     ## 2nd RUN (EVAL): Configuration 3/5 ##
2023-12-15 12:22:34 INFO     use spaCy answer extraction model: positionrank
2023-12-15 12:22:34 INFO     Model `small_finetuned_ckpt/model_nrudfu/epoch_1`
2023-12-15 12:22:34 INFO     	 * Num of GPU in use: 1
2023-12-15 12:22:34 INFO     	 * Prefix: True
2023-12-15 12:22:34 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 12:22:35 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 12:26:47 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 12:30:40 INFO     	Bleu_1: 0.22255506607929187
2023-12-15 12:30:40 INFO     	Bleu_2: 0.12402297094649412
2023-12-15 12:30:40 INFO     	Bleu_3: 0.06888878664142187
2023-12-15 12:30:40 INFO     	Bleu_4: 0.044594802740991514
2023-12-15 12:30:41 INFO     	Bleu_1: 0.2063863657645819
2023-12-15 12:30:41 INFO     	Bleu_2: 0.11454784228169868
2023-12-15 12:30:41 INFO     	Bleu_3: 0.06340655854524996
2023-12-15 12:30:41 INFO     	Bleu_4: 0.04084025123415649
2023-12-15 12:30:57 INFO     use spaCy answer extraction model: positionrank
2023-12-15 12:30:57 INFO     Model `small_finetuned_ckpt/model_nrudfu/epoch_11`
2023-12-15 12:30:57 INFO     	 * Num of GPU in use: 1
2023-12-15 12:30:57 INFO     	 * Prefix: True
2023-12-15 12:30:57 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 12:30:58 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 12:36:12 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 12:41:24 INFO     	Bleu_1: 0.11765543716384325
2023-12-15 12:41:24 INFO     	Bleu_2: 0.06434400635807397
2023-12-15 12:41:24 INFO     	Bleu_3: 0.03396900197867868
2023-12-15 12:41:24 INFO     	Bleu_4: 0.021678055528952873
2023-12-15 12:41:24 INFO     	Bleu_1: 0.11288040324210756
2023-12-15 12:41:24 INFO     	Bleu_2: 0.06179210809956868
2023-12-15 12:41:24 INFO     	Bleu_3: 0.03277914335560192
2023-12-15 12:41:24 INFO     	Bleu_4: 0.020781515818536368
2023-12-15 12:41:36 INFO     use spaCy answer extraction model: positionrank
2023-12-15 12:41:36 INFO     Model `small_finetuned_ckpt/model_nrudfu/epoch_12`
2023-12-15 12:41:36 INFO     	 * Num of GPU in use: 1
2023-12-15 12:41:36 INFO     	 * Prefix: True
2023-12-15 12:41:36 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 12:41:37 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 12:46:46 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 12:51:56 INFO     	Bleu_1: 0.12159241491223634
2023-12-15 12:51:56 INFO     	Bleu_2: 0.06635318398270275
2023-12-15 12:51:56 INFO     	Bleu_3: 0.03490497948352701
2023-12-15 12:51:56 INFO     	Bleu_4: 0.022233626846889926
2023-12-15 12:51:56 INFO     	Bleu_1: 0.11805792628804661
2023-12-15 12:51:56 INFO     	Bleu_2: 0.06498774061511023
2023-12-15 12:51:56 INFO     	Bleu_3: 0.0349033073674635
2023-12-15 12:51:56 INFO     	Bleu_4: 0.022248170644355227
2023-12-15 12:52:10 INFO     use spaCy answer extraction model: positionrank
2023-12-15 12:52:10 INFO     Model `small_finetuned_ckpt/model_nrudfu/epoch_13`
2023-12-15 12:52:10 INFO     	 * Num of GPU in use: 1
2023-12-15 12:52:10 INFO     	 * Prefix: True
2023-12-15 12:52:10 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 12:52:11 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 12:57:22 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 13:02:37 INFO     	Bleu_1: 0.11561773414677692
2023-12-15 13:02:37 INFO     	Bleu_2: 0.0629594094173213
2023-12-15 13:02:37 INFO     	Bleu_3: 0.03286796653278652
2023-12-15 13:02:37 INFO     	Bleu_4: 0.02086103522466977
2023-12-15 13:02:38 INFO     	Bleu_1: 0.11342943362992247
2023-12-15 13:02:38 INFO     	Bleu_2: 0.06274622216702636
2023-12-15 13:02:38 INFO     	Bleu_3: 0.03407290259648245
2023-12-15 13:02:38 INFO     	Bleu_4: 0.021980030326174158
2023-12-15 13:02:48 INFO     use spaCy answer extraction model: positionrank
2023-12-15 13:02:48 INFO     Model `small_finetuned_ckpt/model_nrudfu/epoch_14`
2023-12-15 13:02:48 INFO     	 * Num of GPU in use: 1
2023-12-15 13:02:48 INFO     	 * Prefix: True
2023-12-15 13:02:48 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 13:02:49 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 13:08:03 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 13:13:17 INFO     	Bleu_1: 0.11514206010285896
2023-12-15 13:13:17 INFO     	Bleu_2: 0.0631713829399277
2023-12-15 13:13:17 INFO     	Bleu_3: 0.033568344118544424
2023-12-15 13:13:17 INFO     	Bleu_4: 0.02159603364673942
2023-12-15 13:13:17 INFO     	Bleu_1: 0.11002167729647708
2023-12-15 13:13:17 INFO     	Bleu_2: 0.06075096195638503
2023-12-15 13:13:17 INFO     	Bleu_3: 0.03274119238052089
2023-12-15 13:13:17 INFO     	Bleu_4: 0.020952488532250083
2023-12-15 13:13:31 INFO     use spaCy answer extraction model: positionrank
2023-12-15 13:13:32 INFO     Model `small_finetuned_ckpt/model_nrudfu/epoch_15`
2023-12-15 13:13:32 INFO     	 * Num of GPU in use: 1
2023-12-15 13:13:32 INFO     	 * Prefix: True
2023-12-15 13:13:32 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 13:13:33 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 13:18:50 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 13:24:06 INFO     	Bleu_1: 0.11059907834101314
2023-12-15 13:24:06 INFO     	Bleu_2: 0.060971922747856414
2023-12-15 13:24:06 INFO     	Bleu_3: 0.032542735215707386
2023-12-15 13:24:06 INFO     	Bleu_4: 0.020969311991663585
2023-12-15 13:24:07 INFO     	Bleu_1: 0.11078739602876227
2023-12-15 13:24:07 INFO     	Bleu_2: 0.06082419191642765
2023-12-15 13:24:07 INFO     	Bleu_3: 0.03241342892356153
2023-12-15 13:24:07 INFO     	Bleu_4: 0.020645616408303563
2023-12-15 13:24:24 INFO     use spaCy answer extraction model: positionrank
2023-12-15 13:24:24 INFO     Model `small_finetuned_ckpt/model_nrudfu/epoch_2`
2023-12-15 13:24:24 INFO     	 * Num of GPU in use: 1
2023-12-15 13:24:24 INFO     	 * Prefix: True
2023-12-15 13:24:24 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 13:24:25 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 13:29:10 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 13:33:50 INFO     	Bleu_1: 0.1726529965598387
2023-12-15 13:33:50 INFO     	Bleu_2: 0.09496677001536358
2023-12-15 13:33:50 INFO     	Bleu_3: 0.050735296275318004
2023-12-15 13:33:50 INFO     	Bleu_4: 0.03236520527539009
2023-12-15 13:33:50 INFO     	Bleu_1: 0.1717626392929662
2023-12-15 13:33:50 INFO     	Bleu_2: 0.09404879213385099
2023-12-15 13:33:50 INFO     	Bleu_3: 0.050403381920408726
2023-12-15 13:33:50 INFO     	Bleu_4: 0.031837218051344526
2023-12-15 13:34:07 INFO     use spaCy answer extraction model: positionrank
2023-12-15 13:34:07 INFO     Model `small_finetuned_ckpt/model_nrudfu/epoch_3`
2023-12-15 13:34:07 INFO     	 * Num of GPU in use: 1
2023-12-15 13:34:07 INFO     	 * Prefix: True
2023-12-15 13:34:07 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 13:34:08 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 13:39:09 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 13:44:02 INFO     	Bleu_1: 0.15514108273259025
2023-12-15 13:44:02 INFO     	Bleu_2: 0.08436074291990228
2023-12-15 13:44:02 INFO     	Bleu_3: 0.04396214939218179
2023-12-15 13:44:02 INFO     	Bleu_4: 0.027421135571338063
2023-12-15 13:44:02 INFO     	Bleu_1: 0.15057509037134265
2023-12-15 13:44:02 INFO     	Bleu_2: 0.08245466292725331
2023-12-15 13:44:02 INFO     	Bleu_3: 0.04443944754335066
2023-12-15 13:44:02 INFO     	Bleu_4: 0.02843003746772814
2023-12-15 13:44:17 INFO     use spaCy answer extraction model: positionrank
2023-12-15 13:44:17 INFO     Model `small_finetuned_ckpt/model_nrudfu/epoch_4`
2023-12-15 13:44:17 INFO     	 * Num of GPU in use: 1
2023-12-15 13:44:17 INFO     	 * Prefix: True
2023-12-15 13:44:17 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 13:44:18 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 13:49:26 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 13:54:28 INFO     	Bleu_1: 0.14141547565582524
2023-12-15 13:54:28 INFO     	Bleu_2: 0.07710541502973495
2023-12-15 13:54:28 INFO     	Bleu_3: 0.0401198102501414
2023-12-15 13:54:28 INFO     	Bleu_4: 0.02515104789337203
2023-12-15 13:54:28 INFO     	Bleu_1: 0.13601258110199801
2023-12-15 13:54:28 INFO     	Bleu_2: 0.07441193375926562
2023-12-15 13:54:28 INFO     	Bleu_3: 0.039724710468549224
2023-12-15 13:54:28 INFO     	Bleu_4: 0.02509104675281221
2023-12-15 13:54:36 INFO     use spaCy answer extraction model: positionrank
2023-12-15 13:54:37 INFO     Model `small_finetuned_ckpt/model_nrudfu/epoch_5`
2023-12-15 13:54:37 INFO     	 * Num of GPU in use: 1
2023-12-15 13:54:37 INFO     	 * Prefix: True
2023-12-15 13:54:37 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 13:54:38 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 13:59:48 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 14:04:54 INFO     	Bleu_1: 0.1352631360676802
2023-12-15 14:04:54 INFO     	Bleu_2: 0.07350537613758125
2023-12-15 14:04:54 INFO     	Bleu_3: 0.03789767436150744
2023-12-15 14:04:54 INFO     	Bleu_4: 0.023658109832308528
2023-12-15 14:04:54 INFO     	Bleu_1: 0.1256035036845479
2023-12-15 14:04:54 INFO     	Bleu_2: 0.06876728634280045
2023-12-15 14:04:54 INFO     	Bleu_3: 0.03660614135969604
2023-12-15 14:04:54 INFO     	Bleu_4: 0.023295631944150054
2023-12-15 14:05:08 INFO     use spaCy answer extraction model: positionrank
2023-12-15 14:05:08 INFO     Model `small_finetuned_ckpt/model_nrudfu/epoch_6`
2023-12-15 14:05:08 INFO     	 * Num of GPU in use: 1
2023-12-15 14:05:08 INFO     	 * Prefix: True
2023-12-15 14:05:08 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 14:05:09 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 14:10:19 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 14:15:25 INFO     	Bleu_1: 0.1318453732246825
2023-12-15 14:15:25 INFO     	Bleu_2: 0.07198130930177589
2023-12-15 14:15:25 INFO     	Bleu_3: 0.03790970707532742
2023-12-15 14:15:25 INFO     	Bleu_4: 0.024094792515923766
2023-12-15 14:15:25 INFO     	Bleu_1: 0.12681850255979008
2023-12-15 14:15:25 INFO     	Bleu_2: 0.0697678921828895
2023-12-15 14:15:25 INFO     	Bleu_3: 0.03784811759519804
2023-12-15 14:15:25 INFO     	Bleu_4: 0.024515141656330047
2023-12-15 14:15:34 INFO     use spaCy answer extraction model: positionrank
2023-12-15 14:15:34 INFO     Model `small_finetuned_ckpt/model_nrudfu/epoch_7`
2023-12-15 14:15:34 INFO     	 * Num of GPU in use: 1
2023-12-15 14:15:34 INFO     	 * Prefix: True
2023-12-15 14:15:34 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 14:15:35 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 14:20:49 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 14:26:05 INFO     	Bleu_1: 0.11633712190775389
2023-12-15 14:26:05 INFO     	Bleu_2: 0.06287858188040109
2023-12-15 14:26:05 INFO     	Bleu_3: 0.031964840103718105
2023-12-15 14:26:05 INFO     	Bleu_4: 0.019736252334527745
2023-12-15 14:26:05 INFO     	Bleu_1: 0.11383206675805453
2023-12-15 14:26:05 INFO     	Bleu_2: 0.06216396978992693
2023-12-15 14:26:05 INFO     	Bleu_3: 0.03250224279830606
2023-12-15 14:26:05 INFO     	Bleu_4: 0.02027424620393611
2023-12-15 14:26:17 INFO     use spaCy answer extraction model: positionrank
2023-12-15 14:26:18 INFO     Model `small_finetuned_ckpt/model_nrudfu/epoch_8`
2023-12-15 14:26:18 INFO     	 * Num of GPU in use: 1
2023-12-15 14:26:18 INFO     	 * Prefix: True
2023-12-15 14:26:18 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 14:26:19 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 14:31:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 14:36:44 INFO     	Bleu_1: 0.1197319709232499
2023-12-15 14:36:44 INFO     	Bleu_2: 0.06466570509071846
2023-12-15 14:36:44 INFO     	Bleu_3: 0.033058839557138786
2023-12-15 14:36:44 INFO     	Bleu_4: 0.020481046375409782
2023-12-15 14:36:44 INFO     	Bleu_1: 0.11805598403981739
2023-12-15 14:36:44 INFO     	Bleu_2: 0.0648146092346074
2023-12-15 14:36:44 INFO     	Bleu_3: 0.034729580066933455
2023-12-15 14:36:44 INFO     	Bleu_4: 0.022141352378445243
2023-12-15 14:36:57 INFO     use spaCy answer extraction model: positionrank
2023-12-15 14:36:58 INFO     Model `small_finetuned_ckpt/model_nrudfu/epoch_9`
2023-12-15 14:36:58 INFO     	 * Num of GPU in use: 1
2023-12-15 14:36:58 INFO     	 * Prefix: True
2023-12-15 14:36:58 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 14:36:59 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 14:42:13 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 14:47:27 INFO     	Bleu_1: 0.12368000115805156
2023-12-15 14:47:27 INFO     	Bleu_2: 0.06742242363167357
2023-12-15 14:47:27 INFO     	Bleu_3: 0.035469437114870654
2023-12-15 14:47:27 INFO     	Bleu_4: 0.022495463165880095
2023-12-15 14:47:27 INFO     	Bleu_1: 0.11738234283751209
2023-12-15 14:47:27 INFO     	Bleu_2: 0.06425965923703256
2023-12-15 14:47:27 INFO     	Bleu_3: 0.03431483618532636
2023-12-15 14:47:27 INFO     	Bleu_4: 0.021840965952374444
2023-12-15 14:47:27 INFO     ## 2nd RUN (EVAL): Configuration 4/5 ##
2023-12-15 14:47:40 INFO     use spaCy answer extraction model: positionrank
2023-12-15 14:47:40 INFO     Model `small_finetuned_ckpt/model_woixzh/epoch_1`
2023-12-15 14:47:40 INFO     	 * Num of GPU in use: 1
2023-12-15 14:47:40 INFO     	 * Prefix: True
2023-12-15 14:47:40 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 14:47:41 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 14:52:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 14:57:35 INFO     	Bleu_1: 0.1447561414351988
2023-12-15 14:57:35 INFO     	Bleu_2: 0.07895812302480468
2023-12-15 14:57:35 INFO     	Bleu_3: 0.041278174715215436
2023-12-15 14:57:35 INFO     	Bleu_4: 0.025989332045977327
2023-12-15 14:57:36 INFO     	Bleu_1: 0.1397725214931822
2023-12-15 14:57:36 INFO     	Bleu_2: 0.07701693296719173
2023-12-15 14:57:36 INFO     	Bleu_3: 0.041910661403114904
2023-12-15 14:57:36 INFO     	Bleu_4: 0.02711593446763111
2023-12-15 14:57:49 INFO     use spaCy answer extraction model: positionrank
2023-12-15 14:57:49 INFO     Model `small_finetuned_ckpt/model_woixzh/epoch_11`
2023-12-15 14:57:49 INFO     	 * Num of GPU in use: 1
2023-12-15 14:57:49 INFO     	 * Prefix: True
2023-12-15 14:57:49 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 14:57:50 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 15:03:09 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 15:08:29 INFO     	Bleu_1: 0.10579479329982769
2023-12-15 15:08:29 INFO     	Bleu_2: 0.057545231611828
2023-12-15 15:08:29 INFO     	Bleu_3: 0.02959367688810689
2023-12-15 15:08:29 INFO     	Bleu_4: 0.01845803715155022
2023-12-15 15:08:29 INFO     	Bleu_1: 0.10311272502877697
2023-12-15 15:08:29 INFO     	Bleu_2: 0.056804256195966074
2023-12-15 15:08:29 INFO     	Bleu_3: 0.030271537019886618
2023-12-15 15:08:29 INFO     	Bleu_4: 0.01928634199650454
2023-12-15 15:08:46 INFO     use spaCy answer extraction model: positionrank
2023-12-15 15:08:47 INFO     Model `small_finetuned_ckpt/model_woixzh/epoch_12`
2023-12-15 15:08:47 INFO     	 * Num of GPU in use: 1
2023-12-15 15:08:47 INFO     	 * Prefix: True
2023-12-15 15:08:47 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 15:08:48 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 15:14:06 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 15:19:23 INFO     	Bleu_1: 0.11053572807040045
2023-12-15 15:19:23 INFO     	Bleu_2: 0.060884598274011784
2023-12-15 15:19:23 INFO     	Bleu_3: 0.032302176969653366
2023-12-15 15:19:23 INFO     	Bleu_4: 0.020445548576331424
2023-12-15 15:19:23 INFO     	Bleu_1: 0.10495463134572482
2023-12-15 15:19:23 INFO     	Bleu_2: 0.0573008626746502
2023-12-15 15:19:23 INFO     	Bleu_3: 0.030039299183455118
2023-12-15 15:19:23 INFO     	Bleu_4: 0.01895100949649414
2023-12-15 15:19:38 INFO     use spaCy answer extraction model: positionrank
2023-12-15 15:19:38 INFO     Model `small_finetuned_ckpt/model_woixzh/epoch_13`
2023-12-15 15:19:38 INFO     	 * Num of GPU in use: 1
2023-12-15 15:19:38 INFO     	 * Prefix: True
2023-12-15 15:19:38 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 15:19:39 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 15:24:58 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 15:30:19 INFO     	Bleu_1: 0.10536981957549216
2023-12-15 15:30:19 INFO     	Bleu_2: 0.057881045082489854
2023-12-15 15:30:19 INFO     	Bleu_3: 0.030332561130679038
2023-12-15 15:30:19 INFO     	Bleu_4: 0.019190713802367556
2023-12-15 15:30:19 INFO     	Bleu_1: 0.10187306761953872
2023-12-15 15:30:19 INFO     	Bleu_2: 0.05576066796094829
2023-12-15 15:30:19 INFO     	Bleu_3: 0.02965280960967458
2023-12-15 15:30:19 INFO     	Bleu_4: 0.018898718003737748
2023-12-15 15:30:41 INFO     use spaCy answer extraction model: positionrank
2023-12-15 15:30:41 INFO     Model `small_finetuned_ckpt/model_woixzh/epoch_14`
2023-12-15 15:30:41 INFO     	 * Num of GPU in use: 1
2023-12-15 15:30:41 INFO     	 * Prefix: True
2023-12-15 15:30:41 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 15:30:42 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 15:35:54 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 15:41:08 INFO     	Bleu_1: 0.10944031946090904
2023-12-15 15:41:08 INFO     	Bleu_2: 0.05982761219073408
2023-12-15 15:41:08 INFO     	Bleu_3: 0.031152348541007003
2023-12-15 15:41:08 INFO     	Bleu_4: 0.01953030552991222
2023-12-15 15:41:09 INFO     	Bleu_1: 0.11101211018384821
2023-12-15 15:41:09 INFO     	Bleu_2: 0.06040307499092058
2023-12-15 15:41:09 INFO     	Bleu_3: 0.03178831058709136
2023-12-15 15:41:09 INFO     	Bleu_4: 0.020118355662078067
2023-12-15 15:41:24 INFO     use spaCy answer extraction model: positionrank
2023-12-15 15:41:24 INFO     Model `small_finetuned_ckpt/model_woixzh/epoch_15`
2023-12-15 15:41:24 INFO     	 * Num of GPU in use: 1
2023-12-15 15:41:24 INFO     	 * Prefix: True
2023-12-15 15:41:24 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 15:41:25 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 15:46:41 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 15:51:59 INFO     	Bleu_1: 0.1093156489363029
2023-12-15 15:51:59 INFO     	Bleu_2: 0.06012006697385089
2023-12-15 15:51:59 INFO     	Bleu_3: 0.03144098089851074
2023-12-15 15:51:59 INFO     	Bleu_4: 0.019649806454072632
2023-12-15 15:52:00 INFO     	Bleu_1: 0.10903856572299639
2023-12-15 15:52:00 INFO     	Bleu_2: 0.05997673098134927
2023-12-15 15:52:00 INFO     	Bleu_3: 0.032047713871282
2023-12-15 15:52:00 INFO     	Bleu_4: 0.020466350720257473
2023-12-15 15:52:12 INFO     use spaCy answer extraction model: positionrank
2023-12-15 15:52:12 INFO     Model `small_finetuned_ckpt/model_woixzh/epoch_2`
2023-12-15 15:52:12 INFO     	 * Num of GPU in use: 1
2023-12-15 15:52:12 INFO     	 * Prefix: True
2023-12-15 15:52:12 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 15:52:13 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 15:57:19 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 16:02:17 INFO     	Bleu_1: 0.1330216985175769
2023-12-15 16:02:17 INFO     	Bleu_2: 0.07259524423717972
2023-12-15 16:02:17 INFO     	Bleu_3: 0.03774524426032545
2023-12-15 16:02:17 INFO     	Bleu_4: 0.02366405185167324
2023-12-15 16:02:17 INFO     	Bleu_1: 0.13135994943642576
2023-12-15 16:02:17 INFO     	Bleu_2: 0.07176516047863482
2023-12-15 16:02:17 INFO     	Bleu_3: 0.03795520631608938
2023-12-15 16:02:17 INFO     	Bleu_4: 0.023939871530988425
2023-12-15 16:02:32 INFO     use spaCy answer extraction model: positionrank
2023-12-15 16:02:32 INFO     Model `small_finetuned_ckpt/model_woixzh/epoch_3`
2023-12-15 16:02:32 INFO     	 * Num of GPU in use: 1
2023-12-15 16:02:32 INFO     	 * Prefix: True
2023-12-15 16:02:32 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 16:02:33 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 16:07:48 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 16:13:02 INFO     	Bleu_1: 0.11499221286592856
2023-12-15 16:13:02 INFO     	Bleu_2: 0.06317886106436015
2023-12-15 16:13:02 INFO     	Bleu_3: 0.03373935904895569
2023-12-15 16:13:02 INFO     	Bleu_4: 0.0218002461382499
2023-12-15 16:13:03 INFO     	Bleu_1: 0.11179487179487108
2023-12-15 16:13:03 INFO     	Bleu_2: 0.06140344782105125
2023-12-15 16:13:03 INFO     	Bleu_3: 0.03280640145574717
2023-12-15 16:13:03 INFO     	Bleu_4: 0.02082748340193933
2023-12-15 16:13:16 INFO     use spaCy answer extraction model: positionrank
2023-12-15 16:13:16 INFO     Model `small_finetuned_ckpt/model_woixzh/epoch_4`
2023-12-15 16:13:16 INFO     	 * Num of GPU in use: 1
2023-12-15 16:13:16 INFO     	 * Prefix: True
2023-12-15 16:13:16 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 16:13:17 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 16:18:30 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 16:23:44 INFO     	Bleu_1: 0.11674525935587361
2023-12-15 16:23:44 INFO     	Bleu_2: 0.0641331551633665
2023-12-15 16:23:44 INFO     	Bleu_3: 0.033962509848869844
2023-12-15 16:23:44 INFO     	Bleu_4: 0.02173560516034209
2023-12-15 16:23:44 INFO     	Bleu_1: 0.11537328383713319
2023-12-15 16:23:44 INFO     	Bleu_2: 0.06324212785053236
2023-12-15 16:23:44 INFO     	Bleu_3: 0.0336487478449977
2023-12-15 16:23:44 INFO     	Bleu_4: 0.021506472553420426
2023-12-15 16:23:58 INFO     use spaCy answer extraction model: positionrank
2023-12-15 16:23:58 INFO     Model `small_finetuned_ckpt/model_woixzh/epoch_5`
2023-12-15 16:23:58 INFO     	 * Num of GPU in use: 1
2023-12-15 16:23:58 INFO     	 * Prefix: True
2023-12-15 16:23:58 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 16:23:59 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 16:29:15 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 16:34:33 INFO     	Bleu_1: 0.11105649223383335
2023-12-15 16:34:33 INFO     	Bleu_2: 0.06106938053949282
2023-12-15 16:34:33 INFO     	Bleu_3: 0.032254078140014686
2023-12-15 16:34:33 INFO     	Bleu_4: 0.020603601223380043
2023-12-15 16:34:34 INFO     	Bleu_1: 0.11208694003917287
2023-12-15 16:34:34 INFO     	Bleu_2: 0.06152728462892532
2023-12-15 16:34:34 INFO     	Bleu_3: 0.032892205744679284
2023-12-15 16:34:34 INFO     	Bleu_4: 0.020897693629954212
2023-12-15 16:34:46 INFO     use spaCy answer extraction model: positionrank
2023-12-15 16:34:46 INFO     Model `small_finetuned_ckpt/model_woixzh/epoch_6`
2023-12-15 16:34:46 INFO     	 * Num of GPU in use: 1
2023-12-15 16:34:46 INFO     	 * Prefix: True
2023-12-15 16:34:46 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 16:34:47 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 16:40:05 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 16:45:25 INFO     	Bleu_1: 0.10660648042546488
2023-12-15 16:45:25 INFO     	Bleu_2: 0.05850809248992588
2023-12-15 16:45:25 INFO     	Bleu_3: 0.030799010783448568
2023-12-15 16:45:25 INFO     	Bleu_4: 0.019694400796029514
2023-12-15 16:45:26 INFO     	Bleu_1: 0.10547067520946216
2023-12-15 16:45:26 INFO     	Bleu_2: 0.05750246984796884
2023-12-15 16:45:26 INFO     	Bleu_3: 0.030138622406846772
2023-12-15 16:45:26 INFO     	Bleu_4: 0.018994969405610882
2023-12-15 16:45:41 INFO     use spaCy answer extraction model: positionrank
2023-12-15 16:45:41 INFO     Model `small_finetuned_ckpt/model_woixzh/epoch_7`
2023-12-15 16:45:41 INFO     	 * Num of GPU in use: 1
2023-12-15 16:45:41 INFO     	 * Prefix: True
2023-12-15 16:45:41 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 16:45:42 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 16:51:01 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 16:56:23 INFO     	Bleu_1: 0.10388446740104298
2023-12-15 16:56:23 INFO     	Bleu_2: 0.05724314347489658
2023-12-15 16:56:23 INFO     	Bleu_3: 0.030391669429933044
2023-12-15 16:56:23 INFO     	Bleu_4: 0.019518642794689792
2023-12-15 16:56:23 INFO     	Bleu_1: 0.1039577491564083
2023-12-15 16:56:23 INFO     	Bleu_2: 0.056487640893589194
2023-12-15 16:56:23 INFO     	Bleu_3: 0.029278541297146923
2023-12-15 16:56:23 INFO     	Bleu_4: 0.018247731033794647
2023-12-15 16:56:42 INFO     use spaCy answer extraction model: positionrank
2023-12-15 16:56:42 INFO     Model `small_finetuned_ckpt/model_woixzh/epoch_8`
2023-12-15 16:56:42 INFO     	 * Num of GPU in use: 1
2023-12-15 16:56:42 INFO     	 * Prefix: True
2023-12-15 16:56:42 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 16:56:43 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 17:02:07 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 17:07:25 INFO     	Bleu_1: 0.10594131129240975
2023-12-15 17:07:25 INFO     	Bleu_2: 0.05817117554216497
2023-12-15 17:07:25 INFO     	Bleu_3: 0.030568976774508268
2023-12-15 17:07:25 INFO     	Bleu_4: 0.01934360883212229
2023-12-15 17:07:26 INFO     	Bleu_1: 0.10590641164441174
2023-12-15 17:07:26 INFO     	Bleu_2: 0.05796785208894024
2023-12-15 17:07:26 INFO     	Bleu_3: 0.030608044967265213
2023-12-15 17:07:26 INFO     	Bleu_4: 0.019341449842493967
2023-12-15 17:07:41 INFO     use spaCy answer extraction model: positionrank
2023-12-15 17:07:41 INFO     Model `small_finetuned_ckpt/model_woixzh/epoch_9`
2023-12-15 17:07:41 INFO     	 * Num of GPU in use: 1
2023-12-15 17:07:41 INFO     	 * Prefix: True
2023-12-15 17:07:41 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 17:07:42 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 17:13:01 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqalmqg/t5-small-squad-qag.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 17:18:18 INFO     	Bleu_1: 0.10754106001692626
2023-12-15 17:18:18 INFO     	Bleu_2: 0.058717070536631645
2023-12-15 17:18:18 INFO     	Bleu_3: 0.03055790232052967
2023-12-15 17:18:18 INFO     	Bleu_4: 0.019321644119670054
2023-12-15 17:18:18 INFO     	Bleu_1: 0.1074051011411901
2023-12-15 17:18:18 INFO     	Bleu_2: 0.05835863431060739
2023-12-15 17:18:18 INFO     	Bleu_3: 0.03050233665589353
2023-12-15 17:18:18 INFO     	Bleu_4: 0.019234543057298464
2023-12-15 17:18:19 INFO     2nd RUN RESULTS: 
[('small_finetuned_ckpt/model_nxaqhy/epoch_1', 0.04944739287136915), ('small_finetuned_ckpt/model_vhyoja/epoch_1', 0.04944739287136915), ('small_finetuned_ckpt/model_nxaqhy/epoch_11', 0.047256764247969874), ('small_finetuned_ckpt/model_oprhlh/epoch_1', 0.044594802740991514), ('small_finetuned_ckpt/model_nrudfu/epoch_1', 0.044594802740991514), ('small_finetuned_ckpt/model_nxaqhy/epoch_2', 0.037788504203896324), ('small_finetuned_ckpt/model_vhyoja/epoch_2', 0.037788504203896324), ('small_finetuned_ckpt/model_oprhlh/epoch_11', 0.03332006101802874), ('small_finetuned_ckpt/model_oprhlh/epoch_2', 0.03236520527539009), ('small_finetuned_ckpt/model_nrudfu/epoch_2', 0.03236520527539009), ('small_finetuned_ckpt/model_nxaqhy/epoch_3', 0.03215937900705852), ('small_finetuned_ckpt/model_vhyoja/epoch_3', 0.03215937900705852), ('small_finetuned_ckpt/model_nxaqhy/epoch_4', 0.028807045432486796), ('small_finetuned_ckpt/model_vhyoja/epoch_4', 0.028807045432486796), ('small_finetuned_ckpt/model_nxaqhy/epoch_12', 0.028535151242880794), ('small_finetuned_ckpt/model_oprhlh/epoch_3', 0.027421135571338063), ('small_finetuned_ckpt/model_nrudfu/epoch_3', 0.027421135571338063), ('small_finetuned_ckpt/model_nxaqhy/epoch_6', 0.026206240536265735), ('small_finetuned_ckpt/model_vhyoja/epoch_6', 0.026206240536265735), ('small_finetuned_ckpt/model_woixzh/epoch_1', 0.025989332045977327), ('small_finetuned_ckpt/model_nxaqhy/epoch_5', 0.025754485750608378), ('small_finetuned_ckpt/model_vhyoja/epoch_5', 0.025754485750608378), ('small_finetuned_ckpt/model_oprhlh/epoch_4', 0.02515104789337203), ('small_finetuned_ckpt/model_nrudfu/epoch_4', 0.02515104789337203), ('small_finetuned_ckpt/model_oprhlh/epoch_6', 0.024094792515923766), ('small_finetuned_ckpt/model_nrudfu/epoch_6', 0.024094792515923766), ('small_finetuned_ckpt/model_nxaqhy/epoch_7', 0.02398447675020153), ('small_finetuned_ckpt/model_vhyoja/epoch_7', 0.02398447675020153), ('small_finetuned_ckpt/model_woixzh/epoch_2', 0.02366405185167324), ('small_finetuned_ckpt/model_oprhlh/epoch_5', 0.023658109832308528), ('small_finetuned_ckpt/model_nrudfu/epoch_5', 0.023658109832308528), ('small_finetuned_ckpt/model_nxaqhy/epoch_8', 0.0233810495239621), ('small_finetuned_ckpt/model_vhyoja/epoch_8', 0.0233810495239621), ('small_finetuned_ckpt/model_nxaqhy/epoch_9', 0.023249936720927768), ('small_finetuned_ckpt/model_vhyoja/epoch_9', 0.023249936720927768), ('small_finetuned_ckpt/model_nxaqhy/epoch_10', 0.02285404140578058), ('small_finetuned_ckpt/model_vhyoja/epoch_10', 0.02285404140578058), ('small_finetuned_ckpt/model_nxaqhy/epoch_13', 0.022671142120146877), ('small_finetuned_ckpt/model_vhyoja/epoch_12', 0.022497667675167216), ('small_finetuned_ckpt/model_oprhlh/epoch_9', 0.022495463165880095), ('small_finetuned_ckpt/model_nrudfu/epoch_9', 0.022495463165880095), ('small_finetuned_ckpt/model_vhyoja/epoch_14', 0.02242380427780753), ('small_finetuned_ckpt/model_nrudfu/epoch_12', 0.022233626846889926), ('small_finetuned_ckpt/model_vhyoja/epoch_11', 0.022041354071268265), ('small_finetuned_ckpt/model_vhyoja/epoch_13', 0.021839653431317774), ('small_finetuned_ckpt/model_woixzh/epoch_3', 0.0218002461382499), ('small_finetuned_ckpt/model_woixzh/epoch_4', 0.02173560516034209), ('small_finetuned_ckpt/model_nrudfu/epoch_11', 0.021678055528952873), ('small_finetuned_ckpt/model_nrudfu/epoch_14', 0.02159603364673942), ('small_finetuned_ckpt/model_oprhlh/epoch_12', 0.021356445068363164), ('small_finetuned_ckpt/model_oprhlh/epoch_10', 0.021321283395881168), ('small_finetuned_ckpt/model_nrudfu/epoch_10', 0.021321283395881168), ('small_finetuned_ckpt/model_nrudfu/epoch_15', 0.020969311991663585), ('small_finetuned_ckpt/model_nrudfu/epoch_13', 0.02086103522466977), ('small_finetuned_ckpt/model_vhyoja/epoch_15', 0.020658798699533916), ('small_finetuned_ckpt/model_woixzh/epoch_5', 0.020603601223380043), ('small_finetuned_ckpt/model_nxaqhy/epoch_14', 0.020516478342669786), ('small_finetuned_ckpt/model_oprhlh/epoch_8', 0.020481046375409782), ('small_finetuned_ckpt/model_nrudfu/epoch_8', 0.020481046375409782), ('small_finetuned_ckpt/model_woixzh/epoch_12', 0.020445548576331424), ('small_finetuned_ckpt/model_oprhlh/epoch_7', 0.019736252334527745), ('small_finetuned_ckpt/model_nrudfu/epoch_7', 0.019736252334527745), ('small_finetuned_ckpt/model_woixzh/epoch_6', 0.019694400796029514), ('small_finetuned_ckpt/model_woixzh/epoch_15', 0.019649806454072632), ('small_finetuned_ckpt/model_woixzh/epoch_14', 0.01953030552991222), ('small_finetuned_ckpt/model_woixzh/epoch_7', 0.019518642794689792), ('small_finetuned_ckpt/model_woixzh/epoch_8', 0.01934360883212229), ('small_finetuned_ckpt/model_woixzh/epoch_9', 0.019321644119670054), ('small_finetuned_ckpt/model_woixzh/epoch_13', 0.019190713802367556), ('small_finetuned_ckpt/model_woixzh/epoch_10', 0.019148507855686385), ('small_finetuned_ckpt/model_oprhlh/epoch_13', 0.019035768980693615), ('small_finetuned_ckpt/model_nxaqhy/epoch_15', 0.018661040203592764), ('small_finetuned_ckpt/model_oprhlh/epoch_14', 0.018471772553295678), ('small_finetuned_ckpt/model_woixzh/epoch_11', 0.01845803715155022), ('small_finetuned_ckpt/model_oprhlh/epoch_15', 0.018089911570606795)]
2023-12-15 17:18:19 INFO     	 * rank: 0 | metric: 0.049 | model: small_finetuned_ckpt/model_nxaqhy/epoch_1 |
2023-12-15 17:18:19 INFO     	 * rank: 1 | metric: 0.049 | model: small_finetuned_ckpt/model_vhyoja/epoch_1 |
2023-12-15 17:18:19 INFO     	 * rank: 2 | metric: 0.047 | model: small_finetuned_ckpt/model_nxaqhy/epoch_11 |
2023-12-15 17:18:19 INFO     	 * rank: 3 | metric: 0.045 | model: small_finetuned_ckpt/model_oprhlh/epoch_1 |
2023-12-15 17:18:19 INFO     	 * rank: 4 | metric: 0.045 | model: small_finetuned_ckpt/model_nrudfu/epoch_1 |
2023-12-15 17:18:19 INFO     	 * rank: 5 | metric: 0.038 | model: small_finetuned_ckpt/model_nxaqhy/epoch_2 |
2023-12-15 17:18:19 INFO     	 * rank: 6 | metric: 0.038 | model: small_finetuned_ckpt/model_vhyoja/epoch_2 |
2023-12-15 17:18:19 INFO     	 * rank: 7 | metric: 0.033 | model: small_finetuned_ckpt/model_oprhlh/epoch_11 |
2023-12-15 17:18:19 INFO     	 * rank: 8 | metric: 0.032 | model: small_finetuned_ckpt/model_oprhlh/epoch_2 |
2023-12-15 17:18:19 INFO     	 * rank: 9 | metric: 0.032 | model: small_finetuned_ckpt/model_nrudfu/epoch_2 |
2023-12-15 17:18:19 INFO     	 * rank: 10 | metric: 0.032 | model: small_finetuned_ckpt/model_nxaqhy/epoch_3 |
2023-12-15 17:18:19 INFO     	 * rank: 11 | metric: 0.032 | model: small_finetuned_ckpt/model_vhyoja/epoch_3 |
2023-12-15 17:18:19 INFO     	 * rank: 12 | metric: 0.029 | model: small_finetuned_ckpt/model_nxaqhy/epoch_4 |
2023-12-15 17:18:19 INFO     	 * rank: 13 | metric: 0.029 | model: small_finetuned_ckpt/model_vhyoja/epoch_4 |
2023-12-15 17:18:19 INFO     	 * rank: 14 | metric: 0.029 | model: small_finetuned_ckpt/model_nxaqhy/epoch_12 |
2023-12-15 17:18:19 INFO     	 * rank: 15 | metric: 0.027 | model: small_finetuned_ckpt/model_oprhlh/epoch_3 |
2023-12-15 17:18:19 INFO     	 * rank: 16 | metric: 0.027 | model: small_finetuned_ckpt/model_nrudfu/epoch_3 |
2023-12-15 17:18:19 INFO     	 * rank: 17 | metric: 0.026 | model: small_finetuned_ckpt/model_nxaqhy/epoch_6 |
2023-12-15 17:18:19 INFO     	 * rank: 18 | metric: 0.026 | model: small_finetuned_ckpt/model_vhyoja/epoch_6 |
2023-12-15 17:18:19 INFO     	 * rank: 19 | metric: 0.026 | model: small_finetuned_ckpt/model_woixzh/epoch_1 |
2023-12-15 17:18:19 INFO     	 * rank: 20 | metric: 0.026 | model: small_finetuned_ckpt/model_nxaqhy/epoch_5 |
2023-12-15 17:18:19 INFO     	 * rank: 21 | metric: 0.026 | model: small_finetuned_ckpt/model_vhyoja/epoch_5 |
2023-12-15 17:18:19 INFO     	 * rank: 22 | metric: 0.025 | model: small_finetuned_ckpt/model_oprhlh/epoch_4 |
2023-12-15 17:18:19 INFO     	 * rank: 23 | metric: 0.025 | model: small_finetuned_ckpt/model_nrudfu/epoch_4 |
2023-12-15 17:18:19 INFO     	 * rank: 24 | metric: 0.024 | model: small_finetuned_ckpt/model_oprhlh/epoch_6 |
2023-12-15 17:18:19 INFO     	 * rank: 25 | metric: 0.024 | model: small_finetuned_ckpt/model_nrudfu/epoch_6 |
2023-12-15 17:18:19 INFO     	 * rank: 26 | metric: 0.024 | model: small_finetuned_ckpt/model_nxaqhy/epoch_7 |
2023-12-15 17:18:19 INFO     	 * rank: 27 | metric: 0.024 | model: small_finetuned_ckpt/model_vhyoja/epoch_7 |
2023-12-15 17:18:19 INFO     	 * rank: 28 | metric: 0.024 | model: small_finetuned_ckpt/model_woixzh/epoch_2 |
2023-12-15 17:18:19 INFO     	 * rank: 29 | metric: 0.024 | model: small_finetuned_ckpt/model_oprhlh/epoch_5 |
2023-12-15 17:18:19 INFO     	 * rank: 30 | metric: 0.024 | model: small_finetuned_ckpt/model_nrudfu/epoch_5 |
2023-12-15 17:18:19 INFO     	 * rank: 31 | metric: 0.023 | model: small_finetuned_ckpt/model_nxaqhy/epoch_8 |
2023-12-15 17:18:19 INFO     	 * rank: 32 | metric: 0.023 | model: small_finetuned_ckpt/model_vhyoja/epoch_8 |
2023-12-15 17:18:19 INFO     	 * rank: 33 | metric: 0.023 | model: small_finetuned_ckpt/model_nxaqhy/epoch_9 |
2023-12-15 17:18:19 INFO     	 * rank: 34 | metric: 0.023 | model: small_finetuned_ckpt/model_vhyoja/epoch_9 |
2023-12-15 17:18:19 INFO     	 * rank: 35 | metric: 0.023 | model: small_finetuned_ckpt/model_nxaqhy/epoch_10 |
2023-12-15 17:18:19 INFO     	 * rank: 36 | metric: 0.023 | model: small_finetuned_ckpt/model_vhyoja/epoch_10 |
2023-12-15 17:18:19 INFO     	 * rank: 37 | metric: 0.023 | model: small_finetuned_ckpt/model_nxaqhy/epoch_13 |
2023-12-15 17:18:19 INFO     	 * rank: 38 | metric: 0.022 | model: small_finetuned_ckpt/model_vhyoja/epoch_12 |
2023-12-15 17:18:19 INFO     	 * rank: 39 | metric: 0.022 | model: small_finetuned_ckpt/model_oprhlh/epoch_9 |
2023-12-15 17:18:19 INFO     	 * rank: 40 | metric: 0.022 | model: small_finetuned_ckpt/model_nrudfu/epoch_9 |
2023-12-15 17:18:19 INFO     	 * rank: 41 | metric: 0.022 | model: small_finetuned_ckpt/model_vhyoja/epoch_14 |
2023-12-15 17:18:19 INFO     	 * rank: 42 | metric: 0.022 | model: small_finetuned_ckpt/model_nrudfu/epoch_12 |
2023-12-15 17:18:19 INFO     	 * rank: 43 | metric: 0.022 | model: small_finetuned_ckpt/model_vhyoja/epoch_11 |
2023-12-15 17:18:19 INFO     	 * rank: 44 | metric: 0.022 | model: small_finetuned_ckpt/model_vhyoja/epoch_13 |
2023-12-15 17:18:19 INFO     	 * rank: 45 | metric: 0.022 | model: small_finetuned_ckpt/model_woixzh/epoch_3 |
2023-12-15 17:18:19 INFO     	 * rank: 46 | metric: 0.022 | model: small_finetuned_ckpt/model_woixzh/epoch_4 |
2023-12-15 17:18:19 INFO     	 * rank: 47 | metric: 0.022 | model: small_finetuned_ckpt/model_nrudfu/epoch_11 |
2023-12-15 17:18:19 INFO     	 * rank: 48 | metric: 0.022 | model: small_finetuned_ckpt/model_nrudfu/epoch_14 |
2023-12-15 17:18:19 INFO     	 * rank: 49 | metric: 0.021 | model: small_finetuned_ckpt/model_oprhlh/epoch_12 |
2023-12-15 17:18:19 INFO     	 * rank: 50 | metric: 0.021 | model: small_finetuned_ckpt/model_oprhlh/epoch_10 |
2023-12-15 17:18:19 INFO     	 * rank: 51 | metric: 0.021 | model: small_finetuned_ckpt/model_nrudfu/epoch_10 |
2023-12-15 17:18:19 INFO     	 * rank: 52 | metric: 0.021 | model: small_finetuned_ckpt/model_nrudfu/epoch_15 |
2023-12-15 17:18:19 INFO     	 * rank: 53 | metric: 0.021 | model: small_finetuned_ckpt/model_nrudfu/epoch_13 |
2023-12-15 17:18:19 INFO     	 * rank: 54 | metric: 0.021 | model: small_finetuned_ckpt/model_vhyoja/epoch_15 |
2023-12-15 17:18:19 INFO     	 * rank: 55 | metric: 0.021 | model: small_finetuned_ckpt/model_woixzh/epoch_5 |
2023-12-15 17:18:19 INFO     	 * rank: 56 | metric: 0.021 | model: small_finetuned_ckpt/model_nxaqhy/epoch_14 |
2023-12-15 17:18:19 INFO     	 * rank: 57 | metric: 0.02 | model: small_finetuned_ckpt/model_oprhlh/epoch_8 |
2023-12-15 17:18:19 INFO     	 * rank: 58 | metric: 0.02 | model: small_finetuned_ckpt/model_nrudfu/epoch_8 |
2023-12-15 17:18:19 INFO     	 * rank: 59 | metric: 0.02 | model: small_finetuned_ckpt/model_woixzh/epoch_12 |
2023-12-15 17:18:19 INFO     	 * rank: 60 | metric: 0.02 | model: small_finetuned_ckpt/model_oprhlh/epoch_7 |
2023-12-15 17:18:19 INFO     	 * rank: 61 | metric: 0.02 | model: small_finetuned_ckpt/model_nrudfu/epoch_7 |
2023-12-15 17:18:19 INFO     	 * rank: 62 | metric: 0.02 | model: small_finetuned_ckpt/model_woixzh/epoch_6 |
2023-12-15 17:18:19 INFO     	 * rank: 63 | metric: 0.02 | model: small_finetuned_ckpt/model_woixzh/epoch_15 |
2023-12-15 17:18:19 INFO     	 * rank: 64 | metric: 0.02 | model: small_finetuned_ckpt/model_woixzh/epoch_14 |
2023-12-15 17:18:19 INFO     	 * rank: 65 | metric: 0.02 | model: small_finetuned_ckpt/model_woixzh/epoch_7 |
2023-12-15 17:18:19 INFO     	 * rank: 66 | metric: 0.019 | model: small_finetuned_ckpt/model_woixzh/epoch_8 |
2023-12-15 17:18:19 INFO     	 * rank: 67 | metric: 0.019 | model: small_finetuned_ckpt/model_woixzh/epoch_9 |
2023-12-15 17:18:19 INFO     	 * rank: 68 | metric: 0.019 | model: small_finetuned_ckpt/model_woixzh/epoch_13 |
2023-12-15 17:18:19 INFO     	 * rank: 69 | metric: 0.019 | model: small_finetuned_ckpt/model_woixzh/epoch_10 |
2023-12-15 17:18:19 INFO     	 * rank: 70 | metric: 0.019 | model: small_finetuned_ckpt/model_oprhlh/epoch_13 |
2023-12-15 17:18:19 INFO     	 * rank: 71 | metric: 0.019 | model: small_finetuned_ckpt/model_nxaqhy/epoch_15 |
2023-12-15 17:18:19 INFO     	 * rank: 72 | metric: 0.018 | model: small_finetuned_ckpt/model_oprhlh/epoch_14 |
2023-12-15 17:18:19 INFO     	 * rank: 73 | metric: 0.018 | model: small_finetuned_ckpt/model_woixzh/epoch_11 |
2023-12-15 17:18:19 INFO     	 * rank: 74 | metric: 0.018 | model: small_finetuned_ckpt/model_oprhlh/epoch_15 |
2023-12-15 17:18:19 INFO     creating small_finetuned_ckpt/best_model
2023-12-15 17:18:19 INFO     copying small_finetuned_ckpt/model_nxaqhy/epoch_1/config.json -> small_finetuned_ckpt/best_model
2023-12-15 17:18:19 INFO     copying small_finetuned_ckpt/model_nxaqhy/epoch_1/generation_config.json -> small_finetuned_ckpt/best_model
2023-12-15 17:18:19 INFO     copying small_finetuned_ckpt/model_nxaqhy/epoch_1/pytorch_model.bin -> small_finetuned_ckpt/best_model
2023-12-15 17:18:19 INFO     copying small_finetuned_ckpt/model_nxaqhy/epoch_1/tokenizer_config.json -> small_finetuned_ckpt/best_model
2023-12-15 17:18:19 INFO     copying small_finetuned_ckpt/model_nxaqhy/epoch_1/special_tokens_map.json -> small_finetuned_ckpt/best_model
2023-12-15 17:18:19 INFO     copying small_finetuned_ckpt/model_nxaqhy/epoch_1/added_tokens.json -> small_finetuned_ckpt/best_model
2023-12-15 17:18:19 INFO     copying small_finetuned_ckpt/model_nxaqhy/epoch_1/spiece.model -> small_finetuned_ckpt/best_model
2023-12-15 17:18:19 INFO     copying small_finetuned_ckpt/model_nxaqhy/epoch_1/tokenizer.json -> small_finetuned_ckpt/best_model
2023-12-15 17:18:19 INFO     copying small_finetuned_ckpt/model_nxaqhy/epoch_1/trainer_config.json -> small_finetuned_ckpt/best_model
2023-12-15 17:18:19 INFO     creating small_finetuned_ckpt/best_model/eval
2023-12-15 17:18:19 INFO     copying small_finetuned_ckpt/model_nxaqhy/epoch_1/eval/samples.test.hyp.paragraph.questions_answers.StellarMilk_newsqa.default.txt -> small_finetuned_ckpt/best_model/eval
2023-12-15 17:18:19 INFO     copying small_finetuned_ckpt/model_nxaqhy/epoch_1/eval/samples.validation.hyp.paragraph.questions_answers.StellarMilk_newsqa.default.txt -> small_finetuned_ckpt/best_model/eval
2023-12-15 17:18:19 INFO     copying small_finetuned_ckpt/model_nxaqhy/epoch_1/eval/metric.first.answer.paragraph.questions_answers.StellarMilk_newsqa.default.json -> small_finetuned_ckpt/best_model/eval
