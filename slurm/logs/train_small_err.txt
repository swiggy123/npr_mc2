2023-12-14 22:53:58 INFO     INITIALIZE GRID SEARCHER: 12 configs to try
2023-12-14 22:53:58 INFO     ## 1st RUN: Configuration 0/12 ##
2023-12-14 22:53:58 INFO     initialize model trainer
2023-12-14 22:53:58 INFO     initialize checkpoint at small_trained_ckpt/model_tgwwes
2023-12-14 22:53:58 INFO     hyperparameters
2023-12-14 22:53:58 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-14 22:53:58 INFO     	 * dataset_name: default
2023-12-14 22:53:58 INFO     	 * input_types: ['paragraph']
2023-12-14 22:53:58 INFO     	 * output_types: ['questions_answers']
2023-12-14 22:53:58 INFO     	 * prefix_types: ['qag']
2023-12-14 22:53:58 INFO     	 * model: t5-small
2023-12-14 22:53:58 INFO     	 * max_length: 512
2023-12-14 22:53:58 INFO     	 * max_length_output: 512
2023-12-14 22:53:58 INFO     	 * epoch: 15
2023-12-14 22:53:58 INFO     	 * batch: 2
2023-12-14 22:53:58 INFO     	 * lr: 0.0001
2023-12-14 22:53:58 INFO     	 * fp16: False
2023-12-14 22:53:58 INFO     	 * random_seed: 1
2023-12-14 22:53:58 INFO     	 * gradient_accumulation_steps: 4
2023-12-14 22:53:58 INFO     	 * label_smoothing: 0.15
2023-12-14 22:53:58 INFO     initialize checkpoint with t5-small
2023-12-14 22:54:10 INFO     use spaCy answer extraction model: positionrank
2023-12-14 22:54:12 INFO     Model `t5-small`
2023-12-14 22:54:12 INFO     	 * Num of GPU in use: 1
2023-12-14 22:54:12 INFO     	 * Prefix: True
2023-12-14 22:54:12 INFO     	 * Language: en (ignore at the training phase)
2023-12-14 22:54:12 INFO     dataset preprocessing
/home2/g.torresgamez/.local/lib/python3.10/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=False' instead.
  warnings.warn(
2023-12-14 22:54:14 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-14 22:54:15 INFO     start model training
2023-12-14 22:54:31 INFO     	 * (global step 50: loss: 0.8301887363195419, lr: 0.0001
2023-12-14 22:54:46 INFO     	 * (global step 100: loss: 0.7169673144817352, lr: 0.0001
2023-12-14 22:55:01 INFO     	 * (global step 150: loss: 0.754870593547821, lr: 0.0001
2023-12-14 22:55:16 INFO     	 * (global step 200: loss: 0.5686408802866936, lr: 0.0001
2023-12-14 22:55:31 INFO     	 * (global step 250: loss: 0.7836224287748337, lr: 0.0001
2023-12-14 22:55:47 INFO     	 * (global step 300: loss: 0.48932547867298126, lr: 0.0001
2023-12-14 22:55:50 INFO     [epoch 0/15] average loss: 0.888, lr: 0.0001
2023-12-14 22:55:50 INFO     saving model related files
2023-12-14 22:55:50 INFO     saving model
2023-12-14 22:55:51 INFO     saving tokenizer
2023-12-14 22:55:51 INFO     saving optimizer
2023-12-14 22:55:52 INFO     remove old optimizer files
2023-12-14 22:56:04 INFO     	 * (global step 350: loss: 0.41097644716501236, lr: 0.0001
2023-12-14 22:56:20 INFO     	 * (global step 400: loss: 0.41591834276914597, lr: 0.0001
2023-12-14 22:56:35 INFO     	 * (global step 450: loss: 0.3268912397325039, lr: 0.0001
2023-12-14 22:56:51 INFO     	 * (global step 500: loss: 0.4206017516553402, lr: 0.0001
2023-12-14 22:57:06 INFO     	 * (global step 550: loss: 0.4322343021631241, lr: 0.0001
2023-12-14 22:57:22 INFO     	 * (global step 600: loss: 0.5029958486557007, lr: 0.0001
2023-12-14 22:57:28 INFO     [epoch 1/15] average loss: 0.49, lr: 0.0001
2023-12-14 22:57:28 INFO     saving model related files
2023-12-14 22:57:28 INFO     saving model
2023-12-14 22:57:29 INFO     saving tokenizer
2023-12-14 22:57:29 INFO     saving optimizer
2023-12-14 22:57:30 INFO     remove old optimizer files
2023-12-14 22:57:39 INFO     	 * (global step 650: loss: 0.6376570761203766, lr: 0.0001
2023-12-14 22:57:55 INFO     	 * (global step 700: loss: 0.41285253316164017, lr: 0.0001
2023-12-14 22:58:11 INFO     	 * (global step 750: loss: 0.5026776865124702, lr: 0.0001
2023-12-14 22:58:26 INFO     	 * (global step 800: loss: 0.5838315710425377, lr: 0.0001
2023-12-14 22:58:42 INFO     	 * (global step 850: loss: 0.3259560167789459, lr: 0.0001
2023-12-14 22:58:57 INFO     	 * (global step 900: loss: 0.35467756167054176, lr: 0.0001
2023-12-14 22:59:07 INFO     [epoch 2/15] average loss: 0.449, lr: 0.0001
2023-12-14 22:59:07 INFO     saving model related files
2023-12-14 22:59:07 INFO     saving model
2023-12-14 22:59:08 INFO     saving tokenizer
2023-12-14 22:59:08 INFO     saving optimizer
2023-12-14 22:59:09 INFO     remove old optimizer files
2023-12-14 22:59:15 INFO     	 * (global step 950: loss: 0.600516065955162, lr: 0.0001
2023-12-14 22:59:30 INFO     	 * (global step 1000: loss: 0.4783662334084511, lr: 0.0001
2023-12-14 22:59:46 INFO     	 * (global step 1050: loss: 0.4302617572247982, lr: 0.0001
2023-12-14 23:00:02 INFO     	 * (global step 1100: loss: 0.33881692588329315, lr: 0.0001
2023-12-14 23:00:17 INFO     	 * (global step 1150: loss: 0.4904008209705353, lr: 0.0001
2023-12-14 23:00:33 INFO     	 * (global step 1200: loss: 0.3657698854804039, lr: 0.0001
2023-12-14 23:00:45 INFO     [epoch 3/15] average loss: 0.426, lr: 0.0001
2023-12-14 23:00:45 INFO     saving model related files
2023-12-14 23:00:45 INFO     saving model
2023-12-14 23:00:46 INFO     saving tokenizer
2023-12-14 23:00:46 INFO     saving optimizer
2023-12-14 23:00:47 INFO     remove old optimizer files
2023-12-14 23:00:50 INFO     	 * (global step 1250: loss: 0.3917756527662277, lr: 0.0001
2023-12-14 23:01:06 INFO     	 * (global step 1300: loss: 0.6085117608308792, lr: 0.0001
2023-12-14 23:01:21 INFO     	 * (global step 1350: loss: 0.33919576555490494, lr: 0.0001
2023-12-14 23:01:37 INFO     	 * (global step 1400: loss: 0.32195230200886726, lr: 0.0001
2023-12-14 23:01:52 INFO     	 * (global step 1450: loss: 0.409886859357357, lr: 0.0001
2023-12-14 23:02:08 INFO     	 * (global step 1500: loss: 0.33531492203474045, lr: 0.0001
2023-12-14 23:02:24 INFO     	 * (global step 1550: loss: 0.5768217891454697, lr: 0.0001
2023-12-14 23:02:24 INFO     [epoch 4/15] average loss: 0.409, lr: 0.0001
2023-12-14 23:02:24 INFO     saving model related files
2023-12-14 23:02:24 INFO     saving model
2023-12-14 23:02:24 INFO     saving tokenizer
2023-12-14 23:02:25 INFO     saving optimizer
2023-12-14 23:02:26 INFO     remove old optimizer files
2023-12-14 23:02:41 INFO     	 * (global step 1600: loss: 0.4600207358598709, lr: 0.0001
2023-12-14 23:02:57 INFO     	 * (global step 1650: loss: 0.37354225665330887, lr: 0.0001
2023-12-14 23:03:12 INFO     	 * (global step 1700: loss: 0.36619823426008224, lr: 0.0001
2023-12-14 23:03:28 INFO     	 * (global step 1750: loss: 0.29561852663755417, lr: 0.0001
2023-12-14 23:03:44 INFO     	 * (global step 1800: loss: 0.34286150708794594, lr: 0.0001
2023-12-14 23:03:59 INFO     	 * (global step 1850: loss: 0.40799203515052795, lr: 0.0001
2023-12-14 23:04:03 INFO     [epoch 5/15] average loss: 0.397, lr: 0.0001
2023-12-14 23:04:03 INFO     saving model related files
2023-12-14 23:04:03 INFO     saving model
2023-12-14 23:04:03 INFO     saving tokenizer
2023-12-14 23:04:03 INFO     saving optimizer
2023-12-14 23:04:04 INFO     remove old optimizer files
2023-12-14 23:04:17 INFO     	 * (global step 1900: loss: 0.3286478519439697, lr: 0.0001
2023-12-14 23:04:32 INFO     	 * (global step 1950: loss: 0.2935537099838257, lr: 0.0001
2023-12-14 23:04:48 INFO     	 * (global step 2000: loss: 0.4384095147252083, lr: 0.0001
2023-12-14 23:05:03 INFO     	 * (global step 2050: loss: 0.40853704512119293, lr: 0.0001
2023-12-14 23:05:19 INFO     	 * (global step 2100: loss: 0.3256891295313835, lr: 0.0001
2023-12-14 23:05:35 INFO     	 * (global step 2150: loss: 0.4260861799120903, lr: 0.0001
2023-12-14 23:05:41 INFO     [epoch 6/15] average loss: 0.386, lr: 0.0001
2023-12-14 23:05:41 INFO     saving model related files
2023-12-14 23:05:41 INFO     saving model
2023-12-14 23:05:42 INFO     saving tokenizer
2023-12-14 23:05:42 INFO     saving optimizer
2023-12-14 23:05:43 INFO     remove old optimizer files
2023-12-14 23:05:52 INFO     	 * (global step 2200: loss: 0.4510564059019089, lr: 0.0001
2023-12-14 23:06:08 INFO     	 * (global step 2250: loss: 0.34029605239629745, lr: 0.0001
2023-12-14 23:06:23 INFO     	 * (global step 2300: loss: 0.30229074507951736, lr: 0.0001
2023-12-14 23:06:39 INFO     	 * (global step 2350: loss: 0.41004180908203125, lr: 0.0001
2023-12-14 23:06:55 INFO     	 * (global step 2400: loss: 0.27265677228569984, lr: 0.0001
2023-12-14 23:07:10 INFO     	 * (global step 2450: loss: 0.42053282260894775, lr: 0.0001
2023-12-14 23:07:20 INFO     [epoch 7/15] average loss: 0.376, lr: 0.0001
2023-12-14 23:07:20 INFO     saving model related files
2023-12-14 23:07:20 INFO     saving model
2023-12-14 23:07:20 INFO     saving tokenizer
2023-12-14 23:07:21 INFO     saving optimizer
2023-12-14 23:07:22 INFO     remove old optimizer files
2023-12-14 23:07:28 INFO     	 * (global step 2500: loss: 0.3623160906136036, lr: 0.0001
2023-12-14 23:07:43 INFO     	 * (global step 2550: loss: 0.41658713668584824, lr: 0.0001
2023-12-14 23:07:59 INFO     	 * (global step 2600: loss: 0.3976410999894142, lr: 0.0001
2023-12-14 23:08:15 INFO     	 * (global step 2650: loss: 0.42481620982289314, lr: 0.0001
2023-12-14 23:08:30 INFO     	 * (global step 2700: loss: 0.3209293335676193, lr: 0.0001
2023-12-14 23:08:46 INFO     	 * (global step 2750: loss: 0.31760457158088684, lr: 0.0001
2023-12-14 23:08:58 INFO     [epoch 8/15] average loss: 0.367, lr: 0.0001
2023-12-14 23:08:58 INFO     saving model related files
2023-12-14 23:08:58 INFO     saving model
2023-12-14 23:08:59 INFO     saving tokenizer
2023-12-14 23:08:59 INFO     saving optimizer
2023-12-14 23:09:00 INFO     remove old optimizer files
2023-12-14 23:09:03 INFO     	 * (global step 2800: loss: 0.4149187058210373, lr: 0.0001
2023-12-14 23:09:19 INFO     	 * (global step 2850: loss: 0.4894717261195183, lr: 0.0001
2023-12-14 23:09:35 INFO     	 * (global step 2900: loss: 0.37047696858644485, lr: 0.0001
2023-12-14 23:09:50 INFO     	 * (global step 2950: loss: 0.3764413148164749, lr: 0.0001
2023-12-14 23:10:06 INFO     	 * (global step 3000: loss: 0.30926984921097755, lr: 0.0001
2023-12-14 23:10:22 INFO     	 * (global step 3050: loss: 0.3094777874648571, lr: 0.0001
2023-12-14 23:10:37 INFO     	 * (global step 3100: loss: 0.41946450620889664, lr: 0.0001
2023-12-14 23:10:37 INFO     [epoch 9/15] average loss: 0.359, lr: 0.0001
2023-12-14 23:10:37 INFO     saving model related files
2023-12-14 23:10:37 INFO     saving model
2023-12-14 23:10:38 INFO     saving tokenizer
2023-12-14 23:10:38 INFO     saving optimizer
2023-12-14 23:10:39 INFO     remove old optimizer files
2023-12-14 23:10:39 INFO     complete training: model ckpt was saved at small_trained_ckpt/model_tgwwes
2023-12-14 23:10:39 INFO     ## 1st RUN: Configuration 1/12 ##
2023-12-14 23:10:39 INFO     initialize model trainer
2023-12-14 23:10:39 INFO     initialize checkpoint at small_trained_ckpt/model_eszyci
2023-12-14 23:10:39 INFO     hyperparameters
2023-12-14 23:10:39 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-14 23:10:39 INFO     	 * dataset_name: default
2023-12-14 23:10:39 INFO     	 * input_types: ['paragraph']
2023-12-14 23:10:39 INFO     	 * output_types: ['questions_answers']
2023-12-14 23:10:39 INFO     	 * prefix_types: ['qag']
2023-12-14 23:10:39 INFO     	 * model: t5-small
2023-12-14 23:10:39 INFO     	 * max_length: 512
2023-12-14 23:10:39 INFO     	 * max_length_output: 512
2023-12-14 23:10:39 INFO     	 * epoch: 15
2023-12-14 23:10:39 INFO     	 * batch: 2
2023-12-14 23:10:39 INFO     	 * lr: 0.0001
2023-12-14 23:10:39 INFO     	 * fp16: False
2023-12-14 23:10:39 INFO     	 * random_seed: 1
2023-12-14 23:10:39 INFO     	 * gradient_accumulation_steps: 2
2023-12-14 23:10:39 INFO     	 * label_smoothing: 0.15
2023-12-14 23:10:39 INFO     initialize checkpoint with t5-small
2023-12-14 23:10:41 INFO     use spaCy answer extraction model: positionrank
2023-12-14 23:10:41 INFO     Model `t5-small`
2023-12-14 23:10:41 INFO     	 * Num of GPU in use: 1
2023-12-14 23:10:41 INFO     	 * Prefix: True
2023-12-14 23:10:41 INFO     	 * Language: en (ignore at the training phase)
2023-12-14 23:10:41 INFO     dataset preprocessing
2023-12-14 23:10:43 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-14 23:10:43 INFO     start model training
2023-12-14 23:10:51 INFO     	 * (global step 50: loss: 0.8982174396514893, lr: 0.0001
2023-12-14 23:10:59 INFO     	 * (global step 100: loss: 0.6682791113853455, lr: 0.0001
2023-12-14 23:11:07 INFO     	 * (global step 150: loss: 0.685766726732254, lr: 0.0001
2023-12-14 23:11:15 INFO     	 * (global step 200: loss: 0.5956261456012726, lr: 0.0001
2023-12-14 23:11:23 INFO     	 * (global step 250: loss: 0.6653767824172974, lr: 0.0001
2023-12-14 23:11:31 INFO     	 * (global step 300: loss: 0.4822228103876114, lr: 0.0001
2023-12-14 23:11:40 INFO     	 * (global step 350: loss: 0.5992270708084106, lr: 0.0001
2023-12-14 23:11:48 INFO     	 * (global step 400: loss: 0.44903992116451263, lr: 0.0001
2023-12-14 23:11:56 INFO     	 * (global step 450: loss: 0.5184253305196762, lr: 0.0001
2023-12-14 23:12:04 INFO     	 * (global step 500: loss: 0.8781238496303558, lr: 0.0001
2023-12-14 23:12:12 INFO     	 * (global step 550: loss: 0.4699361175298691, lr: 0.0001
2023-12-14 23:12:20 INFO     	 * (global step 600: loss: 0.4917828142642975, lr: 0.0001
2023-12-14 23:12:23 INFO     [epoch 0/15] average loss: 0.728, lr: 0.0001
2023-12-14 23:12:23 INFO     saving model related files
2023-12-14 23:12:23 INFO     saving model
2023-12-14 23:12:24 INFO     saving tokenizer
2023-12-14 23:12:24 INFO     saving optimizer
2023-12-14 23:12:25 INFO     remove old optimizer files
2023-12-14 23:12:30 INFO     	 * (global step 650: loss: 0.5468820333480835, lr: 0.0001
2023-12-14 23:12:38 INFO     	 * (global step 700: loss: 0.40766745805740356, lr: 0.0001
2023-12-14 23:12:46 INFO     	 * (global step 750: loss: 0.5653561502695084, lr: 0.0001
2023-12-14 23:12:54 INFO     	 * (global step 800: loss: 0.3793777823448181, lr: 0.0001
2023-12-14 23:13:02 INFO     	 * (global step 850: loss: 0.2898809388279915, lr: 0.0001
2023-12-14 23:13:10 INFO     	 * (global step 900: loss: 0.29631708562374115, lr: 0.0001
2023-12-14 23:13:18 INFO     	 * (global step 950: loss: 0.5014292299747467, lr: 0.0001
2023-12-14 23:13:26 INFO     	 * (global step 1000: loss: 0.32680927962064743, lr: 0.0001
2023-12-14 23:13:34 INFO     	 * (global step 1050: loss: 0.47039060294628143, lr: 0.0001
2023-12-14 23:13:43 INFO     	 * (global step 1100: loss: 0.3707634508609772, lr: 0.0001
2023-12-14 23:13:51 INFO     	 * (global step 1150: loss: 0.6593768745660782, lr: 0.0001
2023-12-14 23:13:59 INFO     	 * (global step 1200: loss: 0.5510396957397461, lr: 0.0001
2023-12-14 23:14:06 INFO     [epoch 1/15] average loss: 0.456, lr: 0.0001
2023-12-14 23:14:06 INFO     saving model related files
2023-12-14 23:14:06 INFO     saving model
2023-12-14 23:14:06 INFO     saving tokenizer
2023-12-14 23:14:06 INFO     saving optimizer
2023-12-14 23:14:07 INFO     remove old optimizer files
2023-12-14 23:14:09 INFO     	 * (global step 1250: loss: 0.5287472456693649, lr: 0.0001
2023-12-14 23:14:17 INFO     	 * (global step 1300: loss: 0.4360654130578041, lr: 0.0001
2023-12-14 23:14:25 INFO     	 * (global step 1350: loss: 0.4412996768951416, lr: 0.0001
2023-12-14 23:14:33 INFO     	 * (global step 1400: loss: 0.46109822392463684, lr: 0.0001
2023-12-14 23:14:41 INFO     	 * (global step 1450: loss: 0.5368344634771347, lr: 0.0001
2023-12-14 23:14:49 INFO     	 * (global step 1500: loss: 0.2925279438495636, lr: 0.0001
2023-12-14 23:14:57 INFO     	 * (global step 1550: loss: 0.46427619457244873, lr: 0.0001
2023-12-14 23:15:05 INFO     	 * (global step 1600: loss: 0.33603864908218384, lr: 0.0001
2023-12-14 23:15:13 INFO     	 * (global step 1650: loss: 0.32431529462337494, lr: 0.0001
2023-12-14 23:15:21 INFO     	 * (global step 1700: loss: 0.3729827255010605, lr: 0.0001
2023-12-14 23:15:30 INFO     	 * (global step 1750: loss: 0.33688244223594666, lr: 0.0001
2023-12-14 23:15:38 INFO     	 * (global step 1800: loss: 0.31650038808584213, lr: 0.0001
2023-12-14 23:15:46 INFO     	 * (global step 1850: loss: 0.42309218645095825, lr: 0.0001
2023-12-14 23:15:48 INFO     [epoch 2/15] average loss: 0.421, lr: 0.0001
2023-12-14 23:15:48 INFO     saving model related files
2023-12-14 23:15:48 INFO     saving model
2023-12-14 23:15:48 INFO     saving tokenizer
2023-12-14 23:15:49 INFO     saving optimizer
2023-12-14 23:15:50 INFO     remove old optimizer files
2023-12-14 23:15:56 INFO     	 * (global step 1900: loss: 0.5373504310846329, lr: 0.0001
2023-12-14 23:16:04 INFO     	 * (global step 1950: loss: 0.39386802911758423, lr: 0.0001
2023-12-14 23:16:12 INFO     	 * (global step 2000: loss: 0.3155963197350502, lr: 0.0001
2023-12-14 23:16:20 INFO     	 * (global step 2050: loss: 0.4468197673559189, lr: 0.0001
2023-12-14 23:16:28 INFO     	 * (global step 2100: loss: 0.5401387810707092, lr: 0.0001
2023-12-14 23:16:36 INFO     	 * (global step 2150: loss: 0.3647161275148392, lr: 0.0001
2023-12-14 23:16:44 INFO     	 * (global step 2200: loss: 0.31330618262290955, lr: 0.0001
2023-12-14 23:16:52 INFO     	 * (global step 2250: loss: 0.5429584681987762, lr: 0.0001
2023-12-14 23:17:00 INFO     	 * (global step 2300: loss: 0.434004470705986, lr: 0.0001
2023-12-14 23:17:09 INFO     	 * (global step 2350: loss: 0.36966682970523834, lr: 0.0001
2023-12-14 23:17:17 INFO     	 * (global step 2400: loss: 0.36053283512592316, lr: 0.0001
2023-12-14 23:17:25 INFO     	 * (global step 2450: loss: 0.30703455209732056, lr: 0.0001
2023-12-14 23:17:30 INFO     [epoch 3/15] average loss: 0.4, lr: 0.0001
2023-12-14 23:17:30 INFO     saving model related files
2023-12-14 23:17:30 INFO     saving model
2023-12-14 23:17:31 INFO     saving tokenizer
2023-12-14 23:17:31 INFO     saving optimizer
2023-12-14 23:17:32 INFO     remove old optimizer files
2023-12-14 23:17:35 INFO     	 * (global step 2500: loss: 0.46268264949321747, lr: 0.0001
2023-12-14 23:17:43 INFO     	 * (global step 2550: loss: 0.34826162457466125, lr: 0.0001
2023-12-14 23:17:51 INFO     	 * (global step 2600: loss: 0.36919426918029785, lr: 0.0001
2023-12-14 23:17:59 INFO     	 * (global step 2650: loss: 0.38315220177173615, lr: 0.0001
2023-12-14 23:18:07 INFO     	 * (global step 2700: loss: 0.29465050995349884, lr: 0.0001
2023-12-14 23:18:15 INFO     	 * (global step 2750: loss: 0.3262619078159332, lr: 0.0001
2023-12-14 23:18:23 INFO     	 * (global step 2800: loss: 0.47160473465919495, lr: 0.0001
2023-12-14 23:18:31 INFO     	 * (global step 2850: loss: 0.4251381456851959, lr: 0.0001
2023-12-14 23:18:39 INFO     	 * (global step 2900: loss: 0.38767197728157043, lr: 0.0001
2023-12-14 23:18:47 INFO     	 * (global step 2950: loss: 0.3712562620639801, lr: 0.0001
2023-12-14 23:18:55 INFO     	 * (global step 3000: loss: 0.5169580429792404, lr: 0.0001
2023-12-14 23:19:03 INFO     	 * (global step 3050: loss: 0.37388473749160767, lr: 0.0001
2023-12-14 23:19:11 INFO     	 * (global step 3100: loss: 0.43606650829315186, lr: 0.0001
2023-12-14 23:19:12 INFO     [epoch 4/15] average loss: 0.383, lr: 0.0001
2023-12-14 23:19:12 INFO     saving model related files
2023-12-14 23:19:12 INFO     saving model
2023-12-14 23:19:14 INFO     saving tokenizer
2023-12-14 23:19:14 INFO     saving optimizer
2023-12-14 23:19:16 INFO     remove old optimizer files
2023-12-14 23:19:23 INFO     	 * (global step 3150: loss: 0.3672143667936325, lr: 0.0001
2023-12-14 23:19:31 INFO     	 * (global step 3200: loss: 0.39250944554805756, lr: 0.0001
2023-12-14 23:19:39 INFO     	 * (global step 3250: loss: 0.35659150779247284, lr: 0.0001
2023-12-14 23:19:47 INFO     	 * (global step 3300: loss: 0.26210229098796844, lr: 0.0001
2023-12-14 23:19:55 INFO     	 * (global step 3350: loss: 0.4682949185371399, lr: 0.0001
2023-12-14 23:20:03 INFO     	 * (global step 3400: loss: 0.47681182622909546, lr: 0.0001
2023-12-14 23:20:11 INFO     	 * (global step 3450: loss: 0.27992992103099823, lr: 0.0001
2023-12-14 23:20:20 INFO     	 * (global step 3500: loss: 0.19722742587327957, lr: 0.0001
2023-12-14 23:20:28 INFO     	 * (global step 3550: loss: 0.44961315393447876, lr: 0.0001
2023-12-14 23:20:36 INFO     	 * (global step 3600: loss: 0.43169209361076355, lr: 0.0001
2023-12-14 23:20:44 INFO     	 * (global step 3650: loss: 0.4016174077987671, lr: 0.0001
2023-12-14 23:20:52 INFO     	 * (global step 3700: loss: 0.33356112241744995, lr: 0.0001
2023-12-14 23:20:56 INFO     [epoch 5/15] average loss: 0.37, lr: 0.0001
2023-12-14 23:20:56 INFO     saving model related files
2023-12-14 23:20:56 INFO     saving model
2023-12-14 23:20:57 INFO     saving tokenizer
2023-12-14 23:20:57 INFO     saving optimizer
2023-12-14 23:20:58 INFO     remove old optimizer files
2023-12-14 23:21:02 INFO     	 * (global step 3750: loss: 0.15937866643071175, lr: 0.0001
2023-12-14 23:21:10 INFO     	 * (global step 3800: loss: 0.36719483882188797, lr: 0.0001
2023-12-14 23:21:18 INFO     	 * (global step 3850: loss: 0.23278433829545975, lr: 0.0001
2023-12-14 23:21:26 INFO     	 * (global step 3900: loss: 0.31997914612293243, lr: 0.0001
2023-12-14 23:21:34 INFO     	 * (global step 3950: loss: 0.3062027394771576, lr: 0.0001
2023-12-14 23:21:42 INFO     	 * (global step 4000: loss: 0.3932238668203354, lr: 0.0001
2023-12-14 23:21:50 INFO     	 * (global step 4050: loss: 0.4048128128051758, lr: 0.0001
2023-12-14 23:21:58 INFO     	 * (global step 4100: loss: 0.3991820365190506, lr: 0.0001
2023-12-14 23:22:06 INFO     	 * (global step 4150: loss: 0.362856462597847, lr: 0.0001
2023-12-14 23:22:15 INFO     	 * (global step 4200: loss: 0.3154722973704338, lr: 0.0001
2023-12-14 23:22:23 INFO     	 * (global step 4250: loss: 0.24049700051546097, lr: 0.0001
2023-12-14 23:22:31 INFO     	 * (global step 4300: loss: 0.47682246565818787, lr: 0.0001
2023-12-14 23:22:38 INFO     [epoch 6/15] average loss: 0.358, lr: 0.0001
2023-12-14 23:22:38 INFO     saving model related files
2023-12-14 23:22:38 INFO     saving model
2023-12-14 23:22:39 INFO     saving tokenizer
2023-12-14 23:22:39 INFO     saving optimizer
2023-12-14 23:22:40 INFO     remove old optimizer files
2023-12-14 23:22:41 INFO     	 * (global step 4350: loss: 0.30101555585861206, lr: 0.0001
2023-12-14 23:22:49 INFO     	 * (global step 4400: loss: 0.2810138389468193, lr: 0.0001
2023-12-14 23:22:57 INFO     	 * (global step 4450: loss: 0.2651691287755966, lr: 0.0001
2023-12-14 23:23:05 INFO     	 * (global step 4500: loss: 0.32520030438899994, lr: 0.0001
2023-12-14 23:23:13 INFO     	 * (global step 4550: loss: 0.2487577274441719, lr: 0.0001
2023-12-14 23:23:21 INFO     	 * (global step 4600: loss: 0.34427888691425323, lr: 0.0001
2023-12-14 23:23:29 INFO     	 * (global step 4650: loss: 0.5155353844165802, lr: 0.0001
2023-12-14 23:23:37 INFO     	 * (global step 4700: loss: 0.4154438376426697, lr: 0.0001
2023-12-14 23:23:45 INFO     	 * (global step 4750: loss: 0.35613540560007095, lr: 0.0001
2023-12-14 23:23:53 INFO     	 * (global step 4800: loss: 0.35098113119602203, lr: 0.0001
2023-12-14 23:24:01 INFO     	 * (global step 4850: loss: 0.2745898962020874, lr: 0.0001
2023-12-14 23:24:09 INFO     	 * (global step 4900: loss: 0.2916553467512131, lr: 0.0001
2023-12-14 23:24:17 INFO     	 * (global step 4950: loss: 0.23023412376642227, lr: 0.0001
2023-12-14 23:24:20 INFO     [epoch 7/15] average loss: 0.346, lr: 0.0001
2023-12-14 23:24:20 INFO     saving model related files
2023-12-14 23:24:20 INFO     saving model
2023-12-14 23:24:21 INFO     saving tokenizer
2023-12-14 23:24:21 INFO     saving optimizer
2023-12-14 23:24:22 INFO     remove old optimizer files
2023-12-14 23:24:27 INFO     	 * (global step 5000: loss: 0.3400801196694374, lr: 0.0001
2023-12-14 23:24:35 INFO     	 * (global step 5050: loss: 0.3002518564462662, lr: 0.0001
2023-12-14 23:24:43 INFO     	 * (global step 5100: loss: 0.2925857752561569, lr: 0.0001
2023-12-14 23:24:51 INFO     	 * (global step 5150: loss: 0.20214036107063293, lr: 0.0001
2023-12-14 23:25:00 INFO     	 * (global step 5200: loss: 0.19399606436491013, lr: 0.0001
2023-12-14 23:25:08 INFO     	 * (global step 5250: loss: 0.6247548907995224, lr: 0.0001
2023-12-14 23:25:16 INFO     	 * (global step 5300: loss: 0.2283231019973755, lr: 0.0001
2023-12-14 23:25:24 INFO     	 * (global step 5350: loss: 0.34015341103076935, lr: 0.0001
2023-12-14 23:25:32 INFO     	 * (global step 5400: loss: 0.29983165860176086, lr: 0.0001
2023-12-14 23:25:40 INFO     	 * (global step 5450: loss: 0.2791627198457718, lr: 0.0001
2023-12-14 23:25:48 INFO     	 * (global step 5500: loss: 0.3410445749759674, lr: 0.0001
2023-12-14 23:25:56 INFO     	 * (global step 5550: loss: 0.29849180579185486, lr: 0.0001
2023-12-14 23:26:03 INFO     [epoch 8/15] average loss: 0.336, lr: 0.0001
2023-12-14 23:26:03 INFO     saving model related files
2023-12-14 23:26:03 INFO     saving model
2023-12-14 23:26:03 INFO     saving tokenizer
2023-12-14 23:26:03 INFO     saving optimizer
2023-12-14 23:26:04 INFO     remove old optimizer files
2023-12-14 23:26:06 INFO     	 * (global step 5600: loss: 0.44728951156139374, lr: 0.0001
2023-12-14 23:26:14 INFO     	 * (global step 5650: loss: 0.4226989597082138, lr: 0.0001
2023-12-14 23:26:22 INFO     	 * (global step 5700: loss: 0.24337346106767654, lr: 0.0001
2023-12-14 23:26:31 INFO     	 * (global step 5750: loss: 0.28741392493247986, lr: 0.0001
2023-12-14 23:26:39 INFO     	 * (global step 5800: loss: 0.2832120209932327, lr: 0.0001
2023-12-14 23:26:47 INFO     	 * (global step 5850: loss: 0.40632109344005585, lr: 0.0001
2023-12-14 23:26:55 INFO     	 * (global step 5900: loss: 0.26082710921764374, lr: 0.0001
2023-12-14 23:27:03 INFO     	 * (global step 5950: loss: 0.30924393981695175, lr: 0.0001
2023-12-14 23:27:11 INFO     	 * (global step 6000: loss: 0.34497590363025665, lr: 0.0001
2023-12-14 23:27:19 INFO     	 * (global step 6050: loss: 0.37024693191051483, lr: 0.0001
2023-12-14 23:27:27 INFO     	 * (global step 6100: loss: 0.26730091869831085, lr: 0.0001
2023-12-14 23:27:35 INFO     	 * (global step 6150: loss: 0.3500070571899414, lr: 0.0001
2023-12-14 23:27:43 INFO     	 * (global step 6200: loss: 0.4178972989320755, lr: 0.0001
2023-12-14 23:27:45 INFO     [epoch 9/15] average loss: 0.327, lr: 0.0001
2023-12-14 23:27:45 INFO     saving model related files
2023-12-14 23:27:45 INFO     saving model
2023-12-14 23:27:46 INFO     saving tokenizer
2023-12-14 23:27:46 INFO     saving optimizer
2023-12-14 23:27:47 INFO     remove old optimizer files
2023-12-14 23:27:47 INFO     complete training: model ckpt was saved at small_trained_ckpt/model_eszyci
2023-12-14 23:27:47 INFO     ## 1st RUN: Configuration 2/12 ##
2023-12-14 23:27:47 INFO     initialize model trainer
2023-12-14 23:27:47 INFO     initialize checkpoint at small_trained_ckpt/model_dpyopu
2023-12-14 23:27:47 INFO     hyperparameters
2023-12-14 23:27:47 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-14 23:27:47 INFO     	 * dataset_name: default
2023-12-14 23:27:47 INFO     	 * input_types: ['paragraph']
2023-12-14 23:27:47 INFO     	 * output_types: ['questions_answers']
2023-12-14 23:27:47 INFO     	 * prefix_types: ['qag']
2023-12-14 23:27:47 INFO     	 * model: t5-small
2023-12-14 23:27:47 INFO     	 * max_length: 512
2023-12-14 23:27:47 INFO     	 * max_length_output: 512
2023-12-14 23:27:47 INFO     	 * epoch: 15
2023-12-14 23:27:47 INFO     	 * batch: 2
2023-12-14 23:27:47 INFO     	 * lr: 0.0001
2023-12-14 23:27:47 INFO     	 * fp16: False
2023-12-14 23:27:47 INFO     	 * random_seed: 1
2023-12-14 23:27:47 INFO     	 * gradient_accumulation_steps: 4
2023-12-14 23:27:47 INFO     	 * label_smoothing: 0.0
2023-12-14 23:27:47 INFO     initialize checkpoint with t5-small
2023-12-14 23:27:49 INFO     use spaCy answer extraction model: positionrank
2023-12-14 23:27:49 INFO     Model `t5-small`
2023-12-14 23:27:49 INFO     	 * Num of GPU in use: 1
2023-12-14 23:27:49 INFO     	 * Prefix: True
2023-12-14 23:27:49 INFO     	 * Language: en (ignore at the training phase)
2023-12-14 23:27:49 INFO     dataset preprocessing
2023-12-14 23:27:50 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-14 23:27:51 INFO     start model training
2023-12-14 23:28:06 INFO     	 * (global step 50: loss: 0.8301887363195419, lr: 0.0001
2023-12-14 23:28:22 INFO     	 * (global step 100: loss: 0.7169673144817352, lr: 0.0001
2023-12-14 23:28:38 INFO     	 * (global step 150: loss: 0.754870593547821, lr: 0.0001
2023-12-14 23:28:53 INFO     	 * (global step 200: loss: 0.5686408802866936, lr: 0.0001
2023-12-14 23:29:09 INFO     	 * (global step 250: loss: 0.7836224287748337, lr: 0.0001
2023-12-14 23:29:24 INFO     	 * (global step 300: loss: 0.48932547867298126, lr: 0.0001
2023-12-14 23:29:28 INFO     [epoch 0/15] average loss: 0.888, lr: 0.0001
2023-12-14 23:29:28 INFO     saving model related files
2023-12-14 23:29:28 INFO     saving model
2023-12-14 23:29:28 INFO     saving tokenizer
2023-12-14 23:29:28 INFO     saving optimizer
2023-12-14 23:29:29 INFO     remove old optimizer files
2023-12-14 23:29:42 INFO     	 * (global step 350: loss: 0.41097644716501236, lr: 0.0001
2023-12-14 23:29:57 INFO     	 * (global step 400: loss: 0.41591834276914597, lr: 0.0001
2023-12-14 23:30:13 INFO     	 * (global step 450: loss: 0.3268912397325039, lr: 0.0001
2023-12-14 23:30:29 INFO     	 * (global step 500: loss: 0.4206017516553402, lr: 0.0001
2023-12-14 23:30:44 INFO     	 * (global step 550: loss: 0.4322343021631241, lr: 0.0001
2023-12-14 23:31:00 INFO     	 * (global step 600: loss: 0.5029958486557007, lr: 0.0001
2023-12-14 23:31:06 INFO     [epoch 1/15] average loss: 0.49, lr: 0.0001
2023-12-14 23:31:06 INFO     saving model related files
2023-12-14 23:31:06 INFO     saving model
2023-12-14 23:31:07 INFO     saving tokenizer
2023-12-14 23:31:07 INFO     saving optimizer
2023-12-14 23:31:08 INFO     remove old optimizer files
2023-12-14 23:31:17 INFO     	 * (global step 650: loss: 0.6376570761203766, lr: 0.0001
2023-12-14 23:31:33 INFO     	 * (global step 700: loss: 0.41285253316164017, lr: 0.0001
2023-12-14 23:31:49 INFO     	 * (global step 750: loss: 0.5026776865124702, lr: 0.0001
2023-12-14 23:32:04 INFO     	 * (global step 800: loss: 0.5838315710425377, lr: 0.0001
2023-12-14 23:32:20 INFO     	 * (global step 850: loss: 0.3259560167789459, lr: 0.0001
2023-12-14 23:32:36 INFO     	 * (global step 900: loss: 0.35467756167054176, lr: 0.0001
2023-12-14 23:32:45 INFO     [epoch 2/15] average loss: 0.449, lr: 0.0001
2023-12-14 23:32:45 INFO     saving model related files
2023-12-14 23:32:45 INFO     saving model
2023-12-14 23:32:46 INFO     saving tokenizer
2023-12-14 23:32:46 INFO     saving optimizer
2023-12-14 23:32:47 INFO     remove old optimizer files
2023-12-14 23:32:53 INFO     	 * (global step 950: loss: 0.600516065955162, lr: 0.0001
2023-12-14 23:33:09 INFO     	 * (global step 1000: loss: 0.4783662334084511, lr: 0.0001
2023-12-14 23:33:25 INFO     	 * (global step 1050: loss: 0.4302617572247982, lr: 0.0001
2023-12-14 23:33:40 INFO     	 * (global step 1100: loss: 0.33881692588329315, lr: 0.0001
2023-12-14 23:33:56 INFO     	 * (global step 1150: loss: 0.4904008209705353, lr: 0.0001
2023-12-14 23:34:11 INFO     	 * (global step 1200: loss: 0.3657698854804039, lr: 0.0001
2023-12-14 23:34:24 INFO     [epoch 3/15] average loss: 0.426, lr: 0.0001
2023-12-14 23:34:24 INFO     saving model related files
2023-12-14 23:34:24 INFO     saving model
2023-12-14 23:34:25 INFO     saving tokenizer
2023-12-14 23:34:25 INFO     saving optimizer
2023-12-14 23:34:26 INFO     remove old optimizer files
2023-12-14 23:34:29 INFO     	 * (global step 1250: loss: 0.3917756527662277, lr: 0.0001
2023-12-14 23:34:45 INFO     	 * (global step 1300: loss: 0.6085117608308792, lr: 0.0001
2023-12-14 23:35:00 INFO     	 * (global step 1350: loss: 0.33919576555490494, lr: 0.0001
2023-12-14 23:35:16 INFO     	 * (global step 1400: loss: 0.32195230200886726, lr: 0.0001
2023-12-14 23:35:32 INFO     	 * (global step 1450: loss: 0.409886859357357, lr: 0.0001
2023-12-14 23:35:47 INFO     	 * (global step 1500: loss: 0.33531492203474045, lr: 0.0001
2023-12-14 23:36:03 INFO     	 * (global step 1550: loss: 0.5768217891454697, lr: 0.0001
2023-12-14 23:36:03 INFO     [epoch 4/15] average loss: 0.409, lr: 0.0001
2023-12-14 23:36:03 INFO     saving model related files
2023-12-14 23:36:03 INFO     saving model
2023-12-14 23:36:04 INFO     saving tokenizer
2023-12-14 23:36:04 INFO     saving optimizer
2023-12-14 23:36:05 INFO     remove old optimizer files
2023-12-14 23:36:20 INFO     	 * (global step 1600: loss: 0.4600207358598709, lr: 0.0001
2023-12-14 23:36:36 INFO     	 * (global step 1650: loss: 0.37354225665330887, lr: 0.0001
2023-12-14 23:36:52 INFO     	 * (global step 1700: loss: 0.36619823426008224, lr: 0.0001
2023-12-14 23:37:07 INFO     	 * (global step 1750: loss: 0.29561852663755417, lr: 0.0001
2023-12-14 23:37:23 INFO     	 * (global step 1800: loss: 0.34286150708794594, lr: 0.0001
2023-12-14 23:37:39 INFO     	 * (global step 1850: loss: 0.40799203515052795, lr: 0.0001
2023-12-14 23:37:42 INFO     [epoch 5/15] average loss: 0.397, lr: 0.0001
2023-12-14 23:37:42 INFO     saving model related files
2023-12-14 23:37:42 INFO     saving model
2023-12-14 23:37:42 INFO     saving tokenizer
2023-12-14 23:37:43 INFO     saving optimizer
2023-12-14 23:37:44 INFO     remove old optimizer files
2023-12-14 23:37:56 INFO     	 * (global step 1900: loss: 0.3286478519439697, lr: 0.0001
2023-12-14 23:38:12 INFO     	 * (global step 1950: loss: 0.2935537099838257, lr: 0.0001
2023-12-14 23:38:28 INFO     	 * (global step 2000: loss: 0.4384095147252083, lr: 0.0001
2023-12-14 23:38:43 INFO     	 * (global step 2050: loss: 0.40853704512119293, lr: 0.0001
2023-12-14 23:38:59 INFO     	 * (global step 2100: loss: 0.3256891295313835, lr: 0.0001
2023-12-14 23:39:15 INFO     	 * (global step 2150: loss: 0.4260861799120903, lr: 0.0001
2023-12-14 23:39:21 INFO     [epoch 6/15] average loss: 0.386, lr: 0.0001
2023-12-14 23:39:21 INFO     saving model related files
2023-12-14 23:39:21 INFO     saving model
2023-12-14 23:39:21 INFO     saving tokenizer
2023-12-14 23:39:22 INFO     saving optimizer
2023-12-14 23:39:23 INFO     remove old optimizer files
2023-12-14 23:39:32 INFO     	 * (global step 2200: loss: 0.4510564059019089, lr: 0.0001
2023-12-14 23:39:48 INFO     	 * (global step 2250: loss: 0.34029605239629745, lr: 0.0001
2023-12-14 23:40:03 INFO     	 * (global step 2300: loss: 0.30229074507951736, lr: 0.0001
2023-12-14 23:40:19 INFO     	 * (global step 2350: loss: 0.41004180908203125, lr: 0.0001
2023-12-14 23:40:34 INFO     	 * (global step 2400: loss: 0.27265677228569984, lr: 0.0001
2023-12-14 23:40:50 INFO     	 * (global step 2450: loss: 0.42053282260894775, lr: 0.0001
2023-12-14 23:40:59 INFO     [epoch 7/15] average loss: 0.376, lr: 0.0001
2023-12-14 23:40:59 INFO     saving model related files
2023-12-14 23:40:59 INFO     saving model
2023-12-14 23:41:00 INFO     saving tokenizer
2023-12-14 23:41:00 INFO     saving optimizer
2023-12-14 23:41:01 INFO     remove old optimizer files
2023-12-14 23:41:07 INFO     	 * (global step 2500: loss: 0.3623160906136036, lr: 0.0001
2023-12-14 23:41:23 INFO     	 * (global step 2550: loss: 0.41658713668584824, lr: 0.0001
2023-12-14 23:41:39 INFO     	 * (global step 2600: loss: 0.3976410999894142, lr: 0.0001
2023-12-14 23:41:54 INFO     	 * (global step 2650: loss: 0.42481620982289314, lr: 0.0001
2023-12-14 23:42:10 INFO     	 * (global step 2700: loss: 0.3209293335676193, lr: 0.0001
2023-12-14 23:42:25 INFO     	 * (global step 2750: loss: 0.31760457158088684, lr: 0.0001
2023-12-14 23:42:38 INFO     [epoch 8/15] average loss: 0.367, lr: 0.0001
2023-12-14 23:42:38 INFO     saving model related files
2023-12-14 23:42:38 INFO     saving model
2023-12-14 23:42:39 INFO     saving tokenizer
2023-12-14 23:42:39 INFO     saving optimizer
2023-12-14 23:42:40 INFO     remove old optimizer files
2023-12-14 23:42:43 INFO     	 * (global step 2800: loss: 0.4149187058210373, lr: 0.0001
2023-12-14 23:42:59 INFO     	 * (global step 2850: loss: 0.4894717261195183, lr: 0.0001
2023-12-14 23:43:14 INFO     	 * (global step 2900: loss: 0.37047696858644485, lr: 0.0001
2023-12-14 23:43:30 INFO     	 * (global step 2950: loss: 0.3764413148164749, lr: 0.0001
2023-12-14 23:43:45 INFO     	 * (global step 3000: loss: 0.30926984921097755, lr: 0.0001
2023-12-14 23:44:01 INFO     	 * (global step 3050: loss: 0.3094777874648571, lr: 0.0001
2023-12-14 23:44:17 INFO     	 * (global step 3100: loss: 0.41946450620889664, lr: 0.0001
2023-12-14 23:44:17 INFO     [epoch 9/15] average loss: 0.359, lr: 0.0001
2023-12-14 23:44:17 INFO     saving model related files
2023-12-14 23:44:17 INFO     saving model
2023-12-14 23:44:17 INFO     saving tokenizer
2023-12-14 23:44:18 INFO     saving optimizer
2023-12-14 23:44:19 INFO     remove old optimizer files
2023-12-14 23:44:19 INFO     complete training: model ckpt was saved at small_trained_ckpt/model_dpyopu
2023-12-14 23:44:19 INFO     ## 1st RUN: Configuration 3/12 ##
2023-12-14 23:44:19 INFO     initialize model trainer
2023-12-14 23:44:19 INFO     initialize checkpoint at small_trained_ckpt/model_mzgdpa
2023-12-14 23:44:19 INFO     hyperparameters
2023-12-14 23:44:19 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-14 23:44:19 INFO     	 * dataset_name: default
2023-12-14 23:44:19 INFO     	 * input_types: ['paragraph']
2023-12-14 23:44:19 INFO     	 * output_types: ['questions_answers']
2023-12-14 23:44:19 INFO     	 * prefix_types: ['qag']
2023-12-14 23:44:19 INFO     	 * model: t5-small
2023-12-14 23:44:19 INFO     	 * max_length: 512
2023-12-14 23:44:19 INFO     	 * max_length_output: 512
2023-12-14 23:44:19 INFO     	 * epoch: 15
2023-12-14 23:44:19 INFO     	 * batch: 2
2023-12-14 23:44:19 INFO     	 * lr: 0.0001
2023-12-14 23:44:19 INFO     	 * fp16: False
2023-12-14 23:44:19 INFO     	 * random_seed: 1
2023-12-14 23:44:19 INFO     	 * gradient_accumulation_steps: 2
2023-12-14 23:44:19 INFO     	 * label_smoothing: 0.0
2023-12-14 23:44:19 INFO     initialize checkpoint with t5-small
2023-12-14 23:44:20 INFO     use spaCy answer extraction model: positionrank
2023-12-14 23:44:21 INFO     Model `t5-small`
2023-12-14 23:44:21 INFO     	 * Num of GPU in use: 1
2023-12-14 23:44:21 INFO     	 * Prefix: True
2023-12-14 23:44:21 INFO     	 * Language: en (ignore at the training phase)
2023-12-14 23:44:21 INFO     dataset preprocessing
2023-12-14 23:44:22 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-14 23:44:22 INFO     start model training
2023-12-14 23:44:30 INFO     	 * (global step 50: loss: 0.8982174396514893, lr: 0.0001
2023-12-14 23:44:38 INFO     	 * (global step 100: loss: 0.6682791113853455, lr: 0.0001
2023-12-14 23:44:46 INFO     	 * (global step 150: loss: 0.685766726732254, lr: 0.0001
2023-12-14 23:44:54 INFO     	 * (global step 200: loss: 0.5956261456012726, lr: 0.0001
2023-12-14 23:45:02 INFO     	 * (global step 250: loss: 0.6653767824172974, lr: 0.0001
2023-12-14 23:45:10 INFO     	 * (global step 300: loss: 0.4822228103876114, lr: 0.0001
2023-12-14 23:45:18 INFO     	 * (global step 350: loss: 0.5992270708084106, lr: 0.0001
2023-12-14 23:45:26 INFO     	 * (global step 400: loss: 0.44903992116451263, lr: 0.0001
2023-12-14 23:45:35 INFO     	 * (global step 450: loss: 0.5184253305196762, lr: 0.0001
2023-12-14 23:45:43 INFO     	 * (global step 500: loss: 0.8781238496303558, lr: 0.0001
2023-12-14 23:45:51 INFO     	 * (global step 550: loss: 0.4699361175298691, lr: 0.0001
2023-12-14 23:45:59 INFO     	 * (global step 600: loss: 0.4917828142642975, lr: 0.0001
2023-12-14 23:46:02 INFO     [epoch 0/15] average loss: 0.728, lr: 0.0001
2023-12-14 23:46:02 INFO     saving model related files
2023-12-14 23:46:02 INFO     saving model
2023-12-14 23:46:03 INFO     saving tokenizer
2023-12-14 23:46:03 INFO     saving optimizer
2023-12-14 23:46:04 INFO     remove old optimizer files
2023-12-14 23:46:09 INFO     	 * (global step 650: loss: 0.5468820333480835, lr: 0.0001
2023-12-14 23:46:17 INFO     	 * (global step 700: loss: 0.40766745805740356, lr: 0.0001
2023-12-14 23:46:25 INFO     	 * (global step 750: loss: 0.5653561502695084, lr: 0.0001
2023-12-14 23:46:33 INFO     	 * (global step 800: loss: 0.3793777823448181, lr: 0.0001
2023-12-14 23:46:41 INFO     	 * (global step 850: loss: 0.2898809388279915, lr: 0.0001
2023-12-14 23:46:49 INFO     	 * (global step 900: loss: 0.29631708562374115, lr: 0.0001
2023-12-14 23:46:57 INFO     	 * (global step 950: loss: 0.5014292299747467, lr: 0.0001
2023-12-14 23:47:05 INFO     	 * (global step 1000: loss: 0.32680927962064743, lr: 0.0001
2023-12-14 23:47:13 INFO     	 * (global step 1050: loss: 0.47039060294628143, lr: 0.0001
2023-12-14 23:47:21 INFO     	 * (global step 1100: loss: 0.3707634508609772, lr: 0.0001
2023-12-14 23:47:29 INFO     	 * (global step 1150: loss: 0.6593768745660782, lr: 0.0001
2023-12-14 23:47:38 INFO     	 * (global step 1200: loss: 0.5510396957397461, lr: 0.0001
2023-12-14 23:47:44 INFO     [epoch 1/15] average loss: 0.456, lr: 0.0001
2023-12-14 23:47:44 INFO     saving model related files
2023-12-14 23:47:44 INFO     saving model
2023-12-14 23:47:45 INFO     saving tokenizer
2023-12-14 23:47:45 INFO     saving optimizer
2023-12-14 23:47:46 INFO     remove old optimizer files
2023-12-14 23:47:48 INFO     	 * (global step 1250: loss: 0.5287472456693649, lr: 0.0001
2023-12-14 23:47:56 INFO     	 * (global step 1300: loss: 0.4360654130578041, lr: 0.0001
2023-12-14 23:48:04 INFO     	 * (global step 1350: loss: 0.4412996768951416, lr: 0.0001
2023-12-14 23:48:12 INFO     	 * (global step 1400: loss: 0.46109822392463684, lr: 0.0001
2023-12-14 23:48:20 INFO     	 * (global step 1450: loss: 0.5368344634771347, lr: 0.0001
2023-12-14 23:48:28 INFO     	 * (global step 1500: loss: 0.2925279438495636, lr: 0.0001
2023-12-14 23:48:36 INFO     	 * (global step 1550: loss: 0.46427619457244873, lr: 0.0001
2023-12-14 23:48:44 INFO     	 * (global step 1600: loss: 0.33603864908218384, lr: 0.0001
2023-12-14 23:48:52 INFO     	 * (global step 1650: loss: 0.32431529462337494, lr: 0.0001
2023-12-14 23:49:00 INFO     	 * (global step 1700: loss: 0.3729827255010605, lr: 0.0001
2023-12-14 23:49:08 INFO     	 * (global step 1750: loss: 0.33688244223594666, lr: 0.0001
2023-12-14 23:49:16 INFO     	 * (global step 1800: loss: 0.31650038808584213, lr: 0.0001
2023-12-14 23:49:24 INFO     	 * (global step 1850: loss: 0.42309218645095825, lr: 0.0001
2023-12-14 23:49:27 INFO     [epoch 2/15] average loss: 0.421, lr: 0.0001
2023-12-14 23:49:27 INFO     saving model related files
2023-12-14 23:49:27 INFO     saving model
2023-12-14 23:49:27 INFO     saving tokenizer
2023-12-14 23:49:27 INFO     saving optimizer
2023-12-14 23:49:28 INFO     remove old optimizer files
2023-12-14 23:49:34 INFO     	 * (global step 1900: loss: 0.5373504310846329, lr: 0.0001
2023-12-14 23:49:43 INFO     	 * (global step 1950: loss: 0.39386802911758423, lr: 0.0001
2023-12-14 23:49:51 INFO     	 * (global step 2000: loss: 0.3155963197350502, lr: 0.0001
2023-12-14 23:49:59 INFO     	 * (global step 2050: loss: 0.4468197673559189, lr: 0.0001
2023-12-14 23:50:07 INFO     	 * (global step 2100: loss: 0.5401387810707092, lr: 0.0001
2023-12-14 23:50:15 INFO     	 * (global step 2150: loss: 0.3647161275148392, lr: 0.0001
2023-12-14 23:50:23 INFO     	 * (global step 2200: loss: 0.31330618262290955, lr: 0.0001
2023-12-14 23:50:31 INFO     	 * (global step 2250: loss: 0.5429584681987762, lr: 0.0001
2023-12-14 23:50:39 INFO     	 * (global step 2300: loss: 0.434004470705986, lr: 0.0001
2023-12-14 23:50:47 INFO     	 * (global step 2350: loss: 0.36966682970523834, lr: 0.0001
2023-12-14 23:50:55 INFO     	 * (global step 2400: loss: 0.36053283512592316, lr: 0.0001
2023-12-14 23:51:03 INFO     	 * (global step 2450: loss: 0.30703455209732056, lr: 0.0001
2023-12-14 23:51:09 INFO     [epoch 3/15] average loss: 0.4, lr: 0.0001
2023-12-14 23:51:09 INFO     saving model related files
2023-12-14 23:51:09 INFO     saving model
2023-12-14 23:51:10 INFO     saving tokenizer
2023-12-14 23:51:10 INFO     saving optimizer
2023-12-14 23:51:11 INFO     remove old optimizer files
2023-12-14 23:51:14 INFO     	 * (global step 2500: loss: 0.46268264949321747, lr: 0.0001
2023-12-14 23:51:22 INFO     	 * (global step 2550: loss: 0.34826162457466125, lr: 0.0001
2023-12-14 23:51:30 INFO     	 * (global step 2600: loss: 0.36919426918029785, lr: 0.0001
2023-12-14 23:51:38 INFO     	 * (global step 2650: loss: 0.38315220177173615, lr: 0.0001
2023-12-14 23:51:46 INFO     	 * (global step 2700: loss: 0.29465050995349884, lr: 0.0001
2023-12-14 23:51:54 INFO     	 * (global step 2750: loss: 0.3262619078159332, lr: 0.0001
2023-12-14 23:52:02 INFO     	 * (global step 2800: loss: 0.47160473465919495, lr: 0.0001
2023-12-14 23:52:10 INFO     	 * (global step 2850: loss: 0.4251381456851959, lr: 0.0001
2023-12-14 23:52:18 INFO     	 * (global step 2900: loss: 0.38767197728157043, lr: 0.0001
2023-12-14 23:52:26 INFO     	 * (global step 2950: loss: 0.3712562620639801, lr: 0.0001
2023-12-14 23:52:35 INFO     	 * (global step 3000: loss: 0.5169580429792404, lr: 0.0001
2023-12-14 23:52:43 INFO     	 * (global step 3050: loss: 0.37388473749160767, lr: 0.0001
2023-12-14 23:52:51 INFO     	 * (global step 3100: loss: 0.43606650829315186, lr: 0.0001
2023-12-14 23:52:52 INFO     [epoch 4/15] average loss: 0.383, lr: 0.0001
2023-12-14 23:52:52 INFO     saving model related files
2023-12-14 23:52:52 INFO     saving model
2023-12-14 23:52:52 INFO     saving tokenizer
2023-12-14 23:52:52 INFO     saving optimizer
2023-12-14 23:52:53 INFO     remove old optimizer files
2023-12-14 23:53:01 INFO     	 * (global step 3150: loss: 0.3672143667936325, lr: 0.0001
2023-12-14 23:53:09 INFO     	 * (global step 3200: loss: 0.39250944554805756, lr: 0.0001
2023-12-14 23:53:17 INFO     	 * (global step 3250: loss: 0.35659150779247284, lr: 0.0001
2023-12-14 23:53:25 INFO     	 * (global step 3300: loss: 0.26210229098796844, lr: 0.0001
2023-12-14 23:53:33 INFO     	 * (global step 3350: loss: 0.4682949185371399, lr: 0.0001
2023-12-14 23:53:41 INFO     	 * (global step 3400: loss: 0.47681182622909546, lr: 0.0001
2023-12-14 23:53:49 INFO     	 * (global step 3450: loss: 0.27992992103099823, lr: 0.0001
2023-12-14 23:53:57 INFO     	 * (global step 3500: loss: 0.19722742587327957, lr: 0.0001
2023-12-14 23:54:05 INFO     	 * (global step 3550: loss: 0.44961315393447876, lr: 0.0001
2023-12-14 23:54:13 INFO     	 * (global step 3600: loss: 0.43169209361076355, lr: 0.0001
2023-12-14 23:54:22 INFO     	 * (global step 3650: loss: 0.4016174077987671, lr: 0.0001
2023-12-14 23:54:30 INFO     	 * (global step 3700: loss: 0.33356112241744995, lr: 0.0001
2023-12-14 23:54:34 INFO     [epoch 5/15] average loss: 0.37, lr: 0.0001
2023-12-14 23:54:34 INFO     saving model related files
2023-12-14 23:54:34 INFO     saving model
2023-12-14 23:54:34 INFO     saving tokenizer
2023-12-14 23:54:34 INFO     saving optimizer
2023-12-14 23:54:35 INFO     remove old optimizer files
2023-12-14 23:54:39 INFO     	 * (global step 3750: loss: 0.15937866643071175, lr: 0.0001
2023-12-14 23:54:47 INFO     	 * (global step 3800: loss: 0.36719483882188797, lr: 0.0001
2023-12-14 23:54:56 INFO     	 * (global step 3850: loss: 0.23278433829545975, lr: 0.0001
2023-12-14 23:55:04 INFO     	 * (global step 3900: loss: 0.31997914612293243, lr: 0.0001
2023-12-14 23:55:12 INFO     	 * (global step 3950: loss: 0.3062027394771576, lr: 0.0001
2023-12-14 23:55:20 INFO     	 * (global step 4000: loss: 0.3932238668203354, lr: 0.0001
2023-12-14 23:55:28 INFO     	 * (global step 4050: loss: 0.4048128128051758, lr: 0.0001
2023-12-14 23:55:36 INFO     	 * (global step 4100: loss: 0.3991820365190506, lr: 0.0001
2023-12-14 23:55:44 INFO     	 * (global step 4150: loss: 0.362856462597847, lr: 0.0001
2023-12-14 23:55:52 INFO     	 * (global step 4200: loss: 0.3154722973704338, lr: 0.0001
2023-12-14 23:56:00 INFO     	 * (global step 4250: loss: 0.24049700051546097, lr: 0.0001
2023-12-14 23:56:08 INFO     	 * (global step 4300: loss: 0.47682246565818787, lr: 0.0001
2023-12-14 23:56:16 INFO     [epoch 6/15] average loss: 0.358, lr: 0.0001
2023-12-14 23:56:16 INFO     saving model related files
2023-12-14 23:56:16 INFO     saving model
2023-12-14 23:56:17 INFO     saving tokenizer
2023-12-14 23:56:17 INFO     saving optimizer
2023-12-14 23:56:18 INFO     remove old optimizer files
2023-12-14 23:56:18 INFO     	 * (global step 4350: loss: 0.30101555585861206, lr: 0.0001
2023-12-14 23:56:27 INFO     	 * (global step 4400: loss: 0.2810138389468193, lr: 0.0001
2023-12-14 23:56:35 INFO     	 * (global step 4450: loss: 0.2651691287755966, lr: 0.0001
2023-12-14 23:56:43 INFO     	 * (global step 4500: loss: 0.32520030438899994, lr: 0.0001
2023-12-14 23:56:51 INFO     	 * (global step 4550: loss: 0.2487577274441719, lr: 0.0001
2023-12-14 23:56:59 INFO     	 * (global step 4600: loss: 0.34427888691425323, lr: 0.0001
2023-12-14 23:57:07 INFO     	 * (global step 4650: loss: 0.5155353844165802, lr: 0.0001
2023-12-14 23:57:15 INFO     	 * (global step 4700: loss: 0.4154438376426697, lr: 0.0001
2023-12-14 23:57:23 INFO     	 * (global step 4750: loss: 0.35613540560007095, lr: 0.0001
2023-12-14 23:57:31 INFO     	 * (global step 4800: loss: 0.35098113119602203, lr: 0.0001
2023-12-14 23:57:39 INFO     	 * (global step 4850: loss: 0.2745898962020874, lr: 0.0001
2023-12-14 23:57:47 INFO     	 * (global step 4900: loss: 0.2916553467512131, lr: 0.0001
2023-12-14 23:57:56 INFO     	 * (global step 4950: loss: 0.23023412376642227, lr: 0.0001
2023-12-14 23:57:59 INFO     [epoch 7/15] average loss: 0.346, lr: 0.0001
2023-12-14 23:57:59 INFO     saving model related files
2023-12-14 23:57:59 INFO     saving model
2023-12-14 23:57:59 INFO     saving tokenizer
2023-12-14 23:57:59 INFO     saving optimizer
2023-12-14 23:58:01 INFO     remove old optimizer files
2023-12-14 23:58:06 INFO     	 * (global step 5000: loss: 0.3400801196694374, lr: 0.0001
2023-12-14 23:58:14 INFO     	 * (global step 5050: loss: 0.3002518564462662, lr: 0.0001
2023-12-14 23:58:22 INFO     	 * (global step 5100: loss: 0.2925857752561569, lr: 0.0001
2023-12-14 23:58:30 INFO     	 * (global step 5150: loss: 0.20214036107063293, lr: 0.0001
2023-12-14 23:58:38 INFO     	 * (global step 5200: loss: 0.19399606436491013, lr: 0.0001
2023-12-14 23:58:46 INFO     	 * (global step 5250: loss: 0.6247548907995224, lr: 0.0001
2023-12-14 23:58:54 INFO     	 * (global step 5300: loss: 0.2283231019973755, lr: 0.0001
2023-12-14 23:59:02 INFO     	 * (global step 5350: loss: 0.34015341103076935, lr: 0.0001
2023-12-14 23:59:11 INFO     	 * (global step 5400: loss: 0.29983165860176086, lr: 0.0001
2023-12-14 23:59:19 INFO     	 * (global step 5450: loss: 0.2791627198457718, lr: 0.0001
2023-12-14 23:59:27 INFO     	 * (global step 5500: loss: 0.3410445749759674, lr: 0.0001
2023-12-14 23:59:35 INFO     	 * (global step 5550: loss: 0.29849180579185486, lr: 0.0001
2023-12-14 23:59:41 INFO     [epoch 8/15] average loss: 0.336, lr: 0.0001
2023-12-14 23:59:41 INFO     saving model related files
2023-12-14 23:59:41 INFO     saving model
2023-12-14 23:59:42 INFO     saving tokenizer
2023-12-14 23:59:42 INFO     saving optimizer
2023-12-14 23:59:43 INFO     remove old optimizer files
2023-12-14 23:59:45 INFO     	 * (global step 5600: loss: 0.44728951156139374, lr: 0.0001
2023-12-14 23:59:53 INFO     	 * (global step 5650: loss: 0.4226989597082138, lr: 0.0001
2023-12-15 00:00:01 INFO     	 * (global step 5700: loss: 0.24337346106767654, lr: 0.0001
2023-12-15 00:00:09 INFO     	 * (global step 5750: loss: 0.28741392493247986, lr: 0.0001
2023-12-15 00:00:17 INFO     	 * (global step 5800: loss: 0.2832120209932327, lr: 0.0001
2023-12-15 00:00:25 INFO     	 * (global step 5850: loss: 0.40632109344005585, lr: 0.0001
2023-12-15 00:00:33 INFO     	 * (global step 5900: loss: 0.26082710921764374, lr: 0.0001
2023-12-15 00:00:41 INFO     	 * (global step 5950: loss: 0.30924393981695175, lr: 0.0001
2023-12-15 00:00:50 INFO     	 * (global step 6000: loss: 0.34497590363025665, lr: 0.0001
2023-12-15 00:00:58 INFO     	 * (global step 6050: loss: 0.37024693191051483, lr: 0.0001
2023-12-15 00:01:06 INFO     	 * (global step 6100: loss: 0.26730091869831085, lr: 0.0001
2023-12-15 00:01:14 INFO     	 * (global step 6150: loss: 0.3500070571899414, lr: 0.0001
2023-12-15 00:01:22 INFO     	 * (global step 6200: loss: 0.4178972989320755, lr: 0.0001
2023-12-15 00:01:24 INFO     [epoch 9/15] average loss: 0.327, lr: 0.0001
2023-12-15 00:01:24 INFO     saving model related files
2023-12-15 00:01:24 INFO     saving model
2023-12-15 00:01:24 INFO     saving tokenizer
2023-12-15 00:01:24 INFO     saving optimizer
2023-12-15 00:01:25 INFO     remove old optimizer files
2023-12-15 00:01:25 INFO     complete training: model ckpt was saved at small_trained_ckpt/model_mzgdpa
2023-12-15 00:01:25 INFO     ## 1st RUN: Configuration 4/12 ##
2023-12-15 00:01:25 INFO     initialize model trainer
2023-12-15 00:01:25 INFO     initialize checkpoint at small_trained_ckpt/model_mntyya
2023-12-15 00:01:25 INFO     hyperparameters
2023-12-15 00:01:25 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 00:01:25 INFO     	 * dataset_name: default
2023-12-15 00:01:25 INFO     	 * input_types: ['paragraph']
2023-12-15 00:01:25 INFO     	 * output_types: ['questions_answers']
2023-12-15 00:01:25 INFO     	 * prefix_types: ['qag']
2023-12-15 00:01:25 INFO     	 * model: t5-small
2023-12-15 00:01:25 INFO     	 * max_length: 512
2023-12-15 00:01:25 INFO     	 * max_length_output: 512
2023-12-15 00:01:25 INFO     	 * epoch: 15
2023-12-15 00:01:25 INFO     	 * batch: 2
2023-12-15 00:01:25 INFO     	 * lr: 5e-05
2023-12-15 00:01:25 INFO     	 * fp16: False
2023-12-15 00:01:25 INFO     	 * random_seed: 1
2023-12-15 00:01:25 INFO     	 * gradient_accumulation_steps: 4
2023-12-15 00:01:25 INFO     	 * label_smoothing: 0.15
2023-12-15 00:01:25 INFO     initialize checkpoint with t5-small
2023-12-15 00:01:27 INFO     use spaCy answer extraction model: positionrank
2023-12-15 00:01:28 INFO     Model `t5-small`
2023-12-15 00:01:28 INFO     	 * Num of GPU in use: 1
2023-12-15 00:01:28 INFO     	 * Prefix: True
2023-12-15 00:01:28 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 00:01:28 INFO     dataset preprocessing
2023-12-15 00:01:29 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 00:01:30 INFO     start model training
2023-12-15 00:01:45 INFO     	 * (global step 50: loss: 1.3254896700382233, lr: 5e-05
2023-12-15 00:02:01 INFO     	 * (global step 100: loss: 0.8981009274721146, lr: 5e-05
2023-12-15 00:02:16 INFO     	 * (global step 150: loss: 0.9119456708431244, lr: 5e-05
2023-12-15 00:02:32 INFO     	 * (global step 200: loss: 0.6897153705358505, lr: 5e-05
2023-12-15 00:02:47 INFO     	 * (global step 250: loss: 0.9045827090740204, lr: 5e-05
2023-12-15 00:03:03 INFO     	 * (global step 300: loss: 0.5700984299182892, lr: 5e-05
2023-12-15 00:03:07 INFO     [epoch 0/15] average loss: 1.178, lr: 5e-05
2023-12-15 00:03:07 INFO     saving model related files
2023-12-15 00:03:07 INFO     saving model
2023-12-15 00:03:07 INFO     saving tokenizer
2023-12-15 00:03:07 INFO     saving optimizer
2023-12-15 00:03:08 INFO     remove old optimizer files
2023-12-15 00:03:21 INFO     	 * (global step 350: loss: 0.4940400943160057, lr: 5e-05
2023-12-15 00:03:36 INFO     	 * (global step 400: loss: 0.4914982169866562, lr: 5e-05
2023-12-15 00:03:52 INFO     	 * (global step 450: loss: 0.39232543855905533, lr: 5e-05
2023-12-15 00:04:08 INFO     	 * (global step 500: loss: 0.4878457337617874, lr: 5e-05
2023-12-15 00:04:23 INFO     	 * (global step 550: loss: 0.4989309161901474, lr: 5e-05
2023-12-15 00:04:39 INFO     	 * (global step 600: loss: 0.5362381041049957, lr: 5e-05
2023-12-15 00:04:45 INFO     [epoch 1/15] average loss: 0.566, lr: 5e-05
2023-12-15 00:04:45 INFO     saving model related files
2023-12-15 00:04:45 INFO     saving model
2023-12-15 00:04:46 INFO     saving tokenizer
2023-12-15 00:04:46 INFO     saving optimizer
2023-12-15 00:04:47 INFO     remove old optimizer files
2023-12-15 00:04:56 INFO     	 * (global step 650: loss: 0.7027509585022926, lr: 5e-05
2023-12-15 00:05:12 INFO     	 * (global step 700: loss: 0.45776501297950745, lr: 5e-05
2023-12-15 00:05:27 INFO     	 * (global step 750: loss: 0.5646698027849197, lr: 5e-05
2023-12-15 00:05:43 INFO     	 * (global step 800: loss: 0.6477614045143127, lr: 5e-05
2023-12-15 00:05:59 INFO     	 * (global step 850: loss: 0.3658929839730263, lr: 5e-05
2023-12-15 00:06:14 INFO     	 * (global step 900: loss: 0.3872283771634102, lr: 5e-05
2023-12-15 00:06:24 INFO     [epoch 2/15] average loss: 0.5, lr: 5e-05
2023-12-15 00:06:24 INFO     saving model related files
2023-12-15 00:06:24 INFO     saving model
2023-12-15 00:06:24 INFO     saving tokenizer
2023-12-15 00:06:25 INFO     saving optimizer
2023-12-15 00:06:26 INFO     remove old optimizer files
2023-12-15 00:06:32 INFO     	 * (global step 950: loss: 0.659692645072937, lr: 5e-05
2023-12-15 00:06:48 INFO     	 * (global step 1000: loss: 0.5231571793556213, lr: 5e-05
2023-12-15 00:07:03 INFO     	 * (global step 1050: loss: 0.4832238145172596, lr: 5e-05
2023-12-15 00:07:19 INFO     	 * (global step 1100: loss: 0.37351197749376297, lr: 5e-05
2023-12-15 00:07:34 INFO     	 * (global step 1150: loss: 0.5454632118344307, lr: 5e-05
2023-12-15 00:07:50 INFO     	 * (global step 1200: loss: 0.4034688323736191, lr: 5e-05
2023-12-15 00:08:03 INFO     [epoch 3/15] average loss: 0.472, lr: 5e-05
2023-12-15 00:08:03 INFO     saving model related files
2023-12-15 00:08:03 INFO     saving model
2023-12-15 00:08:03 INFO     saving tokenizer
2023-12-15 00:08:03 INFO     saving optimizer
2023-12-15 00:08:04 INFO     remove old optimizer files
2023-12-15 00:08:08 INFO     	 * (global step 1250: loss: 0.4297929033637047, lr: 5e-05
2023-12-15 00:08:23 INFO     	 * (global step 1300: loss: 0.6594108119606972, lr: 5e-05
2023-12-15 00:08:39 INFO     	 * (global step 1350: loss: 0.36978161334991455, lr: 5e-05
2023-12-15 00:08:55 INFO     	 * (global step 1400: loss: 0.352481909096241, lr: 5e-05
2023-12-15 00:09:10 INFO     	 * (global step 1450: loss: 0.46161432564258575, lr: 5e-05
2023-12-15 00:09:26 INFO     	 * (global step 1500: loss: 0.3683296889066696, lr: 5e-05
2023-12-15 00:09:42 INFO     	 * (global step 1550: loss: 0.6282598376274109, lr: 5e-05
2023-12-15 00:09:42 INFO     [epoch 4/15] average loss: 0.451, lr: 5e-05
2023-12-15 00:09:42 INFO     saving model related files
2023-12-15 00:09:42 INFO     saving model
2023-12-15 00:09:42 INFO     saving tokenizer
2023-12-15 00:09:42 INFO     saving optimizer
2023-12-15 00:09:43 INFO     remove old optimizer files
2023-12-15 00:09:59 INFO     	 * (global step 1600: loss: 0.5040901526808739, lr: 5e-05
2023-12-15 00:10:15 INFO     	 * (global step 1650: loss: 0.40810971707105637, lr: 5e-05
2023-12-15 00:10:31 INFO     	 * (global step 1700: loss: 0.40656714886426926, lr: 5e-05
2023-12-15 00:10:46 INFO     	 * (global step 1750: loss: 0.319843627512455, lr: 5e-05
2023-12-15 00:11:02 INFO     	 * (global step 1800: loss: 0.3748915418982506, lr: 5e-05
2023-12-15 00:11:17 INFO     	 * (global step 1850: loss: 0.44412147998809814, lr: 5e-05
2023-12-15 00:11:21 INFO     [epoch 5/15] average loss: 0.437, lr: 5e-05
2023-12-15 00:11:21 INFO     saving model related files
2023-12-15 00:11:21 INFO     saving model
2023-12-15 00:11:21 INFO     saving tokenizer
2023-12-15 00:11:21 INFO     saving optimizer
2023-12-15 00:11:22 INFO     remove old optimizer files
2023-12-15 00:11:35 INFO     	 * (global step 1900: loss: 0.3636530414223671, lr: 5e-05
2023-12-15 00:11:50 INFO     	 * (global step 1950: loss: 0.3232303373515606, lr: 5e-05
2023-12-15 00:12:06 INFO     	 * (global step 2000: loss: 0.48621904850006104, lr: 5e-05
2023-12-15 00:12:22 INFO     	 * (global step 2050: loss: 0.4487333744764328, lr: 5e-05
2023-12-15 00:12:37 INFO     	 * (global step 2100: loss: 0.36525413393974304, lr: 5e-05
2023-12-15 00:12:53 INFO     	 * (global step 2150: loss: 0.4642280861735344, lr: 5e-05
2023-12-15 00:12:59 INFO     [epoch 6/15] average loss: 0.426, lr: 5e-05
2023-12-15 00:12:59 INFO     saving model related files
2023-12-15 00:12:59 INFO     saving model
2023-12-15 00:13:00 INFO     saving tokenizer
2023-12-15 00:13:00 INFO     saving optimizer
2023-12-15 00:13:01 INFO     remove old optimizer files
2023-12-15 00:13:10 INFO     	 * (global step 2200: loss: 0.49551691114902496, lr: 5e-05
2023-12-15 00:13:26 INFO     	 * (global step 2250: loss: 0.3728572875261307, lr: 5e-05
2023-12-15 00:13:42 INFO     	 * (global step 2300: loss: 0.3395213782787323, lr: 5e-05
2023-12-15 00:13:57 INFO     	 * (global step 2350: loss: 0.45369119942188263, lr: 5e-05
2023-12-15 00:14:13 INFO     	 * (global step 2400: loss: 0.30297956615686417, lr: 5e-05
2023-12-15 00:14:29 INFO     	 * (global step 2450: loss: 0.45672521740198135, lr: 5e-05
2023-12-15 00:14:38 INFO     [epoch 7/15] average loss: 0.415, lr: 5e-05
2023-12-15 00:14:38 INFO     saving model related files
2023-12-15 00:14:38 INFO     saving model
2023-12-15 00:14:39 INFO     saving tokenizer
2023-12-15 00:14:39 INFO     saving optimizer
2023-12-15 00:14:40 INFO     remove old optimizer files
2023-12-15 00:14:46 INFO     	 * (global step 2500: loss: 0.4003717750310898, lr: 5e-05
2023-12-15 00:15:02 INFO     	 * (global step 2550: loss: 0.46041805297136307, lr: 5e-05
2023-12-15 00:15:18 INFO     	 * (global step 2600: loss: 0.4288046658039093, lr: 5e-05
2023-12-15 00:15:33 INFO     	 * (global step 2650: loss: 0.4630977138876915, lr: 5e-05
2023-12-15 00:15:49 INFO     	 * (global step 2700: loss: 0.35659343004226685, lr: 5e-05
2023-12-15 00:16:05 INFO     	 * (global step 2750: loss: 0.3513820096850395, lr: 5e-05
2023-12-15 00:16:17 INFO     [epoch 8/15] average loss: 0.407, lr: 5e-05
2023-12-15 00:16:17 INFO     saving model related files
2023-12-15 00:16:17 INFO     saving model
2023-12-15 00:16:18 INFO     saving tokenizer
2023-12-15 00:16:18 INFO     saving optimizer
2023-12-15 00:16:19 INFO     remove old optimizer files
2023-12-15 00:16:22 INFO     	 * (global step 2800: loss: 0.46546725183725357, lr: 5e-05
2023-12-15 00:16:38 INFO     	 * (global step 2850: loss: 0.5493580773472786, lr: 5e-05
2023-12-15 00:16:53 INFO     	 * (global step 2900: loss: 0.41611748933792114, lr: 5e-05
2023-12-15 00:17:09 INFO     	 * (global step 2950: loss: 0.4201995059847832, lr: 5e-05
2023-12-15 00:17:24 INFO     	 * (global step 3000: loss: 0.3498004972934723, lr: 5e-05
2023-12-15 00:17:40 INFO     	 * (global step 3050: loss: 0.3336162492632866, lr: 5e-05
2023-12-15 00:17:56 INFO     	 * (global step 3100: loss: 0.4677710309624672, lr: 5e-05
2023-12-15 00:17:56 INFO     [epoch 9/15] average loss: 0.399, lr: 5e-05
2023-12-15 00:17:56 INFO     saving model related files
2023-12-15 00:17:56 INFO     saving model
2023-12-15 00:17:56 INFO     saving tokenizer
2023-12-15 00:17:57 INFO     saving optimizer
2023-12-15 00:17:58 INFO     remove old optimizer files
2023-12-15 00:17:58 INFO     complete training: model ckpt was saved at small_trained_ckpt/model_mntyya
2023-12-15 00:17:58 INFO     ## 1st RUN: Configuration 5/12 ##
2023-12-15 00:17:58 INFO     initialize model trainer
2023-12-15 00:17:58 INFO     initialize checkpoint at small_trained_ckpt/model_woixzh
2023-12-15 00:17:58 INFO     hyperparameters
2023-12-15 00:17:58 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 00:17:58 INFO     	 * dataset_name: default
2023-12-15 00:17:58 INFO     	 * input_types: ['paragraph']
2023-12-15 00:17:58 INFO     	 * output_types: ['questions_answers']
2023-12-15 00:17:58 INFO     	 * prefix_types: ['qag']
2023-12-15 00:17:58 INFO     	 * model: t5-small
2023-12-15 00:17:58 INFO     	 * max_length: 512
2023-12-15 00:17:58 INFO     	 * max_length_output: 512
2023-12-15 00:17:58 INFO     	 * epoch: 15
2023-12-15 00:17:58 INFO     	 * batch: 2
2023-12-15 00:17:58 INFO     	 * lr: 5e-05
2023-12-15 00:17:58 INFO     	 * fp16: False
2023-12-15 00:17:58 INFO     	 * random_seed: 1
2023-12-15 00:17:58 INFO     	 * gradient_accumulation_steps: 2
2023-12-15 00:17:58 INFO     	 * label_smoothing: 0.15
2023-12-15 00:17:58 INFO     initialize checkpoint with t5-small
2023-12-15 00:17:59 INFO     use spaCy answer extraction model: positionrank
2023-12-15 00:17:59 INFO     Model `t5-small`
2023-12-15 00:17:59 INFO     	 * Num of GPU in use: 1
2023-12-15 00:17:59 INFO     	 * Prefix: True
2023-12-15 00:17:59 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 00:17:59 INFO     dataset preprocessing
2023-12-15 00:18:01 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 00:18:01 INFO     start model training
2023-12-15 00:18:09 INFO     	 * (global step 50: loss: 1.4092538952827454, lr: 5e-05
2023-12-15 00:18:17 INFO     	 * (global step 100: loss: 0.8925729095935822, lr: 5e-05
2023-12-15 00:18:25 INFO     	 * (global step 150: loss: 0.8196516335010529, lr: 5e-05
2023-12-15 00:18:33 INFO     	 * (global step 200: loss: 0.7100931107997894, lr: 5e-05
2023-12-15 00:18:41 INFO     	 * (global step 250: loss: 0.7927198708057404, lr: 5e-05
2023-12-15 00:18:49 INFO     	 * (global step 300: loss: 0.582080602645874, lr: 5e-05
2023-12-15 00:18:58 INFO     	 * (global step 350: loss: 0.6985580325126648, lr: 5e-05
2023-12-15 00:19:06 INFO     	 * (global step 400: loss: 0.5441485941410065, lr: 5e-05
2023-12-15 00:19:14 INFO     	 * (global step 450: loss: 0.6066481471061707, lr: 5e-05
2023-12-15 00:19:22 INFO     	 * (global step 500: loss: 0.9787241816520691, lr: 5e-05
2023-12-15 00:19:30 INFO     	 * (global step 550: loss: 0.5473348498344421, lr: 5e-05
2023-12-15 00:19:38 INFO     	 * (global step 600: loss: 0.5466890633106232, lr: 5e-05
2023-12-15 00:19:41 INFO     [epoch 0/15] average loss: 0.93, lr: 5e-05
2023-12-15 00:19:41 INFO     saving model related files
2023-12-15 00:19:41 INFO     saving model
2023-12-15 00:19:42 INFO     saving tokenizer
2023-12-15 00:19:42 INFO     saving optimizer
2023-12-15 00:19:43 INFO     remove old optimizer files
2023-12-15 00:19:48 INFO     	 * (global step 650: loss: 0.6004865169525146, lr: 5e-05
2023-12-15 00:19:56 INFO     	 * (global step 700: loss: 0.4539387822151184, lr: 5e-05
2023-12-15 00:20:04 INFO     	 * (global step 750: loss: 0.6251737326383591, lr: 5e-05
2023-12-15 00:20:12 INFO     	 * (global step 800: loss: 0.4313388019800186, lr: 5e-05
2023-12-15 00:20:20 INFO     	 * (global step 850: loss: 0.31939832866191864, lr: 5e-05
2023-12-15 00:20:28 INFO     	 * (global step 900: loss: 0.340692363679409, lr: 5e-05
2023-12-15 00:20:36 INFO     	 * (global step 950: loss: 0.552394837141037, lr: 5e-05
2023-12-15 00:20:45 INFO     	 * (global step 1000: loss: 0.3600849062204361, lr: 5e-05
2023-12-15 00:20:53 INFO     	 * (global step 1050: loss: 0.522358626127243, lr: 5e-05
2023-12-15 00:21:01 INFO     	 * (global step 1100: loss: 0.3982653170824051, lr: 5e-05
2023-12-15 00:21:09 INFO     	 * (global step 1150: loss: 0.7276760339736938, lr: 5e-05
2023-12-15 00:21:17 INFO     	 * (global step 1200: loss: 0.5836630761623383, lr: 5e-05
2023-12-15 00:21:24 INFO     [epoch 1/15] average loss: 0.506, lr: 5e-05
2023-12-15 00:21:24 INFO     saving model related files
2023-12-15 00:21:24 INFO     saving model
2023-12-15 00:21:24 INFO     saving tokenizer
2023-12-15 00:21:25 INFO     saving optimizer
2023-12-15 00:21:26 INFO     remove old optimizer files
2023-12-15 00:21:27 INFO     	 * (global step 1250: loss: 0.5851282775402069, lr: 5e-05
2023-12-15 00:21:35 INFO     	 * (global step 1300: loss: 0.4789549261331558, lr: 5e-05
2023-12-15 00:21:43 INFO     	 * (global step 1350: loss: 0.49678944051265717, lr: 5e-05
2023-12-15 00:21:51 INFO     	 * (global step 1400: loss: 0.5052850395441055, lr: 5e-05
2023-12-15 00:21:59 INFO     	 * (global step 1450: loss: 0.5903183370828629, lr: 5e-05
2023-12-15 00:22:07 INFO     	 * (global step 1500: loss: 0.31918442249298096, lr: 5e-05
2023-12-15 00:22:15 INFO     	 * (global step 1550: loss: 0.5025644749403, lr: 5e-05
2023-12-15 00:22:24 INFO     	 * (global step 1600: loss: 0.3731282949447632, lr: 5e-05
2023-12-15 00:22:32 INFO     	 * (global step 1650: loss: 0.3718061298131943, lr: 5e-05
2023-12-15 00:22:40 INFO     	 * (global step 1700: loss: 0.4068988859653473, lr: 5e-05
2023-12-15 00:22:48 INFO     	 * (global step 1750: loss: 0.37930384278297424, lr: 5e-05
2023-12-15 00:22:56 INFO     	 * (global step 1800: loss: 0.34050821512937546, lr: 5e-05
2023-12-15 00:23:04 INFO     	 * (global step 1850: loss: 0.45124128460884094, lr: 5e-05
2023-12-15 00:23:06 INFO     [epoch 2/15] average loss: 0.462, lr: 5e-05
2023-12-15 00:23:06 INFO     saving model related files
2023-12-15 00:23:06 INFO     saving model
2023-12-15 00:23:07 INFO     saving tokenizer
2023-12-15 00:23:07 INFO     saving optimizer
2023-12-15 00:23:08 INFO     remove old optimizer files
2023-12-15 00:23:14 INFO     	 * (global step 1900: loss: 0.5785777866840363, lr: 5e-05
2023-12-15 00:23:22 INFO     	 * (global step 1950: loss: 0.4268818348646164, lr: 5e-05
2023-12-15 00:23:30 INFO     	 * (global step 2000: loss: 0.3524411469697952, lr: 5e-05
2023-12-15 00:23:38 INFO     	 * (global step 2050: loss: 0.4826643615961075, lr: 5e-05
2023-12-15 00:23:46 INFO     	 * (global step 2100: loss: 0.5808644741773605, lr: 5e-05
2023-12-15 00:23:54 INFO     	 * (global step 2150: loss: 0.40955550968647003, lr: 5e-05
2023-12-15 00:24:03 INFO     	 * (global step 2200: loss: 0.34247517585754395, lr: 5e-05
2023-12-15 00:24:11 INFO     	 * (global step 2250: loss: 0.5951609313488007, lr: 5e-05
2023-12-15 00:24:19 INFO     	 * (global step 2300: loss: 0.4731360375881195, lr: 5e-05
2023-12-15 00:24:27 INFO     	 * (global step 2350: loss: 0.4006525129079819, lr: 5e-05
2023-12-15 00:24:35 INFO     	 * (global step 2400: loss: 0.3970111757516861, lr: 5e-05
2023-12-15 00:24:43 INFO     	 * (global step 2450: loss: 0.33743515610694885, lr: 5e-05
2023-12-15 00:24:49 INFO     [epoch 3/15] average loss: 0.438, lr: 5e-05
2023-12-15 00:24:49 INFO     saving model related files
2023-12-15 00:24:49 INFO     saving model
2023-12-15 00:24:49 INFO     saving tokenizer
2023-12-15 00:24:49 INFO     saving optimizer
2023-12-15 00:24:50 INFO     remove old optimizer files
2023-12-15 00:24:53 INFO     	 * (global step 2500: loss: 0.5057518184185028, lr: 5e-05
2023-12-15 00:25:01 INFO     	 * (global step 2550: loss: 0.3783058822154999, lr: 5e-05
2023-12-15 00:25:09 INFO     	 * (global step 2600: loss: 0.39694856107234955, lr: 5e-05
2023-12-15 00:25:17 INFO     	 * (global step 2650: loss: 0.4204580634832382, lr: 5e-05
2023-12-15 00:25:25 INFO     	 * (global step 2700: loss: 0.32660163938999176, lr: 5e-05
2023-12-15 00:25:33 INFO     	 * (global step 2750: loss: 0.3606703281402588, lr: 5e-05
2023-12-15 00:25:41 INFO     	 * (global step 2800: loss: 0.5049267709255219, lr: 5e-05
2023-12-15 00:25:50 INFO     	 * (global step 2850: loss: 0.4629508852958679, lr: 5e-05
2023-12-15 00:25:58 INFO     	 * (global step 2900: loss: 0.42699331045150757, lr: 5e-05
2023-12-15 00:26:06 INFO     	 * (global step 2950: loss: 0.40720339119434357, lr: 5e-05
2023-12-15 00:26:14 INFO     	 * (global step 3000: loss: 0.5685829073190689, lr: 5e-05
2023-12-15 00:26:22 INFO     	 * (global step 3050: loss: 0.4116494357585907, lr: 5e-05
2023-12-15 00:26:30 INFO     	 * (global step 3100: loss: 0.46713608503341675, lr: 5e-05
2023-12-15 00:26:31 INFO     [epoch 4/15] average loss: 0.42, lr: 5e-05
2023-12-15 00:26:31 INFO     saving model related files
2023-12-15 00:26:31 INFO     saving model
2023-12-15 00:26:31 INFO     saving tokenizer
2023-12-15 00:26:31 INFO     saving optimizer
2023-12-15 00:26:32 INFO     remove old optimizer files
2023-12-15 00:26:40 INFO     	 * (global step 3150: loss: 0.4066362977027893, lr: 5e-05
2023-12-15 00:26:48 INFO     	 * (global step 3200: loss: 0.4360990822315216, lr: 5e-05
2023-12-15 00:26:56 INFO     	 * (global step 3250: loss: 0.3841930478811264, lr: 5e-05
2023-12-15 00:27:04 INFO     	 * (global step 3300: loss: 0.30090536177158356, lr: 5e-05
2023-12-15 00:27:12 INFO     	 * (global step 3350: loss: 0.5038207769393921, lr: 5e-05
2023-12-15 00:27:20 INFO     	 * (global step 3400: loss: 0.5406199991703033, lr: 5e-05
2023-12-15 00:27:29 INFO     	 * (global step 3450: loss: 0.3088754117488861, lr: 5e-05
2023-12-15 00:27:37 INFO     	 * (global step 3500: loss: 0.21375170350074768, lr: 5e-05
2023-12-15 00:27:45 INFO     	 * (global step 3550: loss: 0.49212902784347534, lr: 5e-05
2023-12-15 00:27:53 INFO     	 * (global step 3600: loss: 0.4718747213482857, lr: 5e-05
2023-12-15 00:28:01 INFO     	 * (global step 3650: loss: 0.4324764758348465, lr: 5e-05
2023-12-15 00:28:09 INFO     	 * (global step 3700: loss: 0.3605722188949585, lr: 5e-05
2023-12-15 00:28:13 INFO     [epoch 5/15] average loss: 0.407, lr: 5e-05
2023-12-15 00:28:13 INFO     saving model related files
2023-12-15 00:28:13 INFO     saving model
2023-12-15 00:28:14 INFO     saving tokenizer
2023-12-15 00:28:14 INFO     saving optimizer
2023-12-15 00:28:15 INFO     remove old optimizer files
2023-12-15 00:28:19 INFO     	 * (global step 3750: loss: 0.16982145234942436, lr: 5e-05
2023-12-15 00:28:27 INFO     	 * (global step 3800: loss: 0.41298896074295044, lr: 5e-05
2023-12-15 00:28:35 INFO     	 * (global step 3850: loss: 0.2569720447063446, lr: 5e-05
2023-12-15 00:28:43 INFO     	 * (global step 3900: loss: 0.34787018597126007, lr: 5e-05
2023-12-15 00:28:51 INFO     	 * (global step 3950: loss: 0.32616133987903595, lr: 5e-05
2023-12-15 00:28:59 INFO     	 * (global step 4000: loss: 0.44327685236930847, lr: 5e-05
2023-12-15 00:29:08 INFO     	 * (global step 4050: loss: 0.4436681419610977, lr: 5e-05
2023-12-15 00:29:16 INFO     	 * (global step 4100: loss: 0.4377364218235016, lr: 5e-05
2023-12-15 00:29:24 INFO     	 * (global step 4150: loss: 0.4033365547657013, lr: 5e-05
2023-12-15 00:29:32 INFO     	 * (global step 4200: loss: 0.34388139098882675, lr: 5e-05
2023-12-15 00:29:40 INFO     	 * (global step 4250: loss: 0.2715598791837692, lr: 5e-05
2023-12-15 00:29:48 INFO     	 * (global step 4300: loss: 0.5179559886455536, lr: 5e-05
2023-12-15 00:29:56 INFO     [epoch 6/15] average loss: 0.396, lr: 5e-05
2023-12-15 00:29:56 INFO     saving model related files
2023-12-15 00:29:56 INFO     saving model
2023-12-15 00:29:56 INFO     saving tokenizer
2023-12-15 00:29:56 INFO     saving optimizer
2023-12-15 00:29:57 INFO     remove old optimizer files
2023-12-15 00:29:58 INFO     	 * (global step 4350: loss: 0.34585441648960114, lr: 5e-05
2023-12-15 00:30:06 INFO     	 * (global step 4400: loss: 0.3126178979873657, lr: 5e-05
2023-12-15 00:30:14 INFO     	 * (global step 4450: loss: 0.29098230600357056, lr: 5e-05
2023-12-15 00:30:22 INFO     	 * (global step 4500: loss: 0.36148127913475037, lr: 5e-05
2023-12-15 00:30:30 INFO     	 * (global step 4550: loss: 0.280011847615242, lr: 5e-05
2023-12-15 00:30:38 INFO     	 * (global step 4600: loss: 0.3865074962377548, lr: 5e-05
2023-12-15 00:30:46 INFO     	 * (global step 4650: loss: 0.5773571282625198, lr: 5e-05
2023-12-15 00:30:55 INFO     	 * (global step 4700: loss: 0.46237602829933167, lr: 5e-05
2023-12-15 00:31:03 INFO     	 * (global step 4750: loss: 0.39451834559440613, lr: 5e-05
2023-12-15 00:31:11 INFO     	 * (global step 4800: loss: 0.3926953971385956, lr: 5e-05
2023-12-15 00:31:19 INFO     	 * (global step 4850: loss: 0.30796919763088226, lr: 5e-05
2023-12-15 00:31:27 INFO     	 * (global step 4900: loss: 0.32005469501018524, lr: 5e-05
2023-12-15 00:31:35 INFO     	 * (global step 4950: loss: 0.2539707273244858, lr: 5e-05
2023-12-15 00:31:38 INFO     [epoch 7/15] average loss: 0.385, lr: 5e-05
2023-12-15 00:31:38 INFO     saving model related files
2023-12-15 00:31:38 INFO     saving model
2023-12-15 00:31:38 INFO     saving tokenizer
2023-12-15 00:31:39 INFO     saving optimizer
2023-12-15 00:31:40 INFO     remove old optimizer files
2023-12-15 00:31:45 INFO     	 * (global step 5000: loss: 0.38618311285972595, lr: 5e-05
2023-12-15 00:31:53 INFO     	 * (global step 5050: loss: 0.33932723104953766, lr: 5e-05
2023-12-15 00:32:01 INFO     	 * (global step 5100: loss: 0.323372557759285, lr: 5e-05
2023-12-15 00:32:09 INFO     	 * (global step 5150: loss: 0.22127394378185272, lr: 5e-05
2023-12-15 00:32:17 INFO     	 * (global step 5200: loss: 0.21271099150180817, lr: 5e-05
2023-12-15 00:32:25 INFO     	 * (global step 5250: loss: 0.7082837894558907, lr: 5e-05
2023-12-15 00:32:33 INFO     	 * (global step 5300: loss: 0.26505202800035477, lr: 5e-05
2023-12-15 00:32:41 INFO     	 * (global step 5350: loss: 0.3731112629175186, lr: 5e-05
2023-12-15 00:32:50 INFO     	 * (global step 5400: loss: 0.33397211134433746, lr: 5e-05
2023-12-15 00:32:58 INFO     	 * (global step 5450: loss: 0.31303344666957855, lr: 5e-05
2023-12-15 00:33:06 INFO     	 * (global step 5500: loss: 0.3761341869831085, lr: 5e-05
2023-12-15 00:33:14 INFO     	 * (global step 5550: loss: 0.3218141496181488, lr: 5e-05
2023-12-15 00:33:20 INFO     [epoch 8/15] average loss: 0.376, lr: 5e-05
2023-12-15 00:33:20 INFO     saving model related files
2023-12-15 00:33:20 INFO     saving model
2023-12-15 00:33:21 INFO     saving tokenizer
2023-12-15 00:33:21 INFO     saving optimizer
2023-12-15 00:33:22 INFO     remove old optimizer files
2023-12-15 00:33:24 INFO     	 * (global step 5600: loss: 0.5150308310985565, lr: 5e-05
2023-12-15 00:33:32 INFO     	 * (global step 5650: loss: 0.46271030604839325, lr: 5e-05
2023-12-15 00:33:40 INFO     	 * (global step 5700: loss: 0.26881880313158035, lr: 5e-05
2023-12-15 00:33:48 INFO     	 * (global step 5750: loss: 0.3237970769405365, lr: 5e-05
2023-12-15 00:33:56 INFO     	 * (global step 5800: loss: 0.3281205743551254, lr: 5e-05
2023-12-15 00:34:04 INFO     	 * (global step 5850: loss: 0.46420060098171234, lr: 5e-05
2023-12-15 00:34:12 INFO     	 * (global step 5900: loss: 0.2927240878343582, lr: 5e-05
2023-12-15 00:34:20 INFO     	 * (global step 5950: loss: 0.35176074504852295, lr: 5e-05
2023-12-15 00:34:28 INFO     	 * (global step 6000: loss: 0.3875330686569214, lr: 5e-05
2023-12-15 00:34:37 INFO     	 * (global step 6050: loss: 0.41616472601890564, lr: 5e-05
2023-12-15 00:34:45 INFO     	 * (global step 6100: loss: 0.2893667593598366, lr: 5e-05
2023-12-15 00:34:53 INFO     	 * (global step 6150: loss: 0.3935631066560745, lr: 5e-05
2023-12-15 00:35:01 INFO     	 * (global step 6200: loss: 0.4542161673307419, lr: 5e-05
2023-12-15 00:35:03 INFO     [epoch 9/15] average loss: 0.368, lr: 5e-05
2023-12-15 00:35:03 INFO     saving model related files
2023-12-15 00:35:03 INFO     saving model
2023-12-15 00:35:04 INFO     saving tokenizer
2023-12-15 00:35:04 INFO     saving optimizer
2023-12-15 00:35:06 INFO     remove old optimizer files
2023-12-15 00:35:06 INFO     complete training: model ckpt was saved at small_trained_ckpt/model_woixzh
2023-12-15 00:35:06 INFO     ## 1st RUN: Configuration 6/12 ##
2023-12-15 00:35:06 INFO     initialize model trainer
2023-12-15 00:35:06 INFO     initialize checkpoint at small_trained_ckpt/model_sdkaaa
2023-12-15 00:35:06 INFO     hyperparameters
2023-12-15 00:35:06 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 00:35:06 INFO     	 * dataset_name: default
2023-12-15 00:35:06 INFO     	 * input_types: ['paragraph']
2023-12-15 00:35:06 INFO     	 * output_types: ['questions_answers']
2023-12-15 00:35:06 INFO     	 * prefix_types: ['qag']
2023-12-15 00:35:06 INFO     	 * model: t5-small
2023-12-15 00:35:06 INFO     	 * max_length: 512
2023-12-15 00:35:06 INFO     	 * max_length_output: 512
2023-12-15 00:35:06 INFO     	 * epoch: 15
2023-12-15 00:35:06 INFO     	 * batch: 2
2023-12-15 00:35:06 INFO     	 * lr: 5e-05
2023-12-15 00:35:06 INFO     	 * fp16: False
2023-12-15 00:35:06 INFO     	 * random_seed: 1
2023-12-15 00:35:06 INFO     	 * gradient_accumulation_steps: 4
2023-12-15 00:35:06 INFO     	 * label_smoothing: 0.0
2023-12-15 00:35:06 INFO     initialize checkpoint with t5-small
2023-12-15 00:35:07 INFO     use spaCy answer extraction model: positionrank
2023-12-15 00:35:08 INFO     Model `t5-small`
2023-12-15 00:35:08 INFO     	 * Num of GPU in use: 1
2023-12-15 00:35:08 INFO     	 * Prefix: True
2023-12-15 00:35:08 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 00:35:08 INFO     dataset preprocessing
2023-12-15 00:35:09 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 00:35:09 INFO     start model training
2023-12-15 00:35:25 INFO     	 * (global step 50: loss: 1.3254896700382233, lr: 5e-05
2023-12-15 00:35:41 INFO     	 * (global step 100: loss: 0.8981009274721146, lr: 5e-05
2023-12-15 00:35:56 INFO     	 * (global step 150: loss: 0.9119456708431244, lr: 5e-05
2023-12-15 00:36:12 INFO     	 * (global step 200: loss: 0.6897153705358505, lr: 5e-05
2023-12-15 00:36:27 INFO     	 * (global step 250: loss: 0.9045827090740204, lr: 5e-05
2023-12-15 00:36:43 INFO     	 * (global step 300: loss: 0.5700984299182892, lr: 5e-05
2023-12-15 00:36:46 INFO     [epoch 0/15] average loss: 1.178, lr: 5e-05
2023-12-15 00:36:46 INFO     saving model related files
2023-12-15 00:36:46 INFO     saving model
2023-12-15 00:36:47 INFO     saving tokenizer
2023-12-15 00:36:47 INFO     saving optimizer
2023-12-15 00:36:48 INFO     remove old optimizer files
2023-12-15 00:37:00 INFO     	 * (global step 350: loss: 0.4940400943160057, lr: 5e-05
2023-12-15 00:37:16 INFO     	 * (global step 400: loss: 0.4914982169866562, lr: 5e-05
2023-12-15 00:37:31 INFO     	 * (global step 450: loss: 0.39232543855905533, lr: 5e-05
2023-12-15 00:37:47 INFO     	 * (global step 500: loss: 0.4878457337617874, lr: 5e-05
2023-12-15 00:38:03 INFO     	 * (global step 550: loss: 0.4989309161901474, lr: 5e-05
2023-12-15 00:38:18 INFO     	 * (global step 600: loss: 0.5362381041049957, lr: 5e-05
2023-12-15 00:38:25 INFO     [epoch 1/15] average loss: 0.566, lr: 5e-05
2023-12-15 00:38:25 INFO     saving model related files
2023-12-15 00:38:25 INFO     saving model
2023-12-15 00:38:25 INFO     saving tokenizer
2023-12-15 00:38:25 INFO     saving optimizer
2023-12-15 00:38:26 INFO     remove old optimizer files
2023-12-15 00:38:36 INFO     	 * (global step 650: loss: 0.7027509585022926, lr: 5e-05
2023-12-15 00:38:51 INFO     	 * (global step 700: loss: 0.45776501297950745, lr: 5e-05
2023-12-15 00:39:07 INFO     	 * (global step 750: loss: 0.5646698027849197, lr: 5e-05
2023-12-15 00:39:23 INFO     	 * (global step 800: loss: 0.6477614045143127, lr: 5e-05
2023-12-15 00:39:38 INFO     	 * (global step 850: loss: 0.3658929839730263, lr: 5e-05
2023-12-15 00:39:54 INFO     	 * (global step 900: loss: 0.3872283771634102, lr: 5e-05
2023-12-15 00:40:03 INFO     [epoch 2/15] average loss: 0.5, lr: 5e-05
2023-12-15 00:40:03 INFO     saving model related files
2023-12-15 00:40:03 INFO     saving model
2023-12-15 00:40:04 INFO     saving tokenizer
2023-12-15 00:40:04 INFO     saving optimizer
2023-12-15 00:40:05 INFO     remove old optimizer files
2023-12-15 00:40:12 INFO     	 * (global step 950: loss: 0.659692645072937, lr: 5e-05
2023-12-15 00:40:28 INFO     	 * (global step 1000: loss: 0.5231571793556213, lr: 5e-05
2023-12-15 00:40:43 INFO     	 * (global step 1050: loss: 0.4832238145172596, lr: 5e-05
2023-12-15 00:40:59 INFO     	 * (global step 1100: loss: 0.37351197749376297, lr: 5e-05
2023-12-15 00:41:15 INFO     	 * (global step 1150: loss: 0.5454632118344307, lr: 5e-05
2023-12-15 00:41:30 INFO     	 * (global step 1200: loss: 0.4034688323736191, lr: 5e-05
2023-12-15 00:41:43 INFO     [epoch 3/15] average loss: 0.472, lr: 5e-05
2023-12-15 00:41:43 INFO     saving model related files
2023-12-15 00:41:43 INFO     saving model
2023-12-15 00:41:43 INFO     saving tokenizer
2023-12-15 00:41:44 INFO     saving optimizer
2023-12-15 00:41:45 INFO     remove old optimizer files
2023-12-15 00:41:48 INFO     	 * (global step 1250: loss: 0.4297929033637047, lr: 5e-05
2023-12-15 00:42:03 INFO     	 * (global step 1300: loss: 0.6594108119606972, lr: 5e-05
2023-12-15 00:42:19 INFO     	 * (global step 1350: loss: 0.36978161334991455, lr: 5e-05
2023-12-15 00:42:35 INFO     	 * (global step 1400: loss: 0.352481909096241, lr: 5e-05
2023-12-15 00:42:50 INFO     	 * (global step 1450: loss: 0.46161432564258575, lr: 5e-05
2023-12-15 00:43:06 INFO     	 * (global step 1500: loss: 0.3683296889066696, lr: 5e-05
2023-12-15 00:43:22 INFO     	 * (global step 1550: loss: 0.6282598376274109, lr: 5e-05
2023-12-15 00:43:22 INFO     [epoch 4/15] average loss: 0.451, lr: 5e-05
2023-12-15 00:43:22 INFO     saving model related files
2023-12-15 00:43:22 INFO     saving model
2023-12-15 00:43:22 INFO     saving tokenizer
2023-12-15 00:43:22 INFO     saving optimizer
2023-12-15 00:43:23 INFO     remove old optimizer files
2023-12-15 00:43:39 INFO     	 * (global step 1600: loss: 0.5040901526808739, lr: 5e-05
2023-12-15 00:43:55 INFO     	 * (global step 1650: loss: 0.40810971707105637, lr: 5e-05
2023-12-15 00:44:11 INFO     	 * (global step 1700: loss: 0.40656714886426926, lr: 5e-05
2023-12-15 00:44:26 INFO     	 * (global step 1750: loss: 0.319843627512455, lr: 5e-05
2023-12-15 00:44:42 INFO     	 * (global step 1800: loss: 0.3748915418982506, lr: 5e-05
2023-12-15 00:44:58 INFO     	 * (global step 1850: loss: 0.44412147998809814, lr: 5e-05
2023-12-15 00:45:01 INFO     [epoch 5/15] average loss: 0.437, lr: 5e-05
2023-12-15 00:45:01 INFO     saving model related files
2023-12-15 00:45:01 INFO     saving model
2023-12-15 00:45:01 INFO     saving tokenizer
2023-12-15 00:45:01 INFO     saving optimizer
2023-12-15 00:45:02 INFO     remove old optimizer files
2023-12-15 00:45:15 INFO     	 * (global step 1900: loss: 0.3636530414223671, lr: 5e-05
2023-12-15 00:45:31 INFO     	 * (global step 1950: loss: 0.3232303373515606, lr: 5e-05
2023-12-15 00:45:46 INFO     	 * (global step 2000: loss: 0.48621904850006104, lr: 5e-05
2023-12-15 00:46:02 INFO     	 * (global step 2050: loss: 0.4487333744764328, lr: 5e-05
2023-12-15 00:46:18 INFO     	 * (global step 2100: loss: 0.36525413393974304, lr: 5e-05
2023-12-15 00:46:33 INFO     	 * (global step 2150: loss: 0.4642280861735344, lr: 5e-05
2023-12-15 00:46:40 INFO     [epoch 6/15] average loss: 0.426, lr: 5e-05
2023-12-15 00:46:40 INFO     saving model related files
2023-12-15 00:46:40 INFO     saving model
2023-12-15 00:46:40 INFO     saving tokenizer
2023-12-15 00:46:40 INFO     saving optimizer
2023-12-15 00:46:41 INFO     remove old optimizer files
2023-12-15 00:46:51 INFO     	 * (global step 2200: loss: 0.49551691114902496, lr: 5e-05
2023-12-15 00:47:06 INFO     	 * (global step 2250: loss: 0.3728572875261307, lr: 5e-05
2023-12-15 00:47:22 INFO     	 * (global step 2300: loss: 0.3395213782787323, lr: 5e-05
2023-12-15 00:47:38 INFO     	 * (global step 2350: loss: 0.45369119942188263, lr: 5e-05
2023-12-15 00:47:54 INFO     	 * (global step 2400: loss: 0.30297956615686417, lr: 5e-05
2023-12-15 00:48:09 INFO     	 * (global step 2450: loss: 0.45672521740198135, lr: 5e-05
2023-12-15 00:48:19 INFO     [epoch 7/15] average loss: 0.415, lr: 5e-05
2023-12-15 00:48:19 INFO     saving model related files
2023-12-15 00:48:19 INFO     saving model
2023-12-15 00:48:19 INFO     saving tokenizer
2023-12-15 00:48:19 INFO     saving optimizer
2023-12-15 00:48:20 INFO     remove old optimizer files
2023-12-15 00:48:27 INFO     	 * (global step 2500: loss: 0.4003717750310898, lr: 5e-05
2023-12-15 00:48:42 INFO     	 * (global step 2550: loss: 0.46041805297136307, lr: 5e-05
2023-12-15 00:48:58 INFO     	 * (global step 2600: loss: 0.4288046658039093, lr: 5e-05
2023-12-15 00:49:14 INFO     	 * (global step 2650: loss: 0.4630977138876915, lr: 5e-05
2023-12-15 00:49:29 INFO     	 * (global step 2700: loss: 0.35659343004226685, lr: 5e-05
2023-12-15 00:49:45 INFO     	 * (global step 2750: loss: 0.3513820096850395, lr: 5e-05
2023-12-15 00:49:58 INFO     [epoch 8/15] average loss: 0.407, lr: 5e-05
2023-12-15 00:49:58 INFO     saving model related files
2023-12-15 00:49:58 INFO     saving model
2023-12-15 00:49:58 INFO     saving tokenizer
2023-12-15 00:49:58 INFO     saving optimizer
2023-12-15 00:49:59 INFO     remove old optimizer files
2023-12-15 00:50:02 INFO     	 * (global step 2800: loss: 0.46546725183725357, lr: 5e-05
2023-12-15 00:50:18 INFO     	 * (global step 2850: loss: 0.5493580773472786, lr: 5e-05
2023-12-15 00:50:33 INFO     	 * (global step 2900: loss: 0.41611748933792114, lr: 5e-05
2023-12-15 00:50:49 INFO     	 * (global step 2950: loss: 0.4201995059847832, lr: 5e-05
2023-12-15 00:51:05 INFO     	 * (global step 3000: loss: 0.3498004972934723, lr: 5e-05
2023-12-15 00:51:20 INFO     	 * (global step 3050: loss: 0.3336162492632866, lr: 5e-05
2023-12-15 00:51:36 INFO     	 * (global step 3100: loss: 0.4677710309624672, lr: 5e-05
2023-12-15 00:51:36 INFO     [epoch 9/15] average loss: 0.399, lr: 5e-05
2023-12-15 00:51:36 INFO     saving model related files
2023-12-15 00:51:36 INFO     saving model
2023-12-15 00:51:37 INFO     saving tokenizer
2023-12-15 00:51:37 INFO     saving optimizer
2023-12-15 00:51:38 INFO     remove old optimizer files
2023-12-15 00:51:38 INFO     complete training: model ckpt was saved at small_trained_ckpt/model_sdkaaa
2023-12-15 00:51:38 INFO     ## 1st RUN: Configuration 7/12 ##
2023-12-15 00:51:38 INFO     initialize model trainer
2023-12-15 00:51:38 INFO     initialize checkpoint at small_trained_ckpt/model_uramvg
2023-12-15 00:51:38 INFO     hyperparameters
2023-12-15 00:51:38 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 00:51:38 INFO     	 * dataset_name: default
2023-12-15 00:51:38 INFO     	 * input_types: ['paragraph']
2023-12-15 00:51:38 INFO     	 * output_types: ['questions_answers']
2023-12-15 00:51:38 INFO     	 * prefix_types: ['qag']
2023-12-15 00:51:38 INFO     	 * model: t5-small
2023-12-15 00:51:38 INFO     	 * max_length: 512
2023-12-15 00:51:38 INFO     	 * max_length_output: 512
2023-12-15 00:51:38 INFO     	 * epoch: 15
2023-12-15 00:51:38 INFO     	 * batch: 2
2023-12-15 00:51:38 INFO     	 * lr: 5e-05
2023-12-15 00:51:38 INFO     	 * fp16: False
2023-12-15 00:51:38 INFO     	 * random_seed: 1
2023-12-15 00:51:38 INFO     	 * gradient_accumulation_steps: 2
2023-12-15 00:51:38 INFO     	 * label_smoothing: 0.0
2023-12-15 00:51:38 INFO     initialize checkpoint with t5-small
2023-12-15 00:51:39 INFO     use spaCy answer extraction model: positionrank
2023-12-15 00:51:40 INFO     Model `t5-small`
2023-12-15 00:51:40 INFO     	 * Num of GPU in use: 1
2023-12-15 00:51:40 INFO     	 * Prefix: True
2023-12-15 00:51:40 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 00:51:40 INFO     dataset preprocessing
2023-12-15 00:51:41 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 00:51:41 INFO     start model training
2023-12-15 00:51:50 INFO     	 * (global step 50: loss: 1.4092538952827454, lr: 5e-05
2023-12-15 00:51:58 INFO     	 * (global step 100: loss: 0.8925729095935822, lr: 5e-05
2023-12-15 00:52:06 INFO     	 * (global step 150: loss: 0.8196516335010529, lr: 5e-05
2023-12-15 00:52:14 INFO     	 * (global step 200: loss: 0.7100931107997894, lr: 5e-05
2023-12-15 00:52:22 INFO     	 * (global step 250: loss: 0.7927198708057404, lr: 5e-05
2023-12-15 00:52:30 INFO     	 * (global step 300: loss: 0.582080602645874, lr: 5e-05
2023-12-15 00:52:38 INFO     	 * (global step 350: loss: 0.6985580325126648, lr: 5e-05
2023-12-15 00:52:46 INFO     	 * (global step 400: loss: 0.5441485941410065, lr: 5e-05
2023-12-15 00:52:54 INFO     	 * (global step 450: loss: 0.6066481471061707, lr: 5e-05
2023-12-15 00:53:02 INFO     	 * (global step 500: loss: 0.9787241816520691, lr: 5e-05
2023-12-15 00:53:10 INFO     	 * (global step 550: loss: 0.5473348498344421, lr: 5e-05
2023-12-15 00:53:18 INFO     	 * (global step 600: loss: 0.5466890633106232, lr: 5e-05
2023-12-15 00:53:22 INFO     [epoch 0/15] average loss: 0.93, lr: 5e-05
2023-12-15 00:53:22 INFO     saving model related files
2023-12-15 00:53:22 INFO     saving model
2023-12-15 00:53:22 INFO     saving tokenizer
2023-12-15 00:53:22 INFO     saving optimizer
2023-12-15 00:53:24 INFO     remove old optimizer files
2023-12-15 00:53:28 INFO     	 * (global step 650: loss: 0.6004865169525146, lr: 5e-05
2023-12-15 00:53:36 INFO     	 * (global step 700: loss: 0.4539387822151184, lr: 5e-05
2023-12-15 00:53:44 INFO     	 * (global step 750: loss: 0.6251737326383591, lr: 5e-05
2023-12-15 00:53:52 INFO     	 * (global step 800: loss: 0.4313388019800186, lr: 5e-05
2023-12-15 00:54:01 INFO     	 * (global step 850: loss: 0.31939832866191864, lr: 5e-05
2023-12-15 00:54:09 INFO     	 * (global step 900: loss: 0.340692363679409, lr: 5e-05
2023-12-15 00:54:17 INFO     	 * (global step 950: loss: 0.552394837141037, lr: 5e-05
2023-12-15 00:54:25 INFO     	 * (global step 1000: loss: 0.3600849062204361, lr: 5e-05
2023-12-15 00:54:33 INFO     	 * (global step 1050: loss: 0.522358626127243, lr: 5e-05
2023-12-15 00:54:41 INFO     	 * (global step 1100: loss: 0.3982653170824051, lr: 5e-05
2023-12-15 00:54:49 INFO     	 * (global step 1150: loss: 0.7276760339736938, lr: 5e-05
2023-12-15 00:54:57 INFO     	 * (global step 1200: loss: 0.5836630761623383, lr: 5e-05
2023-12-15 00:55:04 INFO     [epoch 1/15] average loss: 0.506, lr: 5e-05
2023-12-15 00:55:04 INFO     saving model related files
2023-12-15 00:55:04 INFO     saving model
2023-12-15 00:55:05 INFO     saving tokenizer
2023-12-15 00:55:05 INFO     saving optimizer
2023-12-15 00:55:06 INFO     remove old optimizer files
2023-12-15 00:55:07 INFO     	 * (global step 1250: loss: 0.5851282775402069, lr: 5e-05
2023-12-15 00:55:16 INFO     	 * (global step 1300: loss: 0.4789549261331558, lr: 5e-05
2023-12-15 00:55:24 INFO     	 * (global step 1350: loss: 0.49678944051265717, lr: 5e-05
2023-12-15 00:55:32 INFO     	 * (global step 1400: loss: 0.5052850395441055, lr: 5e-05
2023-12-15 00:55:40 INFO     	 * (global step 1450: loss: 0.5903183370828629, lr: 5e-05
2023-12-15 00:55:48 INFO     	 * (global step 1500: loss: 0.31918442249298096, lr: 5e-05
2023-12-15 00:55:56 INFO     	 * (global step 1550: loss: 0.5025644749403, lr: 5e-05
2023-12-15 00:56:04 INFO     	 * (global step 1600: loss: 0.3731282949447632, lr: 5e-05
2023-12-15 00:56:12 INFO     	 * (global step 1650: loss: 0.3718061298131943, lr: 5e-05
2023-12-15 00:56:21 INFO     	 * (global step 1700: loss: 0.4068988859653473, lr: 5e-05
2023-12-15 00:56:29 INFO     	 * (global step 1750: loss: 0.37930384278297424, lr: 5e-05
2023-12-15 00:56:37 INFO     	 * (global step 1800: loss: 0.34050821512937546, lr: 5e-05
2023-12-15 00:56:45 INFO     	 * (global step 1850: loss: 0.45124128460884094, lr: 5e-05
2023-12-15 00:56:47 INFO     [epoch 2/15] average loss: 0.462, lr: 5e-05
2023-12-15 00:56:47 INFO     saving model related files
2023-12-15 00:56:47 INFO     saving model
2023-12-15 00:56:48 INFO     saving tokenizer
2023-12-15 00:56:48 INFO     saving optimizer
2023-12-15 00:56:49 INFO     remove old optimizer files
2023-12-15 00:56:55 INFO     	 * (global step 1900: loss: 0.5785777866840363, lr: 5e-05
2023-12-15 00:57:03 INFO     	 * (global step 1950: loss: 0.4268818348646164, lr: 5e-05
2023-12-15 00:57:11 INFO     	 * (global step 2000: loss: 0.3524411469697952, lr: 5e-05
2023-12-15 00:57:19 INFO     	 * (global step 2050: loss: 0.4826643615961075, lr: 5e-05
2023-12-15 00:57:27 INFO     	 * (global step 2100: loss: 0.5808644741773605, lr: 5e-05
2023-12-15 00:57:35 INFO     	 * (global step 2150: loss: 0.40955550968647003, lr: 5e-05
2023-12-15 00:57:43 INFO     	 * (global step 2200: loss: 0.34247517585754395, lr: 5e-05
2023-12-15 00:57:51 INFO     	 * (global step 2250: loss: 0.5951609313488007, lr: 5e-05
2023-12-15 00:58:00 INFO     	 * (global step 2300: loss: 0.4731360375881195, lr: 5e-05
2023-12-15 00:58:08 INFO     	 * (global step 2350: loss: 0.4006525129079819, lr: 5e-05
2023-12-15 00:58:16 INFO     	 * (global step 2400: loss: 0.3970111757516861, lr: 5e-05
2023-12-15 00:58:24 INFO     	 * (global step 2450: loss: 0.33743515610694885, lr: 5e-05
2023-12-15 00:58:29 INFO     [epoch 3/15] average loss: 0.438, lr: 5e-05
2023-12-15 00:58:29 INFO     saving model related files
2023-12-15 00:58:29 INFO     saving model
2023-12-15 00:58:30 INFO     saving tokenizer
2023-12-15 00:58:30 INFO     saving optimizer
2023-12-15 00:58:31 INFO     remove old optimizer files
2023-12-15 00:58:34 INFO     	 * (global step 2500: loss: 0.5057518184185028, lr: 5e-05
2023-12-15 00:58:42 INFO     	 * (global step 2550: loss: 0.3783058822154999, lr: 5e-05
2023-12-15 00:58:50 INFO     	 * (global step 2600: loss: 0.39694856107234955, lr: 5e-05
2023-12-15 00:58:58 INFO     	 * (global step 2650: loss: 0.4204580634832382, lr: 5e-05
2023-12-15 00:59:06 INFO     	 * (global step 2700: loss: 0.32660163938999176, lr: 5e-05
2023-12-15 00:59:14 INFO     	 * (global step 2750: loss: 0.3606703281402588, lr: 5e-05
2023-12-15 00:59:22 INFO     	 * (global step 2800: loss: 0.5049267709255219, lr: 5e-05
2023-12-15 00:59:31 INFO     	 * (global step 2850: loss: 0.4629508852958679, lr: 5e-05
2023-12-15 00:59:39 INFO     	 * (global step 2900: loss: 0.42699331045150757, lr: 5e-05
2023-12-15 00:59:47 INFO     	 * (global step 2950: loss: 0.40720339119434357, lr: 5e-05
2023-12-15 00:59:55 INFO     	 * (global step 3000: loss: 0.5685829073190689, lr: 5e-05
2023-12-15 01:00:03 INFO     	 * (global step 3050: loss: 0.4116494357585907, lr: 5e-05
2023-12-15 01:00:11 INFO     	 * (global step 3100: loss: 0.46713608503341675, lr: 5e-05
2023-12-15 01:00:12 INFO     [epoch 4/15] average loss: 0.42, lr: 5e-05
2023-12-15 01:00:12 INFO     saving model related files
2023-12-15 01:00:12 INFO     saving model
2023-12-15 01:00:12 INFO     saving tokenizer
2023-12-15 01:00:12 INFO     saving optimizer
2023-12-15 01:00:13 INFO     remove old optimizer files
2023-12-15 01:00:21 INFO     	 * (global step 3150: loss: 0.4066362977027893, lr: 5e-05
2023-12-15 01:00:29 INFO     	 * (global step 3200: loss: 0.4360990822315216, lr: 5e-05
2023-12-15 01:00:37 INFO     	 * (global step 3250: loss: 0.3841930478811264, lr: 5e-05
2023-12-15 01:00:45 INFO     	 * (global step 3300: loss: 0.30090536177158356, lr: 5e-05
2023-12-15 01:00:53 INFO     	 * (global step 3350: loss: 0.5038207769393921, lr: 5e-05
2023-12-15 01:01:01 INFO     	 * (global step 3400: loss: 0.5406199991703033, lr: 5e-05
2023-12-15 01:01:10 INFO     	 * (global step 3450: loss: 0.3088754117488861, lr: 5e-05
2023-12-15 01:01:18 INFO     	 * (global step 3500: loss: 0.21375170350074768, lr: 5e-05
2023-12-15 01:01:26 INFO     	 * (global step 3550: loss: 0.49212902784347534, lr: 5e-05
2023-12-15 01:01:34 INFO     	 * (global step 3600: loss: 0.4718747213482857, lr: 5e-05
2023-12-15 01:01:42 INFO     	 * (global step 3650: loss: 0.4324764758348465, lr: 5e-05
2023-12-15 01:01:50 INFO     	 * (global step 3700: loss: 0.3605722188949585, lr: 5e-05
2023-12-15 01:01:54 INFO     [epoch 5/15] average loss: 0.407, lr: 5e-05
2023-12-15 01:01:54 INFO     saving model related files
2023-12-15 01:01:54 INFO     saving model
2023-12-15 01:01:55 INFO     saving tokenizer
2023-12-15 01:01:55 INFO     saving optimizer
2023-12-15 01:01:56 INFO     remove old optimizer files
2023-12-15 01:02:00 INFO     	 * (global step 3750: loss: 0.16982145234942436, lr: 5e-05
2023-12-15 01:02:08 INFO     	 * (global step 3800: loss: 0.41298896074295044, lr: 5e-05
2023-12-15 01:02:16 INFO     	 * (global step 3850: loss: 0.2569720447063446, lr: 5e-05
2023-12-15 01:02:24 INFO     	 * (global step 3900: loss: 0.34787018597126007, lr: 5e-05
2023-12-15 01:02:33 INFO     	 * (global step 3950: loss: 0.32616133987903595, lr: 5e-05
2023-12-15 01:02:41 INFO     	 * (global step 4000: loss: 0.44327685236930847, lr: 5e-05
2023-12-15 01:02:49 INFO     	 * (global step 4050: loss: 0.4436681419610977, lr: 5e-05
2023-12-15 01:02:57 INFO     	 * (global step 4100: loss: 0.4377364218235016, lr: 5e-05
2023-12-15 01:03:05 INFO     	 * (global step 4150: loss: 0.4033365547657013, lr: 5e-05
2023-12-15 01:03:13 INFO     	 * (global step 4200: loss: 0.34388139098882675, lr: 5e-05
2023-12-15 01:03:21 INFO     	 * (global step 4250: loss: 0.2715598791837692, lr: 5e-05
2023-12-15 01:03:29 INFO     	 * (global step 4300: loss: 0.5179559886455536, lr: 5e-05
2023-12-15 01:03:37 INFO     [epoch 6/15] average loss: 0.396, lr: 5e-05
2023-12-15 01:03:37 INFO     saving model related files
2023-12-15 01:03:37 INFO     saving model
2023-12-15 01:03:37 INFO     saving tokenizer
2023-12-15 01:03:37 INFO     saving optimizer
2023-12-15 01:03:38 INFO     remove old optimizer files
2023-12-15 01:03:39 INFO     	 * (global step 4350: loss: 0.34585441648960114, lr: 5e-05
2023-12-15 01:03:47 INFO     	 * (global step 4400: loss: 0.3126178979873657, lr: 5e-05
2023-12-15 01:03:55 INFO     	 * (global step 4450: loss: 0.29098230600357056, lr: 5e-05
2023-12-15 01:04:03 INFO     	 * (global step 4500: loss: 0.36148127913475037, lr: 5e-05
2023-12-15 01:04:12 INFO     	 * (global step 4550: loss: 0.280011847615242, lr: 5e-05
2023-12-15 01:04:20 INFO     	 * (global step 4600: loss: 0.3865074962377548, lr: 5e-05
2023-12-15 01:04:28 INFO     	 * (global step 4650: loss: 0.5773571282625198, lr: 5e-05
2023-12-15 01:04:36 INFO     	 * (global step 4700: loss: 0.46237602829933167, lr: 5e-05
2023-12-15 01:04:44 INFO     	 * (global step 4750: loss: 0.39451834559440613, lr: 5e-05
2023-12-15 01:04:52 INFO     	 * (global step 4800: loss: 0.3926953971385956, lr: 5e-05
2023-12-15 01:05:00 INFO     	 * (global step 4850: loss: 0.30796919763088226, lr: 5e-05
2023-12-15 01:05:08 INFO     	 * (global step 4900: loss: 0.32005469501018524, lr: 5e-05
2023-12-15 01:05:16 INFO     	 * (global step 4950: loss: 0.2539707273244858, lr: 5e-05
2023-12-15 01:05:19 INFO     [epoch 7/15] average loss: 0.385, lr: 5e-05
2023-12-15 01:05:19 INFO     saving model related files
2023-12-15 01:05:19 INFO     saving model
2023-12-15 01:05:20 INFO     saving tokenizer
2023-12-15 01:05:20 INFO     saving optimizer
2023-12-15 01:05:21 INFO     remove old optimizer files
2023-12-15 01:05:26 INFO     	 * (global step 5000: loss: 0.38618311285972595, lr: 5e-05
2023-12-15 01:05:34 INFO     	 * (global step 5050: loss: 0.33932723104953766, lr: 5e-05
2023-12-15 01:05:42 INFO     	 * (global step 5100: loss: 0.323372557759285, lr: 5e-05
2023-12-15 01:05:50 INFO     	 * (global step 5150: loss: 0.22127394378185272, lr: 5e-05
2023-12-15 01:05:58 INFO     	 * (global step 5200: loss: 0.21271099150180817, lr: 5e-05
2023-12-15 01:06:07 INFO     	 * (global step 5250: loss: 0.7082837894558907, lr: 5e-05
2023-12-15 01:06:15 INFO     	 * (global step 5300: loss: 0.26505202800035477, lr: 5e-05
2023-12-15 01:06:23 INFO     	 * (global step 5350: loss: 0.3731112629175186, lr: 5e-05
2023-12-15 01:06:31 INFO     	 * (global step 5400: loss: 0.33397211134433746, lr: 5e-05
2023-12-15 01:06:39 INFO     	 * (global step 5450: loss: 0.31303344666957855, lr: 5e-05
2023-12-15 01:06:47 INFO     	 * (global step 5500: loss: 0.3761341869831085, lr: 5e-05
2023-12-15 01:06:55 INFO     	 * (global step 5550: loss: 0.3218141496181488, lr: 5e-05
2023-12-15 01:07:02 INFO     [epoch 8/15] average loss: 0.376, lr: 5e-05
2023-12-15 01:07:02 INFO     saving model related files
2023-12-15 01:07:02 INFO     saving model
2023-12-15 01:07:02 INFO     saving tokenizer
2023-12-15 01:07:02 INFO     saving optimizer
2023-12-15 01:07:03 INFO     remove old optimizer files
2023-12-15 01:07:05 INFO     	 * (global step 5600: loss: 0.5150308310985565, lr: 5e-05
2023-12-15 01:07:13 INFO     	 * (global step 5650: loss: 0.46271030604839325, lr: 5e-05
2023-12-15 01:07:21 INFO     	 * (global step 5700: loss: 0.26881880313158035, lr: 5e-05
2023-12-15 01:07:29 INFO     	 * (global step 5750: loss: 0.3237970769405365, lr: 5e-05
2023-12-15 01:07:38 INFO     	 * (global step 5800: loss: 0.3281205743551254, lr: 5e-05
2023-12-15 01:07:46 INFO     	 * (global step 5850: loss: 0.46420060098171234, lr: 5e-05
2023-12-15 01:07:54 INFO     	 * (global step 5900: loss: 0.2927240878343582, lr: 5e-05
2023-12-15 01:08:02 INFO     	 * (global step 5950: loss: 0.35176074504852295, lr: 5e-05
2023-12-15 01:08:10 INFO     	 * (global step 6000: loss: 0.3875330686569214, lr: 5e-05
2023-12-15 01:08:18 INFO     	 * (global step 6050: loss: 0.41616472601890564, lr: 5e-05
2023-12-15 01:08:26 INFO     	 * (global step 6100: loss: 0.2893667593598366, lr: 5e-05
2023-12-15 01:08:34 INFO     	 * (global step 6150: loss: 0.3935631066560745, lr: 5e-05
2023-12-15 01:08:42 INFO     	 * (global step 6200: loss: 0.4542161673307419, lr: 5e-05
2023-12-15 01:08:44 INFO     [epoch 9/15] average loss: 0.368, lr: 5e-05
2023-12-15 01:08:44 INFO     saving model related files
2023-12-15 01:08:44 INFO     saving model
2023-12-15 01:08:45 INFO     saving tokenizer
2023-12-15 01:08:45 INFO     saving optimizer
2023-12-15 01:08:46 INFO     remove old optimizer files
2023-12-15 01:08:46 INFO     complete training: model ckpt was saved at small_trained_ckpt/model_uramvg
2023-12-15 01:08:46 INFO     ## 1st RUN: Configuration 8/12 ##
2023-12-15 01:08:46 INFO     initialize model trainer
2023-12-15 01:08:46 INFO     initialize checkpoint at small_trained_ckpt/model_nxaqhy
2023-12-15 01:08:46 INFO     hyperparameters
2023-12-15 01:08:46 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 01:08:46 INFO     	 * dataset_name: default
2023-12-15 01:08:46 INFO     	 * input_types: ['paragraph']
2023-12-15 01:08:46 INFO     	 * output_types: ['questions_answers']
2023-12-15 01:08:46 INFO     	 * prefix_types: ['qag']
2023-12-15 01:08:46 INFO     	 * model: t5-small
2023-12-15 01:08:46 INFO     	 * max_length: 512
2023-12-15 01:08:46 INFO     	 * max_length_output: 512
2023-12-15 01:08:46 INFO     	 * epoch: 15
2023-12-15 01:08:46 INFO     	 * batch: 2
2023-12-15 01:08:46 INFO     	 * lr: 1e-05
2023-12-15 01:08:46 INFO     	 * fp16: False
2023-12-15 01:08:46 INFO     	 * random_seed: 1
2023-12-15 01:08:46 INFO     	 * gradient_accumulation_steps: 4
2023-12-15 01:08:46 INFO     	 * label_smoothing: 0.15
2023-12-15 01:08:46 INFO     initialize checkpoint with t5-small
2023-12-15 01:08:47 INFO     use spaCy answer extraction model: positionrank
2023-12-15 01:08:48 INFO     Model `t5-small`
2023-12-15 01:08:48 INFO     	 * Num of GPU in use: 1
2023-12-15 01:08:48 INFO     	 * Prefix: True
2023-12-15 01:08:48 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 01:08:48 INFO     dataset preprocessing
2023-12-15 01:08:49 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 01:08:49 INFO     start model training
2023-12-15 01:09:05 INFO     	 * (global step 50: loss: 4.7254626750946045, lr: 1e-05
2023-12-15 01:09:20 INFO     	 * (global step 100: loss: 2.194245398044586, lr: 1e-05
2023-12-15 01:09:36 INFO     	 * (global step 150: loss: 2.079683154821396, lr: 1e-05
2023-12-15 01:09:52 INFO     	 * (global step 200: loss: 1.533323973417282, lr: 1e-05
2023-12-15 01:10:07 INFO     	 * (global step 250: loss: 1.69903364777565, lr: 1e-05
2023-12-15 01:10:23 INFO     	 * (global step 300: loss: 1.1990822851657867, lr: 1e-05
2023-12-15 01:10:26 INFO     [epoch 0/15] average loss: 2.854, lr: 1e-05
2023-12-15 01:10:26 INFO     saving model related files
2023-12-15 01:10:26 INFO     saving model
2023-12-15 01:10:27 INFO     saving tokenizer
2023-12-15 01:10:27 INFO     saving optimizer
2023-12-15 01:10:28 INFO     remove old optimizer files
2023-12-15 01:10:40 INFO     	 * (global step 350: loss: 0.9082247316837311, lr: 1e-05
2023-12-15 01:10:56 INFO     	 * (global step 400: loss: 0.9586096405982971, lr: 1e-05
2023-12-15 01:11:12 INFO     	 * (global step 450: loss: 0.7858358174562454, lr: 1e-05
2023-12-15 01:11:27 INFO     	 * (global step 500: loss: 0.7877277582883835, lr: 1e-05
2023-12-15 01:11:43 INFO     	 * (global step 550: loss: 0.8163514733314514, lr: 1e-05
2023-12-15 01:11:59 INFO     	 * (global step 600: loss: 0.7933409959077835, lr: 1e-05
2023-12-15 01:12:05 INFO     [epoch 1/15] average loss: 0.967, lr: 1e-05
2023-12-15 01:12:05 INFO     saving model related files
2023-12-15 01:12:05 INFO     saving model
2023-12-15 01:12:06 INFO     saving tokenizer
2023-12-15 01:12:06 INFO     saving optimizer
2023-12-15 01:12:07 INFO     remove old optimizer files
2023-12-15 01:12:16 INFO     	 * (global step 650: loss: 1.0206195712089539, lr: 1e-05
2023-12-15 01:12:32 INFO     	 * (global step 700: loss: 0.7090944647789001, lr: 1e-05
2023-12-15 01:12:47 INFO     	 * (global step 750: loss: 0.8640312105417252, lr: 1e-05
2023-12-15 01:13:03 INFO     	 * (global step 800: loss: 0.9867407381534576, lr: 1e-05
2023-12-15 01:13:19 INFO     	 * (global step 850: loss: 0.6101210564374924, lr: 1e-05
2023-12-15 01:13:34 INFO     	 * (global step 900: loss: 0.5985132977366447, lr: 1e-05
2023-12-15 01:13:44 INFO     [epoch 2/15] average loss: 0.765, lr: 1e-05
2023-12-15 01:13:44 INFO     saving model related files
2023-12-15 01:13:44 INFO     saving model
2023-12-15 01:13:44 INFO     saving tokenizer
2023-12-15 01:13:45 INFO     saving optimizer
2023-12-15 01:13:46 INFO     remove old optimizer files
2023-12-15 01:13:52 INFO     	 * (global step 950: loss: 0.8993737697601318, lr: 1e-05
2023-12-15 01:14:08 INFO     	 * (global step 1000: loss: 0.720478281378746, lr: 1e-05
2023-12-15 01:14:23 INFO     	 * (global step 1050: loss: 0.7046075388789177, lr: 1e-05
2023-12-15 01:14:39 INFO     	 * (global step 1100: loss: 0.566312626004219, lr: 1e-05
2023-12-15 01:14:54 INFO     	 * (global step 1150: loss: 0.7824659198522568, lr: 1e-05
2023-12-15 01:15:10 INFO     	 * (global step 1200: loss: 0.591200016438961, lr: 1e-05
2023-12-15 01:15:23 INFO     [epoch 3/15] average loss: 0.68, lr: 1e-05
2023-12-15 01:15:23 INFO     saving model related files
2023-12-15 01:15:23 INFO     saving model
2023-12-15 01:15:23 INFO     saving tokenizer
2023-12-15 01:15:23 INFO     saving optimizer
2023-12-15 01:15:24 INFO     remove old optimizer files
2023-12-15 01:15:28 INFO     	 * (global step 1250: loss: 0.6079748719930649, lr: 1e-05
2023-12-15 01:15:43 INFO     	 * (global step 1300: loss: 0.8639411479234695, lr: 1e-05
2023-12-15 01:15:59 INFO     	 * (global step 1350: loss: 0.5073754116892815, lr: 1e-05
2023-12-15 01:16:15 INFO     	 * (global step 1400: loss: 0.49494417011737823, lr: 1e-05
2023-12-15 01:16:30 INFO     	 * (global step 1450: loss: 0.6688325554132462, lr: 1e-05
2023-12-15 01:16:46 INFO     	 * (global step 1500: loss: 0.5187273919582367, lr: 1e-05
2023-12-15 01:17:02 INFO     	 * (global step 1550: loss: 0.8394150584936142, lr: 1e-05
2023-12-15 01:17:02 INFO     [epoch 4/15] average loss: 0.627, lr: 1e-05
2023-12-15 01:17:02 INFO     saving model related files
2023-12-15 01:17:02 INFO     saving model
2023-12-15 01:17:02 INFO     saving tokenizer
2023-12-15 01:17:02 INFO     saving optimizer
2023-12-15 01:17:03 INFO     remove old optimizer files
2023-12-15 01:17:19 INFO     	 * (global step 1600: loss: 0.6692223399877548, lr: 1e-05
2023-12-15 01:17:35 INFO     	 * (global step 1650: loss: 0.5586731433868408, lr: 1e-05
2023-12-15 01:17:50 INFO     	 * (global step 1700: loss: 0.5567676275968552, lr: 1e-05
2023-12-15 01:18:06 INFO     	 * (global step 1750: loss: 0.447565957903862, lr: 1e-05
2023-12-15 01:18:22 INFO     	 * (global step 1800: loss: 0.5063708946108818, lr: 1e-05
2023-12-15 01:18:37 INFO     	 * (global step 1850: loss: 0.6014319434762001, lr: 1e-05
2023-12-15 01:18:41 INFO     [epoch 5/15] average loss: 0.593, lr: 1e-05
2023-12-15 01:18:41 INFO     saving model related files
2023-12-15 01:18:41 INFO     saving model
2023-12-15 01:18:41 INFO     saving tokenizer
2023-12-15 01:18:41 INFO     saving optimizer
2023-12-15 01:18:42 INFO     remove old optimizer files
2023-12-15 01:18:55 INFO     	 * (global step 1900: loss: 0.49573301523923874, lr: 1e-05
2023-12-15 01:19:11 INFO     	 * (global step 1950: loss: 0.4517539292573929, lr: 1e-05
2023-12-15 01:19:26 INFO     	 * (global step 2000: loss: 0.6298652738332748, lr: 1e-05
2023-12-15 01:19:42 INFO     	 * (global step 2050: loss: 0.587729424238205, lr: 1e-05
2023-12-15 01:19:58 INFO     	 * (global step 2100: loss: 0.488722987473011, lr: 1e-05
2023-12-15 01:20:13 INFO     	 * (global step 2150: loss: 0.5860016867518425, lr: 1e-05
2023-12-15 01:20:20 INFO     [epoch 6/15] average loss: 0.566, lr: 1e-05
2023-12-15 01:20:20 INFO     saving model related files
2023-12-15 01:20:20 INFO     saving model
2023-12-15 01:20:20 INFO     saving tokenizer
2023-12-15 01:20:20 INFO     saving optimizer
2023-12-15 01:20:21 INFO     remove old optimizer files
2023-12-15 01:20:31 INFO     	 * (global step 2200: loss: 0.6470678299665451, lr: 1e-05
2023-12-15 01:20:46 INFO     	 * (global step 2250: loss: 0.48236697912216187, lr: 1e-05
2023-12-15 01:21:02 INFO     	 * (global step 2300: loss: 0.45523447543382645, lr: 1e-05
2023-12-15 01:21:18 INFO     	 * (global step 2350: loss: 0.5895750522613525, lr: 1e-05
2023-12-15 01:21:33 INFO     	 * (global step 2400: loss: 0.389511376619339, lr: 1e-05
2023-12-15 01:21:49 INFO     	 * (global step 2450: loss: 0.571350023150444, lr: 1e-05
2023-12-15 01:21:59 INFO     [epoch 7/15] average loss: 0.542, lr: 1e-05
2023-12-15 01:21:59 INFO     saving model related files
2023-12-15 01:21:59 INFO     saving model
2023-12-15 01:21:59 INFO     saving tokenizer
2023-12-15 01:21:59 INFO     saving optimizer
2023-12-15 01:22:00 INFO     remove old optimizer files
2023-12-15 01:22:07 INFO     	 * (global step 2500: loss: 0.5070558041334152, lr: 1e-05
2023-12-15 01:22:22 INFO     	 * (global step 2550: loss: 0.597995787858963, lr: 1e-05
2023-12-15 01:22:38 INFO     	 * (global step 2600: loss: 0.5401275604963303, lr: 1e-05
2023-12-15 01:22:54 INFO     	 * (global step 2650: loss: 0.5940504372119904, lr: 1e-05
2023-12-15 01:23:09 INFO     	 * (global step 2700: loss: 0.46888844668865204, lr: 1e-05
2023-12-15 01:23:25 INFO     	 * (global step 2750: loss: 0.45623136311769485, lr: 1e-05
2023-12-15 01:23:38 INFO     [epoch 8/15] average loss: 0.525, lr: 1e-05
2023-12-15 01:23:38 INFO     saving model related files
2023-12-15 01:23:38 INFO     saving model
2023-12-15 01:23:38 INFO     saving tokenizer
2023-12-15 01:23:38 INFO     saving optimizer
2023-12-15 01:23:40 INFO     remove old optimizer files
2023-12-15 01:23:43 INFO     	 * (global step 2800: loss: 0.5970074087381363, lr: 1e-05
2023-12-15 01:23:59 INFO     	 * (global step 2850: loss: 0.6758576259016991, lr: 1e-05
2023-12-15 01:24:14 INFO     	 * (global step 2900: loss: 0.5366736277937889, lr: 1e-05
2023-12-15 01:24:30 INFO     	 * (global step 2950: loss: 0.5509486272931099, lr: 1e-05
2023-12-15 01:24:45 INFO     	 * (global step 3000: loss: 0.4430784732103348, lr: 1e-05
2023-12-15 01:25:01 INFO     	 * (global step 3050: loss: 0.4148724526166916, lr: 1e-05
2023-12-15 01:25:17 INFO     	 * (global step 3100: loss: 0.5695320293307304, lr: 1e-05
2023-12-15 01:25:17 INFO     [epoch 9/15] average loss: 0.511, lr: 1e-05
2023-12-15 01:25:17 INFO     saving model related files
2023-12-15 01:25:17 INFO     saving model
2023-12-15 01:25:18 INFO     saving tokenizer
2023-12-15 01:25:18 INFO     saving optimizer
2023-12-15 01:25:19 INFO     remove old optimizer files
2023-12-15 01:25:19 INFO     complete training: model ckpt was saved at small_trained_ckpt/model_nxaqhy
2023-12-15 01:25:19 INFO     ## 1st RUN: Configuration 9/12 ##
2023-12-15 01:25:19 INFO     initialize model trainer
2023-12-15 01:25:19 INFO     initialize checkpoint at small_trained_ckpt/model_oprhlh
2023-12-15 01:25:19 INFO     hyperparameters
2023-12-15 01:25:19 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 01:25:19 INFO     	 * dataset_name: default
2023-12-15 01:25:19 INFO     	 * input_types: ['paragraph']
2023-12-15 01:25:19 INFO     	 * output_types: ['questions_answers']
2023-12-15 01:25:19 INFO     	 * prefix_types: ['qag']
2023-12-15 01:25:19 INFO     	 * model: t5-small
2023-12-15 01:25:19 INFO     	 * max_length: 512
2023-12-15 01:25:19 INFO     	 * max_length_output: 512
2023-12-15 01:25:19 INFO     	 * epoch: 15
2023-12-15 01:25:19 INFO     	 * batch: 2
2023-12-15 01:25:19 INFO     	 * lr: 1e-05
2023-12-15 01:25:19 INFO     	 * fp16: False
2023-12-15 01:25:19 INFO     	 * random_seed: 1
2023-12-15 01:25:19 INFO     	 * gradient_accumulation_steps: 2
2023-12-15 01:25:19 INFO     	 * label_smoothing: 0.15
2023-12-15 01:25:19 INFO     initialize checkpoint with t5-small
2023-12-15 01:25:20 INFO     use spaCy answer extraction model: positionrank
2023-12-15 01:25:21 INFO     Model `t5-small`
2023-12-15 01:25:21 INFO     	 * Num of GPU in use: 1
2023-12-15 01:25:21 INFO     	 * Prefix: True
2023-12-15 01:25:21 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 01:25:21 INFO     dataset preprocessing
2023-12-15 01:25:22 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 01:25:22 INFO     start model training
2023-12-15 01:25:30 INFO     	 * (global step 50: loss: 4.98059868812561, lr: 1e-05
2023-12-15 01:25:38 INFO     	 * (global step 100: loss: 2.327609419822693, lr: 1e-05
2023-12-15 01:25:46 INFO     	 * (global step 150: loss: 1.8425626158714294, lr: 1e-05
2023-12-15 01:25:54 INFO     	 * (global step 200: loss: 1.6577856540679932, lr: 1e-05
2023-12-15 01:26:03 INFO     	 * (global step 250: loss: 1.5848309993743896, lr: 1e-05
2023-12-15 01:26:11 INFO     	 * (global step 300: loss: 1.2796490788459778, lr: 1e-05
2023-12-15 01:26:19 INFO     	 * (global step 350: loss: 1.356551706790924, lr: 1e-05
2023-12-15 01:26:27 INFO     	 * (global step 400: loss: 1.0361078381538391, lr: 1e-05
2023-12-15 01:26:35 INFO     	 * (global step 450: loss: 1.040247619152069, lr: 1e-05
2023-12-15 01:26:43 INFO     	 * (global step 500: loss: 1.4920347929000854, lr: 1e-05
2023-12-15 01:26:51 INFO     	 * (global step 550: loss: 0.863457053899765, lr: 1e-05
2023-12-15 01:26:59 INFO     	 * (global step 600: loss: 0.8804566562175751, lr: 1e-05
2023-12-15 01:27:03 INFO     [epoch 0/15] average loss: 2.03, lr: 1e-05
2023-12-15 01:27:03 INFO     saving model related files
2023-12-15 01:27:03 INFO     saving model
2023-12-15 01:27:03 INFO     saving tokenizer
2023-12-15 01:27:03 INFO     saving optimizer
2023-12-15 01:27:04 INFO     remove old optimizer files
2023-12-15 01:27:09 INFO     	 * (global step 650: loss: 0.9029726088047028, lr: 1e-05
2023-12-15 01:27:17 INFO     	 * (global step 700: loss: 0.7651035785675049, lr: 1e-05
2023-12-15 01:27:25 INFO     	 * (global step 750: loss: 0.9414747357368469, lr: 1e-05
2023-12-15 01:27:33 INFO     	 * (global step 800: loss: 0.7147819399833679, lr: 1e-05
2023-12-15 01:27:41 INFO     	 * (global step 850: loss: 0.529510959982872, lr: 1e-05
2023-12-15 01:27:49 INFO     	 * (global step 900: loss: 0.5772595554590225, lr: 1e-05
2023-12-15 01:27:58 INFO     	 * (global step 950: loss: 0.7951968610286713, lr: 1e-05
2023-12-15 01:28:06 INFO     	 * (global step 1000: loss: 0.5417391806840897, lr: 1e-05
2023-12-15 01:28:14 INFO     	 * (global step 1050: loss: 0.7522474527359009, lr: 1e-05
2023-12-15 01:28:22 INFO     	 * (global step 1100: loss: 0.5840396881103516, lr: 1e-05
2023-12-15 01:28:30 INFO     	 * (global step 1150: loss: 1.038498342037201, lr: 1e-05
2023-12-15 01:28:38 INFO     	 * (global step 1200: loss: 0.7101102769374847, lr: 1e-05
2023-12-15 01:28:45 INFO     [epoch 1/15] average loss: 0.764, lr: 1e-05
2023-12-15 01:28:45 INFO     saving model related files
2023-12-15 01:28:45 INFO     saving model
2023-12-15 01:28:45 INFO     saving tokenizer
2023-12-15 01:28:45 INFO     saving optimizer
2023-12-15 01:28:47 INFO     remove old optimizer files
2023-12-15 01:28:48 INFO     	 * (global step 1250: loss: 0.8279289901256561, lr: 1e-05
2023-12-15 01:28:56 INFO     	 * (global step 1300: loss: 0.7161503285169601, lr: 1e-05
2023-12-15 01:29:04 INFO     	 * (global step 1350: loss: 0.7306906282901764, lr: 1e-05
2023-12-15 01:29:12 INFO     	 * (global step 1400: loss: 0.7189508378505707, lr: 1e-05
2023-12-15 01:29:20 INFO     	 * (global step 1450: loss: 0.8312117457389832, lr: 1e-05
2023-12-15 01:29:28 INFO     	 * (global step 1500: loss: 0.46911299228668213, lr: 1e-05
2023-12-15 01:29:36 INFO     	 * (global step 1550: loss: 0.6531570255756378, lr: 1e-05
2023-12-15 01:29:45 INFO     	 * (global step 1600: loss: 0.5281214565038681, lr: 1e-05
2023-12-15 01:29:53 INFO     	 * (global step 1650: loss: 0.528250128030777, lr: 1e-05
2023-12-15 01:30:01 INFO     	 * (global step 1700: loss: 0.5499285161495209, lr: 1e-05
2023-12-15 01:30:09 INFO     	 * (global step 1750: loss: 0.5536015331745148, lr: 1e-05
2023-12-15 01:30:17 INFO     	 * (global step 1800: loss: 0.46868860721588135, lr: 1e-05
2023-12-15 01:30:25 INFO     	 * (global step 1850: loss: 0.5959600508213043, lr: 1e-05
2023-12-15 01:30:27 INFO     [epoch 2/15] average loss: 0.642, lr: 1e-05
2023-12-15 01:30:27 INFO     saving model related files
2023-12-15 01:30:27 INFO     saving model
2023-12-15 01:30:28 INFO     saving tokenizer
2023-12-15 01:30:28 INFO     saving optimizer
2023-12-15 01:30:29 INFO     remove old optimizer files
2023-12-15 01:30:35 INFO     	 * (global step 1900: loss: 0.7435204386711121, lr: 1e-05
2023-12-15 01:30:43 INFO     	 * (global step 1950: loss: 0.5632871985435486, lr: 1e-05
2023-12-15 01:30:51 INFO     	 * (global step 2000: loss: 0.5177041590213776, lr: 1e-05
2023-12-15 01:30:59 INFO     	 * (global step 2050: loss: 0.6500732004642487, lr: 1e-05
2023-12-15 01:31:07 INFO     	 * (global step 2100: loss: 0.7548327147960663, lr: 1e-05
2023-12-15 01:31:15 INFO     	 * (global step 2150: loss: 0.5402985662221909, lr: 1e-05
2023-12-15 01:31:24 INFO     	 * (global step 2200: loss: 0.4572140723466873, lr: 1e-05
2023-12-15 01:31:32 INFO     	 * (global step 2250: loss: 0.7637749910354614, lr: 1e-05
2023-12-15 01:31:40 INFO     	 * (global step 2300: loss: 0.5777876377105713, lr: 1e-05
2023-12-15 01:31:48 INFO     	 * (global step 2350: loss: 0.51038858294487, lr: 1e-05
2023-12-15 01:31:56 INFO     	 * (global step 2400: loss: 0.5471776872873306, lr: 1e-05
2023-12-15 01:32:04 INFO     	 * (global step 2450: loss: 0.45935986936092377, lr: 1e-05
2023-12-15 01:32:10 INFO     [epoch 3/15] average loss: 0.581, lr: 1e-05
2023-12-15 01:32:10 INFO     saving model related files
2023-12-15 01:32:10 INFO     saving model
2023-12-15 01:32:10 INFO     saving tokenizer
2023-12-15 01:32:10 INFO     saving optimizer
2023-12-15 01:32:11 INFO     remove old optimizer files
2023-12-15 01:32:14 INFO     	 * (global step 2500: loss: 0.6532197594642639, lr: 1e-05
2023-12-15 01:32:22 INFO     	 * (global step 2550: loss: 0.47556959092617035, lr: 1e-05
2023-12-15 01:32:30 INFO     	 * (global step 2600: loss: 0.5075017362833023, lr: 1e-05
2023-12-15 01:32:38 INFO     	 * (global step 2650: loss: 0.5564220249652863, lr: 1e-05
2023-12-15 01:32:46 INFO     	 * (global step 2700: loss: 0.4229992926120758, lr: 1e-05
2023-12-15 01:32:55 INFO     	 * (global step 2750: loss: 0.47293585538864136, lr: 1e-05
2023-12-15 01:33:03 INFO     	 * (global step 2800: loss: 0.6089563965797424, lr: 1e-05
2023-12-15 01:33:11 INFO     	 * (global step 2850: loss: 0.5932329744100571, lr: 1e-05
2023-12-15 01:33:19 INFO     	 * (global step 2900: loss: 0.5312206000089645, lr: 1e-05
2023-12-15 01:33:27 INFO     	 * (global step 2950: loss: 0.5219980925321579, lr: 1e-05
2023-12-15 01:33:35 INFO     	 * (global step 3000: loss: 0.7048918008804321, lr: 1e-05
2023-12-15 01:33:43 INFO     	 * (global step 3050: loss: 0.5298791378736496, lr: 1e-05
2023-12-15 01:33:51 INFO     	 * (global step 3100: loss: 0.5650434046983719, lr: 1e-05
2023-12-15 01:33:52 INFO     [epoch 4/15] average loss: 0.539, lr: 1e-05
2023-12-15 01:33:52 INFO     saving model related files
2023-12-15 01:33:52 INFO     saving model
2023-12-15 01:33:53 INFO     saving tokenizer
2023-12-15 01:33:53 INFO     saving optimizer
2023-12-15 01:33:54 INFO     remove old optimizer files
2023-12-15 01:34:01 INFO     	 * (global step 3150: loss: 0.5075997561216354, lr: 1e-05
2023-12-15 01:34:09 INFO     	 * (global step 3200: loss: 0.5758163034915924, lr: 1e-05
2023-12-15 01:34:17 INFO     	 * (global step 3250: loss: 0.49717435240745544, lr: 1e-05
2023-12-15 01:34:26 INFO     	 * (global step 3300: loss: 0.40033067762851715, lr: 1e-05
2023-12-15 01:34:34 INFO     	 * (global step 3350: loss: 0.6326259672641754, lr: 1e-05
2023-12-15 01:34:42 INFO     	 * (global step 3400: loss: 0.6780581027269363, lr: 1e-05
2023-12-15 01:34:50 INFO     	 * (global step 3450: loss: 0.4069731831550598, lr: 1e-05
2023-12-15 01:34:58 INFO     	 * (global step 3500: loss: 0.2846577763557434, lr: 1e-05
2023-12-15 01:35:06 INFO     	 * (global step 3550: loss: 0.6021698415279388, lr: 1e-05
2023-12-15 01:35:14 INFO     	 * (global step 3600: loss: 0.6108218878507614, lr: 1e-05
2023-12-15 01:35:22 INFO     	 * (global step 3650: loss: 0.5312495529651642, lr: 1e-05
2023-12-15 01:35:30 INFO     	 * (global step 3700: loss: 0.44077667593955994, lr: 1e-05
2023-12-15 01:35:35 INFO     [epoch 5/15] average loss: 0.513, lr: 1e-05
2023-12-15 01:35:35 INFO     saving model related files
2023-12-15 01:35:35 INFO     saving model
2023-12-15 01:35:35 INFO     saving tokenizer
2023-12-15 01:35:35 INFO     saving optimizer
2023-12-15 01:35:36 INFO     remove old optimizer files
2023-12-15 01:35:40 INFO     	 * (global step 3750: loss: 0.22861894965171814, lr: 1e-05
2023-12-15 01:35:48 INFO     	 * (global step 3800: loss: 0.529342383146286, lr: 1e-05
2023-12-15 01:35:56 INFO     	 * (global step 3850: loss: 0.3143666684627533, lr: 1e-05
2023-12-15 01:36:04 INFO     	 * (global step 3900: loss: 0.42237481474876404, lr: 1e-05
2023-12-15 01:36:12 INFO     	 * (global step 3950: loss: 0.40376314520835876, lr: 1e-05
2023-12-15 01:36:20 INFO     	 * (global step 4000: loss: 0.5483021587133408, lr: 1e-05
2023-12-15 01:36:29 INFO     	 * (global step 4050: loss: 0.5681079626083374, lr: 1e-05
2023-12-15 01:36:37 INFO     	 * (global step 4100: loss: 0.5554275065660477, lr: 1e-05
2023-12-15 01:36:45 INFO     	 * (global step 4150: loss: 0.5045499503612518, lr: 1e-05
2023-12-15 01:36:53 INFO     	 * (global step 4200: loss: 0.4330688416957855, lr: 1e-05
2023-12-15 01:37:01 INFO     	 * (global step 4250: loss: 0.36881114542484283, lr: 1e-05
2023-12-15 01:37:09 INFO     	 * (global step 4300: loss: 0.6357844471931458, lr: 1e-05
2023-12-15 01:37:17 INFO     [epoch 6/15] average loss: 0.496, lr: 1e-05
2023-12-15 01:37:17 INFO     saving model related files
2023-12-15 01:37:17 INFO     saving model
2023-12-15 01:37:17 INFO     saving tokenizer
2023-12-15 01:37:17 INFO     saving optimizer
2023-12-15 01:37:18 INFO     remove old optimizer files
2023-12-15 01:37:19 INFO     	 * (global step 4350: loss: 0.4551563262939453, lr: 1e-05
2023-12-15 01:37:27 INFO     	 * (global step 4400: loss: 0.37784498929977417, lr: 1e-05
2023-12-15 01:37:35 INFO     	 * (global step 4450: loss: 0.3563506007194519, lr: 1e-05
2023-12-15 01:37:43 INFO     	 * (global step 4500: loss: 0.44153420627117157, lr: 1e-05
2023-12-15 01:37:51 INFO     	 * (global step 4550: loss: 0.3592182397842407, lr: 1e-05
2023-12-15 01:37:59 INFO     	 * (global step 4600: loss: 0.5075528025627136, lr: 1e-05
2023-12-15 01:38:07 INFO     	 * (global step 4650: loss: 0.770748108625412, lr: 1e-05
2023-12-15 01:38:16 INFO     	 * (global step 4700: loss: 0.5968066155910492, lr: 1e-05
2023-12-15 01:38:24 INFO     	 * (global step 4750: loss: 0.5091223865747452, lr: 1e-05
2023-12-15 01:38:32 INFO     	 * (global step 4800: loss: 0.4680040329694748, lr: 1e-05
2023-12-15 01:38:40 INFO     	 * (global step 4850: loss: 0.3921704441308975, lr: 1e-05
2023-12-15 01:38:48 INFO     	 * (global step 4900: loss: 0.4069429785013199, lr: 1e-05
2023-12-15 01:38:56 INFO     	 * (global step 4950: loss: 0.3111163601279259, lr: 1e-05
2023-12-15 01:38:59 INFO     [epoch 7/15] average loss: 0.481, lr: 1e-05
2023-12-15 01:38:59 INFO     saving model related files
2023-12-15 01:38:59 INFO     saving model
2023-12-15 01:39:00 INFO     saving tokenizer
2023-12-15 01:39:00 INFO     saving optimizer
2023-12-15 01:39:01 INFO     remove old optimizer files
2023-12-15 01:39:06 INFO     	 * (global step 5000: loss: 0.49246031045913696, lr: 1e-05
2023-12-15 01:39:14 INFO     	 * (global step 5050: loss: 0.4134957939386368, lr: 1e-05
2023-12-15 01:39:22 INFO     	 * (global step 5100: loss: 0.41068024933338165, lr: 1e-05
2023-12-15 01:39:30 INFO     	 * (global step 5150: loss: 0.2787094935774803, lr: 1e-05
2023-12-15 01:39:38 INFO     	 * (global step 5200: loss: 0.2696603611111641, lr: 1e-05
2023-12-15 01:39:46 INFO     	 * (global step 5250: loss: 0.8614557534456253, lr: 1e-05
2023-12-15 01:39:54 INFO     	 * (global step 5300: loss: 0.34457969665527344, lr: 1e-05
2023-12-15 01:40:03 INFO     	 * (global step 5350: loss: 0.4644717872142792, lr: 1e-05
2023-12-15 01:40:11 INFO     	 * (global step 5400: loss: 0.4251183867454529, lr: 1e-05
2023-12-15 01:40:19 INFO     	 * (global step 5450: loss: 0.4094025045633316, lr: 1e-05
2023-12-15 01:40:27 INFO     	 * (global step 5500: loss: 0.4556093364953995, lr: 1e-05
2023-12-15 01:40:35 INFO     	 * (global step 5550: loss: 0.38648639619350433, lr: 1e-05
2023-12-15 01:40:41 INFO     [epoch 8/15] average loss: 0.469, lr: 1e-05
2023-12-15 01:40:41 INFO     saving model related files
2023-12-15 01:40:41 INFO     saving model
2023-12-15 01:40:42 INFO     saving tokenizer
2023-12-15 01:40:42 INFO     saving optimizer
2023-12-15 01:40:43 INFO     remove old optimizer files
2023-12-15 01:40:45 INFO     	 * (global step 5600: loss: 0.6416730284690857, lr: 1e-05
2023-12-15 01:40:53 INFO     	 * (global step 5650: loss: 0.5773099958896637, lr: 1e-05
2023-12-15 01:41:01 INFO     	 * (global step 5700: loss: 0.3354850709438324, lr: 1e-05
2023-12-15 01:41:09 INFO     	 * (global step 5750: loss: 0.4181388393044472, lr: 1e-05
2023-12-15 01:41:17 INFO     	 * (global step 5800: loss: 0.4306982159614563, lr: 1e-05
2023-12-15 01:41:25 INFO     	 * (global step 5850: loss: 0.5852713882923126, lr: 1e-05
2023-12-15 01:41:33 INFO     	 * (global step 5900: loss: 0.3560587465763092, lr: 1e-05
2023-12-15 01:41:41 INFO     	 * (global step 5950: loss: 0.43852244317531586, lr: 1e-05
2023-12-15 01:41:49 INFO     	 * (global step 6000: loss: 0.46476687490940094, lr: 1e-05
2023-12-15 01:41:58 INFO     	 * (global step 6050: loss: 0.5273232758045197, lr: 1e-05
2023-12-15 01:42:06 INFO     	 * (global step 6100: loss: 0.3511108160018921, lr: 1e-05
2023-12-15 01:42:14 INFO     	 * (global step 6150: loss: 0.49441832304000854, lr: 1e-05
2023-12-15 01:42:22 INFO     	 * (global step 6200: loss: 0.5331268906593323, lr: 1e-05
2023-12-15 01:42:24 INFO     [epoch 9/15] average loss: 0.46, lr: 1e-05
2023-12-15 01:42:24 INFO     saving model related files
2023-12-15 01:42:24 INFO     saving model
2023-12-15 01:42:24 INFO     saving tokenizer
2023-12-15 01:42:24 INFO     saving optimizer
2023-12-15 01:42:25 INFO     remove old optimizer files
2023-12-15 01:42:25 INFO     complete training: model ckpt was saved at small_trained_ckpt/model_oprhlh
2023-12-15 01:42:25 INFO     ## 1st RUN: Configuration 10/12 ##
2023-12-15 01:42:25 INFO     initialize model trainer
2023-12-15 01:42:25 INFO     initialize checkpoint at small_trained_ckpt/model_vhyoja
2023-12-15 01:42:25 INFO     hyperparameters
2023-12-15 01:42:25 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 01:42:25 INFO     	 * dataset_name: default
2023-12-15 01:42:25 INFO     	 * input_types: ['paragraph']
2023-12-15 01:42:25 INFO     	 * output_types: ['questions_answers']
2023-12-15 01:42:25 INFO     	 * prefix_types: ['qag']
2023-12-15 01:42:25 INFO     	 * model: t5-small
2023-12-15 01:42:25 INFO     	 * max_length: 512
2023-12-15 01:42:25 INFO     	 * max_length_output: 512
2023-12-15 01:42:25 INFO     	 * epoch: 15
2023-12-15 01:42:25 INFO     	 * batch: 2
2023-12-15 01:42:25 INFO     	 * lr: 1e-05
2023-12-15 01:42:25 INFO     	 * fp16: False
2023-12-15 01:42:25 INFO     	 * random_seed: 1
2023-12-15 01:42:25 INFO     	 * gradient_accumulation_steps: 4
2023-12-15 01:42:25 INFO     	 * label_smoothing: 0.0
2023-12-15 01:42:25 INFO     initialize checkpoint with t5-small
2023-12-15 01:42:27 INFO     use spaCy answer extraction model: positionrank
2023-12-15 01:42:27 INFO     Model `t5-small`
2023-12-15 01:42:27 INFO     	 * Num of GPU in use: 1
2023-12-15 01:42:27 INFO     	 * Prefix: True
2023-12-15 01:42:27 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 01:42:27 INFO     dataset preprocessing
2023-12-15 01:42:28 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 01:42:29 INFO     start model training
2023-12-15 01:42:45 INFO     	 * (global step 50: loss: 4.7254626750946045, lr: 1e-05
2023-12-15 01:43:00 INFO     	 * (global step 100: loss: 2.194245398044586, lr: 1e-05
2023-12-15 01:43:16 INFO     	 * (global step 150: loss: 2.079683154821396, lr: 1e-05
2023-12-15 01:43:31 INFO     	 * (global step 200: loss: 1.533323973417282, lr: 1e-05
2023-12-15 01:43:47 INFO     	 * (global step 250: loss: 1.69903364777565, lr: 1e-05
2023-12-15 01:44:03 INFO     	 * (global step 300: loss: 1.1990822851657867, lr: 1e-05
2023-12-15 01:44:06 INFO     [epoch 0/15] average loss: 2.854, lr: 1e-05
2023-12-15 01:44:06 INFO     saving model related files
2023-12-15 01:44:06 INFO     saving model
2023-12-15 01:44:07 INFO     saving tokenizer
2023-12-15 01:44:07 INFO     saving optimizer
2023-12-15 01:44:08 INFO     remove old optimizer files
2023-12-15 01:44:20 INFO     	 * (global step 350: loss: 0.9082247316837311, lr: 1e-05
2023-12-15 01:44:36 INFO     	 * (global step 400: loss: 0.9586096405982971, lr: 1e-05
2023-12-15 01:44:51 INFO     	 * (global step 450: loss: 0.7858358174562454, lr: 1e-05
2023-12-15 01:45:07 INFO     	 * (global step 500: loss: 0.7877277582883835, lr: 1e-05
2023-12-15 01:45:23 INFO     	 * (global step 550: loss: 0.8163514733314514, lr: 1e-05
2023-12-15 01:45:38 INFO     	 * (global step 600: loss: 0.7933409959077835, lr: 1e-05
2023-12-15 01:45:45 INFO     [epoch 1/15] average loss: 0.967, lr: 1e-05
2023-12-15 01:45:45 INFO     saving model related files
2023-12-15 01:45:45 INFO     saving model
2023-12-15 01:45:45 INFO     saving tokenizer
2023-12-15 01:45:45 INFO     saving optimizer
2023-12-15 01:45:46 INFO     remove old optimizer files
2023-12-15 01:45:56 INFO     	 * (global step 650: loss: 1.0206195712089539, lr: 1e-05
2023-12-15 01:46:12 INFO     	 * (global step 700: loss: 0.7090944647789001, lr: 1e-05
2023-12-15 01:46:27 INFO     	 * (global step 750: loss: 0.8640312105417252, lr: 1e-05
2023-12-15 01:46:43 INFO     	 * (global step 800: loss: 0.9867407381534576, lr: 1e-05
2023-12-15 01:46:59 INFO     	 * (global step 850: loss: 0.6101210564374924, lr: 1e-05
2023-12-15 01:47:14 INFO     	 * (global step 900: loss: 0.5985132977366447, lr: 1e-05
2023-12-15 01:47:24 INFO     [epoch 2/15] average loss: 0.765, lr: 1e-05
2023-12-15 01:47:24 INFO     saving model related files
2023-12-15 01:47:24 INFO     saving model
2023-12-15 01:47:25 INFO     saving tokenizer
2023-12-15 01:47:25 INFO     saving optimizer
2023-12-15 01:47:26 INFO     remove old optimizer files
2023-12-15 01:47:32 INFO     	 * (global step 950: loss: 0.8993737697601318, lr: 1e-05
2023-12-15 01:47:48 INFO     	 * (global step 1000: loss: 0.720478281378746, lr: 1e-05
2023-12-15 01:48:03 INFO     	 * (global step 1050: loss: 0.7046075388789177, lr: 1e-05
2023-12-15 01:48:19 INFO     	 * (global step 1100: loss: 0.566312626004219, lr: 1e-05
2023-12-15 01:48:35 INFO     	 * (global step 1150: loss: 0.7824659198522568, lr: 1e-05
2023-12-15 01:48:50 INFO     	 * (global step 1200: loss: 0.591200016438961, lr: 1e-05
2023-12-15 01:49:03 INFO     [epoch 3/15] average loss: 0.68, lr: 1e-05
2023-12-15 01:49:03 INFO     saving model related files
2023-12-15 01:49:03 INFO     saving model
2023-12-15 01:49:03 INFO     saving tokenizer
2023-12-15 01:49:04 INFO     saving optimizer
2023-12-15 01:49:05 INFO     remove old optimizer files
2023-12-15 01:49:08 INFO     	 * (global step 1250: loss: 0.6079748719930649, lr: 1e-05
2023-12-15 01:49:23 INFO     	 * (global step 1300: loss: 0.8639411479234695, lr: 1e-05
2023-12-15 01:49:39 INFO     	 * (global step 1350: loss: 0.5073754116892815, lr: 1e-05
2023-12-15 01:49:55 INFO     	 * (global step 1400: loss: 0.49494417011737823, lr: 1e-05
2023-12-15 01:50:10 INFO     	 * (global step 1450: loss: 0.6688325554132462, lr: 1e-05
2023-12-15 01:50:26 INFO     	 * (global step 1500: loss: 0.5187273919582367, lr: 1e-05
2023-12-15 01:50:42 INFO     	 * (global step 1550: loss: 0.8394150584936142, lr: 1e-05
2023-12-15 01:50:42 INFO     [epoch 4/15] average loss: 0.627, lr: 1e-05
2023-12-15 01:50:42 INFO     saving model related files
2023-12-15 01:50:42 INFO     saving model
2023-12-15 01:50:42 INFO     saving tokenizer
2023-12-15 01:50:42 INFO     saving optimizer
2023-12-15 01:50:44 INFO     remove old optimizer files
2023-12-15 01:50:59 INFO     	 * (global step 1600: loss: 0.6692223399877548, lr: 1e-05
2023-12-15 01:51:15 INFO     	 * (global step 1650: loss: 0.5586731433868408, lr: 1e-05
2023-12-15 01:51:31 INFO     	 * (global step 1700: loss: 0.5567676275968552, lr: 1e-05
2023-12-15 01:51:46 INFO     	 * (global step 1750: loss: 0.447565957903862, lr: 1e-05
2023-12-15 01:52:02 INFO     	 * (global step 1800: loss: 0.5063708946108818, lr: 1e-05
2023-12-15 01:52:18 INFO     	 * (global step 1850: loss: 0.6014319434762001, lr: 1e-05
2023-12-15 01:52:21 INFO     [epoch 5/15] average loss: 0.593, lr: 1e-05
2023-12-15 01:52:21 INFO     saving model related files
2023-12-15 01:52:21 INFO     saving model
2023-12-15 01:52:21 INFO     saving tokenizer
2023-12-15 01:52:21 INFO     saving optimizer
2023-12-15 01:52:22 INFO     remove old optimizer files
2023-12-15 01:52:35 INFO     	 * (global step 1900: loss: 0.49573301523923874, lr: 1e-05
2023-12-15 01:52:51 INFO     	 * (global step 1950: loss: 0.4517539292573929, lr: 1e-05
2023-12-15 01:53:06 INFO     	 * (global step 2000: loss: 0.6298652738332748, lr: 1e-05
2023-12-15 01:53:22 INFO     	 * (global step 2050: loss: 0.587729424238205, lr: 1e-05
2023-12-15 01:53:38 INFO     	 * (global step 2100: loss: 0.488722987473011, lr: 1e-05
2023-12-15 01:53:53 INFO     	 * (global step 2150: loss: 0.5860016867518425, lr: 1e-05
2023-12-15 01:54:00 INFO     [epoch 6/15] average loss: 0.566, lr: 1e-05
2023-12-15 01:54:00 INFO     saving model related files
2023-12-15 01:54:00 INFO     saving model
2023-12-15 01:54:00 INFO     saving tokenizer
2023-12-15 01:54:00 INFO     saving optimizer
2023-12-15 01:54:01 INFO     remove old optimizer files
2023-12-15 01:54:11 INFO     	 * (global step 2200: loss: 0.6470678299665451, lr: 1e-05
2023-12-15 01:54:26 INFO     	 * (global step 2250: loss: 0.48236697912216187, lr: 1e-05
2023-12-15 01:54:42 INFO     	 * (global step 2300: loss: 0.45523447543382645, lr: 1e-05
2023-12-15 01:54:58 INFO     	 * (global step 2350: loss: 0.5895750522613525, lr: 1e-05
2023-12-15 01:55:13 INFO     	 * (global step 2400: loss: 0.389511376619339, lr: 1e-05
2023-12-15 01:55:29 INFO     	 * (global step 2450: loss: 0.571350023150444, lr: 1e-05
2023-12-15 01:55:39 INFO     [epoch 7/15] average loss: 0.542, lr: 1e-05
2023-12-15 01:55:39 INFO     saving model related files
2023-12-15 01:55:39 INFO     saving model
2023-12-15 01:55:39 INFO     saving tokenizer
2023-12-15 01:55:39 INFO     saving optimizer
2023-12-15 01:55:40 INFO     remove old optimizer files
2023-12-15 01:55:47 INFO     	 * (global step 2500: loss: 0.5070558041334152, lr: 1e-05
2023-12-15 01:56:02 INFO     	 * (global step 2550: loss: 0.597995787858963, lr: 1e-05
2023-12-15 01:56:18 INFO     	 * (global step 2600: loss: 0.5401275604963303, lr: 1e-05
2023-12-15 01:56:34 INFO     	 * (global step 2650: loss: 0.5940504372119904, lr: 1e-05
2023-12-15 01:56:49 INFO     	 * (global step 2700: loss: 0.46888844668865204, lr: 1e-05
2023-12-15 01:57:05 INFO     	 * (global step 2750: loss: 0.45623136311769485, lr: 1e-05
2023-12-15 01:57:18 INFO     [epoch 8/15] average loss: 0.525, lr: 1e-05
2023-12-15 01:57:18 INFO     saving model related files
2023-12-15 01:57:18 INFO     saving model
2023-12-15 01:57:18 INFO     saving tokenizer
2023-12-15 01:57:18 INFO     saving optimizer
2023-12-15 01:57:19 INFO     remove old optimizer files
2023-12-15 01:57:23 INFO     	 * (global step 2800: loss: 0.5970074087381363, lr: 1e-05
2023-12-15 01:57:38 INFO     	 * (global step 2850: loss: 0.6758576259016991, lr: 1e-05
2023-12-15 01:57:54 INFO     	 * (global step 2900: loss: 0.5366736277937889, lr: 1e-05
2023-12-15 01:58:10 INFO     	 * (global step 2950: loss: 0.5509486272931099, lr: 1e-05
2023-12-15 01:58:25 INFO     	 * (global step 3000: loss: 0.4430784732103348, lr: 1e-05
2023-12-15 01:58:41 INFO     	 * (global step 3050: loss: 0.4148724526166916, lr: 1e-05
2023-12-15 01:58:57 INFO     	 * (global step 3100: loss: 0.5695320293307304, lr: 1e-05
2023-12-15 01:58:57 INFO     [epoch 9/15] average loss: 0.511, lr: 1e-05
2023-12-15 01:58:57 INFO     saving model related files
2023-12-15 01:58:57 INFO     saving model
2023-12-15 01:58:57 INFO     saving tokenizer
2023-12-15 01:58:57 INFO     saving optimizer
2023-12-15 01:58:58 INFO     remove old optimizer files
2023-12-15 01:58:58 INFO     complete training: model ckpt was saved at small_trained_ckpt/model_vhyoja
2023-12-15 01:58:58 INFO     ## 1st RUN: Configuration 11/12 ##
2023-12-15 01:58:58 INFO     initialize model trainer
2023-12-15 01:58:58 INFO     initialize checkpoint at small_trained_ckpt/model_nrudfu
2023-12-15 01:58:58 INFO     hyperparameters
2023-12-15 01:58:58 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 01:58:58 INFO     	 * dataset_name: default
2023-12-15 01:58:58 INFO     	 * input_types: ['paragraph']
2023-12-15 01:58:58 INFO     	 * output_types: ['questions_answers']
2023-12-15 01:58:58 INFO     	 * prefix_types: ['qag']
2023-12-15 01:58:58 INFO     	 * model: t5-small
2023-12-15 01:58:58 INFO     	 * max_length: 512
2023-12-15 01:58:58 INFO     	 * max_length_output: 512
2023-12-15 01:58:58 INFO     	 * epoch: 15
2023-12-15 01:58:58 INFO     	 * batch: 2
2023-12-15 01:58:58 INFO     	 * lr: 1e-05
2023-12-15 01:58:58 INFO     	 * fp16: False
2023-12-15 01:58:58 INFO     	 * random_seed: 1
2023-12-15 01:58:58 INFO     	 * gradient_accumulation_steps: 2
2023-12-15 01:58:58 INFO     	 * label_smoothing: 0.0
2023-12-15 01:58:58 INFO     initialize checkpoint with t5-small
2023-12-15 01:59:00 INFO     use spaCy answer extraction model: positionrank
2023-12-15 01:59:01 INFO     Model `t5-small`
2023-12-15 01:59:01 INFO     	 * Num of GPU in use: 1
2023-12-15 01:59:01 INFO     	 * Prefix: True
2023-12-15 01:59:01 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 01:59:01 INFO     dataset preprocessing
2023-12-15 01:59:02 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 01:59:02 INFO     start model training
2023-12-15 01:59:10 INFO     	 * (global step 50: loss: 4.98059868812561, lr: 1e-05
2023-12-15 01:59:18 INFO     	 * (global step 100: loss: 2.327609419822693, lr: 1e-05
2023-12-15 01:59:26 INFO     	 * (global step 150: loss: 1.8425626158714294, lr: 1e-05
2023-12-15 01:59:34 INFO     	 * (global step 200: loss: 1.6577856540679932, lr: 1e-05
2023-12-15 01:59:42 INFO     	 * (global step 250: loss: 1.5848309993743896, lr: 1e-05
2023-12-15 01:59:50 INFO     	 * (global step 300: loss: 1.2796490788459778, lr: 1e-05
2023-12-15 01:59:58 INFO     	 * (global step 350: loss: 1.356551706790924, lr: 1e-05
2023-12-15 02:00:06 INFO     	 * (global step 400: loss: 1.0361078381538391, lr: 1e-05
2023-12-15 02:00:15 INFO     	 * (global step 450: loss: 1.040247619152069, lr: 1e-05
2023-12-15 02:00:23 INFO     	 * (global step 500: loss: 1.4920347929000854, lr: 1e-05
2023-12-15 02:00:31 INFO     	 * (global step 550: loss: 0.863457053899765, lr: 1e-05
2023-12-15 02:00:39 INFO     	 * (global step 600: loss: 0.8804566562175751, lr: 1e-05
2023-12-15 02:00:42 INFO     [epoch 0/15] average loss: 2.03, lr: 1e-05
2023-12-15 02:00:42 INFO     saving model related files
2023-12-15 02:00:42 INFO     saving model
2023-12-15 02:00:43 INFO     saving tokenizer
2023-12-15 02:00:43 INFO     saving optimizer
2023-12-15 02:00:44 INFO     remove old optimizer files
2023-12-15 02:00:49 INFO     	 * (global step 650: loss: 0.9029726088047028, lr: 1e-05
2023-12-15 02:00:57 INFO     	 * (global step 700: loss: 0.7651035785675049, lr: 1e-05
2023-12-15 02:01:05 INFO     	 * (global step 750: loss: 0.9414747357368469, lr: 1e-05
2023-12-15 02:01:13 INFO     	 * (global step 800: loss: 0.7147819399833679, lr: 1e-05
2023-12-15 02:01:21 INFO     	 * (global step 850: loss: 0.529510959982872, lr: 1e-05
2023-12-15 02:01:29 INFO     	 * (global step 900: loss: 0.5772595554590225, lr: 1e-05
2023-12-15 02:01:37 INFO     	 * (global step 950: loss: 0.7951968610286713, lr: 1e-05
2023-12-15 02:01:45 INFO     	 * (global step 1000: loss: 0.5417391806840897, lr: 1e-05
2023-12-15 02:01:54 INFO     	 * (global step 1050: loss: 0.7522474527359009, lr: 1e-05
2023-12-15 02:02:02 INFO     	 * (global step 1100: loss: 0.5840396881103516, lr: 1e-05
2023-12-15 02:02:10 INFO     	 * (global step 1150: loss: 1.038498342037201, lr: 1e-05
2023-12-15 02:02:18 INFO     	 * (global step 1200: loss: 0.7101102769374847, lr: 1e-05
2023-12-15 02:02:25 INFO     [epoch 1/15] average loss: 0.764, lr: 1e-05
2023-12-15 02:02:25 INFO     saving model related files
2023-12-15 02:02:25 INFO     saving model
2023-12-15 02:02:25 INFO     saving tokenizer
2023-12-15 02:02:25 INFO     saving optimizer
2023-12-15 02:02:26 INFO     remove old optimizer files
2023-12-15 02:02:28 INFO     	 * (global step 1250: loss: 0.8279289901256561, lr: 1e-05
2023-12-15 02:02:36 INFO     	 * (global step 1300: loss: 0.7161503285169601, lr: 1e-05
2023-12-15 02:02:44 INFO     	 * (global step 1350: loss: 0.7306906282901764, lr: 1e-05
2023-12-15 02:02:52 INFO     	 * (global step 1400: loss: 0.7189508378505707, lr: 1e-05
2023-12-15 02:03:00 INFO     	 * (global step 1450: loss: 0.8312117457389832, lr: 1e-05
2023-12-15 02:03:08 INFO     	 * (global step 1500: loss: 0.46911299228668213, lr: 1e-05
2023-12-15 02:03:16 INFO     	 * (global step 1550: loss: 0.6531570255756378, lr: 1e-05
2023-12-15 02:03:24 INFO     	 * (global step 1600: loss: 0.5281214565038681, lr: 1e-05
2023-12-15 02:03:32 INFO     	 * (global step 1650: loss: 0.528250128030777, lr: 1e-05
2023-12-15 02:03:40 INFO     	 * (global step 1700: loss: 0.5499285161495209, lr: 1e-05
2023-12-15 02:03:48 INFO     	 * (global step 1750: loss: 0.5536015331745148, lr: 1e-05
2023-12-15 02:03:56 INFO     	 * (global step 1800: loss: 0.46868860721588135, lr: 1e-05
2023-12-15 02:04:05 INFO     	 * (global step 1850: loss: 0.5959600508213043, lr: 1e-05
2023-12-15 02:04:07 INFO     [epoch 2/15] average loss: 0.642, lr: 1e-05
2023-12-15 02:04:07 INFO     saving model related files
2023-12-15 02:04:07 INFO     saving model
2023-12-15 02:04:07 INFO     saving tokenizer
2023-12-15 02:04:07 INFO     saving optimizer
2023-12-15 02:04:08 INFO     remove old optimizer files
2023-12-15 02:04:14 INFO     	 * (global step 1900: loss: 0.7435204386711121, lr: 1e-05
2023-12-15 02:04:23 INFO     	 * (global step 1950: loss: 0.5632871985435486, lr: 1e-05
2023-12-15 02:04:31 INFO     	 * (global step 2000: loss: 0.5177041590213776, lr: 1e-05
2023-12-15 02:04:39 INFO     	 * (global step 2050: loss: 0.6500732004642487, lr: 1e-05
2023-12-15 02:04:47 INFO     	 * (global step 2100: loss: 0.7548327147960663, lr: 1e-05
2023-12-15 02:04:55 INFO     	 * (global step 2150: loss: 0.5402985662221909, lr: 1e-05
2023-12-15 02:05:03 INFO     	 * (global step 2200: loss: 0.4572140723466873, lr: 1e-05
2023-12-15 02:05:11 INFO     	 * (global step 2250: loss: 0.7637749910354614, lr: 1e-05
2023-12-15 02:05:19 INFO     	 * (global step 2300: loss: 0.5777876377105713, lr: 1e-05
2023-12-15 02:05:27 INFO     	 * (global step 2350: loss: 0.51038858294487, lr: 1e-05
2023-12-15 02:05:35 INFO     	 * (global step 2400: loss: 0.5471776872873306, lr: 1e-05
2023-12-15 02:05:44 INFO     	 * (global step 2450: loss: 0.45935986936092377, lr: 1e-05
2023-12-15 02:05:49 INFO     [epoch 3/15] average loss: 0.581, lr: 1e-05
2023-12-15 02:05:49 INFO     saving model related files
2023-12-15 02:05:49 INFO     saving model
2023-12-15 02:05:50 INFO     saving tokenizer
2023-12-15 02:05:50 INFO     saving optimizer
2023-12-15 02:05:51 INFO     remove old optimizer files
2023-12-15 02:05:53 INFO     	 * (global step 2500: loss: 0.6532197594642639, lr: 1e-05
2023-12-15 02:06:01 INFO     	 * (global step 2550: loss: 0.47556959092617035, lr: 1e-05
2023-12-15 02:06:09 INFO     	 * (global step 2600: loss: 0.5075017362833023, lr: 1e-05
2023-12-15 02:06:18 INFO     	 * (global step 2650: loss: 0.5564220249652863, lr: 1e-05
2023-12-15 02:06:26 INFO     	 * (global step 2700: loss: 0.4229992926120758, lr: 1e-05
2023-12-15 02:06:34 INFO     	 * (global step 2750: loss: 0.47293585538864136, lr: 1e-05
2023-12-15 02:06:42 INFO     	 * (global step 2800: loss: 0.6089563965797424, lr: 1e-05
2023-12-15 02:06:50 INFO     	 * (global step 2850: loss: 0.5932329744100571, lr: 1e-05
2023-12-15 02:06:58 INFO     	 * (global step 2900: loss: 0.5312206000089645, lr: 1e-05
2023-12-15 02:07:06 INFO     	 * (global step 2950: loss: 0.5219980925321579, lr: 1e-05
2023-12-15 02:07:14 INFO     	 * (global step 3000: loss: 0.7048918008804321, lr: 1e-05
2023-12-15 02:07:22 INFO     	 * (global step 3050: loss: 0.5298791378736496, lr: 1e-05
2023-12-15 02:07:30 INFO     	 * (global step 3100: loss: 0.5650434046983719, lr: 1e-05
2023-12-15 02:07:31 INFO     [epoch 4/15] average loss: 0.539, lr: 1e-05
2023-12-15 02:07:31 INFO     saving model related files
2023-12-15 02:07:31 INFO     saving model
2023-12-15 02:07:32 INFO     saving tokenizer
2023-12-15 02:07:32 INFO     saving optimizer
2023-12-15 02:07:33 INFO     remove old optimizer files
2023-12-15 02:07:40 INFO     	 * (global step 3150: loss: 0.5075997561216354, lr: 1e-05
2023-12-15 02:07:48 INFO     	 * (global step 3200: loss: 0.5758163034915924, lr: 1e-05
2023-12-15 02:07:56 INFO     	 * (global step 3250: loss: 0.49717435240745544, lr: 1e-05
2023-12-15 02:08:05 INFO     	 * (global step 3300: loss: 0.40033067762851715, lr: 1e-05
2023-12-15 02:08:13 INFO     	 * (global step 3350: loss: 0.6326259672641754, lr: 1e-05
2023-12-15 02:08:21 INFO     	 * (global step 3400: loss: 0.6780581027269363, lr: 1e-05
2023-12-15 02:08:29 INFO     	 * (global step 3450: loss: 0.4069731831550598, lr: 1e-05
2023-12-15 02:08:37 INFO     	 * (global step 3500: loss: 0.2846577763557434, lr: 1e-05
2023-12-15 02:08:45 INFO     	 * (global step 3550: loss: 0.6021698415279388, lr: 1e-05
2023-12-15 02:08:53 INFO     	 * (global step 3600: loss: 0.6108218878507614, lr: 1e-05
2023-12-15 02:09:01 INFO     	 * (global step 3650: loss: 0.5312495529651642, lr: 1e-05
2023-12-15 02:09:09 INFO     	 * (global step 3700: loss: 0.44077667593955994, lr: 1e-05
2023-12-15 02:09:13 INFO     [epoch 5/15] average loss: 0.513, lr: 1e-05
2023-12-15 02:09:13 INFO     saving model related files
2023-12-15 02:09:13 INFO     saving model
2023-12-15 02:09:14 INFO     saving tokenizer
2023-12-15 02:09:14 INFO     saving optimizer
2023-12-15 02:09:15 INFO     remove old optimizer files
2023-12-15 02:09:19 INFO     	 * (global step 3750: loss: 0.22861894965171814, lr: 1e-05
2023-12-15 02:09:27 INFO     	 * (global step 3800: loss: 0.529342383146286, lr: 1e-05
2023-12-15 02:09:35 INFO     	 * (global step 3850: loss: 0.3143666684627533, lr: 1e-05
2023-12-15 02:09:43 INFO     	 * (global step 3900: loss: 0.42237481474876404, lr: 1e-05
2023-12-15 02:09:51 INFO     	 * (global step 3950: loss: 0.40376314520835876, lr: 1e-05
2023-12-15 02:10:00 INFO     	 * (global step 4000: loss: 0.5483021587133408, lr: 1e-05
2023-12-15 02:10:08 INFO     	 * (global step 4050: loss: 0.5681079626083374, lr: 1e-05
2023-12-15 02:10:16 INFO     	 * (global step 4100: loss: 0.5554275065660477, lr: 1e-05
2023-12-15 02:10:24 INFO     	 * (global step 4150: loss: 0.5045499503612518, lr: 1e-05
2023-12-15 02:10:32 INFO     	 * (global step 4200: loss: 0.4330688416957855, lr: 1e-05
2023-12-15 02:10:40 INFO     	 * (global step 4250: loss: 0.36881114542484283, lr: 1e-05
2023-12-15 02:10:48 INFO     	 * (global step 4300: loss: 0.6357844471931458, lr: 1e-05
2023-12-15 02:10:56 INFO     [epoch 6/15] average loss: 0.496, lr: 1e-05
2023-12-15 02:10:56 INFO     saving model related files
2023-12-15 02:10:56 INFO     saving model
2023-12-15 02:10:56 INFO     saving tokenizer
2023-12-15 02:10:56 INFO     saving optimizer
2023-12-15 02:10:57 INFO     remove old optimizer files
2023-12-15 02:10:58 INFO     	 * (global step 4350: loss: 0.4551563262939453, lr: 1e-05
2023-12-15 02:11:06 INFO     	 * (global step 4400: loss: 0.37784498929977417, lr: 1e-05
2023-12-15 02:11:14 INFO     	 * (global step 4450: loss: 0.3563506007194519, lr: 1e-05
2023-12-15 02:11:22 INFO     	 * (global step 4500: loss: 0.44153420627117157, lr: 1e-05
2023-12-15 02:11:30 INFO     	 * (global step 4550: loss: 0.3592182397842407, lr: 1e-05
2023-12-15 02:11:38 INFO     	 * (global step 4600: loss: 0.5075528025627136, lr: 1e-05
2023-12-15 02:11:46 INFO     	 * (global step 4650: loss: 0.770748108625412, lr: 1e-05
2023-12-15 02:11:54 INFO     	 * (global step 4700: loss: 0.5968066155910492, lr: 1e-05
2023-12-15 02:12:02 INFO     	 * (global step 4750: loss: 0.5091223865747452, lr: 1e-05
2023-12-15 02:12:10 INFO     	 * (global step 4800: loss: 0.4680040329694748, lr: 1e-05
2023-12-15 02:12:19 INFO     	 * (global step 4850: loss: 0.3921704441308975, lr: 1e-05
2023-12-15 02:12:27 INFO     	 * (global step 4900: loss: 0.4069429785013199, lr: 1e-05
2023-12-15 02:12:35 INFO     	 * (global step 4950: loss: 0.3111163601279259, lr: 1e-05
2023-12-15 02:12:38 INFO     [epoch 7/15] average loss: 0.481, lr: 1e-05
2023-12-15 02:12:38 INFO     saving model related files
2023-12-15 02:12:38 INFO     saving model
2023-12-15 02:12:38 INFO     saving tokenizer
2023-12-15 02:12:38 INFO     saving optimizer
2023-12-15 02:12:39 INFO     remove old optimizer files
2023-12-15 02:12:44 INFO     	 * (global step 5000: loss: 0.49246031045913696, lr: 1e-05
2023-12-15 02:12:53 INFO     	 * (global step 5050: loss: 0.4134957939386368, lr: 1e-05
2023-12-15 02:13:01 INFO     	 * (global step 5100: loss: 0.41068024933338165, lr: 1e-05
2023-12-15 02:13:09 INFO     	 * (global step 5150: loss: 0.2787094935774803, lr: 1e-05
2023-12-15 02:13:17 INFO     	 * (global step 5200: loss: 0.2696603611111641, lr: 1e-05
2023-12-15 02:13:25 INFO     	 * (global step 5250: loss: 0.8614557534456253, lr: 1e-05
2023-12-15 02:13:33 INFO     	 * (global step 5300: loss: 0.34457969665527344, lr: 1e-05
2023-12-15 02:13:41 INFO     	 * (global step 5350: loss: 0.4644717872142792, lr: 1e-05
2023-12-15 02:13:49 INFO     	 * (global step 5400: loss: 0.4251183867454529, lr: 1e-05
2023-12-15 02:13:57 INFO     	 * (global step 5450: loss: 0.4094025045633316, lr: 1e-05
2023-12-15 02:14:05 INFO     	 * (global step 5500: loss: 0.4556093364953995, lr: 1e-05
2023-12-15 02:14:14 INFO     	 * (global step 5550: loss: 0.38648639619350433, lr: 1e-05
2023-12-15 02:14:20 INFO     [epoch 8/15] average loss: 0.469, lr: 1e-05
2023-12-15 02:14:20 INFO     saving model related files
2023-12-15 02:14:20 INFO     saving model
2023-12-15 02:14:20 INFO     saving tokenizer
2023-12-15 02:14:20 INFO     saving optimizer
2023-12-15 02:14:22 INFO     remove old optimizer files
2023-12-15 02:14:23 INFO     	 * (global step 5600: loss: 0.6416730284690857, lr: 1e-05
2023-12-15 02:14:31 INFO     	 * (global step 5650: loss: 0.5773099958896637, lr: 1e-05
2023-12-15 02:14:40 INFO     	 * (global step 5700: loss: 0.3354850709438324, lr: 1e-05
2023-12-15 02:14:48 INFO     	 * (global step 5750: loss: 0.4181388393044472, lr: 1e-05
2023-12-15 02:14:56 INFO     	 * (global step 5800: loss: 0.4306982159614563, lr: 1e-05
2023-12-15 02:15:04 INFO     	 * (global step 5850: loss: 0.5852713882923126, lr: 1e-05
2023-12-15 02:15:12 INFO     	 * (global step 5900: loss: 0.3560587465763092, lr: 1e-05
2023-12-15 02:15:20 INFO     	 * (global step 5950: loss: 0.43852244317531586, lr: 1e-05
2023-12-15 02:15:28 INFO     	 * (global step 6000: loss: 0.46476687490940094, lr: 1e-05
2023-12-15 02:15:36 INFO     	 * (global step 6050: loss: 0.5273232758045197, lr: 1e-05
2023-12-15 02:15:44 INFO     	 * (global step 6100: loss: 0.3511108160018921, lr: 1e-05
2023-12-15 02:15:52 INFO     	 * (global step 6150: loss: 0.49441832304000854, lr: 1e-05
2023-12-15 02:16:00 INFO     	 * (global step 6200: loss: 0.5331268906593323, lr: 1e-05
2023-12-15 02:16:02 INFO     [epoch 9/15] average loss: 0.46, lr: 1e-05
2023-12-15 02:16:02 INFO     saving model related files
2023-12-15 02:16:02 INFO     saving model
2023-12-15 02:16:03 INFO     saving tokenizer
2023-12-15 02:16:03 INFO     saving optimizer
2023-12-15 02:16:04 INFO     remove old optimizer files
2023-12-15 02:16:04 INFO     complete training: model ckpt was saved at small_trained_ckpt/model_nrudfu
2023-12-15 02:16:04 INFO     ## 1st RUN (EVAL): Configuration 0/12 ##
2023-12-15 02:16:33 INFO     use spaCy answer extraction model: positionrank
2023-12-15 02:16:33 INFO     Model `small_trained_ckpt/model_tgwwes/epoch_10`
2023-12-15 02:16:33 INFO     	 * Num of GPU in use: 1
2023-12-15 02:16:33 INFO     	 * Prefix: True
2023-12-15 02:16:33 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 02:16:34 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 02:22:55 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 02:29:06 INFO     	Bleu_1: 0.13011138118827012
2023-12-15 02:29:06 INFO     	Bleu_2: 0.07167919320966919
2023-12-15 02:29:06 INFO     	Bleu_3: 0.03864114921095067
2023-12-15 02:29:06 INFO     	Bleu_4: 0.024770082868451636
2023-12-15 02:29:06 INFO     	Bleu_1: 0.1254805804745422
2023-12-15 02:29:06 INFO     	Bleu_2: 0.06870074744526883
2023-12-15 02:29:06 INFO     	Bleu_3: 0.037311368043104524
2023-12-15 02:29:06 INFO     	Bleu_4: 0.02411720066600268
2023-12-15 02:29:06 INFO     ## 1st RUN (EVAL): Configuration 1/12 ##
2023-12-15 02:29:29 INFO     use spaCy answer extraction model: positionrank
2023-12-15 02:29:30 INFO     Model `small_trained_ckpt/model_eszyci/epoch_10`
2023-12-15 02:29:30 INFO     	 * Num of GPU in use: 1
2023-12-15 02:29:30 INFO     	 * Prefix: True
2023-12-15 02:29:30 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 02:29:31 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 02:35:38 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 02:41:48 INFO     	Bleu_1: 0.13176187092588484
2023-12-15 02:41:48 INFO     	Bleu_2: 0.07207462291789814
2023-12-15 02:41:48 INFO     	Bleu_3: 0.03880043140653511
2023-12-15 02:41:48 INFO     	Bleu_4: 0.024857383229010453
2023-12-15 02:41:49 INFO     	Bleu_1: 0.13063525386828295
2023-12-15 02:41:49 INFO     	Bleu_2: 0.07177394759313371
2023-12-15 02:41:49 INFO     	Bleu_3: 0.03924021487107275
2023-12-15 02:41:49 INFO     	Bleu_4: 0.025478814227181516
2023-12-15 02:41:49 INFO     ## 1st RUN (EVAL): Configuration 2/12 ##
2023-12-15 02:42:10 INFO     use spaCy answer extraction model: positionrank
2023-12-15 02:42:11 INFO     Model `small_trained_ckpt/model_dpyopu/epoch_10`
2023-12-15 02:42:11 INFO     	 * Num of GPU in use: 1
2023-12-15 02:42:11 INFO     	 * Prefix: True
2023-12-15 02:42:11 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 02:42:12 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 02:47:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 02:52:33 INFO     	Bleu_1: 0.13011138118827012
2023-12-15 02:52:33 INFO     	Bleu_2: 0.07167919320966919
2023-12-15 02:52:33 INFO     	Bleu_3: 0.03864114921095067
2023-12-15 02:52:33 INFO     	Bleu_4: 0.024770082868451636
2023-12-15 02:52:34 INFO     	Bleu_1: 0.1254805804745422
2023-12-15 02:52:34 INFO     	Bleu_2: 0.06870074744526883
2023-12-15 02:52:34 INFO     	Bleu_3: 0.037311368043104524
2023-12-15 02:52:34 INFO     	Bleu_4: 0.02411720066600268
2023-12-15 02:52:34 INFO     ## 1st RUN (EVAL): Configuration 3/12 ##
2023-12-15 02:52:48 INFO     use spaCy answer extraction model: positionrank
2023-12-15 02:52:48 INFO     Model `small_trained_ckpt/model_mzgdpa/epoch_10`
2023-12-15 02:52:48 INFO     	 * Num of GPU in use: 1
2023-12-15 02:52:48 INFO     	 * Prefix: True
2023-12-15 02:52:48 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 02:52:49 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 02:57:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 03:02:46 INFO     	Bleu_1: 0.13176187092588484
2023-12-15 03:02:46 INFO     	Bleu_2: 0.07207462291789814
2023-12-15 03:02:46 INFO     	Bleu_3: 0.03880043140653511
2023-12-15 03:02:46 INFO     	Bleu_4: 0.024857383229010453
2023-12-15 03:02:46 INFO     	Bleu_1: 0.13063525386828295
2023-12-15 03:02:46 INFO     	Bleu_2: 0.07177394759313371
2023-12-15 03:02:46 INFO     	Bleu_3: 0.03924021487107275
2023-12-15 03:02:46 INFO     	Bleu_4: 0.025478814227181516
2023-12-15 03:02:46 INFO     ## 1st RUN (EVAL): Configuration 4/12 ##
2023-12-15 03:03:00 INFO     use spaCy answer extraction model: positionrank
2023-12-15 03:03:00 INFO     Model `small_trained_ckpt/model_mntyya/epoch_10`
2023-12-15 03:03:00 INFO     	 * Num of GPU in use: 1
2023-12-15 03:03:00 INFO     	 * Prefix: True
2023-12-15 03:03:00 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 03:03:01 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 03:08:06 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 03:13:15 INFO     	Bleu_1: 0.12728005720110047
2023-12-15 03:13:15 INFO     	Bleu_2: 0.07047727006802477
2023-12-15 03:13:15 INFO     	Bleu_3: 0.03826517393459101
2023-12-15 03:13:15 INFO     	Bleu_4: 0.024686274899235746
2023-12-15 03:13:16 INFO     	Bleu_1: 0.12476340812005683
2023-12-15 03:13:16 INFO     	Bleu_2: 0.06793251016763889
2023-12-15 03:13:16 INFO     	Bleu_3: 0.0359749895064152
2023-12-15 03:13:16 INFO     	Bleu_4: 0.022801206051235205
2023-12-15 03:13:16 INFO     ## 1st RUN (EVAL): Configuration 5/12 ##
2023-12-15 03:13:38 INFO     use spaCy answer extraction model: positionrank
2023-12-15 03:13:38 INFO     Model `small_trained_ckpt/model_woixzh/epoch_10`
2023-12-15 03:13:38 INFO     	 * Num of GPU in use: 1
2023-12-15 03:13:38 INFO     	 * Prefix: True
2023-12-15 03:13:38 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 03:13:39 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 03:18:47 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 03:23:56 INFO     	Bleu_1: 0.12408905231786516
2023-12-15 03:23:56 INFO     	Bleu_2: 0.06846051160375273
2023-12-15 03:23:56 INFO     	Bleu_3: 0.036833860998106746
2023-12-15 03:23:56 INFO     	Bleu_4: 0.023581406957055923
2023-12-15 03:23:56 INFO     	Bleu_1: 0.12678964342767485
2023-12-15 03:23:56 INFO     	Bleu_2: 0.06880724537430627
2023-12-15 03:23:56 INFO     	Bleu_3: 0.03672154716921412
2023-12-15 03:23:56 INFO     	Bleu_4: 0.023509172805561432
2023-12-15 03:23:56 INFO     ## 1st RUN (EVAL): Configuration 6/12 ##
2023-12-15 03:24:16 INFO     use spaCy answer extraction model: positionrank
2023-12-15 03:24:16 INFO     Model `small_trained_ckpt/model_sdkaaa/epoch_10`
2023-12-15 03:24:16 INFO     	 * Num of GPU in use: 1
2023-12-15 03:24:16 INFO     	 * Prefix: True
2023-12-15 03:24:16 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 03:24:17 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 03:29:24 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 03:34:33 INFO     	Bleu_1: 0.12728005720110047
2023-12-15 03:34:33 INFO     	Bleu_2: 0.07047727006802477
2023-12-15 03:34:33 INFO     	Bleu_3: 0.03826517393459101
2023-12-15 03:34:33 INFO     	Bleu_4: 0.024686274899235746
2023-12-15 03:34:33 INFO     	Bleu_1: 0.12476340812005683
2023-12-15 03:34:33 INFO     	Bleu_2: 0.06793251016763889
2023-12-15 03:34:33 INFO     	Bleu_3: 0.0359749895064152
2023-12-15 03:34:33 INFO     	Bleu_4: 0.022801206051235205
2023-12-15 03:34:33 INFO     ## 1st RUN (EVAL): Configuration 7/12 ##
2023-12-15 03:34:49 INFO     use spaCy answer extraction model: positionrank
2023-12-15 03:34:49 INFO     Model `small_trained_ckpt/model_uramvg/epoch_10`
2023-12-15 03:34:49 INFO     	 * Num of GPU in use: 1
2023-12-15 03:34:49 INFO     	 * Prefix: True
2023-12-15 03:34:49 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 03:34:50 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 03:39:59 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 03:45:07 INFO     	Bleu_1: 0.12408905231786516
2023-12-15 03:45:07 INFO     	Bleu_2: 0.06846051160375273
2023-12-15 03:45:07 INFO     	Bleu_3: 0.036833860998106746
2023-12-15 03:45:07 INFO     	Bleu_4: 0.023581406957055923
2023-12-15 03:45:07 INFO     	Bleu_1: 0.12678964342767485
2023-12-15 03:45:07 INFO     	Bleu_2: 0.06880724537430627
2023-12-15 03:45:07 INFO     	Bleu_3: 0.03672154716921412
2023-12-15 03:45:07 INFO     	Bleu_4: 0.023509172805561432
2023-12-15 03:45:07 INFO     ## 1st RUN (EVAL): Configuration 8/12 ##
2023-12-15 03:45:21 INFO     use spaCy answer extraction model: positionrank
2023-12-15 03:45:21 INFO     Model `small_trained_ckpt/model_nxaqhy/epoch_10`
2023-12-15 03:45:21 INFO     	 * Num of GPU in use: 1
2023-12-15 03:45:21 INFO     	 * Prefix: True
2023-12-15 03:45:21 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 03:45:22 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 03:50:37 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 03:55:51 INFO     	Bleu_1: 0.1400774320197748
2023-12-15 03:55:51 INFO     	Bleu_2: 0.07710808797236492
2023-12-15 03:55:51 INFO     	Bleu_3: 0.04177396408884501
2023-12-15 03:55:51 INFO     	Bleu_4: 0.02679595996444357
2023-12-15 03:55:51 INFO     	Bleu_1: 0.13955267913117103
2023-12-15 03:55:51 INFO     	Bleu_2: 0.07673955338144617
2023-12-15 03:55:51 INFO     	Bleu_3: 0.04175831164768515
2023-12-15 03:55:51 INFO     	Bleu_4: 0.026618816932159447
2023-12-15 03:55:51 INFO     ## 1st RUN (EVAL): Configuration 9/12 ##
2023-12-15 03:56:02 INFO     use spaCy answer extraction model: positionrank
2023-12-15 03:56:02 INFO     Model `small_trained_ckpt/model_oprhlh/epoch_10`
2023-12-15 03:56:02 INFO     	 * Num of GPU in use: 1
2023-12-15 03:56:02 INFO     	 * Prefix: True
2023-12-15 03:56:02 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 03:56:03 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 04:01:12 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 04:06:19 INFO     	Bleu_1: 0.13716078215082741
2023-12-15 04:06:19 INFO     	Bleu_2: 0.07627019356065123
2023-12-15 04:06:19 INFO     	Bleu_3: 0.041949527212727045
2023-12-15 04:06:19 INFO     	Bleu_4: 0.027090006600475953
2023-12-15 04:06:19 INFO     	Bleu_1: 0.1347464362412691
2023-12-15 04:06:19 INFO     	Bleu_2: 0.0745145840999375
2023-12-15 04:06:19 INFO     	Bleu_3: 0.04094484875334939
2023-12-15 04:06:19 INFO     	Bleu_4: 0.026552482573880296
2023-12-15 04:06:19 INFO     ## 1st RUN (EVAL): Configuration 10/12 ##
2023-12-15 04:06:39 INFO     use spaCy answer extraction model: positionrank
2023-12-15 04:06:40 INFO     Model `small_trained_ckpt/model_vhyoja/epoch_10`
2023-12-15 04:06:40 INFO     	 * Num of GPU in use: 1
2023-12-15 04:06:40 INFO     	 * Prefix: True
2023-12-15 04:06:40 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 04:06:41 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 04:11:56 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 04:17:11 INFO     	Bleu_1: 0.1400774320197748
2023-12-15 04:17:11 INFO     	Bleu_2: 0.07710808797236492
2023-12-15 04:17:11 INFO     	Bleu_3: 0.04177396408884501
2023-12-15 04:17:11 INFO     	Bleu_4: 0.02679595996444357
2023-12-15 04:17:12 INFO     	Bleu_1: 0.13955267913117103
2023-12-15 04:17:12 INFO     	Bleu_2: 0.07673955338144617
2023-12-15 04:17:12 INFO     	Bleu_3: 0.04175831164768515
2023-12-15 04:17:12 INFO     	Bleu_4: 0.026618816932159447
2023-12-15 04:17:12 INFO     ## 1st RUN (EVAL): Configuration 11/12 ##
2023-12-15 04:17:25 INFO     use spaCy answer extraction model: positionrank
2023-12-15 04:17:26 INFO     Model `small_trained_ckpt/model_nrudfu/epoch_10`
2023-12-15 04:17:26 INFO     	 * Num of GPU in use: 1
2023-12-15 04:17:26 INFO     	 * Prefix: True
2023-12-15 04:17:26 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 04:17:27 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 04:22:36 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 04:27:43 INFO     	Bleu_1: 0.13716078215082741
2023-12-15 04:27:43 INFO     	Bleu_2: 0.07627019356065123
2023-12-15 04:27:43 INFO     	Bleu_3: 0.041949527212727045
2023-12-15 04:27:43 INFO     	Bleu_4: 0.027090006600475953
2023-12-15 04:27:43 INFO     	Bleu_1: 0.1347464362412691
2023-12-15 04:27:43 INFO     	Bleu_2: 0.0745145840999375
2023-12-15 04:27:43 INFO     	Bleu_3: 0.04094484875334939
2023-12-15 04:27:43 INFO     	Bleu_4: 0.026552482573880296
2023-12-15 04:27:43 INFO     1st RUN RESULTS (validation/Bleu_4)
2023-12-15 04:27:43 INFO     	 * rank: 0 | metric: 0.027 | model: small_trained_ckpt/model_oprhlh/epoch_10 |
2023-12-15 04:27:43 INFO     	 * rank: 1 | metric: 0.027 | model: small_trained_ckpt/model_nrudfu/epoch_10 |
2023-12-15 04:27:43 INFO     	 * rank: 2 | metric: 0.027 | model: small_trained_ckpt/model_nxaqhy/epoch_10 |
2023-12-15 04:27:43 INFO     	 * rank: 3 | metric: 0.027 | model: small_trained_ckpt/model_vhyoja/epoch_10 |
2023-12-15 04:27:43 INFO     	 * rank: 4 | metric: 0.025 | model: small_trained_ckpt/model_eszyci/epoch_10 |
2023-12-15 04:27:43 INFO     	 * rank: 5 | metric: 0.025 | model: small_trained_ckpt/model_mzgdpa/epoch_10 |
2023-12-15 04:27:43 INFO     	 * rank: 6 | metric: 0.025 | model: small_trained_ckpt/model_tgwwes/epoch_10 |
2023-12-15 04:27:43 INFO     	 * rank: 7 | metric: 0.025 | model: small_trained_ckpt/model_dpyopu/epoch_10 |
2023-12-15 04:27:43 INFO     	 * rank: 8 | metric: 0.025 | model: small_trained_ckpt/model_mntyya/epoch_10 |
2023-12-15 04:27:43 INFO     	 * rank: 9 | metric: 0.025 | model: small_trained_ckpt/model_sdkaaa/epoch_10 |
2023-12-15 04:27:43 INFO     	 * rank: 10 | metric: 0.024 | model: small_trained_ckpt/model_woixzh/epoch_10 |
2023-12-15 04:27:43 INFO     	 * rank: 11 | metric: 0.024 | model: small_trained_ckpt/model_uramvg/epoch_10 |
2023-12-15 04:27:43 INFO     ## 2nd RUN: Configuration 0/5: validation/Bleu_4 = 0.027090006600475953
2023-12-15 04:27:43 INFO     initialize model trainer
2023-12-15 04:27:43 INFO     load config from existing checkpoint at small_trained_ckpt/model_oprhlh
2023-12-15 04:27:43 INFO     hyperparameters
2023-12-15 04:27:43 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 04:27:43 INFO     	 * dataset_name: default
2023-12-15 04:27:43 INFO     	 * input_types: ['paragraph']
2023-12-15 04:27:43 INFO     	 * output_types: ['questions_answers']
2023-12-15 04:27:43 INFO     	 * prefix_types: ['qag']
2023-12-15 04:27:43 INFO     	 * model: t5-small
2023-12-15 04:27:43 INFO     	 * max_length: 512
2023-12-15 04:27:43 INFO     	 * max_length_output: 512
2023-12-15 04:27:43 INFO     	 * epoch: 15
2023-12-15 04:27:43 INFO     	 * batch: 2
2023-12-15 04:27:43 INFO     	 * lr: 1e-05
2023-12-15 04:27:43 INFO     	 * fp16: False
2023-12-15 04:27:43 INFO     	 * random_seed: 1
2023-12-15 04:27:43 INFO     	 * gradient_accumulation_steps: 2
2023-12-15 04:27:43 INFO     	 * label_smoothing: 0.15
2023-12-15 04:27:43 INFO     load checkpoint from small_trained_ckpt/model_oprhlh/epoch_10
2023-12-15 04:27:44 INFO     use spaCy answer extraction model: positionrank
2023-12-15 04:27:44 INFO     Model `small_trained_ckpt/model_oprhlh/epoch_10`
2023-12-15 04:27:44 INFO     	 * Num of GPU in use: 1
2023-12-15 04:27:44 INFO     	 * Prefix: True
2023-12-15 04:27:44 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 04:27:44 INFO     load optimizer from small_trained_ckpt/model_oprhlh/optimizers/optimizer.10.pt
2023-12-15 04:27:44 INFO     optimizer is loading on cuda
2023-12-15 04:28:09 INFO     dataset preprocessing
2023-12-15 04:28:10 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 04:28:11 INFO     start model training
2023-12-15 04:28:19 INFO     	 * (global step 50: loss: 3.2901601791381836, lr: 1e-05
2023-12-15 04:28:28 INFO     	 * (global step 100: loss: 2.8878512382507324, lr: 1e-05
2023-12-15 04:28:36 INFO     	 * (global step 150: loss: 2.8993669748306274, lr: 1e-05
2023-12-15 04:28:44 INFO     	 * (global step 200: loss: 2.878821015357971, lr: 1e-05
2023-12-15 04:28:52 INFO     	 * (global step 250: loss: 2.8643722534179688, lr: 1e-05
2023-12-15 04:29:01 INFO     	 * (global step 300: loss: 2.7634940147399902, lr: 1e-05
2023-12-15 04:29:09 INFO     	 * (global step 350: loss: 2.983995199203491, lr: 1e-05
2023-12-15 04:29:17 INFO     	 * (global step 400: loss: 2.71738600730896, lr: 1e-05
2023-12-15 04:29:25 INFO     	 * (global step 450: loss: 2.6366394758224487, lr: 1e-05
2023-12-15 04:29:34 INFO     	 * (global step 500: loss: 2.780119299888611, lr: 1e-05
2023-12-15 04:29:42 INFO     	 * (global step 550: loss: 2.6451258659362793, lr: 1e-05
2023-12-15 04:29:50 INFO     	 * (global step 600: loss: 2.932434320449829, lr: 1e-05
2023-12-15 04:29:54 INFO     [epoch 10/15] average loss: 2.922, lr: 1e-05
2023-12-15 04:29:54 INFO     saving model related files
2023-12-15 04:29:54 INFO     saving model
2023-12-15 04:29:54 INFO     saving tokenizer
2023-12-15 04:29:55 INFO     saving optimizer
2023-12-15 04:29:56 INFO     remove old optimizer files
2023-12-15 04:30:01 INFO     	 * (global step 650: loss: 2.77080500125885, lr: 1e-05
2023-12-15 04:30:09 INFO     	 * (global step 700: loss: 2.7031407356262207, lr: 1e-05
2023-12-15 04:30:17 INFO     	 * (global step 750: loss: 2.627203583717346, lr: 1e-05
2023-12-15 04:30:26 INFO     	 * (global step 800: loss: 2.7222037315368652, lr: 1e-05
2023-12-15 04:30:34 INFO     	 * (global step 850: loss: 2.934727430343628, lr: 1e-05
2023-12-15 04:30:42 INFO     	 * (global step 900: loss: 2.8057464361190796, lr: 1e-05
2023-12-15 04:30:51 INFO     	 * (global step 950: loss: 2.6421390771865845, lr: 1e-05
2023-12-15 04:30:59 INFO     	 * (global step 1000: loss: 2.8675148487091064, lr: 1e-05
2023-12-15 04:31:07 INFO     	 * (global step 1050: loss: 2.612825870513916, lr: 1e-05
2023-12-15 04:31:16 INFO     	 * (global step 1100: loss: 2.7340322732925415, lr: 1e-05
2023-12-15 04:31:24 INFO     	 * (global step 1150: loss: 2.511449456214905, lr: 1e-05
2023-12-15 04:31:32 INFO     	 * (global step 1200: loss: 2.9025005102157593, lr: 1e-05
2023-12-15 04:31:39 INFO     [epoch 11/15] average loss: 2.678, lr: 1e-05
2023-12-15 04:31:39 INFO     saving model related files
2023-12-15 04:31:39 INFO     saving model
2023-12-15 04:31:40 INFO     saving tokenizer
2023-12-15 04:31:40 INFO     saving optimizer
2023-12-15 04:31:41 INFO     remove old optimizer files
2023-12-15 04:31:42 INFO     	 * (global step 1250: loss: 2.730367422103882, lr: 1e-05
2023-12-15 04:31:51 INFO     	 * (global step 1300: loss: 2.5317782163619995, lr: 1e-05
2023-12-15 04:31:59 INFO     	 * (global step 1350: loss: 2.436266779899597, lr: 1e-05
2023-12-15 04:32:07 INFO     	 * (global step 1400: loss: 2.692529797554016, lr: 1e-05
2023-12-15 04:32:16 INFO     	 * (global step 1450: loss: 2.682063937187195, lr: 1e-05
2023-12-15 04:32:24 INFO     	 * (global step 1500: loss: 2.478153347969055, lr: 1e-05
2023-12-15 04:32:32 INFO     	 * (global step 1550: loss: 2.541379451751709, lr: 1e-05
2023-12-15 04:32:41 INFO     	 * (global step 1600: loss: 2.5638391971588135, lr: 1e-05
2023-12-15 04:32:49 INFO     	 * (global step 1650: loss: 2.5587902069091797, lr: 1e-05
2023-12-15 04:32:58 INFO     	 * (global step 1700: loss: 2.5802295207977295, lr: 1e-05
2023-12-15 04:33:06 INFO     	 * (global step 1750: loss: 2.5136873722076416, lr: 1e-05
2023-12-15 04:33:14 INFO     	 * (global step 1800: loss: 2.6316064596176147, lr: 1e-05
2023-12-15 04:33:23 INFO     	 * (global step 1850: loss: 2.5205286741256714, lr: 1e-05
2023-12-15 04:33:25 INFO     [epoch 12/15] average loss: 2.624, lr: 1e-05
2023-12-15 04:33:25 INFO     saving model related files
2023-12-15 04:33:25 INFO     saving model
2023-12-15 04:33:25 INFO     saving tokenizer
2023-12-15 04:33:25 INFO     saving optimizer
2023-12-15 04:33:26 INFO     remove old optimizer files
2023-12-15 04:33:32 INFO     	 * (global step 1900: loss: 2.4706249237060547, lr: 1e-05
2023-12-15 04:33:41 INFO     	 * (global step 1950: loss: 2.548195481300354, lr: 1e-05
2023-12-15 04:33:49 INFO     	 * (global step 2000: loss: 2.658315420150757, lr: 1e-05
2023-12-15 04:33:57 INFO     	 * (global step 2050: loss: 2.7273669242858887, lr: 1e-05
2023-12-15 04:34:06 INFO     	 * (global step 2100: loss: 2.578084707260132, lr: 1e-05
2023-12-15 04:34:14 INFO     	 * (global step 2150: loss: 2.548466205596924, lr: 1e-05
2023-12-15 04:34:22 INFO     	 * (global step 2200: loss: 2.64159893989563, lr: 1e-05
2023-12-15 04:34:31 INFO     	 * (global step 2250: loss: 2.5967272520065308, lr: 1e-05
2023-12-15 04:34:39 INFO     	 * (global step 2300: loss: 2.617996335029602, lr: 1e-05
2023-12-15 04:34:47 INFO     	 * (global step 2350: loss: 2.540156126022339, lr: 1e-05
2023-12-15 04:34:56 INFO     	 * (global step 2400: loss: 2.680260181427002, lr: 1e-05
2023-12-15 04:35:04 INFO     	 * (global step 2450: loss: 2.5124926567077637, lr: 1e-05
2023-12-15 04:35:10 INFO     [epoch 13/15] average loss: 2.591, lr: 1e-05
2023-12-15 04:35:10 INFO     saving model related files
2023-12-15 04:35:10 INFO     saving model
2023-12-15 04:35:10 INFO     saving tokenizer
2023-12-15 04:35:10 INFO     saving optimizer
2023-12-15 04:35:11 INFO     remove old optimizer files
2023-12-15 04:35:14 INFO     	 * (global step 2500: loss: 2.4825538396835327, lr: 1e-05
2023-12-15 04:35:22 INFO     	 * (global step 2550: loss: 2.4795562028884888, lr: 1e-05
2023-12-15 04:35:31 INFO     	 * (global step 2600: loss: 2.7419921159744263, lr: 1e-05
2023-12-15 04:35:39 INFO     	 * (global step 2650: loss: 2.6044079065322876, lr: 1e-05
2023-12-15 04:35:47 INFO     	 * (global step 2700: loss: 2.5239622592926025, lr: 1e-05
2023-12-15 04:35:56 INFO     	 * (global step 2750: loss: 2.6096781492233276, lr: 1e-05
2023-12-15 04:36:04 INFO     	 * (global step 2800: loss: 2.9194928407669067, lr: 1e-05
2023-12-15 04:36:12 INFO     	 * (global step 2850: loss: 2.5862152576446533, lr: 1e-05
2023-12-15 04:36:21 INFO     	 * (global step 2900: loss: 2.394441604614258, lr: 1e-05
2023-12-15 04:36:29 INFO     	 * (global step 2950: loss: 2.5208184719085693, lr: 1e-05
2023-12-15 04:36:37 INFO     	 * (global step 3000: loss: 2.7115724086761475, lr: 1e-05
2023-12-15 04:36:46 INFO     	 * (global step 3050: loss: 2.50296413898468, lr: 1e-05
2023-12-15 04:36:54 INFO     	 * (global step 3100: loss: 2.4941844940185547, lr: 1e-05
2023-12-15 04:36:55 INFO     [epoch 14/15] average loss: 2.565, lr: 1e-05
2023-12-15 04:36:55 INFO     saving model related files
2023-12-15 04:36:55 INFO     saving model
2023-12-15 04:36:56 INFO     saving tokenizer
2023-12-15 04:36:56 INFO     saving optimizer
2023-12-15 04:36:56 INFO     remove old optimizer files
2023-12-15 04:36:57 INFO     complete training: model ckpt was saved at small_trained_ckpt/model_oprhlh
2023-12-15 04:36:57 INFO     ## 2nd RUN: Configuration 1/5: validation/Bleu_4 = 0.027090006600475953
2023-12-15 04:36:57 INFO     initialize model trainer
2023-12-15 04:36:57 INFO     load config from existing checkpoint at small_trained_ckpt/model_nrudfu
2023-12-15 04:36:57 INFO     hyperparameters
2023-12-15 04:36:57 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 04:36:57 INFO     	 * dataset_name: default
2023-12-15 04:36:57 INFO     	 * input_types: ['paragraph']
2023-12-15 04:36:57 INFO     	 * output_types: ['questions_answers']
2023-12-15 04:36:57 INFO     	 * prefix_types: ['qag']
2023-12-15 04:36:57 INFO     	 * model: t5-small
2023-12-15 04:36:57 INFO     	 * max_length: 512
2023-12-15 04:36:57 INFO     	 * max_length_output: 512
2023-12-15 04:36:57 INFO     	 * epoch: 15
2023-12-15 04:36:57 INFO     	 * batch: 2
2023-12-15 04:36:57 INFO     	 * lr: 1e-05
2023-12-15 04:36:57 INFO     	 * fp16: False
2023-12-15 04:36:57 INFO     	 * random_seed: 1
2023-12-15 04:36:57 INFO     	 * gradient_accumulation_steps: 2
2023-12-15 04:36:57 INFO     	 * label_smoothing: 0.0
2023-12-15 04:36:57 INFO     load checkpoint from small_trained_ckpt/model_nrudfu/epoch_10
2023-12-15 04:36:58 INFO     use spaCy answer extraction model: positionrank
2023-12-15 04:36:58 INFO     Model `small_trained_ckpt/model_nrudfu/epoch_10`
2023-12-15 04:36:58 INFO     	 * Num of GPU in use: 1
2023-12-15 04:36:58 INFO     	 * Prefix: True
2023-12-15 04:36:58 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 04:36:58 INFO     load optimizer from small_trained_ckpt/model_nrudfu/optimizers/optimizer.10.pt
2023-12-15 04:36:58 INFO     optimizer is loading on cuda
2023-12-15 04:37:28 INFO     dataset preprocessing
2023-12-15 04:37:29 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 04:37:29 INFO     start model training
2023-12-15 04:37:37 INFO     	 * (global step 50: loss: 0.4496455639600754, lr: 1e-05
2023-12-15 04:37:45 INFO     	 * (global step 100: loss: 0.30613577365875244, lr: 1e-05
2023-12-15 04:37:53 INFO     	 * (global step 150: loss: 0.38376612961292267, lr: 1e-05
2023-12-15 04:38:01 INFO     	 * (global step 200: loss: 0.458445280790329, lr: 1e-05
2023-12-15 04:38:09 INFO     	 * (global step 250: loss: 0.4920240044593811, lr: 1e-05
2023-12-15 04:38:17 INFO     	 * (global step 300: loss: 0.3872182220220566, lr: 1e-05
2023-12-15 04:38:25 INFO     	 * (global step 350: loss: 0.6425428837537766, lr: 1e-05
2023-12-15 04:38:33 INFO     	 * (global step 400: loss: 0.3977912664413452, lr: 1e-05
2023-12-15 04:38:41 INFO     	 * (global step 450: loss: 0.32791201770305634, lr: 1e-05
2023-12-15 04:38:49 INFO     	 * (global step 500: loss: 0.500120684504509, lr: 1e-05
2023-12-15 04:38:58 INFO     	 * (global step 550: loss: 0.3513919413089752, lr: 1e-05
2023-12-15 04:39:06 INFO     	 * (global step 600: loss: 0.6888209879398346, lr: 1e-05
2023-12-15 04:39:09 INFO     [epoch 10/15] average loss: 0.451, lr: 1e-05
2023-12-15 04:39:09 INFO     saving model related files
2023-12-15 04:39:09 INFO     saving model
2023-12-15 04:39:10 INFO     saving tokenizer
2023-12-15 04:39:10 INFO     saving optimizer
2023-12-15 04:39:11 INFO     remove old optimizer files
2023-12-15 04:39:15 INFO     	 * (global step 650: loss: 0.5290793180465698, lr: 1e-05
2023-12-15 04:39:24 INFO     	 * (global step 700: loss: 0.45102354884147644, lr: 1e-05
2023-12-15 04:39:32 INFO     	 * (global step 750: loss: 0.3763030469417572, lr: 1e-05
2023-12-15 04:39:40 INFO     	 * (global step 800: loss: 0.4612008184194565, lr: 1e-05
2023-12-15 04:39:48 INFO     	 * (global step 850: loss: 0.7232697457075119, lr: 1e-05
2023-12-15 04:39:56 INFO     	 * (global step 900: loss: 0.5947788506746292, lr: 1e-05
2023-12-15 04:40:04 INFO     	 * (global step 950: loss: 0.40346062183380127, lr: 1e-05
2023-12-15 04:40:12 INFO     	 * (global step 1000: loss: 0.6752543896436691, lr: 1e-05
2023-12-15 04:40:20 INFO     	 * (global step 1050: loss: 0.4102638363838196, lr: 1e-05
2023-12-15 04:40:28 INFO     	 * (global step 1100: loss: 0.5255715847015381, lr: 1e-05
2023-12-15 04:40:36 INFO     	 * (global step 1150: loss: 0.28671179711818695, lr: 1e-05
2023-12-15 04:40:44 INFO     	 * (global step 1200: loss: 0.6991834938526154, lr: 1e-05
2023-12-15 04:40:51 INFO     [epoch 11/15] average loss: 0.443, lr: 1e-05
2023-12-15 04:40:51 INFO     saving model related files
2023-12-15 04:40:51 INFO     saving model
2023-12-15 04:40:52 INFO     saving tokenizer
2023-12-15 04:40:52 INFO     saving optimizer
2023-12-15 04:40:53 INFO     remove old optimizer files
2023-12-15 04:40:54 INFO     	 * (global step 1250: loss: 0.5564457476139069, lr: 1e-05
2023-12-15 04:41:02 INFO     	 * (global step 1300: loss: 0.32367490977048874, lr: 1e-05
2023-12-15 04:41:10 INFO     	 * (global step 1350: loss: 0.21319925785064697, lr: 1e-05
2023-12-15 04:41:18 INFO     	 * (global step 1400: loss: 0.5183595269918442, lr: 1e-05
2023-12-15 04:41:26 INFO     	 * (global step 1450: loss: 0.47640539705753326, lr: 1e-05
2023-12-15 04:41:34 INFO     	 * (global step 1500: loss: 0.2823924794793129, lr: 1e-05
2023-12-15 04:41:43 INFO     	 * (global step 1550: loss: 0.3638869971036911, lr: 1e-05
2023-12-15 04:41:51 INFO     	 * (global step 1600: loss: 0.38998228311538696, lr: 1e-05
2023-12-15 04:41:59 INFO     	 * (global step 1650: loss: 0.3735499233007431, lr: 1e-05
2023-12-15 04:42:07 INFO     	 * (global step 1700: loss: 0.38802631199359894, lr: 1e-05
2023-12-15 04:42:15 INFO     	 * (global step 1750: loss: 0.33881308138370514, lr: 1e-05
2023-12-15 04:42:23 INFO     	 * (global step 1800: loss: 0.4489271193742752, lr: 1e-05
2023-12-15 04:42:31 INFO     	 * (global step 1850: loss: 0.3634178936481476, lr: 1e-05
2023-12-15 04:42:33 INFO     [epoch 12/15] average loss: 0.436, lr: 1e-05
2023-12-15 04:42:33 INFO     saving model related files
2023-12-15 04:42:33 INFO     saving model
2023-12-15 04:42:34 INFO     saving tokenizer
2023-12-15 04:42:34 INFO     saving optimizer
2023-12-15 04:42:35 INFO     remove old optimizer files
2023-12-15 04:42:41 INFO     	 * (global step 1900: loss: 0.2913345471024513, lr: 1e-05
2023-12-15 04:42:49 INFO     	 * (global step 1950: loss: 0.3443600535392761, lr: 1e-05
2023-12-15 04:42:57 INFO     	 * (global step 2000: loss: 0.5048031806945801, lr: 1e-05
2023-12-15 04:43:05 INFO     	 * (global step 2050: loss: 0.5577161908149719, lr: 1e-05
2023-12-15 04:43:13 INFO     	 * (global step 2100: loss: 0.40951181948184967, lr: 1e-05
2023-12-15 04:43:21 INFO     	 * (global step 2150: loss: 0.36173637211322784, lr: 1e-05
2023-12-15 04:43:29 INFO     	 * (global step 2200: loss: 0.4944714456796646, lr: 1e-05
2023-12-15 04:43:37 INFO     	 * (global step 2250: loss: 0.4377359002828598, lr: 1e-05
2023-12-15 04:43:46 INFO     	 * (global step 2300: loss: 0.4580064117908478, lr: 1e-05
2023-12-15 04:43:54 INFO     	 * (global step 2350: loss: 0.3993697017431259, lr: 1e-05
2023-12-15 04:44:02 INFO     	 * (global step 2400: loss: 0.534403383731842, lr: 1e-05
2023-12-15 04:44:10 INFO     	 * (global step 2450: loss: 0.3606875315308571, lr: 1e-05
2023-12-15 04:44:15 INFO     [epoch 13/15] average loss: 0.43, lr: 1e-05
2023-12-15 04:44:15 INFO     saving model related files
2023-12-15 04:44:15 INFO     saving model
2023-12-15 04:44:16 INFO     saving tokenizer
2023-12-15 04:44:16 INFO     saving optimizer
2023-12-15 04:44:17 INFO     remove old optimizer files
2023-12-15 04:44:20 INFO     	 * (global step 2500: loss: 0.34043489396572113, lr: 1e-05
2023-12-15 04:44:28 INFO     	 * (global step 2550: loss: 0.3219597190618515, lr: 1e-05
2023-12-15 04:44:36 INFO     	 * (global step 2600: loss: 0.6133054345846176, lr: 1e-05
2023-12-15 04:44:44 INFO     	 * (global step 2650: loss: 0.4854634255170822, lr: 1e-05
2023-12-15 04:44:52 INFO     	 * (global step 2700: loss: 0.3649510443210602, lr: 1e-05
2023-12-15 04:45:00 INFO     	 * (global step 2750: loss: 0.443041667342186, lr: 1e-05
2023-12-15 04:45:08 INFO     	 * (global step 2800: loss: 0.777031421661377, lr: 1e-05
2023-12-15 04:45:16 INFO     	 * (global step 2850: loss: 0.46113404631614685, lr: 1e-05
2023-12-15 04:45:24 INFO     	 * (global step 2900: loss: 0.26523688435554504, lr: 1e-05
2023-12-15 04:45:32 INFO     	 * (global step 2950: loss: 0.38095948100090027, lr: 1e-05
2023-12-15 04:45:40 INFO     	 * (global step 3000: loss: 0.5737696439027786, lr: 1e-05
2023-12-15 04:45:48 INFO     	 * (global step 3050: loss: 0.3565344363451004, lr: 1e-05
2023-12-15 04:45:57 INFO     	 * (global step 3100: loss: 0.35718637704849243, lr: 1e-05
2023-12-15 04:45:57 INFO     [epoch 14/15] average loss: 0.424, lr: 1e-05
2023-12-15 04:45:57 INFO     saving model related files
2023-12-15 04:45:57 INFO     saving model
2023-12-15 04:45:58 INFO     saving tokenizer
2023-12-15 04:45:58 INFO     saving optimizer
2023-12-15 04:45:59 INFO     remove old optimizer files
2023-12-15 04:45:59 INFO     complete training: model ckpt was saved at small_trained_ckpt/model_nrudfu
2023-12-15 04:45:59 INFO     ## 2nd RUN: Configuration 2/5: validation/Bleu_4 = 0.02679595996444357
2023-12-15 04:45:59 INFO     initialize model trainer
2023-12-15 04:45:59 INFO     load config from existing checkpoint at small_trained_ckpt/model_nxaqhy
2023-12-15 04:45:59 INFO     hyperparameters
2023-12-15 04:45:59 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 04:45:59 INFO     	 * dataset_name: default
2023-12-15 04:45:59 INFO     	 * input_types: ['paragraph']
2023-12-15 04:45:59 INFO     	 * output_types: ['questions_answers']
2023-12-15 04:45:59 INFO     	 * prefix_types: ['qag']
2023-12-15 04:45:59 INFO     	 * model: t5-small
2023-12-15 04:45:59 INFO     	 * max_length: 512
2023-12-15 04:45:59 INFO     	 * max_length_output: 512
2023-12-15 04:45:59 INFO     	 * epoch: 15
2023-12-15 04:45:59 INFO     	 * batch: 2
2023-12-15 04:45:59 INFO     	 * lr: 1e-05
2023-12-15 04:45:59 INFO     	 * fp16: False
2023-12-15 04:45:59 INFO     	 * random_seed: 1
2023-12-15 04:45:59 INFO     	 * gradient_accumulation_steps: 4
2023-12-15 04:45:59 INFO     	 * label_smoothing: 0.15
2023-12-15 04:45:59 INFO     load checkpoint from small_trained_ckpt/model_nxaqhy/epoch_10
2023-12-15 04:46:00 INFO     use spaCy answer extraction model: positionrank
2023-12-15 04:46:00 INFO     Model `small_trained_ckpt/model_nxaqhy/epoch_10`
2023-12-15 04:46:00 INFO     	 * Num of GPU in use: 1
2023-12-15 04:46:00 INFO     	 * Prefix: True
2023-12-15 04:46:00 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 04:46:00 INFO     load optimizer from small_trained_ckpt/model_nxaqhy/optimizers/optimizer.10.pt
2023-12-15 04:46:00 INFO     optimizer is loading on cuda
2023-12-15 04:46:24 INFO     dataset preprocessing
2023-12-15 04:46:26 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 04:46:26 INFO     start model training
2023-12-15 04:46:41 INFO     	 * (global step 50: loss: 3.2142703533172607, lr: 1e-05
2023-12-15 04:46:57 INFO     	 * (global step 100: loss: 3.0722952485084534, lr: 1e-05
2023-12-15 04:47:13 INFO     	 * (global step 150: loss: 2.9771822094917297, lr: 1e-05
2023-12-15 04:47:29 INFO     	 * (global step 200: loss: 2.8646212220191956, lr: 1e-05
2023-12-15 04:47:45 INFO     	 * (global step 250: loss: 2.938095986843109, lr: 1e-05
2023-12-15 04:48:01 INFO     	 * (global step 300: loss: 2.963389575481415, lr: 1e-05
2023-12-15 04:48:04 INFO     [epoch 10/15] average loss: 3.137, lr: 1e-05
2023-12-15 04:48:04 INFO     saving model related files
2023-12-15 04:48:04 INFO     saving model
2023-12-15 04:48:05 INFO     saving tokenizer
2023-12-15 04:48:05 INFO     saving optimizer
2023-12-15 04:48:06 INFO     remove old optimizer files
2023-12-15 04:48:19 INFO     	 * (global step 350: loss: 2.7846925854682922, lr: 1e-05
2023-12-15 04:48:35 INFO     	 * (global step 400: loss: 2.798571288585663, lr: 1e-05
2023-12-15 04:48:51 INFO     	 * (global step 450: loss: 2.8977245688438416, lr: 1e-05
2023-12-15 04:49:07 INFO     	 * (global step 500: loss: 2.8426042199134827, lr: 1e-05
2023-12-15 04:49:23 INFO     	 * (global step 550: loss: 2.905411124229431, lr: 1e-05
2023-12-15 04:49:39 INFO     	 * (global step 600: loss: 2.8397856950759888, lr: 1e-05
2023-12-15 04:49:45 INFO     [epoch 11/15] average loss: 2.805, lr: 1e-05
2023-12-15 04:49:45 INFO     saving model related files
2023-12-15 04:49:45 INFO     saving model
2023-12-15 04:49:46 INFO     saving tokenizer
2023-12-15 04:49:46 INFO     saving optimizer
2023-12-15 04:49:47 INFO     remove old optimizer files
2023-12-15 04:49:56 INFO     	 * (global step 650: loss: 2.7019346952438354, lr: 1e-05
2023-12-15 04:50:12 INFO     	 * (global step 700: loss: 2.694447338581085, lr: 1e-05
2023-12-15 04:50:28 INFO     	 * (global step 750: loss: 2.716581881046295, lr: 1e-05
2023-12-15 04:50:44 INFO     	 * (global step 800: loss: 2.7039403319358826, lr: 1e-05
2023-12-15 04:51:00 INFO     	 * (global step 850: loss: 2.8813031911849976, lr: 1e-05
2023-12-15 04:51:16 INFO     	 * (global step 900: loss: 2.8476189374923706, lr: 1e-05
2023-12-15 04:51:26 INFO     [epoch 12/15] average loss: 2.737, lr: 1e-05
2023-12-15 04:51:26 INFO     saving model related files
2023-12-15 04:51:26 INFO     saving model
2023-12-15 04:51:27 INFO     saving tokenizer
2023-12-15 04:51:27 INFO     saving optimizer
2023-12-15 04:51:28 INFO     remove old optimizer files
2023-12-15 04:51:34 INFO     	 * (global step 950: loss: 2.821207106113434, lr: 1e-05
2023-12-15 04:51:50 INFO     	 * (global step 1000: loss: 2.670786142349243, lr: 1e-05
2023-12-15 04:52:06 INFO     	 * (global step 1050: loss: 2.686358392238617, lr: 1e-05
2023-12-15 04:52:22 INFO     	 * (global step 1100: loss: 2.619095742702484, lr: 1e-05
2023-12-15 04:52:38 INFO     	 * (global step 1150: loss: 2.600089132785797, lr: 1e-05
2023-12-15 04:52:54 INFO     	 * (global step 1200: loss: 2.684172749519348, lr: 1e-05
2023-12-15 04:53:07 INFO     [epoch 13/15] average loss: 2.696, lr: 1e-05
2023-12-15 04:53:07 INFO     saving model related files
2023-12-15 04:53:07 INFO     saving model
2023-12-15 04:53:08 INFO     saving tokenizer
2023-12-15 04:53:08 INFO     saving optimizer
2023-12-15 04:53:09 INFO     remove old optimizer files
2023-12-15 04:53:12 INFO     	 * (global step 1250: loss: 2.684213876724243, lr: 1e-05
2023-12-15 04:53:28 INFO     	 * (global step 1300: loss: 2.5506439208984375, lr: 1e-05
2023-12-15 04:53:44 INFO     	 * (global step 1350: loss: 2.673559069633484, lr: 1e-05
2023-12-15 04:54:00 INFO     	 * (global step 1400: loss: 2.5952293276786804, lr: 1e-05
2023-12-15 04:54:16 INFO     	 * (global step 1450: loss: 2.5965030789375305, lr: 1e-05
2023-12-15 04:54:32 INFO     	 * (global step 1500: loss: 2.7942174077033997, lr: 1e-05
2023-12-15 04:54:48 INFO     	 * (global step 1550: loss: 2.5923580527305603, lr: 1e-05
2023-12-15 04:54:48 INFO     [epoch 14/15] average loss: 2.664, lr: 1e-05
2023-12-15 04:54:48 INFO     saving model related files
2023-12-15 04:54:48 INFO     saving model
2023-12-15 04:54:48 INFO     saving tokenizer
2023-12-15 04:54:48 INFO     saving optimizer
2023-12-15 04:54:49 INFO     remove old optimizer files
2023-12-15 04:54:49 INFO     complete training: model ckpt was saved at small_trained_ckpt/model_nxaqhy
2023-12-15 04:54:49 INFO     ## 2nd RUN: Configuration 3/5: validation/Bleu_4 = 0.02679595996444357
2023-12-15 04:54:50 INFO     initialize model trainer
2023-12-15 04:54:50 INFO     load config from existing checkpoint at small_trained_ckpt/model_vhyoja
2023-12-15 04:54:50 INFO     hyperparameters
2023-12-15 04:54:50 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 04:54:50 INFO     	 * dataset_name: default
2023-12-15 04:54:50 INFO     	 * input_types: ['paragraph']
2023-12-15 04:54:50 INFO     	 * output_types: ['questions_answers']
2023-12-15 04:54:50 INFO     	 * prefix_types: ['qag']
2023-12-15 04:54:50 INFO     	 * model: t5-small
2023-12-15 04:54:50 INFO     	 * max_length: 512
2023-12-15 04:54:50 INFO     	 * max_length_output: 512
2023-12-15 04:54:50 INFO     	 * epoch: 15
2023-12-15 04:54:50 INFO     	 * batch: 2
2023-12-15 04:54:50 INFO     	 * lr: 1e-05
2023-12-15 04:54:50 INFO     	 * fp16: False
2023-12-15 04:54:50 INFO     	 * random_seed: 1
2023-12-15 04:54:50 INFO     	 * gradient_accumulation_steps: 4
2023-12-15 04:54:50 INFO     	 * label_smoothing: 0.0
2023-12-15 04:54:50 INFO     load checkpoint from small_trained_ckpt/model_vhyoja/epoch_10
2023-12-15 04:54:50 INFO     use spaCy answer extraction model: positionrank
2023-12-15 04:54:51 INFO     Model `small_trained_ckpt/model_vhyoja/epoch_10`
2023-12-15 04:54:51 INFO     	 * Num of GPU in use: 1
2023-12-15 04:54:51 INFO     	 * Prefix: True
2023-12-15 04:54:51 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 04:54:51 INFO     load optimizer from small_trained_ckpt/model_vhyoja/optimizers/optimizer.10.pt
2023-12-15 04:54:51 INFO     optimizer is loading on cuda
2023-12-15 04:55:16 INFO     dataset preprocessing
2023-12-15 04:55:17 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 04:55:18 INFO     start model training
2023-12-15 04:55:33 INFO     	 * (global step 50: loss: 0.41874635964632034, lr: 1e-05
2023-12-15 04:55:49 INFO     	 * (global step 100: loss: 0.4871516674757004, lr: 1e-05
2023-12-15 04:56:04 INFO     	 * (global step 150: loss: 0.47077561914920807, lr: 1e-05
2023-12-15 04:56:20 INFO     	 * (global step 200: loss: 0.4403390884399414, lr: 1e-05
2023-12-15 04:56:35 INFO     	 * (global step 250: loss: 0.5523229911923409, lr: 1e-05
2023-12-15 04:56:51 INFO     	 * (global step 300: loss: 0.6081260517239571, lr: 1e-05
2023-12-15 04:56:54 INFO     [epoch 10/15] average loss: 0.5, lr: 1e-05
2023-12-15 04:56:54 INFO     saving model related files
2023-12-15 04:56:54 INFO     saving model
2023-12-15 04:56:55 INFO     saving tokenizer
2023-12-15 04:56:55 INFO     saving optimizer
2023-12-15 04:56:56 INFO     remove old optimizer files
2023-12-15 04:57:09 INFO     	 * (global step 350: loss: 0.4443930983543396, lr: 1e-05
2023-12-15 04:57:24 INFO     	 * (global step 400: loss: 0.4513590931892395, lr: 1e-05
2023-12-15 04:57:40 INFO     	 * (global step 450: loss: 0.5981110110878944, lr: 1e-05
2023-12-15 04:57:55 INFO     	 * (global step 500: loss: 0.5734274387359619, lr: 1e-05
2023-12-15 04:58:11 INFO     	 * (global step 550: loss: 0.6219593212008476, lr: 1e-05
2023-12-15 04:58:26 INFO     	 * (global step 600: loss: 0.5617053881287575, lr: 1e-05
2023-12-15 04:58:33 INFO     [epoch 11/15] average loss: 0.491, lr: 1e-05
2023-12-15 04:58:33 INFO     saving model related files
2023-12-15 04:58:33 INFO     saving model
2023-12-15 04:58:33 INFO     saving tokenizer
2023-12-15 04:58:33 INFO     saving optimizer
2023-12-15 04:58:34 INFO     remove old optimizer files
2023-12-15 04:58:44 INFO     	 * (global step 650: loss: 0.4467748776078224, lr: 1e-05
2023-12-15 04:59:00 INFO     	 * (global step 700: loss: 0.4235490597784519, lr: 1e-05
2023-12-15 04:59:15 INFO     	 * (global step 750: loss: 0.45581451058387756, lr: 1e-05
2023-12-15 04:59:31 INFO     	 * (global step 800: loss: 0.4585486948490143, lr: 1e-05
2023-12-15 04:59:46 INFO     	 * (global step 850: loss: 0.6416958495974541, lr: 1e-05
2023-12-15 05:00:02 INFO     	 * (global step 900: loss: 0.6153014451265335, lr: 1e-05
2023-12-15 05:00:12 INFO     [epoch 12/15] average loss: 0.482, lr: 1e-05
2023-12-15 05:00:12 INFO     saving model related files
2023-12-15 05:00:12 INFO     saving model
2023-12-15 05:00:12 INFO     saving tokenizer
2023-12-15 05:00:12 INFO     saving optimizer
2023-12-15 05:00:13 INFO     remove old optimizer files
2023-12-15 05:00:20 INFO     	 * (global step 950: loss: 0.5819598659873009, lr: 1e-05
2023-12-15 05:00:35 INFO     	 * (global step 1000: loss: 0.4502054937183857, lr: 1e-05
2023-12-15 05:00:51 INFO     	 * (global step 1050: loss: 0.45738520473241806, lr: 1e-05
2023-12-15 05:01:06 INFO     	 * (global step 1100: loss: 0.394372895359993, lr: 1e-05
2023-12-15 05:01:22 INFO     	 * (global step 1150: loss: 0.36679207533597946, lr: 1e-05
2023-12-15 05:01:37 INFO     	 * (global step 1200: loss: 0.4751862660050392, lr: 1e-05
2023-12-15 05:01:50 INFO     [epoch 13/15] average loss: 0.475, lr: 1e-05
2023-12-15 05:01:50 INFO     saving model related files
2023-12-15 05:01:50 INFO     saving model
2023-12-15 05:01:50 INFO     saving tokenizer
2023-12-15 05:01:50 INFO     saving optimizer
2023-12-15 05:01:51 INFO     remove old optimizer files
2023-12-15 05:01:54 INFO     	 * (global step 1250: loss: 0.47825879603624344, lr: 1e-05
2023-12-15 05:02:10 INFO     	 * (global step 1300: loss: 0.33165206760168076, lr: 1e-05
2023-12-15 05:02:25 INFO     	 * (global step 1350: loss: 0.47139114141464233, lr: 1e-05
2023-12-15 05:02:41 INFO     	 * (global step 1400: loss: 0.37907901406288147, lr: 1e-05
2023-12-15 05:02:57 INFO     	 * (global step 1450: loss: 0.4027574732899666, lr: 1e-05
2023-12-15 05:03:12 INFO     	 * (global step 1500: loss: 0.6063618138432503, lr: 1e-05
2023-12-15 05:03:28 INFO     	 * (global step 1550: loss: 0.40627144277095795, lr: 1e-05
2023-12-15 05:03:28 INFO     [epoch 14/15] average loss: 0.468, lr: 1e-05
2023-12-15 05:03:28 INFO     saving model related files
2023-12-15 05:03:28 INFO     saving model
2023-12-15 05:03:29 INFO     saving tokenizer
2023-12-15 05:03:29 INFO     saving optimizer
2023-12-15 05:03:30 INFO     remove old optimizer files
2023-12-15 05:03:30 INFO     complete training: model ckpt was saved at small_trained_ckpt/model_vhyoja
2023-12-15 05:03:30 INFO     ## 2nd RUN: Configuration 4/5: validation/Bleu_4 = 0.024857383229010453
2023-12-15 05:03:30 INFO     initialize model trainer
2023-12-15 05:03:30 INFO     load config from existing checkpoint at small_trained_ckpt/model_eszyci
2023-12-15 05:03:30 INFO     hyperparameters
2023-12-15 05:03:30 INFO     	 * dataset_path: StellarMilk/newsqa
2023-12-15 05:03:30 INFO     	 * dataset_name: default
2023-12-15 05:03:30 INFO     	 * input_types: ['paragraph']
2023-12-15 05:03:30 INFO     	 * output_types: ['questions_answers']
2023-12-15 05:03:30 INFO     	 * prefix_types: ['qag']
2023-12-15 05:03:30 INFO     	 * model: t5-small
2023-12-15 05:03:30 INFO     	 * max_length: 512
2023-12-15 05:03:30 INFO     	 * max_length_output: 512
2023-12-15 05:03:30 INFO     	 * epoch: 15
2023-12-15 05:03:30 INFO     	 * batch: 2
2023-12-15 05:03:30 INFO     	 * lr: 0.0001
2023-12-15 05:03:30 INFO     	 * fp16: False
2023-12-15 05:03:30 INFO     	 * random_seed: 1
2023-12-15 05:03:30 INFO     	 * gradient_accumulation_steps: 2
2023-12-15 05:03:30 INFO     	 * label_smoothing: 0.15
2023-12-15 05:03:30 INFO     load checkpoint from small_trained_ckpt/model_eszyci/epoch_10
2023-12-15 05:03:30 INFO     use spaCy answer extraction model: positionrank
2023-12-15 05:03:31 INFO     Model `small_trained_ckpt/model_eszyci/epoch_10`
2023-12-15 05:03:31 INFO     	 * Num of GPU in use: 1
2023-12-15 05:03:31 INFO     	 * Prefix: True
2023-12-15 05:03:31 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 05:03:31 INFO     load optimizer from small_trained_ckpt/model_eszyci/optimizers/optimizer.10.pt
2023-12-15 05:03:31 INFO     optimizer is loading on cuda
2023-12-15 05:03:54 INFO     dataset preprocessing
2023-12-15 05:03:55 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-15 05:03:56 INFO     start model training
2023-12-15 05:04:04 INFO     	 * (global step 50: loss: 2.6573402881622314, lr: 0.0001
2023-12-15 05:04:12 INFO     	 * (global step 100: loss: 2.4304548501968384, lr: 0.0001
2023-12-15 05:04:20 INFO     	 * (global step 150: loss: 2.448151111602783, lr: 0.0001
2023-12-15 05:04:29 INFO     	 * (global step 200: loss: 2.4905405044555664, lr: 0.0001
2023-12-15 05:04:37 INFO     	 * (global step 250: loss: 2.477782368659973, lr: 0.0001
2023-12-15 05:04:45 INFO     	 * (global step 300: loss: 2.412869453430176, lr: 0.0001
2023-12-15 05:04:53 INFO     	 * (global step 350: loss: 2.592427968978882, lr: 0.0001
2023-12-15 05:05:02 INFO     	 * (global step 400: loss: 2.4013975858688354, lr: 0.0001
2023-12-15 05:05:10 INFO     	 * (global step 450: loss: 2.343756914138794, lr: 0.0001
2023-12-15 05:05:18 INFO     	 * (global step 500: loss: 2.448922872543335, lr: 0.0001
2023-12-15 05:05:27 INFO     	 * (global step 550: loss: 2.343118667602539, lr: 0.0001
2023-12-15 05:05:35 INFO     	 * (global step 600: loss: 2.5957717895507812, lr: 0.0001
2023-12-15 05:05:39 INFO     [epoch 10/15] average loss: 2.516, lr: 0.0001
2023-12-15 05:05:39 INFO     saving model related files
2023-12-15 05:05:39 INFO     saving model
2023-12-15 05:05:39 INFO     saving tokenizer
2023-12-15 05:05:39 INFO     saving optimizer
2023-12-15 05:05:40 INFO     remove old optimizer files
2023-12-15 05:05:45 INFO     	 * (global step 650: loss: 2.454154133796692, lr: 0.0001
2023-12-15 05:05:54 INFO     	 * (global step 700: loss: 2.4253381490707397, lr: 0.0001
2023-12-15 05:06:02 INFO     	 * (global step 750: loss: 2.3677607774734497, lr: 0.0001
2023-12-15 05:06:10 INFO     	 * (global step 800: loss: 2.4064334630966187, lr: 0.0001
2023-12-15 05:06:19 INFO     	 * (global step 850: loss: 2.597411036491394, lr: 0.0001
2023-12-15 05:06:27 INFO     	 * (global step 900: loss: 2.4830089807510376, lr: 0.0001
2023-12-15 05:06:36 INFO     	 * (global step 950: loss: 2.374028444290161, lr: 0.0001
2023-12-15 05:06:44 INFO     	 * (global step 1000: loss: 2.5464727878570557, lr: 0.0001
2023-12-15 05:06:52 INFO     	 * (global step 1050: loss: 2.3602633476257324, lr: 0.0001
2023-12-15 05:07:01 INFO     	 * (global step 1100: loss: 2.456254243850708, lr: 0.0001
2023-12-15 05:07:09 INFO     	 * (global step 1150: loss: 2.28319251537323, lr: 0.0001
2023-12-15 05:07:17 INFO     	 * (global step 1200: loss: 2.56051504611969, lr: 0.0001
2023-12-15 05:07:24 INFO     [epoch 11/15] average loss: 2.397, lr: 0.0001
2023-12-15 05:07:24 INFO     saving model related files
2023-12-15 05:07:24 INFO     saving model
2023-12-15 05:07:25 INFO     saving tokenizer
2023-12-15 05:07:25 INFO     saving optimizer
2023-12-15 05:07:26 INFO     remove old optimizer files
2023-12-15 05:07:27 INFO     	 * (global step 1250: loss: 2.456742286682129, lr: 0.0001
2023-12-15 05:07:36 INFO     	 * (global step 1300: loss: 2.2950721979141235, lr: 0.0001
2023-12-15 05:07:44 INFO     	 * (global step 1350: loss: 2.2273194789886475, lr: 0.0001
2023-12-15 05:07:52 INFO     	 * (global step 1400: loss: 2.4430224895477295, lr: 0.0001
2023-12-15 05:08:01 INFO     	 * (global step 1450: loss: 2.412691354751587, lr: 0.0001
2023-12-15 05:08:09 INFO     	 * (global step 1500: loss: 2.27191162109375, lr: 0.0001
2023-12-15 05:08:18 INFO     	 * (global step 1550: loss: 2.324491262435913, lr: 0.0001
2023-12-15 05:08:26 INFO     	 * (global step 1600: loss: 2.3396339416503906, lr: 0.0001
2023-12-15 05:08:34 INFO     	 * (global step 1650: loss: 2.333460211753845, lr: 0.0001
2023-12-15 05:08:43 INFO     	 * (global step 1700: loss: 2.344846487045288, lr: 0.0001
2023-12-15 05:08:51 INFO     	 * (global step 1750: loss: 2.312524437904358, lr: 0.0001
2023-12-15 05:08:59 INFO     	 * (global step 1800: loss: 2.382927179336548, lr: 0.0001
2023-12-15 05:09:08 INFO     	 * (global step 1850: loss: 2.3124643564224243, lr: 0.0001
2023-12-15 05:09:10 INFO     [epoch 12/15] average loss: 2.375, lr: 0.0001
2023-12-15 05:09:10 INFO     saving model related files
2023-12-15 05:09:10 INFO     saving model
2023-12-15 05:09:11 INFO     saving tokenizer
2023-12-15 05:09:11 INFO     saving optimizer
2023-12-15 05:09:11 INFO     remove old optimizer files
2023-12-15 05:09:18 INFO     	 * (global step 1900: loss: 2.263749122619629, lr: 0.0001
2023-12-15 05:09:26 INFO     	 * (global step 1950: loss: 2.319665789604187, lr: 0.0001
2023-12-15 05:09:34 INFO     	 * (global step 2000: loss: 2.4105972051620483, lr: 0.0001
2023-12-15 05:09:43 INFO     	 * (global step 2050: loss: 2.4563419818878174, lr: 0.0001
2023-12-15 05:09:51 INFO     	 * (global step 2100: loss: 2.3530722856521606, lr: 0.0001
2023-12-15 05:09:59 INFO     	 * (global step 2150: loss: 2.329246759414673, lr: 0.0001
2023-12-15 05:10:08 INFO     	 * (global step 2200: loss: 2.405771255493164, lr: 0.0001
2023-12-15 05:10:16 INFO     	 * (global step 2250: loss: 2.362202763557434, lr: 0.0001
2023-12-15 05:10:25 INFO     	 * (global step 2300: loss: 2.380563259124756, lr: 0.0001
2023-12-15 05:10:33 INFO     	 * (global step 2350: loss: 2.3398555517196655, lr: 0.0001
2023-12-15 05:10:41 INFO     	 * (global step 2400: loss: 2.4385091066360474, lr: 0.0001
2023-12-15 05:10:50 INFO     	 * (global step 2450: loss: 2.3183001279830933, lr: 0.0001
2023-12-15 05:10:55 INFO     [epoch 13/15] average loss: 2.36, lr: 0.0001
2023-12-15 05:10:55 INFO     saving model related files
2023-12-15 05:10:55 INFO     saving model
2023-12-15 05:10:56 INFO     saving tokenizer
2023-12-15 05:10:56 INFO     saving optimizer
2023-12-15 05:10:57 INFO     remove old optimizer files
2023-12-15 05:11:00 INFO     	 * (global step 2500: loss: 2.2885518074035645, lr: 0.0001
2023-12-15 05:11:08 INFO     	 * (global step 2550: loss: 2.28608238697052, lr: 0.0001
2023-12-15 05:11:16 INFO     	 * (global step 2600: loss: 2.459415912628174, lr: 0.0001
2023-12-15 05:11:25 INFO     	 * (global step 2650: loss: 2.374444603919983, lr: 0.0001
2023-12-15 05:11:33 INFO     	 * (global step 2700: loss: 2.304111361503601, lr: 0.0001
2023-12-15 05:11:41 INFO     	 * (global step 2750: loss: 2.3563361167907715, lr: 0.0001
2023-12-15 05:11:50 INFO     	 * (global step 2800: loss: 2.575770854949951, lr: 0.0001
2023-12-15 05:11:58 INFO     	 * (global step 2850: loss: 2.362609624862671, lr: 0.0001
2023-12-15 05:12:06 INFO     	 * (global step 2900: loss: 2.2415114641189575, lr: 0.0001
2023-12-15 05:12:15 INFO     	 * (global step 2950: loss: 2.3122963905334473, lr: 0.0001
2023-12-15 05:12:23 INFO     	 * (global step 3000: loss: 2.4369800090789795, lr: 0.0001
2023-12-15 05:12:31 INFO     	 * (global step 3050: loss: 2.3096522092819214, lr: 0.0001
2023-12-15 05:12:40 INFO     	 * (global step 3100: loss: 2.308119535446167, lr: 0.0001
2023-12-15 05:12:41 INFO     [epoch 14/15] average loss: 2.349, lr: 0.0001
2023-12-15 05:12:41 INFO     saving model related files
2023-12-15 05:12:41 INFO     saving model
2023-12-15 05:12:41 INFO     saving tokenizer
2023-12-15 05:12:41 INFO     saving optimizer
2023-12-15 05:12:42 INFO     remove old optimizer files
2023-12-15 05:12:42 INFO     complete training: model ckpt was saved at small_trained_ckpt/model_eszyci
2023-12-15 05:12:42 INFO     ## 2nd RUN (EVAL): Configuration 0/5 ##
2023-12-15 05:13:01 INFO     use spaCy answer extraction model: positionrank
2023-12-15 05:13:01 INFO     Model `small_trained_ckpt/model_oprhlh/epoch_1`
2023-12-15 05:13:01 INFO     	 * Num of GPU in use: 1
2023-12-15 05:13:01 INFO     	 * Prefix: True
2023-12-15 05:13:01 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 05:13:02 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 05:18:41 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 05:24:19 WARNING  prediction not found at the evaluation
2023-12-15 05:24:19 WARNING  prediction not found at the evaluation
2023-12-15 05:24:19 INFO     	Bleu_1: 0.0
2023-12-15 05:24:19 INFO     	Bleu_2: 0.0
2023-12-15 05:24:19 INFO     	Bleu_3: 0.0
2023-12-15 05:24:19 INFO     	Bleu_4: 0.0
2023-12-15 05:24:20 WARNING  prediction not found at the evaluation
2023-12-15 05:24:20 WARNING  prediction not found at the evaluation
2023-12-15 05:24:20 INFO     	Bleu_1: 0.0
2023-12-15 05:24:20 INFO     	Bleu_2: 0.0
2023-12-15 05:24:20 INFO     	Bleu_3: 0.0
2023-12-15 05:24:20 INFO     	Bleu_4: 0.0
2023-12-15 05:24:40 INFO     use spaCy answer extraction model: positionrank
2023-12-15 05:24:41 INFO     Model `small_trained_ckpt/model_oprhlh/epoch_11`
2023-12-15 05:24:41 INFO     	 * Num of GPU in use: 1
2023-12-15 05:24:41 INFO     	 * Prefix: True
2023-12-15 05:24:41 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 05:24:42 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 05:29:31 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 05:34:27 INFO     	Bleu_1: 0.16311791656619046
2023-12-15 05:34:27 INFO     	Bleu_2: 0.09046008709134094
2023-12-15 05:34:27 INFO     	Bleu_3: 0.049235823516480294
2023-12-15 05:34:27 INFO     	Bleu_4: 0.03176736117417117
2023-12-15 05:34:27 INFO     	Bleu_1: 0.16231687202871636
2023-12-15 05:34:27 INFO     	Bleu_2: 0.08932449176371555
2023-12-15 05:34:27 INFO     	Bleu_3: 0.04808532765452977
2023-12-15 05:34:27 INFO     	Bleu_4: 0.030187326027319727
2023-12-15 05:34:34 INFO     use spaCy answer extraction model: positionrank
2023-12-15 05:34:34 INFO     Model `small_trained_ckpt/model_oprhlh/epoch_12`
2023-12-15 05:34:34 INFO     	 * Num of GPU in use: 1
2023-12-15 05:34:34 INFO     	 * Prefix: True
2023-12-15 05:34:34 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 05:34:35 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 05:39:51 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 05:45:08 INFO     	Bleu_1: 0.12244407794661487
2023-12-15 05:45:08 INFO     	Bleu_2: 0.06812808044694102
2023-12-15 05:45:08 INFO     	Bleu_3: 0.036764751404655724
2023-12-15 05:45:08 INFO     	Bleu_4: 0.023561801308342623
2023-12-15 05:45:09 INFO     	Bleu_1: 0.1277156188851527
2023-12-15 05:45:09 INFO     	Bleu_2: 0.07108712897312193
2023-12-15 05:45:09 INFO     	Bleu_3: 0.03842797578154592
2023-12-15 05:45:09 INFO     	Bleu_4: 0.0244478861017986
2023-12-15 05:45:23 INFO     use spaCy answer extraction model: positionrank
2023-12-15 05:45:23 INFO     Model `small_trained_ckpt/model_oprhlh/epoch_13`
2023-12-15 05:45:23 INFO     	 * Num of GPU in use: 1
2023-12-15 05:45:23 INFO     	 * Prefix: True
2023-12-15 05:45:23 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 05:45:24 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 05:50:47 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 05:56:11 INFO     	Bleu_1: 0.11773531327531989
2023-12-15 05:56:11 INFO     	Bleu_2: 0.06491541402195065
2023-12-15 05:56:11 INFO     	Bleu_3: 0.03462775187929386
2023-12-15 05:56:11 INFO     	Bleu_4: 0.02183067950134625
2023-12-15 05:56:11 INFO     	Bleu_1: 0.11395049140397008
2023-12-15 05:56:11 INFO     	Bleu_2: 0.0630828399208654
2023-12-15 05:56:11 INFO     	Bleu_3: 0.03371471011644435
2023-12-15 05:56:11 INFO     	Bleu_4: 0.021403741165953023
2023-12-15 05:56:28 INFO     use spaCy answer extraction model: positionrank
2023-12-15 05:56:28 INFO     Model `small_trained_ckpt/model_oprhlh/epoch_14`
2023-12-15 05:56:28 INFO     	 * Num of GPU in use: 1
2023-12-15 05:56:28 INFO     	 * Prefix: True
2023-12-15 05:56:28 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 05:56:29 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 06:01:51 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 06:07:14 INFO     	Bleu_1: 0.1128573668887169
2023-12-15 06:07:14 INFO     	Bleu_2: 0.06206293868849736
2023-12-15 06:07:14 INFO     	Bleu_3: 0.03297047398772786
2023-12-15 06:07:14 INFO     	Bleu_4: 0.020800321588617866
2023-12-15 06:07:14 INFO     	Bleu_1: 0.11121871175133419
2023-12-15 06:07:14 INFO     	Bleu_2: 0.06131475682199701
2023-12-15 06:07:14 INFO     	Bleu_3: 0.03281626039883236
2023-12-15 06:07:14 INFO     	Bleu_4: 0.020856858466263405
2023-12-15 06:07:29 INFO     use spaCy answer extraction model: positionrank
2023-12-15 06:07:30 INFO     Model `small_trained_ckpt/model_oprhlh/epoch_15`
2023-12-15 06:07:30 INFO     	 * Num of GPU in use: 1
2023-12-15 06:07:30 INFO     	 * Prefix: True
2023-12-15 06:07:30 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 06:07:31 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 06:12:55 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 06:18:20 INFO     	Bleu_1: 0.10973144862366525
2023-12-15 06:18:20 INFO     	Bleu_2: 0.05990778908399197
2023-12-15 06:18:20 INFO     	Bleu_3: 0.031022555931322155
2023-12-15 06:18:20 INFO     	Bleu_4: 0.019132843985108217
2023-12-15 06:18:20 INFO     	Bleu_1: 0.10755334518071451
2023-12-15 06:18:20 INFO     	Bleu_2: 0.059102478782485444
2023-12-15 06:18:20 INFO     	Bleu_3: 0.031458527671949114
2023-12-15 06:18:20 INFO     	Bleu_4: 0.02001185626345796
2023-12-15 06:18:31 INFO     use spaCy answer extraction model: positionrank
2023-12-15 06:18:32 INFO     Model `small_trained_ckpt/model_oprhlh/epoch_2`
2023-12-15 06:18:32 INFO     	 * Num of GPU in use: 1
2023-12-15 06:18:32 INFO     	 * Prefix: True
2023-12-15 06:18:32 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 06:18:33 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 06:24:13 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 06:29:50 WARNING  prediction not found at the evaluation
2023-12-15 06:29:50 WARNING  prediction not found at the evaluation
2023-12-15 06:29:50 INFO     	Bleu_1: 0.0
2023-12-15 06:29:50 INFO     	Bleu_2: 0.0
2023-12-15 06:29:50 INFO     	Bleu_3: 0.0
2023-12-15 06:29:50 INFO     	Bleu_4: 0.0
2023-12-15 06:29:50 WARNING  prediction not found at the evaluation
2023-12-15 06:29:50 WARNING  prediction not found at the evaluation
2023-12-15 06:29:51 INFO     	Bleu_1: 0.0
2023-12-15 06:29:51 INFO     	Bleu_2: 0.0
2023-12-15 06:29:51 INFO     	Bleu_3: 0.0
2023-12-15 06:29:51 INFO     	Bleu_4: 0.0
2023-12-15 06:30:10 INFO     use spaCy answer extraction model: positionrank
2023-12-15 06:30:10 INFO     Model `small_trained_ckpt/model_oprhlh/epoch_3`
2023-12-15 06:30:10 INFO     	 * Num of GPU in use: 1
2023-12-15 06:30:10 INFO     	 * Prefix: True
2023-12-15 06:30:10 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 06:30:11 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 06:35:38 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 06:41:03 WARNING  prediction not found at the evaluation
2023-12-15 06:41:03 WARNING  prediction not found at the evaluation
2023-12-15 06:41:03 INFO     	Bleu_1: 0.0
2023-12-15 06:41:03 INFO     	Bleu_2: 0.0
2023-12-15 06:41:03 INFO     	Bleu_3: 0.0
2023-12-15 06:41:03 INFO     	Bleu_4: 0.0
2023-12-15 06:41:03 WARNING  prediction not found at the evaluation
2023-12-15 06:41:03 WARNING  prediction not found at the evaluation
2023-12-15 06:41:03 INFO     	Bleu_1: 0.0
2023-12-15 06:41:03 INFO     	Bleu_2: 0.0
2023-12-15 06:41:03 INFO     	Bleu_3: 0.0
2023-12-15 06:41:03 INFO     	Bleu_4: 0.0
2023-12-15 06:41:17 INFO     use spaCy answer extraction model: positionrank
2023-12-15 06:41:18 INFO     Model `small_trained_ckpt/model_oprhlh/epoch_4`
2023-12-15 06:41:18 INFO     	 * Num of GPU in use: 1
2023-12-15 06:41:18 INFO     	 * Prefix: True
2023-12-15 06:41:18 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 06:41:19 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 06:46:46 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 06:52:12 WARNING  prediction not found at the evaluation
2023-12-15 06:52:12 WARNING  prediction not found at the evaluation
2023-12-15 06:52:12 INFO     	Bleu_1: 6.539730149434986e-85
2023-12-15 06:52:12 INFO     	Bleu_2: 3.374263033314095e-85
2023-12-15 06:52:12 INFO     	Bleu_3: 1.6595259972966896e-85
2023-12-15 06:52:12 INFO     	Bleu_4: 1.1046995181675649e-85
2023-12-15 06:52:12 WARNING  prediction not found at the evaluation
2023-12-15 06:52:12 WARNING  prediction not found at the evaluation
2023-12-15 06:52:12 INFO     	Bleu_1: 2.386252141432754e-40
2023-12-15 06:52:12 INFO     	Bleu_2: 1.239363554945379e-40
2023-12-15 06:52:12 INFO     	Bleu_3: 4.80531134794778e-41
2023-12-15 06:52:12 INFO     	Bleu_4: 3.7719069613620317e-45
2023-12-15 06:52:27 INFO     use spaCy answer extraction model: positionrank
2023-12-15 06:52:27 INFO     Model `small_trained_ckpt/model_oprhlh/epoch_5`
2023-12-15 06:52:27 INFO     	 * Num of GPU in use: 1
2023-12-15 06:52:27 INFO     	 * Prefix: True
2023-12-15 06:52:27 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 06:52:28 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 06:57:43 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 07:02:57 INFO     	Bleu_1: 0.13324637579662324
2023-12-15 07:02:57 INFO     	Bleu_2: 0.07398431119691543
2023-12-15 07:02:57 INFO     	Bleu_3: 0.04041700791028718
2023-12-15 07:02:57 INFO     	Bleu_4: 0.025943565572567634
2023-12-15 07:02:57 WARNING  prediction not found at the evaluation
2023-12-15 07:02:57 WARNING  prediction not found at the evaluation
2023-12-15 07:02:57 INFO     	Bleu_1: 0.1373168044587213
2023-12-15 07:02:57 INFO     	Bleu_2: 0.07480260508483894
2023-12-15 07:02:57 INFO     	Bleu_3: 0.04015920941305496
2023-12-15 07:02:57 INFO     	Bleu_4: 0.025076724553575288
2023-12-15 07:03:13 INFO     use spaCy answer extraction model: positionrank
2023-12-15 07:03:13 INFO     Model `small_trained_ckpt/model_oprhlh/epoch_6`
2023-12-15 07:03:13 INFO     	 * Num of GPU in use: 1
2023-12-15 07:03:13 INFO     	 * Prefix: True
2023-12-15 07:03:13 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 07:03:15 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 07:08:28 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 07:13:39 INFO     	Bleu_1: 0.13922412631086942
2023-12-15 07:13:39 INFO     	Bleu_2: 0.07688093182098787
2023-12-15 07:13:39 INFO     	Bleu_3: 0.0420357337437764
2023-12-15 07:13:39 INFO     	Bleu_4: 0.027103802649494652
2023-12-15 07:13:40 INFO     	Bleu_1: 0.1392329383209582
2023-12-15 07:13:40 INFO     	Bleu_2: 0.07635165586146143
2023-12-15 07:13:40 INFO     	Bleu_3: 0.041048712206771004
2023-12-15 07:13:40 INFO     	Bleu_4: 0.025892216436099878
2023-12-15 07:13:53 INFO     use spaCy answer extraction model: positionrank
2023-12-15 07:13:53 INFO     Model `small_trained_ckpt/model_oprhlh/epoch_7`
2023-12-15 07:13:53 INFO     	 * Num of GPU in use: 1
2023-12-15 07:13:53 INFO     	 * Prefix: True
2023-12-15 07:13:53 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 07:13:54 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 07:19:10 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 07:24:24 INFO     	Bleu_1: 0.1371949892489955
2023-12-15 07:24:24 INFO     	Bleu_2: 0.07591258188471403
2023-12-15 07:24:24 INFO     	Bleu_3: 0.040705198636164466
2023-12-15 07:24:24 INFO     	Bleu_4: 0.025866209687817512
2023-12-15 07:24:24 INFO     	Bleu_1: 0.13027777316574962
2023-12-15 07:24:24 INFO     	Bleu_2: 0.07204969910144729
2023-12-15 07:24:24 INFO     	Bleu_3: 0.03948643985627756
2023-12-15 07:24:24 INFO     	Bleu_4: 0.025202015825166468
2023-12-15 07:24:37 INFO     use spaCy answer extraction model: positionrank
2023-12-15 07:24:37 INFO     Model `small_trained_ckpt/model_oprhlh/epoch_8`
2023-12-15 07:24:37 INFO     	 * Num of GPU in use: 1
2023-12-15 07:24:37 INFO     	 * Prefix: True
2023-12-15 07:24:37 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 07:24:38 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 07:29:50 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 07:35:02 INFO     	Bleu_1: 0.1333293502103931
2023-12-15 07:35:02 INFO     	Bleu_2: 0.07400485911372721
2023-12-15 07:35:02 INFO     	Bleu_3: 0.04034412632132814
2023-12-15 07:35:02 INFO     	Bleu_4: 0.026045591670829718
2023-12-15 07:35:03 INFO     	Bleu_1: 0.13327942685238203
2023-12-15 07:35:03 INFO     	Bleu_2: 0.07320924715536298
2023-12-15 07:35:03 INFO     	Bleu_3: 0.03969668727513085
2023-12-15 07:35:03 INFO     	Bleu_4: 0.025222717878741953
2023-12-15 07:35:17 INFO     use spaCy answer extraction model: positionrank
2023-12-15 07:35:17 INFO     Model `small_trained_ckpt/model_oprhlh/epoch_9`
2023-12-15 07:35:17 INFO     	 * Num of GPU in use: 1
2023-12-15 07:35:17 INFO     	 * Prefix: True
2023-12-15 07:35:17 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 07:35:18 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 07:40:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 07:45:46 INFO     	Bleu_1: 0.12895851917930315
2023-12-15 07:45:46 INFO     	Bleu_2: 0.07168411120869225
2023-12-15 07:45:46 INFO     	Bleu_3: 0.039197315965426346
2023-12-15 07:45:46 INFO     	Bleu_4: 0.02523639356886681
2023-12-15 07:45:46 INFO     	Bleu_1: 0.1241194306601506
2023-12-15 07:45:46 INFO     	Bleu_2: 0.06876221325584186
2023-12-15 07:45:46 INFO     	Bleu_3: 0.03740664024517338
2023-12-15 07:45:46 INFO     	Bleu_4: 0.02389583899699245
2023-12-15 07:45:46 INFO     ## 2nd RUN (EVAL): Configuration 1/5 ##
2023-12-15 07:46:02 INFO     use spaCy answer extraction model: positionrank
2023-12-15 07:46:02 INFO     Model `small_trained_ckpt/model_nrudfu/epoch_1`
2023-12-15 07:46:02 INFO     	 * Num of GPU in use: 1
2023-12-15 07:46:02 INFO     	 * Prefix: True
2023-12-15 07:46:02 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 07:46:03 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 07:51:42 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 07:57:18 WARNING  prediction not found at the evaluation
2023-12-15 07:57:18 WARNING  prediction not found at the evaluation
2023-12-15 07:57:18 INFO     	Bleu_1: 0.0
2023-12-15 07:57:18 INFO     	Bleu_2: 0.0
2023-12-15 07:57:18 INFO     	Bleu_3: 0.0
2023-12-15 07:57:18 INFO     	Bleu_4: 0.0
2023-12-15 07:57:18 WARNING  prediction not found at the evaluation
2023-12-15 07:57:18 WARNING  prediction not found at the evaluation
2023-12-15 07:57:19 INFO     	Bleu_1: 0.0
2023-12-15 07:57:19 INFO     	Bleu_2: 0.0
2023-12-15 07:57:19 INFO     	Bleu_3: 0.0
2023-12-15 07:57:19 INFO     	Bleu_4: 0.0
2023-12-15 07:57:37 INFO     use spaCy answer extraction model: positionrank
2023-12-15 07:57:37 INFO     Model `small_trained_ckpt/model_nrudfu/epoch_11`
2023-12-15 07:57:37 INFO     	 * Num of GPU in use: 1
2023-12-15 07:57:37 INFO     	 * Prefix: True
2023-12-15 07:57:37 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 07:57:38 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 08:02:50 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 08:08:01 INFO     	Bleu_1: 0.1285958313618236
2023-12-15 08:08:01 INFO     	Bleu_2: 0.07093060273814887
2023-12-15 08:08:01 INFO     	Bleu_3: 0.03858836521668193
2023-12-15 08:08:01 INFO     	Bleu_4: 0.02478816904382595
2023-12-15 08:08:02 INFO     	Bleu_1: 0.1273349526995031
2023-12-15 08:08:02 INFO     	Bleu_2: 0.07053339575123373
2023-12-15 08:08:02 INFO     	Bleu_3: 0.0386837852527073
2023-12-15 08:08:02 INFO     	Bleu_4: 0.024986123491425722
2023-12-15 08:08:14 INFO     use spaCy answer extraction model: positionrank
2023-12-15 08:08:14 INFO     Model `small_trained_ckpt/model_nrudfu/epoch_12`
2023-12-15 08:08:14 INFO     	 * Num of GPU in use: 1
2023-12-15 08:08:14 INFO     	 * Prefix: True
2023-12-15 08:08:14 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 08:08:15 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 08:13:23 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 08:18:32 INFO     	Bleu_1: 0.1287734886422495
2023-12-15 08:18:32 INFO     	Bleu_2: 0.0708794769806859
2023-12-15 08:18:32 INFO     	Bleu_3: 0.03831004610461662
2023-12-15 08:18:32 INFO     	Bleu_4: 0.024512428881700302
2023-12-15 08:18:33 INFO     	Bleu_1: 0.12919311687694812
2023-12-15 08:18:33 INFO     	Bleu_2: 0.07139534507712494
2023-12-15 08:18:33 INFO     	Bleu_3: 0.03895640961488837
2023-12-15 08:18:33 INFO     	Bleu_4: 0.025147663166495075
2023-12-15 08:18:44 INFO     use spaCy answer extraction model: positionrank
2023-12-15 08:18:45 INFO     Model `small_trained_ckpt/model_nrudfu/epoch_13`
2023-12-15 08:18:45 INFO     	 * Num of GPU in use: 1
2023-12-15 08:18:45 INFO     	 * Prefix: True
2023-12-15 08:18:45 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 08:18:46 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 08:24:06 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 08:29:19 INFO     	Bleu_1: 0.1282533685147299
2023-12-15 08:29:19 INFO     	Bleu_2: 0.07084165518409741
2023-12-15 08:29:19 INFO     	Bleu_3: 0.03855708848972053
2023-12-15 08:29:19 INFO     	Bleu_4: 0.02473292667619303
2023-12-15 08:29:19 INFO     	Bleu_1: 0.12401700239142892
2023-12-15 08:29:19 INFO     	Bleu_2: 0.06762696576378945
2023-12-15 08:29:19 INFO     	Bleu_3: 0.03613196975006324
2023-12-15 08:29:19 INFO     	Bleu_4: 0.023064973916850053
2023-12-15 08:29:33 INFO     use spaCy answer extraction model: positionrank
2023-12-15 08:29:34 INFO     Model `small_trained_ckpt/model_nrudfu/epoch_14`
2023-12-15 08:29:34 INFO     	 * Num of GPU in use: 1
2023-12-15 08:29:34 INFO     	 * Prefix: True
2023-12-15 08:29:34 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 08:29:35 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 08:34:44 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 08:39:54 INFO     	Bleu_1: 0.12847159014460074
2023-12-15 08:39:54 INFO     	Bleu_2: 0.0702470932840796
2023-12-15 08:39:54 INFO     	Bleu_3: 0.037425633986398435
2023-12-15 08:39:54 INFO     	Bleu_4: 0.023539991228829972
2023-12-15 08:39:54 INFO     	Bleu_1: 0.1261292671966271
2023-12-15 08:39:54 INFO     	Bleu_2: 0.06936758023910404
2023-12-15 08:39:54 INFO     	Bleu_3: 0.0376608332612408
2023-12-15 08:39:54 INFO     	Bleu_4: 0.02443585577250259
2023-12-15 08:40:09 INFO     use spaCy answer extraction model: positionrank
2023-12-15 08:40:09 INFO     Model `small_trained_ckpt/model_nrudfu/epoch_15`
2023-12-15 08:40:09 INFO     	 * Num of GPU in use: 1
2023-12-15 08:40:09 INFO     	 * Prefix: True
2023-12-15 08:40:09 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 08:40:10 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 08:45:22 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 08:50:33 INFO     	Bleu_1: 0.12057223688623715
2023-12-15 08:50:33 INFO     	Bleu_2: 0.06672480426112286
2023-12-15 08:50:33 INFO     	Bleu_3: 0.036082614528554276
2023-12-15 08:50:33 INFO     	Bleu_4: 0.023073055136000147
2023-12-15 08:50:34 INFO     	Bleu_1: 0.11692543340527664
2023-12-15 08:50:34 INFO     	Bleu_2: 0.06428431019547141
2023-12-15 08:50:34 INFO     	Bleu_3: 0.03478343451253507
2023-12-15 08:50:34 INFO     	Bleu_4: 0.022505136609824606
2023-12-15 08:50:48 INFO     use spaCy answer extraction model: positionrank
2023-12-15 08:50:49 INFO     Model `small_trained_ckpt/model_nrudfu/epoch_2`
2023-12-15 08:50:49 INFO     	 * Num of GPU in use: 1
2023-12-15 08:50:49 INFO     	 * Prefix: True
2023-12-15 08:50:49 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 08:50:50 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 08:56:29 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 09:02:07 WARNING  prediction not found at the evaluation
2023-12-15 09:02:07 WARNING  prediction not found at the evaluation
2023-12-15 09:02:07 INFO     	Bleu_1: 0.0
2023-12-15 09:02:07 INFO     	Bleu_2: 0.0
2023-12-15 09:02:07 INFO     	Bleu_3: 0.0
2023-12-15 09:02:07 INFO     	Bleu_4: 0.0
2023-12-15 09:02:07 WARNING  prediction not found at the evaluation
2023-12-15 09:02:07 WARNING  prediction not found at the evaluation
2023-12-15 09:02:08 INFO     	Bleu_1: 0.0
2023-12-15 09:02:08 INFO     	Bleu_2: 0.0
2023-12-15 09:02:08 INFO     	Bleu_3: 0.0
2023-12-15 09:02:08 INFO     	Bleu_4: 0.0
2023-12-15 09:02:23 INFO     use spaCy answer extraction model: positionrank
2023-12-15 09:02:23 INFO     Model `small_trained_ckpt/model_nrudfu/epoch_3`
2023-12-15 09:02:23 INFO     	 * Num of GPU in use: 1
2023-12-15 09:02:23 INFO     	 * Prefix: True
2023-12-15 09:02:23 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 09:02:24 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 09:07:52 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 09:13:18 WARNING  prediction not found at the evaluation
2023-12-15 09:13:18 WARNING  prediction not found at the evaluation
2023-12-15 09:13:18 INFO     	Bleu_1: 0.0
2023-12-15 09:13:18 INFO     	Bleu_2: 0.0
2023-12-15 09:13:18 INFO     	Bleu_3: 0.0
2023-12-15 09:13:18 INFO     	Bleu_4: 0.0
2023-12-15 09:13:18 WARNING  prediction not found at the evaluation
2023-12-15 09:13:18 WARNING  prediction not found at the evaluation
2023-12-15 09:13:18 INFO     	Bleu_1: 0.0
2023-12-15 09:13:18 INFO     	Bleu_2: 0.0
2023-12-15 09:13:18 INFO     	Bleu_3: 0.0
2023-12-15 09:13:18 INFO     	Bleu_4: 0.0
2023-12-15 09:13:30 INFO     use spaCy answer extraction model: positionrank
2023-12-15 09:13:30 INFO     Model `small_trained_ckpt/model_nrudfu/epoch_4`
2023-12-15 09:13:30 INFO     	 * Num of GPU in use: 1
2023-12-15 09:13:30 INFO     	 * Prefix: True
2023-12-15 09:13:30 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 09:13:31 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 09:18:59 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 09:24:26 WARNING  prediction not found at the evaluation
2023-12-15 09:24:26 WARNING  prediction not found at the evaluation
2023-12-15 09:24:26 INFO     	Bleu_1: 6.539730149434986e-85
2023-12-15 09:24:26 INFO     	Bleu_2: 3.374263033314095e-85
2023-12-15 09:24:26 INFO     	Bleu_3: 1.6595259972966896e-85
2023-12-15 09:24:26 INFO     	Bleu_4: 1.1046995181675649e-85
2023-12-15 09:24:26 WARNING  prediction not found at the evaluation
2023-12-15 09:24:26 WARNING  prediction not found at the evaluation
2023-12-15 09:24:26 INFO     	Bleu_1: 2.386252141432754e-40
2023-12-15 09:24:26 INFO     	Bleu_2: 1.239363554945379e-40
2023-12-15 09:24:26 INFO     	Bleu_3: 4.80531134794778e-41
2023-12-15 09:24:26 INFO     	Bleu_4: 3.7719069613620317e-45
2023-12-15 09:24:43 INFO     use spaCy answer extraction model: positionrank
2023-12-15 09:24:44 INFO     Model `small_trained_ckpt/model_nrudfu/epoch_5`
2023-12-15 09:24:44 INFO     	 * Num of GPU in use: 1
2023-12-15 09:24:44 INFO     	 * Prefix: True
2023-12-15 09:24:44 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 09:24:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 09:29:58 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 09:35:12 INFO     	Bleu_1: 0.13324637579662324
2023-12-15 09:35:12 INFO     	Bleu_2: 0.07398431119691543
2023-12-15 09:35:12 INFO     	Bleu_3: 0.04041700791028718
2023-12-15 09:35:12 INFO     	Bleu_4: 0.025943565572567634
2023-12-15 09:35:12 WARNING  prediction not found at the evaluation
2023-12-15 09:35:12 WARNING  prediction not found at the evaluation
2023-12-15 09:35:12 INFO     	Bleu_1: 0.1373168044587213
2023-12-15 09:35:12 INFO     	Bleu_2: 0.07480260508483894
2023-12-15 09:35:12 INFO     	Bleu_3: 0.04015920941305496
2023-12-15 09:35:12 INFO     	Bleu_4: 0.025076724553575288
2023-12-15 09:35:29 INFO     use spaCy answer extraction model: positionrank
2023-12-15 09:35:29 INFO     Model `small_trained_ckpt/model_nrudfu/epoch_6`
2023-12-15 09:35:29 INFO     	 * Num of GPU in use: 1
2023-12-15 09:35:29 INFO     	 * Prefix: True
2023-12-15 09:35:29 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 09:35:30 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 09:40:44 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 09:45:56 INFO     	Bleu_1: 0.13922412631086942
2023-12-15 09:45:56 INFO     	Bleu_2: 0.07688093182098787
2023-12-15 09:45:56 INFO     	Bleu_3: 0.0420357337437764
2023-12-15 09:45:56 INFO     	Bleu_4: 0.027103802649494652
2023-12-15 09:45:57 INFO     	Bleu_1: 0.1392329383209582
2023-12-15 09:45:57 INFO     	Bleu_2: 0.07635165586146143
2023-12-15 09:45:57 INFO     	Bleu_3: 0.041048712206771004
2023-12-15 09:45:57 INFO     	Bleu_4: 0.025892216436099878
2023-12-15 09:46:12 INFO     use spaCy answer extraction model: positionrank
2023-12-15 09:46:12 INFO     Model `small_trained_ckpt/model_nrudfu/epoch_7`
2023-12-15 09:46:12 INFO     	 * Num of GPU in use: 1
2023-12-15 09:46:12 INFO     	 * Prefix: True
2023-12-15 09:46:12 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 09:46:13 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 09:51:29 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 09:56:43 INFO     	Bleu_1: 0.1371949892489955
2023-12-15 09:56:43 INFO     	Bleu_2: 0.07591258188471403
2023-12-15 09:56:43 INFO     	Bleu_3: 0.040705198636164466
2023-12-15 09:56:43 INFO     	Bleu_4: 0.025866209687817512
2023-12-15 09:56:44 INFO     	Bleu_1: 0.13027777316574962
2023-12-15 09:56:44 INFO     	Bleu_2: 0.07204969910144729
2023-12-15 09:56:44 INFO     	Bleu_3: 0.03948643985627756
2023-12-15 09:56:44 INFO     	Bleu_4: 0.025202015825166468
2023-12-15 09:56:57 INFO     use spaCy answer extraction model: positionrank
2023-12-15 09:56:57 INFO     Model `small_trained_ckpt/model_nrudfu/epoch_8`
2023-12-15 09:56:57 INFO     	 * Num of GPU in use: 1
2023-12-15 09:56:57 INFO     	 * Prefix: True
2023-12-15 09:56:57 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 09:56:58 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 10:02:10 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 10:07:22 INFO     	Bleu_1: 0.1333293502103931
2023-12-15 10:07:22 INFO     	Bleu_2: 0.07400485911372721
2023-12-15 10:07:22 INFO     	Bleu_3: 0.04034412632132814
2023-12-15 10:07:22 INFO     	Bleu_4: 0.026045591670829718
2023-12-15 10:07:22 INFO     	Bleu_1: 0.13327942685238203
2023-12-15 10:07:22 INFO     	Bleu_2: 0.07320924715536298
2023-12-15 10:07:22 INFO     	Bleu_3: 0.03969668727513085
2023-12-15 10:07:22 INFO     	Bleu_4: 0.025222717878741953
2023-12-15 10:07:38 INFO     use spaCy answer extraction model: positionrank
2023-12-15 10:07:38 INFO     Model `small_trained_ckpt/model_nrudfu/epoch_9`
2023-12-15 10:07:38 INFO     	 * Num of GPU in use: 1
2023-12-15 10:07:38 INFO     	 * Prefix: True
2023-12-15 10:07:38 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 10:07:39 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 10:12:55 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 10:18:10 INFO     	Bleu_1: 0.12895851917930315
2023-12-15 10:18:10 INFO     	Bleu_2: 0.07168411120869225
2023-12-15 10:18:10 INFO     	Bleu_3: 0.039197315965426346
2023-12-15 10:18:10 INFO     	Bleu_4: 0.02523639356886681
2023-12-15 10:18:10 INFO     	Bleu_1: 0.1241194306601506
2023-12-15 10:18:10 INFO     	Bleu_2: 0.06876221325584186
2023-12-15 10:18:10 INFO     	Bleu_3: 0.03740664024517338
2023-12-15 10:18:10 INFO     	Bleu_4: 0.02389583899699245
2023-12-15 10:18:10 INFO     ## 2nd RUN (EVAL): Configuration 2/5 ##
2023-12-15 10:18:27 INFO     use spaCy answer extraction model: positionrank
2023-12-15 10:18:27 INFO     Model `small_trained_ckpt/model_nxaqhy/epoch_1`
2023-12-15 10:18:27 INFO     	 * Num of GPU in use: 1
2023-12-15 10:18:27 INFO     	 * Prefix: True
2023-12-15 10:18:27 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 10:18:28 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 10:24:16 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 10:30:01 WARNING  prediction not found at the evaluation
2023-12-15 10:30:01 WARNING  prediction not found at the evaluation
2023-12-15 10:30:02 INFO     	Bleu_1: 0.0
2023-12-15 10:30:02 INFO     	Bleu_2: 0.0
2023-12-15 10:30:02 INFO     	Bleu_3: 0.0
2023-12-15 10:30:02 INFO     	Bleu_4: 0.0
2023-12-15 10:30:02 INFO     	Bleu_1: 0.0
2023-12-15 10:30:02 INFO     	Bleu_2: 0.0
2023-12-15 10:30:02 INFO     	Bleu_3: 0.0
2023-12-15 10:30:02 INFO     	Bleu_4: 0.0
2023-12-15 10:30:41 INFO     use spaCy answer extraction model: positionrank
2023-12-15 10:30:42 INFO     Model `small_trained_ckpt/model_nxaqhy/epoch_11`
2023-12-15 10:30:42 INFO     	 * Num of GPU in use: 1
2023-12-15 10:30:42 INFO     	 * Prefix: True
2023-12-15 10:30:42 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 10:30:43 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 10:35:46 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 10:40:51 INFO     	Bleu_1: 0.1674519068511621
2023-12-15 10:40:51 INFO     	Bleu_2: 0.09247134103079621
2023-12-15 10:40:51 INFO     	Bleu_3: 0.04934099049046423
2023-12-15 10:40:51 INFO     	Bleu_4: 0.031331580430390686
2023-12-15 10:40:52 INFO     	Bleu_1: 0.17679566330371665
2023-12-15 10:40:52 INFO     	Bleu_2: 0.09551093885669841
2023-12-15 10:40:52 INFO     	Bleu_3: 0.049280334020847576
2023-12-15 10:40:52 INFO     	Bleu_4: 0.029834293031023418
2023-12-15 10:41:17 INFO     use spaCy answer extraction model: positionrank
2023-12-15 10:41:17 INFO     Model `small_trained_ckpt/model_nxaqhy/epoch_12`
2023-12-15 10:41:17 INFO     	 * Num of GPU in use: 1
2023-12-15 10:41:17 INFO     	 * Prefix: True
2023-12-15 10:41:17 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 10:41:18 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 10:46:35 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 10:51:53 INFO     	Bleu_1: 0.1321088233841887
2023-12-15 10:51:53 INFO     	Bleu_2: 0.07254759399372168
2023-12-15 10:51:53 INFO     	Bleu_3: 0.03797423583099793
2023-12-15 10:51:53 INFO     	Bleu_4: 0.02368081225497924
2023-12-15 10:51:54 INFO     	Bleu_1: 0.13695366665648434
2023-12-15 10:51:54 INFO     	Bleu_2: 0.07555426980075779
2023-12-15 10:51:54 INFO     	Bleu_3: 0.04008687146883753
2023-12-15 10:51:54 INFO     	Bleu_4: 0.024884879514022695
2023-12-15 10:52:11 INFO     use spaCy answer extraction model: positionrank
2023-12-15 10:52:12 INFO     Model `small_trained_ckpt/model_nxaqhy/epoch_13`
2023-12-15 10:52:12 INFO     	 * Num of GPU in use: 1
2023-12-15 10:52:12 INFO     	 * Prefix: True
2023-12-15 10:52:12 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 10:52:13 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 10:57:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 11:02:51 INFO     	Bleu_1: 0.12353585157259522
2023-12-15 11:02:51 INFO     	Bleu_2: 0.06818885940578373
2023-12-15 11:02:51 INFO     	Bleu_3: 0.03572460923509176
2023-12-15 11:02:51 INFO     	Bleu_4: 0.02232041755858136
2023-12-15 11:02:51 INFO     	Bleu_1: 0.12612028754402163
2023-12-15 11:02:51 INFO     	Bleu_2: 0.06955975414350801
2023-12-15 11:02:51 INFO     	Bleu_3: 0.03697302321555274
2023-12-15 11:02:51 INFO     	Bleu_4: 0.022983942453045286
2023-12-15 11:03:06 INFO     use spaCy answer extraction model: positionrank
2023-12-15 11:03:07 INFO     Model `small_trained_ckpt/model_nxaqhy/epoch_14`
2023-12-15 11:03:07 INFO     	 * Num of GPU in use: 1
2023-12-15 11:03:07 INFO     	 * Prefix: True
2023-12-15 11:03:07 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 11:03:08 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 11:08:28 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 11:13:51 INFO     	Bleu_1: 0.12192410611486416
2023-12-15 11:13:51 INFO     	Bleu_2: 0.06722688274742078
2023-12-15 11:13:51 INFO     	Bleu_3: 0.03536221407311233
2023-12-15 11:13:51 INFO     	Bleu_4: 0.022262380450618565
2023-12-15 11:13:51 INFO     	Bleu_1: 0.11824092553324093
2023-12-15 11:13:51 INFO     	Bleu_2: 0.0643347509557009
2023-12-15 11:13:51 INFO     	Bleu_3: 0.033269180338329186
2023-12-15 11:13:51 INFO     	Bleu_4: 0.020466584201503116
2023-12-15 11:14:12 INFO     use spaCy answer extraction model: positionrank
2023-12-15 11:14:12 INFO     Model `small_trained_ckpt/model_nxaqhy/epoch_15`
2023-12-15 11:14:12 INFO     	 * Num of GPU in use: 1
2023-12-15 11:14:12 INFO     	 * Prefix: True
2023-12-15 11:14:12 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 11:14:13 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 11:19:34 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 11:24:57 INFO     	Bleu_1: 0.11725714968950046
2023-12-15 11:24:57 INFO     	Bleu_2: 0.06455848232317764
2023-12-15 11:24:57 INFO     	Bleu_3: 0.033780072486673765
2023-12-15 11:24:57 INFO     	Bleu_4: 0.021043411912699612
2023-12-15 11:24:58 INFO     	Bleu_1: 0.11861836562763177
2023-12-15 11:24:58 INFO     	Bleu_2: 0.06560605314957163
2023-12-15 11:24:58 INFO     	Bleu_3: 0.03495276844983717
2023-12-15 11:24:58 INFO     	Bleu_4: 0.022001212614113825
2023-12-15 11:25:11 INFO     use spaCy answer extraction model: positionrank
2023-12-15 11:25:11 INFO     Model `small_trained_ckpt/model_nxaqhy/epoch_2`
2023-12-15 11:25:11 INFO     	 * Num of GPU in use: 1
2023-12-15 11:25:11 INFO     	 * Prefix: True
2023-12-15 11:25:11 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 11:25:12 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 11:30:52 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 11:36:30 WARNING  prediction not found at the evaluation
2023-12-15 11:36:30 WARNING  prediction not found at the evaluation
2023-12-15 11:36:30 INFO     	Bleu_1: 0.0
2023-12-15 11:36:30 INFO     	Bleu_2: 0.0
2023-12-15 11:36:30 INFO     	Bleu_3: 0.0
2023-12-15 11:36:30 INFO     	Bleu_4: 0.0
2023-12-15 11:36:30 WARNING  prediction not found at the evaluation
2023-12-15 11:36:30 WARNING  prediction not found at the evaluation
2023-12-15 11:36:30 INFO     	Bleu_1: 0.0
2023-12-15 11:36:30 INFO     	Bleu_2: 0.0
2023-12-15 11:36:30 INFO     	Bleu_3: 0.0
2023-12-15 11:36:30 INFO     	Bleu_4: 0.0
2023-12-15 11:36:44 INFO     use spaCy answer extraction model: positionrank
2023-12-15 11:36:44 INFO     Model `small_trained_ckpt/model_nxaqhy/epoch_3`
2023-12-15 11:36:44 INFO     	 * Num of GPU in use: 1
2023-12-15 11:36:44 INFO     	 * Prefix: True
2023-12-15 11:36:44 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 11:36:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 11:42:23 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 11:48:01 WARNING  prediction not found at the evaluation
2023-12-15 11:48:01 WARNING  prediction not found at the evaluation
2023-12-15 11:48:01 INFO     	Bleu_1: 0.0
2023-12-15 11:48:01 INFO     	Bleu_2: 0.0
2023-12-15 11:48:01 INFO     	Bleu_3: 0.0
2023-12-15 11:48:01 INFO     	Bleu_4: 0.0
2023-12-15 11:48:01 WARNING  prediction not found at the evaluation
2023-12-15 11:48:01 WARNING  prediction not found at the evaluation
2023-12-15 11:48:01 INFO     	Bleu_1: 0.0
2023-12-15 11:48:01 INFO     	Bleu_2: 0.0
2023-12-15 11:48:01 INFO     	Bleu_3: 0.0
2023-12-15 11:48:01 INFO     	Bleu_4: 0.0
2023-12-15 11:48:14 INFO     use spaCy answer extraction model: positionrank
2023-12-15 11:48:14 INFO     Model `small_trained_ckpt/model_nxaqhy/epoch_4`
2023-12-15 11:48:14 INFO     	 * Num of GPU in use: 1
2023-12-15 11:48:14 INFO     	 * Prefix: True
2023-12-15 11:48:14 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 11:48:15 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 11:53:54 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 11:59:32 WARNING  prediction not found at the evaluation
2023-12-15 11:59:32 WARNING  prediction not found at the evaluation
2023-12-15 11:59:32 INFO     	Bleu_1: 0.0
2023-12-15 11:59:32 INFO     	Bleu_2: 0.0
2023-12-15 11:59:32 INFO     	Bleu_3: 0.0
2023-12-15 11:59:32 INFO     	Bleu_4: 0.0
2023-12-15 11:59:32 WARNING  prediction not found at the evaluation
2023-12-15 11:59:32 WARNING  prediction not found at the evaluation
2023-12-15 11:59:33 INFO     	Bleu_1: 0.0
2023-12-15 11:59:33 INFO     	Bleu_2: 0.0
2023-12-15 11:59:33 INFO     	Bleu_3: 0.0
2023-12-15 11:59:33 INFO     	Bleu_4: 0.0
2023-12-15 11:59:44 INFO     use spaCy answer extraction model: positionrank
2023-12-15 11:59:44 INFO     Model `small_trained_ckpt/model_nxaqhy/epoch_5`
2023-12-15 11:59:44 INFO     	 * Num of GPU in use: 1
2023-12-15 11:59:44 INFO     	 * Prefix: True
2023-12-15 11:59:44 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 11:59:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 12:05:15 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 12:10:46 WARNING  prediction not found at the evaluation
2023-12-15 12:10:46 WARNING  prediction not found at the evaluation
2023-12-15 12:10:46 INFO     	Bleu_1: 0.0
2023-12-15 12:10:46 INFO     	Bleu_2: 0.0
2023-12-15 12:10:46 INFO     	Bleu_3: 0.0
2023-12-15 12:10:46 INFO     	Bleu_4: 0.0
2023-12-15 12:10:46 WARNING  prediction not found at the evaluation
2023-12-15 12:10:46 WARNING  prediction not found at the evaluation
2023-12-15 12:10:46 INFO     	Bleu_1: 0.0
2023-12-15 12:10:46 INFO     	Bleu_2: 0.0
2023-12-15 12:10:46 INFO     	Bleu_3: 0.0
2023-12-15 12:10:46 INFO     	Bleu_4: 0.0
2023-12-15 12:10:58 INFO     use spaCy answer extraction model: positionrank
2023-12-15 12:10:58 INFO     Model `small_trained_ckpt/model_nxaqhy/epoch_6`
2023-12-15 12:10:58 INFO     	 * Num of GPU in use: 1
2023-12-15 12:10:58 INFO     	 * Prefix: True
2023-12-15 12:10:58 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 12:10:59 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 12:16:26 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 12:21:53 WARNING  prediction not found at the evaluation
2023-12-15 12:21:53 WARNING  prediction not found at the evaluation
2023-12-15 12:21:53 INFO     	Bleu_1: 0.0
2023-12-15 12:21:53 INFO     	Bleu_2: 0.0
2023-12-15 12:21:53 INFO     	Bleu_3: 0.0
2023-12-15 12:21:53 INFO     	Bleu_4: 0.0
2023-12-15 12:21:53 WARNING  prediction not found at the evaluation
2023-12-15 12:21:53 WARNING  prediction not found at the evaluation
2023-12-15 12:21:54 INFO     	Bleu_1: 0.0
2023-12-15 12:21:54 INFO     	Bleu_2: 0.0
2023-12-15 12:21:54 INFO     	Bleu_3: 0.0
2023-12-15 12:21:54 INFO     	Bleu_4: 0.0
2023-12-15 12:22:10 INFO     use spaCy answer extraction model: positionrank
2023-12-15 12:22:10 INFO     Model `small_trained_ckpt/model_nxaqhy/epoch_7`
2023-12-15 12:22:10 INFO     	 * Num of GPU in use: 1
2023-12-15 12:22:10 INFO     	 * Prefix: True
2023-12-15 12:22:10 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 12:22:11 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 12:27:39 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 12:33:07 WARNING  prediction not found at the evaluation
2023-12-15 12:33:07 WARNING  prediction not found at the evaluation
2023-12-15 12:33:07 INFO     	Bleu_1: 1.4457768320184939e-09
2023-12-15 12:33:07 INFO     	Bleu_2: 8.065014592095308e-10
2023-12-15 12:33:07 INFO     	Bleu_3: 4.159933379154798e-10
2023-12-15 12:33:07 INFO     	Bleu_4: 2.4028529521773015e-10
2023-12-15 12:33:07 WARNING  prediction not found at the evaluation
2023-12-15 12:33:07 WARNING  prediction not found at the evaluation
2023-12-15 12:33:08 INFO     	Bleu_1: 1.8794093830962696e-23
2023-12-15 12:33:08 INFO     	Bleu_2: 9.494881753205959e-24
2023-12-15 12:33:08 INFO     	Bleu_3: 4.1995221755964736e-24
2023-12-15 12:33:08 INFO     	Bleu_4: 2.528484196220065e-24
2023-12-15 12:33:20 INFO     use spaCy answer extraction model: positionrank
2023-12-15 12:33:20 INFO     Model `small_trained_ckpt/model_nxaqhy/epoch_8`
2023-12-15 12:33:20 INFO     	 * Num of GPU in use: 1
2023-12-15 12:33:20 INFO     	 * Prefix: True
2023-12-15 12:33:20 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 12:33:21 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 12:38:39 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 12:44:00 INFO     	Bleu_1: 0.12310655353703501
2023-12-15 12:44:00 INFO     	Bleu_2: 0.06789799336029342
2023-12-15 12:44:00 INFO     	Bleu_3: 0.036231148681282827
2023-12-15 12:44:00 INFO     	Bleu_4: 0.022801779901596805
2023-12-15 12:44:01 WARNING  prediction not found at the evaluation
2023-12-15 12:44:01 WARNING  prediction not found at the evaluation
2023-12-15 12:44:01 INFO     	Bleu_1: 0.13655322147523058
2023-12-15 12:44:01 INFO     	Bleu_2: 0.07371424568345102
2023-12-15 12:44:01 INFO     	Bleu_3: 0.03839977980926254
2023-12-15 12:44:01 INFO     	Bleu_4: 0.02352173581235142
2023-12-15 12:44:11 INFO     use spaCy answer extraction model: positionrank
2023-12-15 12:44:11 INFO     Model `small_trained_ckpt/model_nxaqhy/epoch_9`
2023-12-15 12:44:11 INFO     	 * Num of GPU in use: 1
2023-12-15 12:44:11 INFO     	 * Prefix: True
2023-12-15 12:44:11 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 12:44:12 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 12:49:28 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 12:54:46 INFO     	Bleu_1: 0.12686274189063085
2023-12-15 12:54:46 INFO     	Bleu_2: 0.06993829395946108
2023-12-15 12:54:46 INFO     	Bleu_3: 0.03728058151044918
2023-12-15 12:54:46 INFO     	Bleu_4: 0.023512186017587956
2023-12-15 12:54:46 INFO     	Bleu_1: 0.13546774179350052
2023-12-15 12:54:46 INFO     	Bleu_2: 0.07400972454718086
2023-12-15 12:54:46 INFO     	Bleu_3: 0.039652880494789586
2023-12-15 12:54:46 INFO     	Bleu_4: 0.02494692687129966
2023-12-15 12:54:46 INFO     ## 2nd RUN (EVAL): Configuration 3/5 ##
2023-12-15 12:55:01 INFO     use spaCy answer extraction model: positionrank
2023-12-15 12:55:01 INFO     Model `small_trained_ckpt/model_vhyoja/epoch_1`
2023-12-15 12:55:01 INFO     	 * Num of GPU in use: 1
2023-12-15 12:55:01 INFO     	 * Prefix: True
2023-12-15 12:55:01 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 12:55:02 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 13:00:49 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 13:06:35 WARNING  prediction not found at the evaluation
2023-12-15 13:06:35 WARNING  prediction not found at the evaluation
2023-12-15 13:06:35 INFO     	Bleu_1: 0.0
2023-12-15 13:06:35 INFO     	Bleu_2: 0.0
2023-12-15 13:06:35 INFO     	Bleu_3: 0.0
2023-12-15 13:06:35 INFO     	Bleu_4: 0.0
2023-12-15 13:06:35 INFO     	Bleu_1: 0.0
2023-12-15 13:06:35 INFO     	Bleu_2: 0.0
2023-12-15 13:06:35 INFO     	Bleu_3: 0.0
2023-12-15 13:06:35 INFO     	Bleu_4: 0.0
2023-12-15 13:06:50 INFO     use spaCy answer extraction model: positionrank
2023-12-15 13:06:51 INFO     Model `small_trained_ckpt/model_vhyoja/epoch_11`
2023-12-15 13:06:51 INFO     	 * Num of GPU in use: 1
2023-12-15 13:06:51 INFO     	 * Prefix: True
2023-12-15 13:06:51 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 13:06:52 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 13:12:01 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 13:17:11 INFO     	Bleu_1: 0.14300260105708307
2023-12-15 13:17:11 INFO     	Bleu_2: 0.0790018010760377
2023-12-15 13:17:11 INFO     	Bleu_3: 0.04279272206388648
2023-12-15 13:17:11 INFO     	Bleu_4: 0.02757159557841438
2023-12-15 13:17:12 INFO     	Bleu_1: 0.1436803360899254
2023-12-15 13:17:12 INFO     	Bleu_2: 0.07894185255059226
2023-12-15 13:17:12 INFO     	Bleu_3: 0.042884474729152994
2023-12-15 13:17:12 INFO     	Bleu_4: 0.02723258248473806
2023-12-15 13:17:31 INFO     use spaCy answer extraction model: positionrank
2023-12-15 13:17:32 INFO     Model `small_trained_ckpt/model_vhyoja/epoch_12`
2023-12-15 13:17:32 INFO     	 * Num of GPU in use: 1
2023-12-15 13:17:32 INFO     	 * Prefix: True
2023-12-15 13:17:32 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 13:17:33 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 13:22:42 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 13:27:51 INFO     	Bleu_1: 0.1393126181241646
2023-12-15 13:27:51 INFO     	Bleu_2: 0.07726915851757278
2023-12-15 13:27:51 INFO     	Bleu_3: 0.04227285602649666
2023-12-15 13:27:51 INFO     	Bleu_4: 0.027481948649672926
2023-12-15 13:27:51 INFO     	Bleu_1: 0.13767376952391053
2023-12-15 13:27:51 INFO     	Bleu_2: 0.07600149645139086
2023-12-15 13:27:51 INFO     	Bleu_3: 0.04151007447680932
2023-12-15 13:27:51 INFO     	Bleu_4: 0.026492388356535414
2023-12-15 13:28:08 INFO     use spaCy answer extraction model: positionrank
2023-12-15 13:28:08 INFO     Model `small_trained_ckpt/model_vhyoja/epoch_13`
2023-12-15 13:28:08 INFO     	 * Num of GPU in use: 1
2023-12-15 13:28:08 INFO     	 * Prefix: True
2023-12-15 13:28:08 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 13:28:09 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 13:33:20 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 13:38:32 INFO     	Bleu_1: 0.13556461454573662
2023-12-15 13:38:32 INFO     	Bleu_2: 0.07563986517347268
2023-12-15 13:38:32 INFO     	Bleu_3: 0.041864565014802485
2023-12-15 13:38:32 INFO     	Bleu_4: 0.027413165013277195
2023-12-15 13:38:33 INFO     	Bleu_1: 0.13545881626774453
2023-12-15 13:38:33 INFO     	Bleu_2: 0.07475815612550382
2023-12-15 13:38:33 INFO     	Bleu_3: 0.04100624613363718
2023-12-15 13:38:33 INFO     	Bleu_4: 0.026443707212633463
2023-12-15 13:38:45 INFO     use spaCy answer extraction model: positionrank
2023-12-15 13:38:45 INFO     Model `small_trained_ckpt/model_vhyoja/epoch_14`
2023-12-15 13:38:45 INFO     	 * Num of GPU in use: 1
2023-12-15 13:38:45 INFO     	 * Prefix: True
2023-12-15 13:38:45 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 13:38:47 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 13:44:00 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 13:49:11 INFO     	Bleu_1: 0.13456356728178248
2023-12-15 13:49:11 INFO     	Bleu_2: 0.07521529737746277
2023-12-15 13:49:11 INFO     	Bleu_3: 0.041679509552008304
2023-12-15 13:49:11 INFO     	Bleu_4: 0.0272744564461658
2023-12-15 13:49:11 INFO     	Bleu_1: 0.1302981181446393
2023-12-15 13:49:11 INFO     	Bleu_2: 0.0718931115871281
2023-12-15 13:49:11 INFO     	Bleu_3: 0.03906965894366097
2023-12-15 13:49:11 INFO     	Bleu_4: 0.024718634878587505
2023-12-15 13:49:24 INFO     use spaCy answer extraction model: positionrank
2023-12-15 13:49:24 INFO     Model `small_trained_ckpt/model_vhyoja/epoch_15`
2023-12-15 13:49:24 INFO     	 * Num of GPU in use: 1
2023-12-15 13:49:24 INFO     	 * Prefix: True
2023-12-15 13:49:24 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 13:49:26 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 13:54:36 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 13:59:48 INFO     	Bleu_1: 0.1327278786868844
2023-12-15 13:59:48 INFO     	Bleu_2: 0.07407485569952281
2023-12-15 13:59:48 INFO     	Bleu_3: 0.04122664023130911
2023-12-15 13:59:48 INFO     	Bleu_4: 0.027043384598371824
2023-12-15 13:59:48 INFO     	Bleu_1: 0.12404066503637302
2023-12-15 13:59:48 INFO     	Bleu_2: 0.06838517084955174
2023-12-15 13:59:48 INFO     	Bleu_3: 0.036848767951017956
2023-12-15 13:59:48 INFO     	Bleu_4: 0.023347251863220446
2023-12-15 14:00:01 INFO     use spaCy answer extraction model: positionrank
2023-12-15 14:00:01 INFO     Model `small_trained_ckpt/model_vhyoja/epoch_2`
2023-12-15 14:00:01 INFO     	 * Num of GPU in use: 1
2023-12-15 14:00:01 INFO     	 * Prefix: True
2023-12-15 14:00:01 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 14:00:02 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 14:05:40 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 14:11:18 WARNING  prediction not found at the evaluation
2023-12-15 14:11:18 WARNING  prediction not found at the evaluation
2023-12-15 14:11:18 INFO     	Bleu_1: 0.0
2023-12-15 14:11:18 INFO     	Bleu_2: 0.0
2023-12-15 14:11:18 INFO     	Bleu_3: 0.0
2023-12-15 14:11:18 INFO     	Bleu_4: 0.0
2023-12-15 14:11:18 WARNING  prediction not found at the evaluation
2023-12-15 14:11:18 WARNING  prediction not found at the evaluation
2023-12-15 14:11:19 INFO     	Bleu_1: 0.0
2023-12-15 14:11:19 INFO     	Bleu_2: 0.0
2023-12-15 14:11:19 INFO     	Bleu_3: 0.0
2023-12-15 14:11:19 INFO     	Bleu_4: 0.0
2023-12-15 14:11:31 INFO     use spaCy answer extraction model: positionrank
2023-12-15 14:11:31 INFO     Model `small_trained_ckpt/model_vhyoja/epoch_3`
2023-12-15 14:11:31 INFO     	 * Num of GPU in use: 1
2023-12-15 14:11:31 INFO     	 * Prefix: True
2023-12-15 14:11:31 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 14:11:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 14:17:11 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 14:22:49 WARNING  prediction not found at the evaluation
2023-12-15 14:22:49 WARNING  prediction not found at the evaluation
2023-12-15 14:22:49 INFO     	Bleu_1: 0.0
2023-12-15 14:22:49 INFO     	Bleu_2: 0.0
2023-12-15 14:22:49 INFO     	Bleu_3: 0.0
2023-12-15 14:22:49 INFO     	Bleu_4: 0.0
2023-12-15 14:22:49 WARNING  prediction not found at the evaluation
2023-12-15 14:22:49 WARNING  prediction not found at the evaluation
2023-12-15 14:22:49 INFO     	Bleu_1: 0.0
2023-12-15 14:22:49 INFO     	Bleu_2: 0.0
2023-12-15 14:22:49 INFO     	Bleu_3: 0.0
2023-12-15 14:22:49 INFO     	Bleu_4: 0.0
2023-12-15 14:22:59 INFO     use spaCy answer extraction model: positionrank
2023-12-15 14:23:00 INFO     Model `small_trained_ckpt/model_vhyoja/epoch_4`
2023-12-15 14:23:00 INFO     	 * Num of GPU in use: 1
2023-12-15 14:23:00 INFO     	 * Prefix: True
2023-12-15 14:23:00 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 14:23:01 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 14:28:39 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 14:34:17 WARNING  prediction not found at the evaluation
2023-12-15 14:34:17 WARNING  prediction not found at the evaluation
2023-12-15 14:34:17 INFO     	Bleu_1: 0.0
2023-12-15 14:34:17 INFO     	Bleu_2: 0.0
2023-12-15 14:34:17 INFO     	Bleu_3: 0.0
2023-12-15 14:34:17 INFO     	Bleu_4: 0.0
2023-12-15 14:34:17 WARNING  prediction not found at the evaluation
2023-12-15 14:34:17 WARNING  prediction not found at the evaluation
2023-12-15 14:34:17 INFO     	Bleu_1: 0.0
2023-12-15 14:34:17 INFO     	Bleu_2: 0.0
2023-12-15 14:34:17 INFO     	Bleu_3: 0.0
2023-12-15 14:34:17 INFO     	Bleu_4: 0.0
2023-12-15 14:34:30 INFO     use spaCy answer extraction model: positionrank
2023-12-15 14:34:31 INFO     Model `small_trained_ckpt/model_vhyoja/epoch_5`
2023-12-15 14:34:31 INFO     	 * Num of GPU in use: 1
2023-12-15 14:34:31 INFO     	 * Prefix: True
2023-12-15 14:34:31 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 14:34:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 14:40:02 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 14:45:32 WARNING  prediction not found at the evaluation
2023-12-15 14:45:32 WARNING  prediction not found at the evaluation
2023-12-15 14:45:32 INFO     	Bleu_1: 0.0
2023-12-15 14:45:32 INFO     	Bleu_2: 0.0
2023-12-15 14:45:32 INFO     	Bleu_3: 0.0
2023-12-15 14:45:32 INFO     	Bleu_4: 0.0
2023-12-15 14:45:32 WARNING  prediction not found at the evaluation
2023-12-15 14:45:32 WARNING  prediction not found at the evaluation
2023-12-15 14:45:32 INFO     	Bleu_1: 0.0
2023-12-15 14:45:32 INFO     	Bleu_2: 0.0
2023-12-15 14:45:32 INFO     	Bleu_3: 0.0
2023-12-15 14:45:32 INFO     	Bleu_4: 0.0
2023-12-15 14:45:47 INFO     use spaCy answer extraction model: positionrank
2023-12-15 14:45:47 INFO     Model `small_trained_ckpt/model_vhyoja/epoch_6`
2023-12-15 14:45:47 INFO     	 * Num of GPU in use: 1
2023-12-15 14:45:47 INFO     	 * Prefix: True
2023-12-15 14:45:47 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 14:45:48 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 14:51:14 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 14:56:39 WARNING  prediction not found at the evaluation
2023-12-15 14:56:39 WARNING  prediction not found at the evaluation
2023-12-15 14:56:39 INFO     	Bleu_1: 0.0
2023-12-15 14:56:39 INFO     	Bleu_2: 0.0
2023-12-15 14:56:39 INFO     	Bleu_3: 0.0
2023-12-15 14:56:39 INFO     	Bleu_4: 0.0
2023-12-15 14:56:39 WARNING  prediction not found at the evaluation
2023-12-15 14:56:39 WARNING  prediction not found at the evaluation
2023-12-15 14:56:39 INFO     	Bleu_1: 0.0
2023-12-15 14:56:39 INFO     	Bleu_2: 0.0
2023-12-15 14:56:39 INFO     	Bleu_3: 0.0
2023-12-15 14:56:39 INFO     	Bleu_4: 0.0
2023-12-15 14:56:51 INFO     use spaCy answer extraction model: positionrank
2023-12-15 14:56:52 INFO     Model `small_trained_ckpt/model_vhyoja/epoch_7`
2023-12-15 14:56:52 INFO     	 * Num of GPU in use: 1
2023-12-15 14:56:52 INFO     	 * Prefix: True
2023-12-15 14:56:52 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 14:56:53 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 15:02:21 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 15:07:48 WARNING  prediction not found at the evaluation
2023-12-15 15:07:48 WARNING  prediction not found at the evaluation
2023-12-15 15:07:48 INFO     	Bleu_1: 1.4457768320184939e-09
2023-12-15 15:07:48 INFO     	Bleu_2: 8.065014592095308e-10
2023-12-15 15:07:48 INFO     	Bleu_3: 4.159933379154798e-10
2023-12-15 15:07:48 INFO     	Bleu_4: 2.4028529521773015e-10
2023-12-15 15:07:48 WARNING  prediction not found at the evaluation
2023-12-15 15:07:48 WARNING  prediction not found at the evaluation
2023-12-15 15:07:48 INFO     	Bleu_1: 1.8794093830962696e-23
2023-12-15 15:07:48 INFO     	Bleu_2: 9.494881753205959e-24
2023-12-15 15:07:48 INFO     	Bleu_3: 4.1995221755964736e-24
2023-12-15 15:07:48 INFO     	Bleu_4: 2.528484196220065e-24
2023-12-15 15:07:59 INFO     use spaCy answer extraction model: positionrank
2023-12-15 15:08:00 INFO     Model `small_trained_ckpt/model_vhyoja/epoch_8`
2023-12-15 15:08:00 INFO     	 * Num of GPU in use: 1
2023-12-15 15:08:00 INFO     	 * Prefix: True
2023-12-15 15:08:00 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 15:08:01 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 15:13:19 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 15:18:40 INFO     	Bleu_1: 0.12310655353703501
2023-12-15 15:18:40 INFO     	Bleu_2: 0.06789799336029342
2023-12-15 15:18:40 INFO     	Bleu_3: 0.036231148681282827
2023-12-15 15:18:40 INFO     	Bleu_4: 0.022801779901596805
2023-12-15 15:18:40 WARNING  prediction not found at the evaluation
2023-12-15 15:18:40 WARNING  prediction not found at the evaluation
2023-12-15 15:18:40 INFO     	Bleu_1: 0.13655322147523058
2023-12-15 15:18:40 INFO     	Bleu_2: 0.07371424568345102
2023-12-15 15:18:40 INFO     	Bleu_3: 0.03839977980926254
2023-12-15 15:18:40 INFO     	Bleu_4: 0.02352173581235142
2023-12-15 15:18:55 INFO     use spaCy answer extraction model: positionrank
2023-12-15 15:18:55 INFO     Model `small_trained_ckpt/model_vhyoja/epoch_9`
2023-12-15 15:18:55 INFO     	 * Num of GPU in use: 1
2023-12-15 15:18:55 INFO     	 * Prefix: True
2023-12-15 15:18:55 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 15:18:56 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 15:24:12 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 15:29:29 INFO     	Bleu_1: 0.12686274189063085
2023-12-15 15:29:29 INFO     	Bleu_2: 0.06993829395946108
2023-12-15 15:29:29 INFO     	Bleu_3: 0.03728058151044918
2023-12-15 15:29:29 INFO     	Bleu_4: 0.023512186017587956
2023-12-15 15:29:29 INFO     	Bleu_1: 0.13546774179350052
2023-12-15 15:29:29 INFO     	Bleu_2: 0.07400972454718086
2023-12-15 15:29:29 INFO     	Bleu_3: 0.039652880494789586
2023-12-15 15:29:29 INFO     	Bleu_4: 0.02494692687129966
2023-12-15 15:29:29 INFO     ## 2nd RUN (EVAL): Configuration 4/5 ##
2023-12-15 15:29:48 INFO     use spaCy answer extraction model: positionrank
2023-12-15 15:29:48 INFO     Model `small_trained_ckpt/model_eszyci/epoch_1`
2023-12-15 15:29:48 INFO     	 * Num of GPU in use: 1
2023-12-15 15:29:48 INFO     	 * Prefix: True
2023-12-15 15:29:48 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 15:29:49 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 15:35:02 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 15:40:12 INFO     	Bleu_1: 0.13186086496495064
2023-12-15 15:40:12 INFO     	Bleu_2: 0.07403603473450224
2023-12-15 15:40:12 INFO     	Bleu_3: 0.04153108233409842
2023-12-15 15:40:12 INFO     	Bleu_4: 0.027390287824979653
2023-12-15 15:40:13 INFO     	Bleu_1: 0.134648289382044
2023-12-15 15:40:13 INFO     	Bleu_2: 0.0741290258097373
2023-12-15 15:40:13 INFO     	Bleu_3: 0.03977829770917986
2023-12-15 15:40:13 INFO     	Bleu_4: 0.025103892563693976
2023-12-15 15:40:32 INFO     use spaCy answer extraction model: positionrank
2023-12-15 15:40:33 INFO     Model `small_trained_ckpt/model_eszyci/epoch_11`
2023-12-15 15:40:33 INFO     	 * Num of GPU in use: 1
2023-12-15 15:40:33 INFO     	 * Prefix: True
2023-12-15 15:40:33 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 15:40:34 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 15:45:55 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 15:51:13 INFO     	Bleu_1: 0.11054324606289365
2023-12-15 15:51:13 INFO     	Bleu_2: 0.060834987241706
2023-12-15 15:51:13 INFO     	Bleu_3: 0.032426608499317354
2023-12-15 15:51:13 INFO     	Bleu_4: 0.020712856650882728
2023-12-15 15:51:14 INFO     	Bleu_1: 0.1043303746438157
2023-12-15 15:51:14 INFO     	Bleu_2: 0.05681444173842128
2023-12-15 15:51:14 INFO     	Bleu_3: 0.030223776306204302
2023-12-15 15:51:14 INFO     	Bleu_4: 0.019224327739727494
2023-12-15 15:51:24 INFO     use spaCy answer extraction model: positionrank
2023-12-15 15:51:24 INFO     Model `small_trained_ckpt/model_eszyci/epoch_12`
2023-12-15 15:51:24 INFO     	 * Num of GPU in use: 1
2023-12-15 15:51:24 INFO     	 * Prefix: True
2023-12-15 15:51:24 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 15:51:25 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 15:56:26 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 16:01:30 INFO     	Bleu_1: 0.13002831762287032
2023-12-15 16:01:30 INFO     	Bleu_2: 0.07175376624959028
2023-12-15 16:01:30 INFO     	Bleu_3: 0.0390544964276916
2023-12-15 16:01:30 INFO     	Bleu_4: 0.02520759247675772
2023-12-15 16:01:31 INFO     	Bleu_1: 0.1269571503276006
2023-12-15 16:01:31 INFO     	Bleu_2: 0.06865138607858934
2023-12-15 16:01:31 INFO     	Bleu_3: 0.03623440581950873
2023-12-15 16:01:31 INFO     	Bleu_4: 0.02265097675001839
2023-12-15 16:01:44 INFO     use spaCy answer extraction model: positionrank
2023-12-15 16:01:44 INFO     Model `small_trained_ckpt/model_eszyci/epoch_13`
2023-12-15 16:01:44 INFO     	 * Num of GPU in use: 1
2023-12-15 16:01:44 INFO     	 * Prefix: True
2023-12-15 16:01:44 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 16:01:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 16:06:58 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 16:12:09 INFO     	Bleu_1: 0.12142207441516642
2023-12-15 16:12:09 INFO     	Bleu_2: 0.06712030864606104
2023-12-15 16:12:09 INFO     	Bleu_3: 0.036374044606830074
2023-12-15 16:12:09 INFO     	Bleu_4: 0.023460671974797107
2023-12-15 16:12:10 INFO     	Bleu_1: 0.11701791960682868
2023-12-15 16:12:10 INFO     	Bleu_2: 0.06388660040691248
2023-12-15 16:12:10 INFO     	Bleu_3: 0.03441507531582178
2023-12-15 16:12:10 INFO     	Bleu_4: 0.02228007272342147
2023-12-15 16:12:24 INFO     use spaCy answer extraction model: positionrank
2023-12-15 16:12:25 INFO     Model `small_trained_ckpt/model_eszyci/epoch_14`
2023-12-15 16:12:25 INFO     	 * Num of GPU in use: 1
2023-12-15 16:12:25 INFO     	 * Prefix: True
2023-12-15 16:12:25 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 16:12:26 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 16:17:25 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 16:22:13 INFO     	Bleu_1: 0.1410618615307745
2023-12-15 16:22:13 INFO     	Bleu_2: 0.0774221473353551
2023-12-15 16:22:13 INFO     	Bleu_3: 0.042198318272947614
2023-12-15 16:22:13 INFO     	Bleu_4: 0.027336465605706295
2023-12-15 16:22:14 INFO     	Bleu_1: 0.13561982340204498
2023-12-15 16:22:14 INFO     	Bleu_2: 0.07387976725599917
2023-12-15 16:22:14 INFO     	Bleu_3: 0.04038183802753627
2023-12-15 16:22:14 INFO     	Bleu_4: 0.026232291089197838
2023-12-15 16:22:26 INFO     use spaCy answer extraction model: positionrank
2023-12-15 16:22:26 INFO     Model `small_trained_ckpt/model_eszyci/epoch_15`
2023-12-15 16:22:26 INFO     	 * Num of GPU in use: 1
2023-12-15 16:22:26 INFO     	 * Prefix: True
2023-12-15 16:22:26 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 16:22:27 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 16:27:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 16:32:34 INFO     	Bleu_1: 0.13369224950306902
2023-12-15 16:32:34 INFO     	Bleu_2: 0.07393958160070922
2023-12-15 16:32:34 INFO     	Bleu_3: 0.03998850487908336
2023-12-15 16:32:34 INFO     	Bleu_4: 0.025745957196009424
2023-12-15 16:32:35 INFO     	Bleu_1: 0.12215245974294489
2023-12-15 16:32:35 INFO     	Bleu_2: 0.06548845570318873
2023-12-15 16:32:35 INFO     	Bleu_3: 0.03353140992635884
2023-12-15 16:32:35 INFO     	Bleu_4: 0.020628795885771517
2023-12-15 16:32:49 INFO     use spaCy answer extraction model: positionrank
2023-12-15 16:32:50 INFO     Model `small_trained_ckpt/model_eszyci/epoch_2`
2023-12-15 16:32:50 INFO     	 * Num of GPU in use: 1
2023-12-15 16:32:50 INFO     	 * Prefix: True
2023-12-15 16:32:50 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 16:32:51 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 16:38:04 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 16:43:17 INFO     	Bleu_1: 0.12979207672182563
2023-12-15 16:43:17 INFO     	Bleu_2: 0.07142466615816363
2023-12-15 16:43:17 INFO     	Bleu_3: 0.037800733743145555
2023-12-15 16:43:17 INFO     	Bleu_4: 0.02372433657030973
2023-12-15 16:43:17 INFO     	Bleu_1: 0.12607384064798513
2023-12-15 16:43:17 INFO     	Bleu_2: 0.06884849807337702
2023-12-15 16:43:17 INFO     	Bleu_3: 0.03646320694177464
2023-12-15 16:43:17 INFO     	Bleu_4: 0.0230629725628783
2023-12-15 16:43:31 INFO     use spaCy answer extraction model: positionrank
2023-12-15 16:43:31 INFO     Model `small_trained_ckpt/model_eszyci/epoch_3`
2023-12-15 16:43:31 INFO     	 * Num of GPU in use: 1
2023-12-15 16:43:31 INFO     	 * Prefix: True
2023-12-15 16:43:31 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 16:43:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 16:48:50 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 16:54:11 INFO     	Bleu_1: 0.10639926227502773
2023-12-15 16:54:11 INFO     	Bleu_2: 0.058375024186875936
2023-12-15 16:54:11 INFO     	Bleu_3: 0.03056384030517866
2023-12-15 16:54:11 INFO     	Bleu_4: 0.019047613556888634
2023-12-15 16:54:12 INFO     	Bleu_1: 0.10841315007240601
2023-12-15 16:54:12 INFO     	Bleu_2: 0.05925198301078961
2023-12-15 16:54:12 INFO     	Bleu_3: 0.031029148098859503
2023-12-15 16:54:12 INFO     	Bleu_4: 0.019527974413281295
2023-12-15 16:54:25 INFO     use spaCy answer extraction model: positionrank
2023-12-15 16:54:25 INFO     Model `small_trained_ckpt/model_eszyci/epoch_4`
2023-12-15 16:54:25 INFO     	 * Num of GPU in use: 1
2023-12-15 16:54:25 INFO     	 * Prefix: True
2023-12-15 16:54:25 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 16:54:26 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 16:59:15 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 17:04:08 INFO     	Bleu_1: 0.14563725981468356
2023-12-15 17:04:08 INFO     	Bleu_2: 0.0802736996241654
2023-12-15 17:04:08 INFO     	Bleu_3: 0.04330706295636639
2023-12-15 17:04:08 INFO     	Bleu_4: 0.027769626613283686
2023-12-15 17:04:08 INFO     	Bleu_1: 0.1416469083837268
2023-12-15 17:04:08 INFO     	Bleu_2: 0.075920246665428
2023-12-15 17:04:08 INFO     	Bleu_3: 0.039424628938714484
2023-12-15 17:04:08 INFO     	Bleu_4: 0.024565462649126138
2023-12-15 17:04:18 INFO     use spaCy answer extraction model: positionrank
2023-12-15 17:04:19 INFO     Model `small_trained_ckpt/model_eszyci/epoch_5`
2023-12-15 17:04:19 INFO     	 * Num of GPU in use: 1
2023-12-15 17:04:19 INFO     	 * Prefix: True
2023-12-15 17:04:19 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 17:04:20 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 17:09:38 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 17:14:59 INFO     	Bleu_1: 0.11197257814412727
2023-12-15 17:14:59 INFO     	Bleu_2: 0.06179678278081122
2023-12-15 17:14:59 INFO     	Bleu_3: 0.03340756706588114
2023-12-15 17:14:59 INFO     	Bleu_4: 0.021535665199828014
2023-12-15 17:15:00 INFO     	Bleu_1: 0.1087032399101073
2023-12-15 17:15:00 INFO     	Bleu_2: 0.05927001187491635
2023-12-15 17:15:00 INFO     	Bleu_3: 0.03165361743963718
2023-12-15 17:15:00 INFO     	Bleu_4: 0.020374683928038404
2023-12-15 17:15:10 INFO     use spaCy answer extraction model: positionrank
2023-12-15 17:15:11 INFO     Model `small_trained_ckpt/model_eszyci/epoch_6`
2023-12-15 17:15:11 INFO     	 * Num of GPU in use: 1
2023-12-15 17:15:11 INFO     	 * Prefix: True
2023-12-15 17:15:11 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 17:15:12 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 17:20:31 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 17:25:56 INFO     	Bleu_1: 0.10579846466607941
2023-12-15 17:25:56 INFO     	Bleu_2: 0.057686437937773304
2023-12-15 17:25:56 INFO     	Bleu_3: 0.03011484621343243
2023-12-15 17:25:56 INFO     	Bleu_4: 0.01886370860681624
2023-12-15 17:25:56 INFO     	Bleu_1: 0.10497161390109465
2023-12-15 17:25:56 INFO     	Bleu_2: 0.057098983706022725
2023-12-15 17:25:56 INFO     	Bleu_3: 0.03003713827928289
2023-12-15 17:25:56 INFO     	Bleu_4: 0.019131082809887102
2023-12-15 17:26:09 INFO     use spaCy answer extraction model: positionrank
2023-12-15 17:26:09 INFO     Model `small_trained_ckpt/model_eszyci/epoch_7`
2023-12-15 17:26:09 INFO     	 * Num of GPU in use: 1
2023-12-15 17:26:09 INFO     	 * Prefix: True
2023-12-15 17:26:09 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 17:26:10 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 17:31:25 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 17:36:30 INFO     	Bleu_1: 0.12148474910758249
2023-12-15 17:36:30 INFO     	Bleu_2: 0.06712741511821825
2023-12-15 17:36:30 INFO     	Bleu_3: 0.036133622575684185
2023-12-15 17:36:30 INFO     	Bleu_4: 0.023263683864965624
2023-12-15 17:36:31 INFO     	Bleu_1: 0.1179594982832309
2023-12-15 17:36:31 INFO     	Bleu_2: 0.06428787713543933
2023-12-15 17:36:31 INFO     	Bleu_3: 0.034207104290257626
2023-12-15 17:36:31 INFO     	Bleu_4: 0.021932144496960685
2023-12-15 17:36:44 INFO     use spaCy answer extraction model: positionrank
2023-12-15 17:36:45 INFO     Model `small_trained_ckpt/model_eszyci/epoch_8`
2023-12-15 17:36:45 INFO     	 * Num of GPU in use: 1
2023-12-15 17:36:45 INFO     	 * Prefix: True
2023-12-15 17:36:45 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 17:36:46 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 17:41:53 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 17:47:10 INFO     	Bleu_1: 0.12583967756381456
2023-12-15 17:47:10 INFO     	Bleu_2: 0.06937993701947204
2023-12-15 17:47:10 INFO     	Bleu_3: 0.03770316033541618
2023-12-15 17:47:10 INFO     	Bleu_4: 0.02445090705265281
2023-12-15 17:47:10 INFO     	Bleu_1: 0.12879518346708277
2023-12-15 17:47:10 INFO     	Bleu_2: 0.0708633653760199
2023-12-15 17:47:10 INFO     	Bleu_3: 0.038980358739261485
2023-12-15 17:47:10 INFO     	Bleu_4: 0.025676932902786356
2023-12-15 17:47:26 INFO     use spaCy answer extraction model: positionrank
2023-12-15 17:47:26 INFO     Model `small_trained_ckpt/model_eszyci/epoch_9`
2023-12-15 17:47:26 INFO     	 * Num of GPU in use: 1
2023-12-15 17:47:26 INFO     	 * Prefix: True
2023-12-15 17:47:26 INFO     	 * Language: en (ignore at the training phase)
2023-12-15 17:47:27 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-15 17:52:34 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-15 17:57:39 INFO     	Bleu_1: 0.13157477820025243
2023-12-15 17:57:39 INFO     	Bleu_2: 0.0725418571267589
2023-12-15 17:57:39 INFO     	Bleu_3: 0.03948183929266031
2023-12-15 17:57:39 INFO     	Bleu_4: 0.025555989270799318
2023-12-15 17:57:39 INFO     	Bleu_1: 0.12999345631309883
2023-12-15 17:57:39 INFO     	Bleu_2: 0.07172902475135783
2023-12-15 17:57:39 INFO     	Bleu_3: 0.03949221154970444
2023-12-15 17:57:39 INFO     	Bleu_4: 0.025764265808455995
2023-12-15 17:57:39 INFO     2nd RUN RESULTS: 
[('small_trained_ckpt/model_oprhlh/epoch_11', 0.03176736117417117), ('small_trained_ckpt/model_nxaqhy/epoch_11', 0.031331580430390686), ('small_trained_ckpt/model_eszyci/epoch_4', 0.027769626613283686), ('small_trained_ckpt/model_vhyoja/epoch_11', 0.02757159557841438), ('small_trained_ckpt/model_vhyoja/epoch_12', 0.027481948649672926), ('small_trained_ckpt/model_vhyoja/epoch_13', 0.027413165013277195), ('small_trained_ckpt/model_eszyci/epoch_1', 0.027390287824979653), ('small_trained_ckpt/model_eszyci/epoch_14', 0.027336465605706295), ('small_trained_ckpt/model_vhyoja/epoch_14', 0.0272744564461658), ('small_trained_ckpt/model_oprhlh/epoch_6', 0.027103802649494652), ('small_trained_ckpt/model_nrudfu/epoch_6', 0.027103802649494652), ('small_trained_ckpt/model_oprhlh/epoch_10', 0.027090006600475953), ('small_trained_ckpt/model_nrudfu/epoch_10', 0.027090006600475953), ('small_trained_ckpt/model_vhyoja/epoch_15', 0.027043384598371824), ('small_trained_ckpt/model_nxaqhy/epoch_10', 0.02679595996444357), ('small_trained_ckpt/model_vhyoja/epoch_10', 0.02679595996444357), ('small_trained_ckpt/model_oprhlh/epoch_8', 0.026045591670829718), ('small_trained_ckpt/model_nrudfu/epoch_8', 0.026045591670829718), ('small_trained_ckpt/model_oprhlh/epoch_5', 0.025943565572567634), ('small_trained_ckpt/model_nrudfu/epoch_5', 0.025943565572567634), ('small_trained_ckpt/model_oprhlh/epoch_7', 0.025866209687817512), ('small_trained_ckpt/model_nrudfu/epoch_7', 0.025866209687817512), ('small_trained_ckpt/model_eszyci/epoch_15', 0.025745957196009424), ('small_trained_ckpt/model_eszyci/epoch_9', 0.025555989270799318), ('small_trained_ckpt/model_oprhlh/epoch_9', 0.02523639356886681), ('small_trained_ckpt/model_nrudfu/epoch_9', 0.02523639356886681), ('small_trained_ckpt/model_eszyci/epoch_12', 0.02520759247675772), ('small_trained_ckpt/model_eszyci/epoch_10', 0.024857383229010453), ('small_trained_ckpt/model_nrudfu/epoch_11', 0.02478816904382595), ('small_trained_ckpt/model_nrudfu/epoch_13', 0.02473292667619303), ('small_trained_ckpt/model_nrudfu/epoch_12', 0.024512428881700302), ('small_trained_ckpt/model_eszyci/epoch_8', 0.02445090705265281), ('small_trained_ckpt/model_eszyci/epoch_2', 0.02372433657030973), ('small_trained_ckpt/model_nxaqhy/epoch_12', 0.02368081225497924), ('small_trained_ckpt/model_oprhlh/epoch_12', 0.023561801308342623), ('small_trained_ckpt/model_nrudfu/epoch_14', 0.023539991228829972), ('small_trained_ckpt/model_nxaqhy/epoch_9', 0.023512186017587956), ('small_trained_ckpt/model_vhyoja/epoch_9', 0.023512186017587956), ('small_trained_ckpt/model_eszyci/epoch_13', 0.023460671974797107), ('small_trained_ckpt/model_eszyci/epoch_7', 0.023263683864965624), ('small_trained_ckpt/model_nrudfu/epoch_15', 0.023073055136000147), ('small_trained_ckpt/model_nxaqhy/epoch_8', 0.022801779901596805), ('small_trained_ckpt/model_vhyoja/epoch_8', 0.022801779901596805), ('small_trained_ckpt/model_nxaqhy/epoch_13', 0.02232041755858136), ('small_trained_ckpt/model_nxaqhy/epoch_14', 0.022262380450618565), ('small_trained_ckpt/model_oprhlh/epoch_13', 0.02183067950134625), ('small_trained_ckpt/model_eszyci/epoch_5', 0.021535665199828014), ('small_trained_ckpt/model_nxaqhy/epoch_15', 0.021043411912699612), ('small_trained_ckpt/model_oprhlh/epoch_14', 0.020800321588617866), ('small_trained_ckpt/model_eszyci/epoch_11', 0.020712856650882728), ('small_trained_ckpt/model_oprhlh/epoch_15', 0.019132843985108217), ('small_trained_ckpt/model_eszyci/epoch_3', 0.019047613556888634), ('small_trained_ckpt/model_eszyci/epoch_6', 0.01886370860681624), ('small_trained_ckpt/model_nxaqhy/epoch_7', 2.4028529521773015e-10), ('small_trained_ckpt/model_vhyoja/epoch_7', 2.4028529521773015e-10), ('small_trained_ckpt/model_oprhlh/epoch_4', 1.1046995181675649e-85), ('small_trained_ckpt/model_nrudfu/epoch_4', 1.1046995181675649e-85), ('small_trained_ckpt/model_oprhlh/epoch_1', 0.0), ('small_trained_ckpt/model_oprhlh/epoch_2', 0.0), ('small_trained_ckpt/model_oprhlh/epoch_3', 0.0), ('small_trained_ckpt/model_nrudfu/epoch_1', 0.0), ('small_trained_ckpt/model_nrudfu/epoch_2', 0.0), ('small_trained_ckpt/model_nrudfu/epoch_3', 0.0), ('small_trained_ckpt/model_nxaqhy/epoch_1', 0.0), ('small_trained_ckpt/model_nxaqhy/epoch_2', 0.0), ('small_trained_ckpt/model_nxaqhy/epoch_3', 0.0), ('small_trained_ckpt/model_nxaqhy/epoch_4', 0.0), ('small_trained_ckpt/model_nxaqhy/epoch_5', 0.0), ('small_trained_ckpt/model_nxaqhy/epoch_6', 0.0), ('small_trained_ckpt/model_vhyoja/epoch_1', 0.0), ('small_trained_ckpt/model_vhyoja/epoch_2', 0.0), ('small_trained_ckpt/model_vhyoja/epoch_3', 0.0), ('small_trained_ckpt/model_vhyoja/epoch_4', 0.0), ('small_trained_ckpt/model_vhyoja/epoch_5', 0.0), ('small_trained_ckpt/model_vhyoja/epoch_6', 0.0)]
2023-12-15 17:57:39 INFO     	 * rank: 0 | metric: 0.032 | model: small_trained_ckpt/model_oprhlh/epoch_11 |
2023-12-15 17:57:39 INFO     	 * rank: 1 | metric: 0.031 | model: small_trained_ckpt/model_nxaqhy/epoch_11 |
2023-12-15 17:57:39 INFO     	 * rank: 2 | metric: 0.028 | model: small_trained_ckpt/model_eszyci/epoch_4 |
2023-12-15 17:57:39 INFO     	 * rank: 3 | metric: 0.028 | model: small_trained_ckpt/model_vhyoja/epoch_11 |
2023-12-15 17:57:39 INFO     	 * rank: 4 | metric: 0.027 | model: small_trained_ckpt/model_vhyoja/epoch_12 |
2023-12-15 17:57:39 INFO     	 * rank: 5 | metric: 0.027 | model: small_trained_ckpt/model_vhyoja/epoch_13 |
2023-12-15 17:57:39 INFO     	 * rank: 6 | metric: 0.027 | model: small_trained_ckpt/model_eszyci/epoch_1 |
2023-12-15 17:57:39 INFO     	 * rank: 7 | metric: 0.027 | model: small_trained_ckpt/model_eszyci/epoch_14 |
2023-12-15 17:57:39 INFO     	 * rank: 8 | metric: 0.027 | model: small_trained_ckpt/model_vhyoja/epoch_14 |
2023-12-15 17:57:39 INFO     	 * rank: 9 | metric: 0.027 | model: small_trained_ckpt/model_oprhlh/epoch_6 |
2023-12-15 17:57:39 INFO     	 * rank: 10 | metric: 0.027 | model: small_trained_ckpt/model_nrudfu/epoch_6 |
2023-12-15 17:57:39 INFO     	 * rank: 11 | metric: 0.027 | model: small_trained_ckpt/model_oprhlh/epoch_10 |
2023-12-15 17:57:39 INFO     	 * rank: 12 | metric: 0.027 | model: small_trained_ckpt/model_nrudfu/epoch_10 |
2023-12-15 17:57:39 INFO     	 * rank: 13 | metric: 0.027 | model: small_trained_ckpt/model_vhyoja/epoch_15 |
2023-12-15 17:57:39 INFO     	 * rank: 14 | metric: 0.027 | model: small_trained_ckpt/model_nxaqhy/epoch_10 |
2023-12-15 17:57:39 INFO     	 * rank: 15 | metric: 0.027 | model: small_trained_ckpt/model_vhyoja/epoch_10 |
2023-12-15 17:57:39 INFO     	 * rank: 16 | metric: 0.026 | model: small_trained_ckpt/model_oprhlh/epoch_8 |
2023-12-15 17:57:39 INFO     	 * rank: 17 | metric: 0.026 | model: small_trained_ckpt/model_nrudfu/epoch_8 |
2023-12-15 17:57:39 INFO     	 * rank: 18 | metric: 0.026 | model: small_trained_ckpt/model_oprhlh/epoch_5 |
2023-12-15 17:57:39 INFO     	 * rank: 19 | metric: 0.026 | model: small_trained_ckpt/model_nrudfu/epoch_5 |
2023-12-15 17:57:39 INFO     	 * rank: 20 | metric: 0.026 | model: small_trained_ckpt/model_oprhlh/epoch_7 |
2023-12-15 17:57:39 INFO     	 * rank: 21 | metric: 0.026 | model: small_trained_ckpt/model_nrudfu/epoch_7 |
2023-12-15 17:57:39 INFO     	 * rank: 22 | metric: 0.026 | model: small_trained_ckpt/model_eszyci/epoch_15 |
2023-12-15 17:57:39 INFO     	 * rank: 23 | metric: 0.026 | model: small_trained_ckpt/model_eszyci/epoch_9 |
2023-12-15 17:57:39 INFO     	 * rank: 24 | metric: 0.025 | model: small_trained_ckpt/model_oprhlh/epoch_9 |
2023-12-15 17:57:39 INFO     	 * rank: 25 | metric: 0.025 | model: small_trained_ckpt/model_nrudfu/epoch_9 |
2023-12-15 17:57:39 INFO     	 * rank: 26 | metric: 0.025 | model: small_trained_ckpt/model_eszyci/epoch_12 |
2023-12-15 17:57:39 INFO     	 * rank: 27 | metric: 0.025 | model: small_trained_ckpt/model_eszyci/epoch_10 |
2023-12-15 17:57:39 INFO     	 * rank: 28 | metric: 0.025 | model: small_trained_ckpt/model_nrudfu/epoch_11 |
2023-12-15 17:57:39 INFO     	 * rank: 29 | metric: 0.025 | model: small_trained_ckpt/model_nrudfu/epoch_13 |
2023-12-15 17:57:39 INFO     	 * rank: 30 | metric: 0.025 | model: small_trained_ckpt/model_nrudfu/epoch_12 |
2023-12-15 17:57:39 INFO     	 * rank: 31 | metric: 0.024 | model: small_trained_ckpt/model_eszyci/epoch_8 |
2023-12-15 17:57:39 INFO     	 * rank: 32 | metric: 0.024 | model: small_trained_ckpt/model_eszyci/epoch_2 |
2023-12-15 17:57:39 INFO     	 * rank: 33 | metric: 0.024 | model: small_trained_ckpt/model_nxaqhy/epoch_12 |
2023-12-15 17:57:39 INFO     	 * rank: 34 | metric: 0.024 | model: small_trained_ckpt/model_oprhlh/epoch_12 |
2023-12-15 17:57:39 INFO     	 * rank: 35 | metric: 0.024 | model: small_trained_ckpt/model_nrudfu/epoch_14 |
2023-12-15 17:57:39 INFO     	 * rank: 36 | metric: 0.024 | model: small_trained_ckpt/model_nxaqhy/epoch_9 |
2023-12-15 17:57:39 INFO     	 * rank: 37 | metric: 0.024 | model: small_trained_ckpt/model_vhyoja/epoch_9 |
2023-12-15 17:57:39 INFO     	 * rank: 38 | metric: 0.023 | model: small_trained_ckpt/model_eszyci/epoch_13 |
2023-12-15 17:57:39 INFO     	 * rank: 39 | metric: 0.023 | model: small_trained_ckpt/model_eszyci/epoch_7 |
2023-12-15 17:57:39 INFO     	 * rank: 40 | metric: 0.023 | model: small_trained_ckpt/model_nrudfu/epoch_15 |
2023-12-15 17:57:39 INFO     	 * rank: 41 | metric: 0.023 | model: small_trained_ckpt/model_nxaqhy/epoch_8 |
2023-12-15 17:57:39 INFO     	 * rank: 42 | metric: 0.023 | model: small_trained_ckpt/model_vhyoja/epoch_8 |
2023-12-15 17:57:39 INFO     	 * rank: 43 | metric: 0.022 | model: small_trained_ckpt/model_nxaqhy/epoch_13 |
2023-12-15 17:57:39 INFO     	 * rank: 44 | metric: 0.022 | model: small_trained_ckpt/model_nxaqhy/epoch_14 |
2023-12-15 17:57:39 INFO     	 * rank: 45 | metric: 0.022 | model: small_trained_ckpt/model_oprhlh/epoch_13 |
2023-12-15 17:57:39 INFO     	 * rank: 46 | metric: 0.022 | model: small_trained_ckpt/model_eszyci/epoch_5 |
2023-12-15 17:57:39 INFO     	 * rank: 47 | metric: 0.021 | model: small_trained_ckpt/model_nxaqhy/epoch_15 |
2023-12-15 17:57:39 INFO     	 * rank: 48 | metric: 0.021 | model: small_trained_ckpt/model_oprhlh/epoch_14 |
2023-12-15 17:57:39 INFO     	 * rank: 49 | metric: 0.021 | model: small_trained_ckpt/model_eszyci/epoch_11 |
2023-12-15 17:57:39 INFO     	 * rank: 50 | metric: 0.019 | model: small_trained_ckpt/model_oprhlh/epoch_15 |
2023-12-15 17:57:39 INFO     	 * rank: 51 | metric: 0.019 | model: small_trained_ckpt/model_eszyci/epoch_3 |
2023-12-15 17:57:39 INFO     	 * rank: 52 | metric: 0.019 | model: small_trained_ckpt/model_eszyci/epoch_6 |
2023-12-15 17:57:39 INFO     	 * rank: 53 | metric: 0.0 | model: small_trained_ckpt/model_nxaqhy/epoch_7 |
2023-12-15 17:57:39 INFO     	 * rank: 54 | metric: 0.0 | model: small_trained_ckpt/model_vhyoja/epoch_7 |
2023-12-15 17:57:39 INFO     	 * rank: 55 | metric: 0.0 | model: small_trained_ckpt/model_oprhlh/epoch_4 |
2023-12-15 17:57:39 INFO     	 * rank: 56 | metric: 0.0 | model: small_trained_ckpt/model_nrudfu/epoch_4 |
2023-12-15 17:57:39 INFO     	 * rank: 57 | metric: 0.0 | model: small_trained_ckpt/model_oprhlh/epoch_1 |
2023-12-15 17:57:39 INFO     	 * rank: 58 | metric: 0.0 | model: small_trained_ckpt/model_oprhlh/epoch_2 |
2023-12-15 17:57:39 INFO     	 * rank: 59 | metric: 0.0 | model: small_trained_ckpt/model_oprhlh/epoch_3 |
2023-12-15 17:57:39 INFO     	 * rank: 60 | metric: 0.0 | model: small_trained_ckpt/model_nrudfu/epoch_1 |
2023-12-15 17:57:39 INFO     	 * rank: 61 | metric: 0.0 | model: small_trained_ckpt/model_nrudfu/epoch_2 |
2023-12-15 17:57:39 INFO     	 * rank: 62 | metric: 0.0 | model: small_trained_ckpt/model_nrudfu/epoch_3 |
2023-12-15 17:57:39 INFO     	 * rank: 63 | metric: 0.0 | model: small_trained_ckpt/model_nxaqhy/epoch_1 |
2023-12-15 17:57:39 INFO     	 * rank: 64 | metric: 0.0 | model: small_trained_ckpt/model_nxaqhy/epoch_2 |
2023-12-15 17:57:39 INFO     	 * rank: 65 | metric: 0.0 | model: small_trained_ckpt/model_nxaqhy/epoch_3 |
2023-12-15 17:57:39 INFO     	 * rank: 66 | metric: 0.0 | model: small_trained_ckpt/model_nxaqhy/epoch_4 |
2023-12-15 17:57:39 INFO     	 * rank: 67 | metric: 0.0 | model: small_trained_ckpt/model_nxaqhy/epoch_5 |
2023-12-15 17:57:39 INFO     	 * rank: 68 | metric: 0.0 | model: small_trained_ckpt/model_nxaqhy/epoch_6 |
2023-12-15 17:57:39 INFO     	 * rank: 69 | metric: 0.0 | model: small_trained_ckpt/model_vhyoja/epoch_1 |
2023-12-15 17:57:39 INFO     	 * rank: 70 | metric: 0.0 | model: small_trained_ckpt/model_vhyoja/epoch_2 |
2023-12-15 17:57:39 INFO     	 * rank: 71 | metric: 0.0 | model: small_trained_ckpt/model_vhyoja/epoch_3 |
2023-12-15 17:57:39 INFO     	 * rank: 72 | metric: 0.0 | model: small_trained_ckpt/model_vhyoja/epoch_4 |
2023-12-15 17:57:39 INFO     	 * rank: 73 | metric: 0.0 | model: small_trained_ckpt/model_vhyoja/epoch_5 |
2023-12-15 17:57:39 INFO     	 * rank: 74 | metric: 0.0 | model: small_trained_ckpt/model_vhyoja/epoch_6 |
2023-12-15 17:57:39 INFO     creating small_trained_ckpt/best_model
2023-12-15 17:57:39 INFO     copying small_trained_ckpt/model_oprhlh/epoch_11/config.json -> small_trained_ckpt/best_model
2023-12-15 17:57:39 INFO     copying small_trained_ckpt/model_oprhlh/epoch_11/generation_config.json -> small_trained_ckpt/best_model
2023-12-15 17:57:39 INFO     copying small_trained_ckpt/model_oprhlh/epoch_11/pytorch_model.bin -> small_trained_ckpt/best_model
2023-12-15 17:57:46 INFO     copying small_trained_ckpt/model_oprhlh/epoch_11/tokenizer_config.json -> small_trained_ckpt/best_model
2023-12-15 17:57:46 INFO     copying small_trained_ckpt/model_oprhlh/epoch_11/special_tokens_map.json -> small_trained_ckpt/best_model
2023-12-15 17:57:46 INFO     copying small_trained_ckpt/model_oprhlh/epoch_11/added_tokens.json -> small_trained_ckpt/best_model
2023-12-15 17:57:46 INFO     copying small_trained_ckpt/model_oprhlh/epoch_11/spiece.model -> small_trained_ckpt/best_model
2023-12-15 17:57:47 INFO     copying small_trained_ckpt/model_oprhlh/epoch_11/tokenizer.json -> small_trained_ckpt/best_model
2023-12-15 17:57:47 INFO     copying small_trained_ckpt/model_oprhlh/epoch_11/trainer_config.json -> small_trained_ckpt/best_model
2023-12-15 17:57:47 INFO     creating small_trained_ckpt/best_model/eval
2023-12-15 17:57:47 INFO     copying small_trained_ckpt/model_oprhlh/epoch_11/eval/samples.test.hyp.paragraph.questions_answers.StellarMilk_newsqa.default.txt -> small_trained_ckpt/best_model/eval
2023-12-15 17:57:47 INFO     copying small_trained_ckpt/model_oprhlh/epoch_11/eval/samples.validation.hyp.paragraph.questions_answers.StellarMilk_newsqa.default.txt -> small_trained_ckpt/best_model/eval
2023-12-15 17:57:47 INFO     copying small_trained_ckpt/model_oprhlh/epoch_11/eval/metric.first.answer.paragraph.questions_answers.StellarMilk_newsqa.default.json -> small_trained_ckpt/best_model/eval
