2023-12-20 10:57:28 INFO     INITIALIZE GRID SEARCHER: 12 configs to try
2023-12-20 10:57:28 INFO     ## 1st RUN: Configuration 0/12 ##
2023-12-20 10:57:28 INFO     initialize model trainer
2023-12-20 10:57:28 INFO     initialize checkpoint at small_combined_trained_ckpt/model_lwtqag
2023-12-20 10:57:28 INFO     hyperparameters
2023-12-20 10:57:28 INFO     	 * dataset_path: StellarMilk/squad_newsqa
2023-12-20 10:57:28 INFO     	 * dataset_name: default
2023-12-20 10:57:28 INFO     	 * input_types: ['paragraph']
2023-12-20 10:57:28 INFO     	 * output_types: ['questions_answers']
2023-12-20 10:57:28 INFO     	 * prefix_types: ['qag']
2023-12-20 10:57:28 INFO     	 * model: t5-small
2023-12-20 10:57:28 INFO     	 * max_length: 512
2023-12-20 10:57:28 INFO     	 * max_length_output: 512
2023-12-20 10:57:28 INFO     	 * epoch: 15
2023-12-20 10:57:28 INFO     	 * batch: 2
2023-12-20 10:57:28 INFO     	 * lr: 0.0001
2023-12-20 10:57:28 INFO     	 * fp16: False
2023-12-20 10:57:28 INFO     	 * random_seed: 1
2023-12-20 10:57:28 INFO     	 * gradient_accumulation_steps: 4
2023-12-20 10:57:28 INFO     	 * label_smoothing: 0.15
2023-12-20 10:57:28 INFO     initialize checkpoint with t5-small
2023-12-20 10:57:33 INFO     use spaCy answer extraction model: positionrank
2023-12-20 10:57:35 INFO     Model `t5-small`
2023-12-20 10:57:35 INFO     	 * Num of GPU in use: 1
2023-12-20 10:57:35 INFO     	 * Prefix: True
2023-12-20 10:57:35 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 10:57:35 INFO     dataset preprocessing
/home2/g.torresgamez/.local/lib/python3.10/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=False' instead.
  warnings.warn(
Downloading readme:   0%|          | 0.00/390 [00:00<?, ?B/s]Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 390/390 [00:00<00:00, 4.02MB/s]
Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]
Downloading data:   0%|          | 0.00/40.2M [00:00<?, ?B/s][A
Downloading data:  10%|â–ˆ         | 4.19M/40.2M [00:00<00:07, 4.55MB/s][A
Downloading data:  31%|â–ˆâ–ˆâ–ˆâ–      | 12.6M/40.2M [00:01<00:03, 7.91MB/s][A
Downloading data:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 21.0M/40.2M [00:02<00:02, 8.67MB/s][A
Downloading data:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 29.4M/40.2M [00:03<00:01, 10.4MB/s][A
Downloading data:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 37.7M/40.2M [00:03<00:00, 12.3MB/s][ADownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40.2M/40.2M [00:03<00:00, 10.9MB/s]
Downloading data files:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.71s/it]
Downloading data:   0%|          | 0.00/3.57M [00:00<?, ?B/s][A
Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.57M/3.57M [00:00<00:00, 6.76MB/s][ADownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.57M/3.57M [00:00<00:00, 6.68MB/s]
Downloading data files:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:04<00:01,  1.85s/it]
Downloading data:   0%|          | 0.00/3.69M [00:00<?, ?B/s][A
Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.69M/3.69M [00:00<00:00, 4.24MB/s][ADownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.69M/3.69M [00:00<00:00, 4.21MB/s]
Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.41s/it]Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.72s/it]
Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 165.11it/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 10000 examples [00:00, 43809.23 examples/s]Generating train split: 26789 examples [00:00, 77602.74 examples/s]Generating train split: 26789 examples [00:00, 61016.90 examples/s]
Generating validation split: 0 examples [00:00, ? examples/s]Generating validation split: 2641 examples [00:00, 83969.38 examples/s]
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 3003 examples [00:00, 95469.60 examples/s]
2023-12-20 10:57:43 INFO     encode all the data       : 26789
  0%|          | 0/26789 [00:00<?, ?it/s]  0%|          | 19/26789 [00:00<02:21, 189.19it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors
  0%|          | 129/26789 [00:00<00:36, 723.82it/s]  1%|          | 236/26789 [00:00<00:30, 881.50it/s]  1%|          | 326/26789 [00:00<00:29, 886.71it/s]  2%|â–         | 419/26789 [00:00<00:29, 902.16it/s]  2%|â–         | 520/26789 [00:00<00:28, 935.67it/s]  2%|â–         | 622/26789 [00:00<00:27, 959.71it/s]  3%|â–Ž         | 718/26789 [00:00<00:27, 947.26it/s]  3%|â–Ž         | 813/26789 [00:00<00:27, 928.40it/s]  3%|â–Ž         | 906/26789 [00:01<00:27, 924.45it/s]  4%|â–Ž         | 1001/26789 [00:01<00:27, 931.89it/s]  4%|â–         | 1097/26789 [00:01<00:27, 938.64it/s]  4%|â–         | 1191/26789 [00:01<00:27, 932.73it/s]  5%|â–         | 1287/26789 [00:01<00:27, 938.54it/s]  5%|â–Œ         | 1381/26789 [00:01<00:27, 933.30it/s]  6%|â–Œ         | 1475/26789 [00:01<00:27, 914.92it/s]  6%|â–Œ         | 1567/26789 [00:01<00:27, 906.05it/s]  6%|â–Œ         | 1663/26789 [00:01<00:27, 919.23it/s]  7%|â–‹         | 1758/26789 [00:01<00:27, 926.51it/s]  7%|â–‹         | 1851/26789 [00:02<00:26, 927.11it/s]  7%|â–‹         | 1944/26789 [00:02<00:27, 915.33it/s]  8%|â–Š         | 2036/26789 [00:02<00:27, 913.63it/s]  8%|â–Š         | 2128/26789 [00:02<00:27, 913.07it/s]  8%|â–Š         | 2224/26789 [00:02<00:26, 926.21it/s]  9%|â–Š         | 2317/26789 [00:02<00:26, 920.11it/s]  9%|â–‰         | 2410/26789 [00:02<00:28, 848.18it/s]  9%|â–‰         | 2496/26789 [00:02<00:28, 846.87it/s] 10%|â–‰         | 2588/26789 [00:02<00:27, 866.78it/s] 10%|â–‰         | 2676/26789 [00:02<00:27, 861.43it/s] 10%|â–ˆ         | 2767/26789 [00:03<00:27, 871.96it/s] 11%|â–ˆ         | 2860/26789 [00:03<00:26, 888.69it/s] 11%|â–ˆ         | 2952/26789 [00:03<00:26, 897.00it/s] 11%|â–ˆâ–        | 3042/26789 [00:03<00:26, 889.14it/s] 12%|â–ˆâ–        | 3132/26789 [00:03<00:26, 892.06it/s] 12%|â–ˆâ–        | 3224/26789 [00:03<00:26, 898.60it/s] 12%|â–ˆâ–        | 3314/26789 [00:03<00:26, 892.19it/s] 13%|â–ˆâ–Ž        | 3404/26789 [00:03<00:26, 881.83it/s] 13%|â–ˆâ–Ž        | 3494/26789 [00:03<00:26, 885.73it/s] 13%|â–ˆâ–Ž        | 3583/26789 [00:03<00:26, 879.68it/s] 14%|â–ˆâ–Ž        | 3676/26789 [00:04<00:25, 894.09it/s] 14%|â–ˆâ–        | 3767/26789 [00:04<00:25, 898.41it/s] 14%|â–ˆâ–        | 3857/26789 [00:04<00:25, 889.18it/s] 15%|â–ˆâ–        | 3949/26789 [00:04<00:25, 897.19it/s] 15%|â–ˆâ–Œ        | 4039/26789 [00:04<00:25, 893.01it/s] 15%|â–ˆâ–Œ        | 4130/26789 [00:04<00:25, 897.03it/s] 16%|â–ˆâ–Œ        | 4220/26789 [00:04<00:25, 894.17it/s] 16%|â–ˆâ–Œ        | 4310/26789 [00:04<00:25, 893.93it/s] 16%|â–ˆâ–‹        | 4402/26789 [00:04<00:24, 899.24it/s] 17%|â–ˆâ–‹        | 4492/26789 [00:05<00:24, 895.51it/s] 17%|â–ˆâ–‹        | 4585/26789 [00:05<00:24, 903.56it/s] 17%|â–ˆâ–‹        | 4680/26789 [00:05<00:24, 916.43it/s] 18%|â–ˆâ–Š        | 4772/26789 [00:05<00:24, 915.50it/s] 18%|â–ˆâ–Š        | 4864/26789 [00:05<00:24, 895.30it/s] 18%|â–ˆâ–Š        | 4954/26789 [00:05<00:24, 892.18it/s] 19%|â–ˆâ–‰        | 5044/26789 [00:05<00:24, 884.51it/s] 19%|â–ˆâ–‰        | 5136/26789 [00:05<00:24, 893.48it/s] 20%|â–ˆâ–‰        | 5226/26789 [00:05<00:24, 887.61it/s] 20%|â–ˆâ–‰        | 5315/26789 [00:05<00:24, 876.32it/s] 20%|â–ˆâ–ˆ        | 5407/26789 [00:06<00:24, 887.84it/s] 21%|â–ˆâ–ˆ        | 5499/26789 [00:06<00:23, 897.26it/s] 21%|â–ˆâ–ˆ        | 5589/26789 [00:06<00:23, 893.57it/s] 21%|â–ˆâ–ˆ        | 5684/26789 [00:06<00:23, 908.03it/s] 22%|â–ˆâ–ˆâ–       | 5776/26789 [00:06<00:23, 911.13it/s] 22%|â–ˆâ–ˆâ–       | 5868/26789 [00:06<00:23, 904.00it/s] 22%|â–ˆâ–ˆâ–       | 5960/26789 [00:06<00:22, 906.21it/s] 23%|â–ˆâ–ˆâ–Ž       | 6055/26789 [00:06<00:22, 919.15it/s] 23%|â–ˆâ–ˆâ–Ž       | 6147/26789 [00:06<00:22, 906.41it/s] 23%|â–ˆâ–ˆâ–Ž       | 6238/26789 [00:06<00:23, 889.48it/s] 24%|â–ˆâ–ˆâ–Ž       | 6333/26789 [00:07<00:22, 905.76it/s] 24%|â–ˆâ–ˆâ–       | 6425/26789 [00:07<00:22, 909.57it/s] 24%|â–ˆâ–ˆâ–       | 6520/26789 [00:07<00:21, 921.43it/s] 25%|â–ˆâ–ˆâ–       | 6616/26789 [00:07<00:21, 932.42it/s] 25%|â–ˆâ–ˆâ–Œ       | 6710/26789 [00:07<00:21, 930.10it/s] 25%|â–ˆâ–ˆâ–Œ       | 6804/26789 [00:07<00:21, 924.56it/s] 26%|â–ˆâ–ˆâ–Œ       | 6897/26789 [00:07<00:21, 921.50it/s] 26%|â–ˆâ–ˆâ–Œ       | 6990/26789 [00:07<00:21, 912.40it/s] 26%|â–ˆâ–ˆâ–‹       | 7082/26789 [00:07<00:21, 905.29it/s] 27%|â–ˆâ–ˆâ–‹       | 7178/26789 [00:07<00:21, 919.39it/s] 27%|â–ˆâ–ˆâ–‹       | 7270/26789 [00:08<00:21, 909.00it/s] 27%|â–ˆâ–ˆâ–‹       | 7361/26789 [00:08<00:21, 897.64it/s] 28%|â–ˆâ–ˆâ–Š       | 7456/26789 [00:08<00:21, 908.75it/s] 28%|â–ˆâ–ˆâ–Š       | 7547/26789 [00:08<00:21, 902.59it/s] 29%|â–ˆâ–ˆâ–Š       | 7644/26789 [00:08<00:20, 920.62it/s] 29%|â–ˆâ–ˆâ–‰       | 7740/26789 [00:08<00:20, 931.23it/s] 29%|â–ˆâ–ˆâ–‰       | 7834/26789 [00:08<00:20, 918.47it/s] 30%|â–ˆâ–ˆâ–‰       | 7929/26789 [00:08<00:20, 926.42it/s] 30%|â–ˆâ–ˆâ–‰       | 8022/26789 [00:08<00:20, 916.31it/s] 30%|â–ˆâ–ˆâ–ˆ       | 8114/26789 [00:08<00:20, 913.70it/s] 31%|â–ˆâ–ˆâ–ˆ       | 8206/26789 [00:09<00:20, 913.90it/s] 31%|â–ˆâ–ˆâ–ˆ       | 8298/26789 [00:09<00:20, 912.31it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 8390/26789 [00:09<00:20, 903.59it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 8483/26789 [00:09<00:20, 909.56it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 8574/26789 [00:09<00:20, 886.53it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 8663/26789 [00:09<00:20, 881.13it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 8752/26789 [00:09<00:20, 879.18it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 8842/26789 [00:09<00:20, 884.61it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 8943/26789 [00:09<00:19, 921.12it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 9036/26789 [00:10<00:19, 912.79it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 9128/26789 [00:10<00:19, 909.88it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 9223/26789 [00:10<00:19, 921.40it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 9316/26789 [00:10<00:19, 913.18it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 9409/26789 [00:10<00:18, 916.91it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 9505/26789 [00:10<00:18, 928.50it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9598/26789 [00:10<00:18, 916.58it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9690/26789 [00:10<00:18, 903.75it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 9782/26789 [00:10<00:18, 906.84it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 9873/26789 [00:10<00:18, 906.70it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 9964/26789 [00:11<00:18, 888.57it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 10058/26789 [00:11<00:18, 903.07it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 10150/26789 [00:11<00:18, 907.58it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 10241/26789 [00:11<00:18, 908.22it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 10335/26789 [00:11<00:17, 915.75it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 10427/26789 [00:11<00:18, 905.85it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 10521/26789 [00:11<00:17, 913.14it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 10617/26789 [00:11<00:17, 924.91it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 10712/26789 [00:11<00:17, 930.74it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10809/26789 [00:11<00:16, 940.17it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10904/26789 [00:12<00:17, 910.00it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11001/26789 [00:12<00:17, 925.61it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11094/26789 [00:12<00:17, 922.52it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11187/26789 [00:12<00:17, 910.19it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11285/26789 [00:12<00:16, 928.23it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11378/26789 [00:12<00:16, 924.52it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 11472/26789 [00:12<00:16, 928.03it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 11566/26789 [00:12<00:16, 929.32it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 11659/26789 [00:12<00:16, 920.63it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11752/26789 [00:12<00:16, 913.82it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11844/26789 [00:13<00:16, 915.22it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11936/26789 [00:13<00:16, 915.42it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12032/26789 [00:13<00:16, 915.14it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 12126/26789 [00:13<00:15, 919.89it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 12219/26789 [00:13<00:15, 913.05it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 12315/26789 [00:13<00:15, 924.71it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 12408/26789 [00:13<00:15, 925.43it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 12503/26789 [00:13<00:15, 931.12it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 12601/26789 [00:13<00:15, 941.93it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 12696/26789 [00:14<00:15, 922.57it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12790/26789 [00:14<00:15, 926.38it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12883/26789 [00:14<00:15, 906.74it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12980/26789 [00:14<00:14, 925.05it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 13075/26789 [00:14<00:14, 931.37it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 13169/26789 [00:14<00:14, 922.23it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 13262/26789 [00:14<00:15, 894.03it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 13355/26789 [00:14<00:14, 902.70it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 13448/26789 [00:14<00:14, 907.22it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 13542/26789 [00:14<00:14, 915.43it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 13637/26789 [00:15<00:14, 922.96it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13730/26789 [00:15<00:14, 920.68it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13823/26789 [00:15<00:14, 920.71it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13916/26789 [00:15<00:14, 901.61it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14007/26789 [00:15<00:14, 902.45it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 14099/26789 [00:15<00:14, 906.05it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 14197/26789 [00:15<00:13, 925.93it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 14291/26789 [00:15<00:13, 927.84it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 14384/26789 [00:15<00:13, 907.02it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14475/26789 [00:15<00:13, 894.05it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14565/26789 [00:16<00:24, 504.96it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14656/26789 [00:16<00:20, 581.45it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14750/26789 [00:16<00:18, 657.72it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14844/26789 [00:16<00:16, 722.58it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14941/26789 [00:16<00:15, 783.58it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15036/26789 [00:16<00:14, 825.00it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 15134/26789 [00:16<00:13, 866.71it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 15228/26789 [00:17<00:13, 886.96it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 15321/26789 [00:17<00:12, 888.24it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 15413/26789 [00:17<00:12, 896.42it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 15505/26789 [00:17<00:12, 901.48it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 15599/26789 [00:17<00:12, 911.08it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 15692/26789 [00:17<00:12, 902.15it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 15787/26789 [00:17<00:12, 913.43it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 15879/26789 [00:17<00:12, 885.76it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 15971/26789 [00:17<00:12, 895.33it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16062/26789 [00:17<00:11, 896.26it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 16157/26789 [00:18<00:11, 911.61it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 16249/26789 [00:18<00:11, 911.18it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 16344/26789 [00:18<00:11, 922.41it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16437/26789 [00:18<00:11, 923.51it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16530/26789 [00:18<00:12, 846.05it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16616/26789 [00:18<00:12, 784.74it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16697/26789 [00:18<00:13, 742.82it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 16773/26789 [00:18<00:13, 721.56it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 16846/26789 [00:18<00:13, 714.01it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 16918/26789 [00:19<00:14, 690.95it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 16988/26789 [00:19<00:14, 686.39it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17057/26789 [00:19<00:14, 678.15it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 17125/26789 [00:19<00:14, 673.79it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 17193/26789 [00:19<00:14, 670.66it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 17264/26789 [00:19<00:14, 678.32it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 17332/26789 [00:19<00:13, 675.89it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 17403/26789 [00:19<00:13, 684.97it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 17472/26789 [00:19<00:14, 662.65it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 17539/26789 [00:19<00:13, 663.52it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 17606/26789 [00:20<00:14, 652.87it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 17672/26789 [00:20<00:14, 632.74it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 17736/26789 [00:20<00:14, 631.46it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 17806/26789 [00:20<00:13, 648.35it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 17874/26789 [00:20<00:13, 656.24it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 17943/26789 [00:20<00:13, 663.47it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18013/26789 [00:20<00:13, 672.80it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18081/26789 [00:20<00:12, 673.79it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 18149/26789 [00:20<00:13, 660.90it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 18216/26789 [00:21<00:13, 654.61it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 18283/26789 [00:21<00:12, 656.67it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 18353/26789 [00:21<00:12, 667.93it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 18425/26789 [00:21<00:12, 679.98it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 18494/26789 [00:21<00:12, 671.71it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 18563/26789 [00:21<00:12, 676.05it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 18631/26789 [00:21<00:12, 673.66it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 18699/26789 [00:21<00:12, 664.29it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 18766/26789 [00:21<00:12, 655.50it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 18833/26789 [00:21<00:12, 657.48it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 18902/26789 [00:22<00:11, 666.24it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 18969/26789 [00:22<00:11, 666.77it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19037/26789 [00:22<00:11, 668.76it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 19105/26789 [00:22<00:11, 670.15it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 19174/26789 [00:22<00:11, 674.25it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 19242/26789 [00:22<00:11, 672.27it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 19311/26789 [00:22<00:11, 676.35it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 19379/26789 [00:22<00:11, 673.55it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 19447/26789 [00:22<00:10, 669.98it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 19516/26789 [00:22<00:10, 675.13it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 19584/26789 [00:23<00:10, 675.96it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 19652/26789 [00:23<00:10, 669.08it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 19722/26789 [00:23<00:10, 675.73it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 19790/26789 [00:23<00:10, 676.80it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 19860/26789 [00:23<00:10, 681.98it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 19929/26789 [00:23<00:10, 681.04it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 19998/26789 [00:23<00:09, 680.84it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20067/26789 [00:23<00:10, 672.15it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 20135/26789 [00:23<00:09, 666.29it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 20207/26789 [00:23<00:09, 677.43it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 20280/26789 [00:24<00:09, 690.09it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 20350/26789 [00:24<00:09, 685.73it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 20419/26789 [00:24<00:09, 680.27it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 20488/26789 [00:24<00:09, 668.57it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 20558/26789 [00:24<00:09, 676.06it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 20628/26789 [00:24<00:09, 681.62it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 20697/26789 [00:24<00:08, 678.89it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 20765/26789 [00:24<00:08, 670.39it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 20834/26789 [00:24<00:08, 674.13it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 20903/26789 [00:25<00:08, 677.45it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 20972/26789 [00:25<00:08, 678.47it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21043/26789 [00:25<00:08, 686.87it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 21112/26789 [00:25<00:08, 674.76it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 21183/26789 [00:25<00:08, 682.95it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 21252/26789 [00:25<00:08, 680.93it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 21323/26789 [00:25<00:07, 687.64it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 21392/26789 [00:25<00:07, 684.69it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 21461/26789 [00:25<00:07, 682.44it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 21531/26789 [00:25<00:07, 684.44it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 21600/26789 [00:26<00:07, 677.30it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 21671/26789 [00:26<00:07, 685.96it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 21740/26789 [00:26<00:07, 673.88it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21812/26789 [00:26<00:07, 687.10it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21882/26789 [00:26<00:07, 687.82it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21951/26789 [00:26<00:07, 676.61it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22023/26789 [00:26<00:06, 688.98it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22092/26789 [00:26<00:07, 665.06it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 22159/26789 [00:26<00:07, 661.25it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 22226/26789 [00:26<00:06, 662.19it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 22296/26789 [00:27<00:06, 673.21it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 22365/26789 [00:27<00:06, 675.37it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 22433/26789 [00:27<00:06, 671.30it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22501/26789 [00:27<00:06, 669.14it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22571/26789 [00:27<00:06, 678.03it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22642/26789 [00:27<00:06, 684.12it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22712/26789 [00:27<00:05, 687.87it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 22781/26789 [00:27<00:05, 685.55it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 22850/26789 [00:27<00:05, 674.61it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 22922/26789 [00:27<00:05, 685.43it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 22991/26789 [00:28<00:05, 678.52it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23059/26789 [00:28<00:05, 677.42it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 23128/26789 [00:28<00:05, 679.19it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 23198/26789 [00:28<00:05, 682.35it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 23267/26789 [00:28<00:05, 681.69it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 23336/26789 [00:28<00:05, 678.76it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 23404/26789 [00:28<00:05, 670.37it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 23473/26789 [00:28<00:04, 674.19it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 23541/26789 [00:28<00:04, 675.21it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 23611/26789 [00:29<00:04, 678.46it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 23680/26789 [00:29<00:04, 677.79it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 23748/26789 [00:29<00:04, 677.01it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 23819/26789 [00:29<00:04, 684.79it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 23890/26789 [00:29<00:04, 689.80it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 23962/26789 [00:29<00:04, 697.48it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24032/26789 [00:29<00:04, 683.30it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24101/26789 [00:29<00:03, 679.36it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 24174/26789 [00:29<00:03, 692.32it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 24244/26789 [00:29<00:03, 692.76it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 24314/26789 [00:30<00:03, 685.82it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 24383/26789 [00:30<00:03, 684.04it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 24452/26789 [00:30<00:03, 677.19it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 24524/26789 [00:30<00:03, 687.75it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 24593/26789 [00:30<00:03, 687.77it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 24662/26789 [00:30<00:03, 680.76it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 24731/26789 [00:30<00:03, 678.16it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 24799/26789 [00:30<00:02, 669.94it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 24868/26789 [00:30<00:02, 674.35it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 24942/26789 [00:30<00:02, 692.15it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25013/26789 [00:31<00:02, 696.45it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25083/26789 [00:31<00:02, 696.11it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 25153/26789 [00:31<00:02, 696.87it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 25223/26789 [00:31<00:02, 691.63it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 25293/26789 [00:31<00:02, 676.24it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 25361/26789 [00:31<00:02, 673.77it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 25436/26789 [00:31<00:01, 692.48it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 25507/26789 [00:31<00:01, 696.64it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 25577/26789 [00:31<00:01, 694.00it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 25647/26789 [00:31<00:01, 683.63it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 25719/26789 [00:32<00:01, 692.62it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 25789/26789 [00:32<00:01, 691.30it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 25859/26789 [00:32<00:01, 689.31it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 25928/26789 [00:32<00:01, 687.58it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 25997/26789 [00:32<00:01, 682.76it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26067/26789 [00:32<00:01, 686.24it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 26136/26789 [00:32<00:00, 684.07it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 26205/26789 [00:32<00:00, 682.83it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 26274/26789 [00:32<00:00, 677.75it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 26342/26789 [00:32<00:00, 674.81it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 26414/26789 [00:33<00:00, 688.02it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 26483/26789 [00:33<00:00, 678.76it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 26551/26789 [00:33<00:00, 675.64it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 26621/26789 [00:33<00:00, 682.14it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 26690/26789 [00:33<00:00, 682.47it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 26760/26789 [00:33<00:00, 685.46it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26789/26789 [00:33<00:00, 796.22it/s]
2023-12-20 10:58:21 INFO     after remove the overflow : 18894
2023-12-20 10:58:23 INFO     preprocessed feature is saved at /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/squad_newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-20 10:58:23 INFO     start model training
2023-12-20 10:58:38 INFO     	 * (global step 50: loss: 0.794698640704155, lr: 0.0001
2023-12-20 10:58:52 INFO     	 * (global step 100: loss: 0.8158146739006042, lr: 0.0001
2023-12-20 10:59:08 INFO     	 * (global step 150: loss: 0.5092844516038895, lr: 0.0001
2023-12-20 10:59:23 INFO     	 * (global step 200: loss: 0.48985590040683746, lr: 0.0001
2023-12-20 10:59:38 INFO     	 * (global step 250: loss: 0.5685335397720337, lr: 0.0001
2023-12-20 10:59:53 INFO     	 * (global step 300: loss: 0.4591766595840454, lr: 0.0001
2023-12-20 11:00:09 INFO     	 * (global step 350: loss: 0.49949783831834793, lr: 0.0001
2023-12-20 11:00:24 INFO     	 * (global step 400: loss: 0.32772936671972275, lr: 0.0001
2023-12-20 11:00:39 INFO     	 * (global step 450: loss: 0.38464515656232834, lr: 0.0001
2023-12-20 11:00:55 INFO     	 * (global step 500: loss: 0.3578670769929886, lr: 0.0001
2023-12-20 11:01:10 INFO     	 * (global step 550: loss: 0.42995817959308624, lr: 0.0001
2023-12-20 11:01:26 INFO     	 * (global step 600: loss: 0.4401578903198242, lr: 0.0001
2023-12-20 11:01:41 INFO     	 * (global step 650: loss: 0.3297661617398262, lr: 0.0001
2023-12-20 11:01:57 INFO     	 * (global step 700: loss: 0.3470190726220608, lr: 0.0001
2023-12-20 11:02:12 INFO     	 * (global step 750: loss: 0.46080951392650604, lr: 0.0001
2023-12-20 11:02:28 INFO     	 * (global step 800: loss: 0.36384502053260803, lr: 0.0001
2023-12-20 11:02:43 INFO     	 * (global step 850: loss: 0.3340333253145218, lr: 0.0001
2023-12-20 11:02:59 INFO     	 * (global step 900: loss: 0.34148379415273666, lr: 0.0001
2023-12-20 11:03:14 INFO     	 * (global step 950: loss: 0.3397870473563671, lr: 0.0001
2023-12-20 11:03:30 INFO     	 * (global step 1000: loss: 0.362209677696228, lr: 0.0001
2023-12-20 11:03:45 INFO     	 * (global step 1050: loss: 0.3599195182323456, lr: 0.0001
2023-12-20 11:04:00 INFO     	 * (global step 1100: loss: 0.38159240037202835, lr: 0.0001
2023-12-20 11:04:16 INFO     	 * (global step 1150: loss: 0.5759630724787712, lr: 0.0001
2023-12-20 11:04:31 INFO     	 * (global step 1200: loss: 0.3516113795340061, lr: 0.0001
2023-12-20 11:04:47 INFO     	 * (global step 1250: loss: 0.30679260566830635, lr: 0.0001
2023-12-20 11:05:02 INFO     	 * (global step 1300: loss: 0.3484619930386543, lr: 0.0001
2023-12-20 11:05:18 INFO     	 * (global step 1350: loss: 0.30273113772273064, lr: 0.0001
2023-12-20 11:05:33 INFO     	 * (global step 1400: loss: 0.3991699293255806, lr: 0.0001
2023-12-20 11:05:49 INFO     	 * (global step 1450: loss: 0.3522299453616142, lr: 0.0001
2023-12-20 11:06:04 INFO     	 * (global step 1500: loss: 0.44122854992747307, lr: 0.0001
2023-12-20 11:06:20 INFO     	 * (global step 1550: loss: 0.2924797087907791, lr: 0.0001
2023-12-20 11:06:35 INFO     	 * (global step 1600: loss: 0.29544708877801895, lr: 0.0001
2023-12-20 11:06:51 INFO     	 * (global step 1650: loss: 0.4055129364132881, lr: 0.0001
2023-12-20 11:07:06 INFO     	 * (global step 1700: loss: 0.3162122517824173, lr: 0.0001
2023-12-20 11:07:22 INFO     	 * (global step 1750: loss: 0.3017149865627289, lr: 0.0001
2023-12-20 11:07:37 INFO     	 * (global step 1800: loss: 0.30947345495224, lr: 0.0001
2023-12-20 11:07:53 INFO     	 * (global step 1850: loss: 0.24946700781583786, lr: 0.0001
2023-12-20 11:08:08 INFO     	 * (global step 1900: loss: 0.3348980024456978, lr: 0.0001
2023-12-20 11:08:24 INFO     	 * (global step 1950: loss: 0.30491098016500473, lr: 0.0001
2023-12-20 11:08:39 INFO     	 * (global step 2000: loss: 0.25571754574775696, lr: 0.0001
2023-12-20 11:08:55 INFO     	 * (global step 2050: loss: 0.2758674770593643, lr: 0.0001
2023-12-20 11:09:10 INFO     	 * (global step 2100: loss: 0.2771972827613354, lr: 0.0001
2023-12-20 11:09:26 INFO     	 * (global step 2150: loss: 0.419622078537941, lr: 0.0001
2023-12-20 11:09:41 INFO     	 * (global step 2200: loss: 0.4423096254467964, lr: 0.0001
2023-12-20 11:09:57 INFO     	 * (global step 2250: loss: 0.28373606503009796, lr: 0.0001
2023-12-20 11:10:12 INFO     	 * (global step 2300: loss: 0.3657923713326454, lr: 0.0001
2023-12-20 11:10:28 INFO     	 * (global step 2350: loss: 0.34129631146788597, lr: 0.0001
2023-12-20 11:10:31 INFO     [epoch 0/15] average loss: 0.436, lr: 0.0001
2023-12-20 11:10:31 INFO     saving model related files
2023-12-20 11:10:31 INFO     saving model
2023-12-20 11:10:32 INFO     saving tokenizer
2023-12-20 11:10:32 INFO     saving optimizer
2023-12-20 11:10:33 INFO     remove old optimizer files
2023-12-20 11:10:45 INFO     	 * (global step 2400: loss: 0.32220079004764557, lr: 0.0001
2023-12-20 11:11:00 INFO     	 * (global step 2450: loss: 0.3519979864358902, lr: 0.0001
2023-12-20 11:11:16 INFO     	 * (global step 2500: loss: 0.33408186212182045, lr: 0.0001
2023-12-20 11:11:31 INFO     	 * (global step 2550: loss: 0.31032033264636993, lr: 0.0001
2023-12-20 11:11:47 INFO     	 * (global step 2600: loss: 0.3125217743217945, lr: 0.0001
2023-12-20 11:12:02 INFO     	 * (global step 2650: loss: 0.5425296351313591, lr: 0.0001
2023-12-20 11:12:18 INFO     	 * (global step 2700: loss: 0.3850136175751686, lr: 0.0001
2023-12-20 11:12:33 INFO     	 * (global step 2750: loss: 0.2798687443137169, lr: 0.0001
2023-12-20 11:12:49 INFO     	 * (global step 2800: loss: 0.42221230268478394, lr: 0.0001
2023-12-20 11:13:04 INFO     	 * (global step 2850: loss: 0.4185946062207222, lr: 0.0001
2023-12-20 11:13:20 INFO     	 * (global step 2900: loss: 0.40989673137664795, lr: 0.0001
2023-12-20 11:13:35 INFO     	 * (global step 2950: loss: 0.3224378377199173, lr: 0.0001
2023-12-20 11:13:51 INFO     	 * (global step 3000: loss: 0.3858010545372963, lr: 0.0001
2023-12-20 11:14:06 INFO     	 * (global step 3050: loss: 0.35008156672120094, lr: 0.0001
2023-12-20 11:14:22 INFO     	 * (global step 3100: loss: 0.33248430490493774, lr: 0.0001
2023-12-20 11:14:37 INFO     	 * (global step 3150: loss: 0.47973456233739853, lr: 0.0001
2023-12-20 11:14:53 INFO     	 * (global step 3200: loss: 0.3016412705183029, lr: 0.0001
2023-12-20 11:15:08 INFO     	 * (global step 3250: loss: 0.3782372921705246, lr: 0.0001
2023-12-20 11:15:24 INFO     	 * (global step 3300: loss: 0.3296907916665077, lr: 0.0001
2023-12-20 11:15:39 INFO     	 * (global step 3350: loss: 0.2881235219538212, lr: 0.0001
2023-12-20 11:15:55 INFO     	 * (global step 3400: loss: 0.2685377597808838, lr: 0.0001
2023-12-20 11:16:11 INFO     	 * (global step 3450: loss: 0.3674388974905014, lr: 0.0001
2023-12-20 11:16:26 INFO     	 * (global step 3500: loss: 0.48867956548929214, lr: 0.0001
2023-12-20 11:16:42 INFO     	 * (global step 3550: loss: 0.31830860674381256, lr: 0.0001
2023-12-20 11:16:57 INFO     	 * (global step 3600: loss: 0.35436491668224335, lr: 0.0001
2023-12-20 11:17:13 INFO     	 * (global step 3650: loss: 0.31908390671014786, lr: 0.0001
2023-12-20 11:17:28 INFO     	 * (global step 3700: loss: 0.3549884781241417, lr: 0.0001
2023-12-20 11:17:44 INFO     	 * (global step 3750: loss: 0.33602409437298775, lr: 0.0001
2023-12-20 11:17:59 INFO     	 * (global step 3800: loss: 0.25316664949059486, lr: 0.0001
2023-12-20 11:18:15 INFO     	 * (global step 3850: loss: 0.34346091747283936, lr: 0.0001
2023-12-20 11:18:30 INFO     	 * (global step 3900: loss: 0.27555112913250923, lr: 0.0001
2023-12-20 11:18:46 INFO     	 * (global step 3950: loss: 0.34766818583011627, lr: 0.0001
2023-12-20 11:19:01 INFO     	 * (global step 4000: loss: 0.2862633913755417, lr: 0.0001
2023-12-20 11:19:17 INFO     	 * (global step 4050: loss: 0.39706095308065414, lr: 0.0001
2023-12-20 11:19:32 INFO     	 * (global step 4100: loss: 0.2631145939230919, lr: 0.0001
2023-12-20 11:19:48 INFO     	 * (global step 4150: loss: 0.2628025487065315, lr: 0.0001
2023-12-20 11:20:03 INFO     	 * (global step 4200: loss: 0.35479169338941574, lr: 0.0001
2023-12-20 11:20:19 INFO     	 * (global step 4250: loss: 0.3913527838885784, lr: 0.0001
2023-12-20 11:20:34 INFO     	 * (global step 4300: loss: 0.28558509051799774, lr: 0.0001
2023-12-20 11:20:50 INFO     	 * (global step 4350: loss: 0.26326051726937294, lr: 0.0001
2023-12-20 11:21:05 INFO     	 * (global step 4400: loss: 0.29415087401866913, lr: 0.0001
2023-12-20 11:21:21 INFO     	 * (global step 4450: loss: 0.262018371373415, lr: 0.0001
2023-12-20 11:21:36 INFO     	 * (global step 4500: loss: 0.264301136136055, lr: 0.0001
2023-12-20 11:21:52 INFO     	 * (global step 4550: loss: 0.32391639053821564, lr: 0.0001
2023-12-20 11:22:07 INFO     	 * (global step 4600: loss: 0.21828724816441536, lr: 0.0001
2023-12-20 11:22:23 INFO     	 * (global step 4650: loss: 0.3331867344677448, lr: 0.0001
2023-12-20 11:22:38 INFO     	 * (global step 4700: loss: 0.3040569983422756, lr: 0.0001
2023-12-20 11:22:45 INFO     [epoch 1/15] average loss: 0.333, lr: 0.0001
2023-12-20 11:22:45 INFO     saving model related files
2023-12-20 11:22:45 INFO     saving model
2023-12-20 11:22:46 INFO     saving tokenizer
2023-12-20 11:22:46 INFO     saving optimizer
2023-12-20 11:22:47 INFO     remove old optimizer files
2023-12-20 11:22:56 INFO     	 * (global step 4750: loss: 0.4743799492716789, lr: 0.0001
2023-12-20 11:23:11 INFO     	 * (global step 4800: loss: 0.28612641245126724, lr: 0.0001
2023-12-20 11:23:27 INFO     	 * (global step 4850: loss: 0.242323137819767, lr: 0.0001
2023-12-20 11:23:42 INFO     	 * (global step 4900: loss: 0.2775350734591484, lr: 0.0001
2023-12-20 11:23:58 INFO     	 * (global step 4950: loss: 0.28253724053502083, lr: 0.0001
2023-12-20 11:24:13 INFO     	 * (global step 5000: loss: 0.4605114310979843, lr: 0.0001
2023-12-20 11:24:29 INFO     	 * (global step 5050: loss: 0.3534008339047432, lr: 0.0001
2023-12-20 11:24:44 INFO     	 * (global step 5100: loss: 0.25451648607850075, lr: 0.0001
2023-12-20 11:25:00 INFO     	 * (global step 5150: loss: 0.30367235839366913, lr: 0.0001
2023-12-20 11:25:15 INFO     	 * (global step 5200: loss: 0.36998898535966873, lr: 0.0001
2023-12-20 11:25:31 INFO     	 * (global step 5250: loss: 0.2979753576219082, lr: 0.0001
2023-12-20 11:25:46 INFO     	 * (global step 5300: loss: 0.322780542075634, lr: 0.0001
2023-12-20 11:26:02 INFO     	 * (global step 5350: loss: 0.2674420177936554, lr: 0.0001
2023-12-20 11:26:17 INFO     	 * (global step 5400: loss: 0.21772386506199837, lr: 0.0001
2023-12-20 11:26:33 INFO     	 * (global step 5450: loss: 0.4315117672085762, lr: 0.0001
2023-12-20 11:26:48 INFO     	 * (global step 5500: loss: 0.41965071856975555, lr: 0.0001
2023-12-20 11:27:04 INFO     	 * (global step 5550: loss: 0.2781887762248516, lr: 0.0001
2023-12-20 11:27:19 INFO     	 * (global step 5600: loss: 0.24984923005104065, lr: 0.0001
2023-12-20 11:27:35 INFO     	 * (global step 5650: loss: 0.2629379481077194, lr: 0.0001
2023-12-20 11:27:50 INFO     	 * (global step 5700: loss: 0.4250052943825722, lr: 0.0001
2023-12-20 11:28:06 INFO     	 * (global step 5750: loss: 0.35296111553907394, lr: 0.0001
2023-12-20 11:28:21 INFO     	 * (global step 5800: loss: 0.3252169191837311, lr: 0.0001
2023-12-20 11:28:36 INFO     	 * (global step 5850: loss: 0.37009841203689575, lr: 0.0001
2023-12-20 11:28:52 INFO     	 * (global step 5900: loss: 0.316234327852726, lr: 0.0001
2023-12-20 11:29:07 INFO     	 * (global step 5950: loss: 0.37720517069101334, lr: 0.0001
2023-12-20 11:29:23 INFO     	 * (global step 6000: loss: 0.36206046119332314, lr: 0.0001
2023-12-20 11:29:38 INFO     	 * (global step 6050: loss: 0.2820034362375736, lr: 0.0001
2023-12-20 11:29:54 INFO     	 * (global step 6100: loss: 0.3602571561932564, lr: 0.0001
2023-12-20 11:30:09 INFO     	 * (global step 6150: loss: 0.3613818734884262, lr: 0.0001
2023-12-20 11:30:25 INFO     	 * (global step 6200: loss: 0.28925982490181923, lr: 0.0001
2023-12-20 11:30:40 INFO     	 * (global step 6250: loss: 0.25868910551071167, lr: 0.0001
2023-12-20 11:30:56 INFO     	 * (global step 6300: loss: 0.357315294444561, lr: 0.0001
2023-12-20 11:31:11 INFO     	 * (global step 6350: loss: 0.38496094197034836, lr: 0.0001
2023-12-20 11:31:27 INFO     	 * (global step 6400: loss: 0.2727784737944603, lr: 0.0001
2023-12-20 11:31:42 INFO     	 * (global step 6450: loss: 0.30949050188064575, lr: 0.0001
2023-12-20 11:31:58 INFO     	 * (global step 6500: loss: 0.2983803078532219, lr: 0.0001
2023-12-20 11:32:13 INFO     	 * (global step 6550: loss: 0.41341155767440796, lr: 0.0001
2023-12-20 11:32:29 INFO     	 * (global step 6600: loss: 0.3885060325264931, lr: 0.0001
2023-12-20 11:32:44 INFO     	 * (global step 6650: loss: 0.29920003563165665, lr: 0.0001
2023-12-20 11:33:00 INFO     	 * (global step 6700: loss: 0.303361427038908, lr: 0.0001
2023-12-20 11:33:15 INFO     	 * (global step 6750: loss: 0.36094309017062187, lr: 0.0001
2023-12-20 11:33:31 INFO     	 * (global step 6800: loss: 0.30268706008791924, lr: 0.0001
2023-12-20 11:33:46 INFO     	 * (global step 6850: loss: 0.2471390888094902, lr: 0.0001
2023-12-20 11:34:02 INFO     	 * (global step 6900: loss: 0.38052524253726006, lr: 0.0001
2023-12-20 11:34:17 INFO     	 * (global step 6950: loss: 0.28337113931775093, lr: 0.0001
2023-12-20 11:34:33 INFO     	 * (global step 7000: loss: 0.28006818890571594, lr: 0.0001
2023-12-20 11:34:48 INFO     	 * (global step 7050: loss: 0.29546940326690674, lr: 0.0001
2023-12-20 11:34:59 INFO     [epoch 2/15] average loss: 0.314, lr: 0.0001
2023-12-20 11:34:59 INFO     saving model related files
2023-12-20 11:34:59 INFO     saving model
2023-12-20 11:34:59 INFO     saving tokenizer
2023-12-20 11:34:59 INFO     saving optimizer
2023-12-20 11:35:00 INFO     remove old optimizer files
2023-12-20 11:35:05 INFO     	 * (global step 7100: loss: 0.26735008135437965, lr: 0.0001
2023-12-20 11:35:21 INFO     	 * (global step 7150: loss: 0.42639103904366493, lr: 0.0001
2023-12-20 11:35:37 INFO     	 * (global step 7200: loss: 0.3046858534216881, lr: 0.0001
2023-12-20 11:35:52 INFO     	 * (global step 7250: loss: 0.23654332011938095, lr: 0.0001
2023-12-20 11:36:08 INFO     	 * (global step 7300: loss: 0.4119284451007843, lr: 0.0001
2023-12-20 11:36:23 INFO     	 * (global step 7350: loss: 0.26185332983732224, lr: 0.0001
2023-12-20 11:36:39 INFO     	 * (global step 7400: loss: 0.3289632089436054, lr: 0.0001
2023-12-20 11:36:54 INFO     	 * (global step 7450: loss: 0.256744135171175, lr: 0.0001
2023-12-20 11:37:10 INFO     	 * (global step 7500: loss: 0.2654450982809067, lr: 0.0001
2023-12-20 11:37:25 INFO     	 * (global step 7550: loss: 0.31848692893981934, lr: 0.0001
2023-12-20 11:37:41 INFO     	 * (global step 7600: loss: 0.3086092323064804, lr: 0.0001
2023-12-20 11:37:56 INFO     	 * (global step 7650: loss: 0.26864271610975266, lr: 0.0001
2023-12-20 11:38:12 INFO     	 * (global step 7700: loss: 0.3414244130253792, lr: 0.0001
2023-12-20 11:38:27 INFO     	 * (global step 7750: loss: 0.3125448524951935, lr: 0.0001
2023-12-20 11:38:43 INFO     	 * (global step 7800: loss: 0.2596990503370762, lr: 0.0001
2023-12-20 11:38:58 INFO     	 * (global step 7850: loss: 0.23763268440961838, lr: 0.0001
2023-12-20 11:39:14 INFO     	 * (global step 7900: loss: 0.24257349595427513, lr: 0.0001
2023-12-20 11:39:29 INFO     	 * (global step 7950: loss: 0.23198523372411728, lr: 0.0001
2023-12-20 11:39:45 INFO     	 * (global step 8000: loss: 0.32308872044086456, lr: 0.0001
2023-12-20 11:40:00 INFO     	 * (global step 8050: loss: 0.2656877897679806, lr: 0.0001
2023-12-20 11:40:16 INFO     	 * (global step 8100: loss: 0.4199715442955494, lr: 0.0001
2023-12-20 11:40:31 INFO     	 * (global step 8150: loss: 0.2506383880972862, lr: 0.0001
2023-12-20 11:40:47 INFO     	 * (global step 8200: loss: 0.27415624260902405, lr: 0.0001
2023-12-20 11:41:03 INFO     	 * (global step 8250: loss: 0.2923671007156372, lr: 0.0001
2023-12-20 11:41:18 INFO     	 * (global step 8300: loss: 0.2640255056321621, lr: 0.0001
2023-12-20 11:41:34 INFO     	 * (global step 8350: loss: 0.34015386551618576, lr: 0.0001
2023-12-20 11:41:49 INFO     	 * (global step 8400: loss: 0.2899046540260315, lr: 0.0001
2023-12-20 11:42:05 INFO     	 * (global step 8450: loss: 0.291726965457201, lr: 0.0001
2023-12-20 11:42:20 INFO     	 * (global step 8500: loss: 0.29617438837885857, lr: 0.0001
2023-12-20 11:42:36 INFO     	 * (global step 8550: loss: 0.23272128775715828, lr: 0.0001
2023-12-20 11:42:51 INFO     	 * (global step 8600: loss: 0.3375900164246559, lr: 0.0001
2023-12-20 11:43:07 INFO     	 * (global step 8650: loss: 0.4255778379738331, lr: 0.0001
2023-12-20 11:43:22 INFO     	 * (global step 8700: loss: 0.34871598705649376, lr: 0.0001
2023-12-20 11:43:38 INFO     	 * (global step 8750: loss: 0.2308386228978634, lr: 0.0001
2023-12-20 11:43:53 INFO     	 * (global step 8800: loss: 0.30984029918909073, lr: 0.0001
2023-12-20 11:44:09 INFO     	 * (global step 8850: loss: 0.2931140214204788, lr: 0.0001
2023-12-20 11:44:24 INFO     	 * (global step 8900: loss: 0.2331311460584402, lr: 0.0001
2023-12-20 11:44:40 INFO     	 * (global step 8950: loss: 0.2816232368350029, lr: 0.0001
2023-12-20 11:44:55 INFO     	 * (global step 9000: loss: 0.32486873865127563, lr: 0.0001
2023-12-20 11:45:11 INFO     	 * (global step 9050: loss: 0.27955763041973114, lr: 0.0001
2023-12-20 11:45:26 INFO     	 * (global step 9100: loss: 0.2993909865617752, lr: 0.0001
2023-12-20 11:45:42 INFO     	 * (global step 9150: loss: 0.2571081444621086, lr: 0.0001
2023-12-20 11:45:57 INFO     	 * (global step 9200: loss: 0.2419983223080635, lr: 0.0001
2023-12-20 11:46:13 INFO     	 * (global step 9250: loss: 0.20178828574717045, lr: 0.0001
2023-12-20 11:46:28 INFO     	 * (global step 9300: loss: 0.24090223386883736, lr: 0.0001
2023-12-20 11:46:44 INFO     	 * (global step 9350: loss: 0.30274274200201035, lr: 0.0001
2023-12-20 11:46:59 INFO     	 * (global step 9400: loss: 0.33377840369939804, lr: 0.0001
2023-12-20 11:47:13 INFO     [epoch 3/15] average loss: 0.301, lr: 0.0001
2023-12-20 11:47:13 INFO     saving model related files
2023-12-20 11:47:13 INFO     saving model
2023-12-20 11:47:14 INFO     saving tokenizer
2023-12-20 11:47:14 INFO     saving optimizer
2023-12-20 11:47:15 INFO     remove old optimizer files
2023-12-20 11:47:16 INFO     	 * (global step 9450: loss: 0.2306361123919487, lr: 0.0001
2023-12-20 11:47:32 INFO     	 * (global step 9500: loss: 0.368644155561924, lr: 0.0001
2023-12-20 11:47:47 INFO     	 * (global step 9550: loss: 0.29723208025097847, lr: 0.0001
2023-12-20 11:48:03 INFO     	 * (global step 9600: loss: 0.3680744469165802, lr: 0.0001
2023-12-20 11:48:19 INFO     	 * (global step 9650: loss: 0.29484302550554276, lr: 0.0001
2023-12-20 11:48:34 INFO     	 * (global step 9700: loss: 0.27184395119547844, lr: 0.0001
2023-12-20 11:48:50 INFO     	 * (global step 9750: loss: 0.2659461833536625, lr: 0.0001
2023-12-20 11:49:05 INFO     	 * (global step 9800: loss: 0.2607880160212517, lr: 0.0001
2023-12-20 11:49:21 INFO     	 * (global step 9850: loss: 0.24633964896202087, lr: 0.0001
2023-12-20 11:49:36 INFO     	 * (global step 9900: loss: 0.3415997177362442, lr: 0.0001
2023-12-20 11:49:52 INFO     	 * (global step 9950: loss: 0.3708193376660347, lr: 0.0001
2023-12-20 11:50:07 INFO     	 * (global step 10000: loss: 0.2465841993689537, lr: 0.0001
2023-12-20 11:50:23 INFO     	 * (global step 10050: loss: 0.41812875866889954, lr: 0.0001
2023-12-20 11:50:38 INFO     	 * (global step 10100: loss: 0.21324950829148293, lr: 0.0001
2023-12-20 11:50:54 INFO     	 * (global step 10150: loss: 0.40309975296258926, lr: 0.0001
2023-12-20 11:51:09 INFO     	 * (global step 10200: loss: 0.3538661040365696, lr: 0.0001
2023-12-20 11:51:25 INFO     	 * (global step 10250: loss: 0.19044896587729454, lr: 0.0001
2023-12-20 11:51:40 INFO     	 * (global step 10300: loss: 0.24244306981563568, lr: 0.0001
2023-12-20 11:51:56 INFO     	 * (global step 10350: loss: 0.34641698375344276, lr: 0.0001
2023-12-20 11:52:11 INFO     	 * (global step 10400: loss: 0.21581478044390678, lr: 0.0001
2023-12-20 11:52:27 INFO     	 * (global step 10450: loss: 0.37193620949983597, lr: 0.0001
2023-12-20 11:52:43 INFO     	 * (global step 10500: loss: 0.32558927685022354, lr: 0.0001
2023-12-20 11:52:58 INFO     	 * (global step 10550: loss: 0.2667055279016495, lr: 0.0001
2023-12-20 11:53:14 INFO     	 * (global step 10600: loss: 0.316746324300766, lr: 0.0001
2023-12-20 11:53:29 INFO     	 * (global step 10650: loss: 0.30861596390604973, lr: 0.0001
2023-12-20 11:53:45 INFO     	 * (global step 10700: loss: 0.42267896980047226, lr: 0.0001
2023-12-20 11:54:00 INFO     	 * (global step 10750: loss: 0.4446251168847084, lr: 0.0001
2023-12-20 11:54:16 INFO     	 * (global step 10800: loss: 0.26104601100087166, lr: 0.0001
2023-12-20 11:54:31 INFO     	 * (global step 10850: loss: 0.27269813790917397, lr: 0.0001
2023-12-20 11:54:47 INFO     	 * (global step 10900: loss: 0.2943039536476135, lr: 0.0001
2023-12-20 11:55:02 INFO     	 * (global step 10950: loss: 0.17876771837472916, lr: 0.0001
2023-12-20 11:55:18 INFO     	 * (global step 11000: loss: 0.28435633331537247, lr: 0.0001
2023-12-20 11:55:33 INFO     	 * (global step 11050: loss: 0.316412840038538, lr: 0.0001
2023-12-20 11:55:49 INFO     	 * (global step 11100: loss: 0.23709474876523018, lr: 0.0001
2023-12-20 11:56:04 INFO     	 * (global step 11150: loss: 0.251188050955534, lr: 0.0001
2023-12-20 11:56:20 INFO     	 * (global step 11200: loss: 0.3402763418853283, lr: 0.0001
2023-12-20 11:56:35 INFO     	 * (global step 11250: loss: 0.35046662762761116, lr: 0.0001
2023-12-20 11:56:51 INFO     	 * (global step 11300: loss: 0.2774139642715454, lr: 0.0001
2023-12-20 11:57:06 INFO     	 * (global step 11350: loss: 0.28885577619075775, lr: 0.0001
2023-12-20 11:57:22 INFO     	 * (global step 11400: loss: 0.24490999802947044, lr: 0.0001
2023-12-20 11:57:37 INFO     	 * (global step 11450: loss: 0.2750055268406868, lr: 0.0001
2023-12-20 11:57:53 INFO     	 * (global step 11500: loss: 0.21591392904520035, lr: 0.0001
2023-12-20 11:58:08 INFO     	 * (global step 11550: loss: 0.29298005998134613, lr: 0.0001
2023-12-20 11:58:24 INFO     	 * (global step 11600: loss: 0.32462434098124504, lr: 0.0001
2023-12-20 11:58:39 INFO     	 * (global step 11650: loss: 0.3111067973077297, lr: 0.0001
2023-12-20 11:58:55 INFO     	 * (global step 11700: loss: 0.32593970745801926, lr: 0.0001
2023-12-20 11:59:11 INFO     	 * (global step 11750: loss: 0.27469387650489807, lr: 0.0001
2023-12-20 11:59:26 INFO     	 * (global step 11800: loss: 0.35244496911764145, lr: 0.0001
2023-12-20 11:59:28 INFO     [epoch 4/15] average loss: 0.291, lr: 0.0001
2023-12-20 11:59:28 INFO     saving model related files
2023-12-20 11:59:28 INFO     saving model
2023-12-20 11:59:28 INFO     saving tokenizer
2023-12-20 11:59:28 INFO     saving optimizer
2023-12-20 11:59:29 INFO     remove old optimizer files
2023-12-20 11:59:43 INFO     	 * (global step 11850: loss: 0.397550992667675, lr: 0.0001
2023-12-20 11:59:59 INFO     	 * (global step 11900: loss: 0.3010600805282593, lr: 0.0001
2023-12-20 12:00:14 INFO     	 * (global step 11950: loss: 0.2657768651843071, lr: 0.0001
2023-12-20 12:00:30 INFO     	 * (global step 12000: loss: 0.2984388880431652, lr: 0.0001
2023-12-20 12:00:45 INFO     	 * (global step 12050: loss: 0.2874509394168854, lr: 0.0001
2023-12-20 12:01:01 INFO     	 * (global step 12100: loss: 0.3442595899105072, lr: 0.0001
2023-12-20 12:01:16 INFO     	 * (global step 12150: loss: 0.34044282883405685, lr: 0.0001
2023-12-20 12:01:32 INFO     	 * (global step 12200: loss: 0.34013159573078156, lr: 0.0001
2023-12-20 12:01:47 INFO     	 * (global step 12250: loss: 0.22736457362771034, lr: 0.0001
2023-12-20 12:02:03 INFO     	 * (global step 12300: loss: 0.299767404794693, lr: 0.0001
2023-12-20 12:02:18 INFO     	 * (global step 12350: loss: 0.23718631640076637, lr: 0.0001
2023-12-20 12:02:34 INFO     	 * (global step 12400: loss: 0.3067733906209469, lr: 0.0001
2023-12-20 12:02:49 INFO     	 * (global step 12450: loss: 0.2812284305691719, lr: 0.0001
2023-12-20 12:03:05 INFO     	 * (global step 12500: loss: 0.22798864357173443, lr: 0.0001
2023-12-20 12:03:21 INFO     	 * (global step 12550: loss: 0.3094985857605934, lr: 0.0001
2023-12-20 12:03:36 INFO     	 * (global step 12600: loss: 0.3309333994984627, lr: 0.0001
2023-12-20 12:03:52 INFO     	 * (global step 12650: loss: 0.2833055928349495, lr: 0.0001
2023-12-20 12:04:07 INFO     	 * (global step 12700: loss: 0.29347480461001396, lr: 0.0001
2023-12-20 12:04:23 INFO     	 * (global step 12750: loss: 0.27481988817453384, lr: 0.0001
2023-12-20 12:04:38 INFO     	 * (global step 12800: loss: 0.3099144771695137, lr: 0.0001
2023-12-20 12:04:54 INFO     	 * (global step 12850: loss: 0.2031956948339939, lr: 0.0001
2023-12-20 12:05:09 INFO     	 * (global step 12900: loss: 0.28555772453546524, lr: 0.0001
2023-12-20 12:05:25 INFO     	 * (global step 12950: loss: 0.22975089401006699, lr: 0.0001
2023-12-20 12:05:40 INFO     	 * (global step 13000: loss: 0.3479439839720726, lr: 0.0001
2023-12-20 12:05:56 INFO     	 * (global step 13050: loss: 0.2158633917570114, lr: 0.0001
2023-12-20 12:06:11 INFO     	 * (global step 13100: loss: 0.2748282589018345, lr: 0.0001
2023-12-20 12:06:27 INFO     	 * (global step 13150: loss: 0.26097943633794785, lr: 0.0001
2023-12-20 12:06:42 INFO     	 * (global step 13200: loss: 0.2268790863454342, lr: 0.0001
2023-12-20 12:06:58 INFO     	 * (global step 13250: loss: 0.2758730798959732, lr: 0.0001
2023-12-20 12:07:13 INFO     	 * (global step 13300: loss: 0.34892909601330757, lr: 0.0001
2023-12-20 12:07:29 INFO     	 * (global step 13350: loss: 0.28610897436738014, lr: 0.0001
2023-12-20 12:07:44 INFO     	 * (global step 13400: loss: 0.29486948251724243, lr: 0.0001
2023-12-20 12:08:00 INFO     	 * (global step 13450: loss: 0.26001979783177376, lr: 0.0001
2023-12-20 12:08:15 INFO     	 * (global step 13500: loss: 0.22170386090874672, lr: 0.0001
2023-12-20 12:08:31 INFO     	 * (global step 13550: loss: 0.3186596632003784, lr: 0.0001
2023-12-20 12:08:46 INFO     	 * (global step 13600: loss: 0.3214953690767288, lr: 0.0001
2023-12-20 12:09:02 INFO     	 * (global step 13650: loss: 0.2872379384934902, lr: 0.0001
2023-12-20 12:09:17 INFO     	 * (global step 13700: loss: 0.3301205337047577, lr: 0.0001
2023-12-20 12:09:33 INFO     	 * (global step 13750: loss: 0.19089411944150925, lr: 0.0001
2023-12-20 12:09:48 INFO     	 * (global step 13800: loss: 0.29973988980054855, lr: 0.0001
2023-12-20 12:10:04 INFO     	 * (global step 13850: loss: 0.3123796693980694, lr: 0.0001
2023-12-20 12:10:19 INFO     	 * (global step 13900: loss: 0.3156430013477802, lr: 0.0001
2023-12-20 12:10:35 INFO     	 * (global step 13950: loss: 0.36395326629281044, lr: 0.0001
2023-12-20 12:10:50 INFO     	 * (global step 14000: loss: 0.2860645242035389, lr: 0.0001
2023-12-20 12:11:06 INFO     	 * (global step 14050: loss: 0.26120808720588684, lr: 0.0001
2023-12-20 12:11:21 INFO     	 * (global step 14100: loss: 0.35797078162431717, lr: 0.0001
2023-12-20 12:11:37 INFO     	 * (global step 14150: loss: 0.30805156752467155, lr: 0.0001
2023-12-20 12:11:42 INFO     [epoch 5/15] average loss: 0.284, lr: 0.0001
2023-12-20 12:11:42 INFO     saving model related files
2023-12-20 12:11:42 INFO     saving model
2023-12-20 12:11:43 INFO     saving tokenizer
2023-12-20 12:11:43 INFO     saving optimizer
2023-12-20 12:11:43 INFO     remove old optimizer files
2023-12-20 12:11:54 INFO     	 * (global step 14200: loss: 0.3243229500949383, lr: 0.0001
2023-12-20 12:12:10 INFO     	 * (global step 14250: loss: 0.32747844606637955, lr: 0.0001
2023-12-20 12:12:25 INFO     	 * (global step 14300: loss: 0.28909366950392723, lr: 0.0001
2023-12-20 12:12:41 INFO     	 * (global step 14350: loss: 0.2658029831945896, lr: 0.0001
2023-12-20 12:12:56 INFO     	 * (global step 14400: loss: 0.3384033255279064, lr: 0.0001
2023-12-20 12:13:12 INFO     	 * (global step 14450: loss: 0.25982842594385147, lr: 0.0001
2023-12-20 12:13:27 INFO     	 * (global step 14500: loss: 0.35240892693400383, lr: 0.0001
2023-12-20 12:13:43 INFO     	 * (global step 14550: loss: 0.3347499594092369, lr: 0.0001
2023-12-20 12:13:58 INFO     	 * (global step 14600: loss: 0.26602066680788994, lr: 0.0001
2023-12-20 12:14:14 INFO     	 * (global step 14650: loss: 0.2840142846107483, lr: 0.0001
2023-12-20 12:14:29 INFO     	 * (global step 14700: loss: 0.23039864003658295, lr: 0.0001
2023-12-20 12:14:45 INFO     	 * (global step 14750: loss: 0.3099052980542183, lr: 0.0001
2023-12-20 12:15:00 INFO     	 * (global step 14800: loss: 0.3101281560957432, lr: 0.0001
2023-12-20 12:15:16 INFO     	 * (global step 14850: loss: 0.25714149698615074, lr: 0.0001
2023-12-20 12:15:31 INFO     	 * (global step 14900: loss: 0.23760640248656273, lr: 0.0001
2023-12-20 12:15:47 INFO     	 * (global step 14950: loss: 0.35595183819532394, lr: 0.0001
2023-12-20 12:16:02 INFO     	 * (global step 15000: loss: 0.3122466281056404, lr: 0.0001
2023-12-20 12:16:18 INFO     	 * (global step 15050: loss: 0.28230585902929306, lr: 0.0001
2023-12-20 12:16:33 INFO     	 * (global step 15100: loss: 0.2869769148528576, lr: 0.0001
2023-12-20 12:16:49 INFO     	 * (global step 15150: loss: 0.2247777208685875, lr: 0.0001
2023-12-20 12:17:04 INFO     	 * (global step 15200: loss: 0.3381291702389717, lr: 0.0001
2023-12-20 12:17:20 INFO     	 * (global step 15250: loss: 0.1844586282968521, lr: 0.0001
2023-12-20 12:17:35 INFO     	 * (global step 15300: loss: 0.20849664136767387, lr: 0.0001
2023-12-20 12:17:51 INFO     	 * (global step 15350: loss: 0.1982424221932888, lr: 0.0001
2023-12-20 12:18:06 INFO     	 * (global step 15400: loss: 0.2828752398490906, lr: 0.0001
2023-12-20 12:18:22 INFO     	 * (global step 15450: loss: 0.2748861685395241, lr: 0.0001
2023-12-20 12:18:37 INFO     	 * (global step 15500: loss: 0.29030678421258926, lr: 0.0001
2023-12-20 12:18:53 INFO     	 * (global step 15550: loss: 0.23059413209557533, lr: 0.0001
2023-12-20 12:19:08 INFO     	 * (global step 15600: loss: 0.2544834278523922, lr: 0.0001
2023-12-20 12:19:24 INFO     	 * (global step 15650: loss: 0.2850787565112114, lr: 0.0001
2023-12-20 12:19:39 INFO     	 * (global step 15700: loss: 0.25854117423295975, lr: 0.0001
2023-12-20 12:19:55 INFO     	 * (global step 15750: loss: 0.286511093378067, lr: 0.0001
2023-12-20 12:20:10 INFO     	 * (global step 15800: loss: 0.2580495122820139, lr: 0.0001
2023-12-20 12:20:26 INFO     	 * (global step 15850: loss: 0.3291594311594963, lr: 0.0001
2023-12-20 12:20:42 INFO     	 * (global step 15900: loss: 0.2921023294329643, lr: 0.0001
2023-12-20 12:20:57 INFO     	 * (global step 15950: loss: 0.23735458962619305, lr: 0.0001
2023-12-20 12:21:13 INFO     	 * (global step 16000: loss: 0.20947272703051567, lr: 0.0001
2023-12-20 12:21:28 INFO     	 * (global step 16050: loss: 0.22744100913405418, lr: 0.0001
2023-12-20 12:21:44 INFO     	 * (global step 16100: loss: 0.28969164192676544, lr: 0.0001
2023-12-20 12:21:59 INFO     	 * (global step 16150: loss: 0.2648211158812046, lr: 0.0001
2023-12-20 12:22:15 INFO     	 * (global step 16200: loss: 0.22785421833395958, lr: 0.0001
2023-12-20 12:22:30 INFO     	 * (global step 16250: loss: 0.25519610568881035, lr: 0.0001
2023-12-20 12:22:46 INFO     	 * (global step 16300: loss: 0.28295033425092697, lr: 0.0001
2023-12-20 12:23:01 INFO     	 * (global step 16350: loss: 0.29985906556248665, lr: 0.0001
2023-12-20 12:23:17 INFO     	 * (global step 16400: loss: 0.21443424373865128, lr: 0.0001
2023-12-20 12:23:32 INFO     	 * (global step 16450: loss: 0.3260040730237961, lr: 0.0001
2023-12-20 12:23:48 INFO     	 * (global step 16500: loss: 0.2191232368350029, lr: 0.0001
2023-12-20 12:23:56 INFO     [epoch 6/15] average loss: 0.277, lr: 0.0001
2023-12-20 12:23:56 INFO     saving model related files
2023-12-20 12:23:56 INFO     saving model
2023-12-20 12:23:57 INFO     saving tokenizer
2023-12-20 12:23:57 INFO     saving optimizer
2023-12-20 12:23:58 INFO     remove old optimizer files
2023-12-20 12:24:05 INFO     	 * (global step 16550: loss: 0.311431348323822, lr: 0.0001
2023-12-20 12:24:20 INFO     	 * (global step 16600: loss: 0.20643962919712067, lr: 0.0001
2023-12-20 12:24:36 INFO     	 * (global step 16650: loss: 0.24480168521404266, lr: 0.0001
2023-12-20 12:24:51 INFO     	 * (global step 16700: loss: 0.28725095093250275, lr: 0.0001
2023-12-20 12:25:07 INFO     	 * (global step 16750: loss: 0.30810751765966415, lr: 0.0001
2023-12-20 12:25:22 INFO     	 * (global step 16800: loss: 0.30752822756767273, lr: 0.0001
2023-12-20 12:25:38 INFO     	 * (global step 16850: loss: 0.2516601160168648, lr: 0.0001
2023-12-20 12:25:53 INFO     	 * (global step 16900: loss: 0.23863399773836136, lr: 0.0001
2023-12-20 12:26:09 INFO     	 * (global step 16950: loss: 0.3437451086938381, lr: 0.0001
2023-12-20 12:26:24 INFO     	 * (global step 17000: loss: 0.2373247742652893, lr: 0.0001
2023-12-20 12:26:40 INFO     	 * (global step 17050: loss: 0.34652645140886307, lr: 0.0001
2023-12-20 12:26:55 INFO     	 * (global step 17100: loss: 0.3965378925204277, lr: 0.0001
2023-12-20 12:27:11 INFO     	 * (global step 17150: loss: 0.25654174387454987, lr: 0.0001
2023-12-20 12:27:26 INFO     	 * (global step 17200: loss: 0.2117989882826805, lr: 0.0001
2023-12-20 12:27:42 INFO     	 * (global step 17250: loss: 0.3182353600859642, lr: 0.0001
2023-12-20 12:27:57 INFO     	 * (global step 17300: loss: 0.2903923951089382, lr: 0.0001
2023-12-20 12:28:13 INFO     	 * (global step 17350: loss: 0.40127794072031975, lr: 0.0001
2023-12-20 12:28:28 INFO     	 * (global step 17400: loss: 0.2495391145348549, lr: 0.0001
2023-12-20 12:28:44 INFO     	 * (global step 17450: loss: 0.21892603486776352, lr: 0.0001
2023-12-20 12:28:59 INFO     	 * (global step 17500: loss: 0.25060872733592987, lr: 0.0001
2023-12-20 12:29:15 INFO     	 * (global step 17550: loss: 0.255885012447834, lr: 0.0001
2023-12-20 12:29:30 INFO     	 * (global step 17600: loss: 0.2805434390902519, lr: 0.0001
2023-12-20 12:29:45 INFO     	 * (global step 17650: loss: 0.274538267403841, lr: 0.0001
2023-12-20 12:30:01 INFO     	 * (global step 17700: loss: 0.32267458736896515, lr: 0.0001
2023-12-20 12:30:16 INFO     	 * (global step 17750: loss: 0.2127838134765625, lr: 0.0001
2023-12-20 12:30:32 INFO     	 * (global step 17800: loss: 0.2246692106127739, lr: 0.0001
2023-12-20 12:30:47 INFO     	 * (global step 17850: loss: 0.2431018464267254, lr: 0.0001
2023-12-20 12:31:03 INFO     	 * (global step 17900: loss: 0.2245984897017479, lr: 0.0001
2023-12-20 12:31:18 INFO     	 * (global step 17950: loss: 0.27818556129932404, lr: 0.0001
2023-12-20 12:31:34 INFO     	 * (global step 18000: loss: 0.2719857208430767, lr: 0.0001
2023-12-20 12:31:49 INFO     	 * (global step 18050: loss: 0.26991256326436996, lr: 0.0001
2023-12-20 12:32:05 INFO     	 * (global step 18100: loss: 0.2380734160542488, lr: 0.0001
2023-12-20 12:32:20 INFO     	 * (global step 18150: loss: 0.2809135131537914, lr: 0.0001
2023-12-20 12:32:36 INFO     	 * (global step 18200: loss: 0.2695728428661823, lr: 0.0001
2023-12-20 12:32:51 INFO     	 * (global step 18250: loss: 0.166603721678257, lr: 0.0001
2023-12-20 12:33:07 INFO     	 * (global step 18300: loss: 0.3439089059829712, lr: 0.0001
2023-12-20 12:33:22 INFO     	 * (global step 18350: loss: 0.40637853741645813, lr: 0.0001
2023-12-20 12:33:38 INFO     	 * (global step 18400: loss: 0.24314631894230843, lr: 0.0001
2023-12-20 12:33:53 INFO     	 * (global step 18450: loss: 0.18052418157458305, lr: 0.0001
2023-12-20 12:34:09 INFO     	 * (global step 18500: loss: 0.34149548783898354, lr: 0.0001
2023-12-20 12:34:24 INFO     	 * (global step 18550: loss: 0.18449593521654606, lr: 0.0001
2023-12-20 12:34:40 INFO     	 * (global step 18600: loss: 0.24255840107798576, lr: 0.0001
2023-12-20 12:34:55 INFO     	 * (global step 18650: loss: 0.22848133742809296, lr: 0.0001
2023-12-20 12:35:10 INFO     	 * (global step 18700: loss: 0.3663252852857113, lr: 0.0001
2023-12-20 12:35:26 INFO     	 * (global step 18750: loss: 0.40421993657946587, lr: 0.0001
2023-12-20 12:35:41 INFO     	 * (global step 18800: loss: 0.25788069888949394, lr: 0.0001
2023-12-20 12:35:57 INFO     	 * (global step 18850: loss: 0.18800209276378155, lr: 0.0001
2023-12-20 12:36:09 INFO     [epoch 7/15] average loss: 0.271, lr: 0.0001
2023-12-20 12:36:09 INFO     saving model related files
2023-12-20 12:36:09 INFO     saving model
2023-12-20 12:36:09 INFO     saving tokenizer
2023-12-20 12:36:09 INFO     saving optimizer
2023-12-20 12:36:10 INFO     remove old optimizer files
2023-12-20 12:36:14 INFO     	 * (global step 18900: loss: 0.2897914834320545, lr: 0.0001
2023-12-20 12:36:30 INFO     	 * (global step 18950: loss: 0.2933793477714062, lr: 0.0001
2023-12-20 12:36:45 INFO     	 * (global step 19000: loss: 0.23124147206544876, lr: 0.0001
2023-12-20 12:37:00 INFO     	 * (global step 19050: loss: 0.26128723099827766, lr: 0.0001
2023-12-20 12:37:16 INFO     	 * (global step 19100: loss: 0.24848582968115807, lr: 0.0001
2023-12-20 12:37:31 INFO     	 * (global step 19150: loss: 0.25422216206789017, lr: 0.0001
2023-12-20 12:37:47 INFO     	 * (global step 19200: loss: 0.29268041998147964, lr: 0.0001
2023-12-20 12:38:02 INFO     	 * (global step 19250: loss: 0.2420947626233101, lr: 0.0001
2023-12-20 12:38:18 INFO     	 * (global step 19300: loss: 0.22748566791415215, lr: 0.0001
2023-12-20 12:38:33 INFO     	 * (global step 19350: loss: 0.23568953573703766, lr: 0.0001
2023-12-20 12:38:49 INFO     	 * (global step 19400: loss: 0.26348330453038216, lr: 0.0001
2023-12-20 12:39:04 INFO     	 * (global step 19450: loss: 0.2548668123781681, lr: 0.0001
2023-12-20 12:39:20 INFO     	 * (global step 19500: loss: 0.3850899301469326, lr: 0.0001
2023-12-20 12:39:35 INFO     	 * (global step 19550: loss: 0.21553673222661018, lr: 0.0001
2023-12-20 12:39:51 INFO     	 * (global step 19600: loss: 0.23256201297044754, lr: 0.0001
2023-12-20 12:40:06 INFO     	 * (global step 19650: loss: 0.23970747739076614, lr: 0.0001
2023-12-20 12:40:22 INFO     	 * (global step 19700: loss: 0.33320897072553635, lr: 0.0001
2023-12-20 12:40:37 INFO     	 * (global step 19750: loss: 0.20020775869488716, lr: 0.0001
2023-12-20 12:40:53 INFO     	 * (global step 19800: loss: 0.24242104589939117, lr: 0.0001
2023-12-20 12:41:08 INFO     	 * (global step 19850: loss: 0.2589806579053402, lr: 0.0001
2023-12-20 12:41:24 INFO     	 * (global step 19900: loss: 0.24368271231651306, lr: 0.0001
2023-12-20 12:41:39 INFO     	 * (global step 19950: loss: 0.2672891542315483, lr: 0.0001
2023-12-20 12:41:55 INFO     	 * (global step 20000: loss: 0.23918800801038742, lr: 0.0001
2023-12-20 12:42:10 INFO     	 * (global step 20050: loss: 0.22363650240004063, lr: 0.0001
2023-12-20 12:42:26 INFO     	 * (global step 20100: loss: 0.27785252034664154, lr: 0.0001
2023-12-20 12:42:41 INFO     	 * (global step 20150: loss: 0.2577386423945427, lr: 0.0001
2023-12-20 12:42:57 INFO     	 * (global step 20200: loss: 0.4617351330816746, lr: 0.0001
2023-12-20 12:43:12 INFO     	 * (global step 20250: loss: 0.2891774885356426, lr: 0.0001
2023-12-20 12:43:28 INFO     	 * (global step 20300: loss: 0.26684610173106194, lr: 0.0001
2023-12-20 12:43:43 INFO     	 * (global step 20350: loss: 0.2749948650598526, lr: 0.0001
2023-12-20 12:43:59 INFO     	 * (global step 20400: loss: 0.2181677371263504, lr: 0.0001
2023-12-20 12:44:14 INFO     	 * (global step 20450: loss: 0.3336305133998394, lr: 0.0001
2023-12-20 12:44:30 INFO     	 * (global step 20500: loss: 0.19211263582110405, lr: 0.0001
2023-12-20 12:44:45 INFO     	 * (global step 20550: loss: 0.27820904552936554, lr: 0.0001
2023-12-20 12:45:01 INFO     	 * (global step 20600: loss: 0.28792131692171097, lr: 0.0001
2023-12-20 12:45:16 INFO     	 * (global step 20650: loss: 0.23849576711654663, lr: 0.0001
2023-12-20 12:45:32 INFO     	 * (global step 20700: loss: 0.2723420225083828, lr: 0.0001
2023-12-20 12:45:47 INFO     	 * (global step 20750: loss: 0.2679600939154625, lr: 0.0001
2023-12-20 12:46:02 INFO     	 * (global step 20800: loss: 0.2943212352693081, lr: 0.0001
2023-12-20 12:46:18 INFO     	 * (global step 20850: loss: 0.28812069073319435, lr: 0.0001
2023-12-20 12:46:33 INFO     	 * (global step 20900: loss: 0.3080320656299591, lr: 0.0001
2023-12-20 12:46:49 INFO     	 * (global step 20950: loss: 0.28351806104183197, lr: 0.0001
2023-12-20 12:47:04 INFO     	 * (global step 21000: loss: 0.27354574389755726, lr: 0.0001
2023-12-20 12:47:20 INFO     	 * (global step 21050: loss: 0.2059126552194357, lr: 0.0001
2023-12-20 12:47:35 INFO     	 * (global step 21100: loss: 0.21221406757831573, lr: 0.0001
2023-12-20 12:47:51 INFO     	 * (global step 21150: loss: 0.23915284126996994, lr: 0.0001
2023-12-20 12:48:06 INFO     	 * (global step 21200: loss: 0.29714278504252434, lr: 0.0001
2023-12-20 12:48:22 INFO     [epoch 8/15] average loss: 0.265, lr: 0.0001
2023-12-20 12:48:22 INFO     saving model related files
2023-12-20 12:48:22 INFO     saving model
2023-12-20 12:48:22 INFO     saving tokenizer
2023-12-20 12:48:22 INFO     saving optimizer
2023-12-20 12:48:23 INFO     remove old optimizer files
2023-12-20 12:48:24 INFO     	 * (global step 21250: loss: 0.30239661782979965, lr: 0.0001
2023-12-20 12:48:39 INFO     	 * (global step 21300: loss: 0.2207176312804222, lr: 0.0001
2023-12-20 12:48:55 INFO     	 * (global step 21350: loss: 0.25877102464437485, lr: 0.0001
2023-12-20 12:49:10 INFO     	 * (global step 21400: loss: 0.30564454570412636, lr: 0.0001
2023-12-20 12:49:25 INFO     	 * (global step 21450: loss: 0.3158227354288101, lr: 0.0001
2023-12-20 12:49:41 INFO     	 * (global step 21500: loss: 0.24125304445624352, lr: 0.0001
2023-12-20 12:49:56 INFO     	 * (global step 21550: loss: 0.28680386394262314, lr: 0.0001
2023-12-20 12:50:12 INFO     	 * (global step 21600: loss: 0.22200078889727592, lr: 0.0001
2023-12-20 12:50:27 INFO     	 * (global step 21650: loss: 0.2736923284828663, lr: 0.0001
2023-12-20 12:50:43 INFO     	 * (global step 21700: loss: 0.26165393367409706, lr: 0.0001
2023-12-20 12:50:58 INFO     	 * (global step 21750: loss: 0.21833612769842148, lr: 0.0001
2023-12-20 12:51:14 INFO     	 * (global step 21800: loss: 0.37150420621037483, lr: 0.0001
2023-12-20 12:51:29 INFO     	 * (global step 21850: loss: 0.2699969634413719, lr: 0.0001
2023-12-20 12:51:45 INFO     	 * (global step 21900: loss: 0.3560263440012932, lr: 0.0001
2023-12-20 12:52:00 INFO     	 * (global step 21950: loss: 0.3618002310395241, lr: 0.0001
2023-12-20 12:52:16 INFO     	 * (global step 22000: loss: 0.31443318352103233, lr: 0.0001
2023-12-20 12:52:31 INFO     	 * (global step 22050: loss: 0.33166268467903137, lr: 0.0001
2023-12-20 12:52:47 INFO     	 * (global step 22100: loss: 0.23336530476808548, lr: 0.0001
2023-12-20 12:53:02 INFO     	 * (global step 22150: loss: 0.24523918703198433, lr: 0.0001
2023-12-20 12:53:18 INFO     	 * (global step 22200: loss: 0.1797492355108261, lr: 0.0001
2023-12-20 12:53:33 INFO     	 * (global step 22250: loss: 0.2749163843691349, lr: 0.0001
2023-12-20 12:53:48 INFO     	 * (global step 22300: loss: 0.2533942684531212, lr: 0.0001
2023-12-20 12:54:04 INFO     	 * (global step 22350: loss: 0.3049536943435669, lr: 0.0001
2023-12-20 12:54:19 INFO     	 * (global step 22400: loss: 0.28700394555926323, lr: 0.0001
2023-12-20 12:54:35 INFO     	 * (global step 22450: loss: 0.2347981482744217, lr: 0.0001
2023-12-20 12:54:50 INFO     	 * (global step 22500: loss: 0.20324342884123325, lr: 0.0001
2023-12-20 12:55:06 INFO     	 * (global step 22550: loss: 0.2966163530945778, lr: 0.0001
2023-12-20 12:55:21 INFO     	 * (global step 22600: loss: 0.3140881545841694, lr: 0.0001
2023-12-20 12:55:37 INFO     	 * (global step 22650: loss: 0.261786051094532, lr: 0.0001
2023-12-20 12:55:52 INFO     	 * (global step 22700: loss: 0.23507140949368477, lr: 0.0001
2023-12-20 12:56:08 INFO     	 * (global step 22750: loss: 0.2944134194403887, lr: 0.0001
2023-12-20 12:56:23 INFO     	 * (global step 22800: loss: 0.22796881943941116, lr: 0.0001
2023-12-20 12:56:39 INFO     	 * (global step 22850: loss: 0.27746254950761795, lr: 0.0001
2023-12-20 12:56:54 INFO     	 * (global step 22900: loss: 0.23419520258903503, lr: 0.0001
2023-12-20 12:57:10 INFO     	 * (global step 22950: loss: 0.2990921884775162, lr: 0.0001
2023-12-20 12:57:25 INFO     	 * (global step 23000: loss: 0.24701180681586266, lr: 0.0001
2023-12-20 12:57:41 INFO     	 * (global step 23050: loss: 0.22759873792529106, lr: 0.0001
2023-12-20 12:57:56 INFO     	 * (global step 23100: loss: 0.310496736317873, lr: 0.0001
2023-12-20 12:58:12 INFO     	 * (global step 23150: loss: 0.3067304491996765, lr: 0.0001
2023-12-20 12:58:27 INFO     	 * (global step 23200: loss: 0.27253561839461327, lr: 0.0001
2023-12-20 12:58:43 INFO     	 * (global step 23250: loss: 0.243511114269495, lr: 0.0001
2023-12-20 12:58:58 INFO     	 * (global step 23300: loss: 0.21969323605298996, lr: 0.0001
2023-12-20 12:59:14 INFO     	 * (global step 23350: loss: 0.2802576385438442, lr: 0.0001
2023-12-20 12:59:29 INFO     	 * (global step 23400: loss: 0.26982683688402176, lr: 0.0001
2023-12-20 12:59:45 INFO     	 * (global step 23450: loss: 0.24352477118372917, lr: 0.0001
2023-12-20 13:00:00 INFO     	 * (global step 23500: loss: 0.24836770445108414, lr: 0.0001
2023-12-20 13:00:16 INFO     	 * (global step 23550: loss: 0.32831593602895737, lr: 0.0001
2023-12-20 13:00:31 INFO     	 * (global step 23600: loss: 0.21863803640007973, lr: 0.0001
2023-12-20 13:00:35 INFO     [epoch 9/15] average loss: 0.261, lr: 0.0001
2023-12-20 13:00:35 INFO     saving model related files
2023-12-20 13:00:35 INFO     saving model
2023-12-20 13:00:35 INFO     saving tokenizer
2023-12-20 13:00:35 INFO     saving optimizer
2023-12-20 13:00:36 INFO     remove old optimizer files
2023-12-20 13:00:36 INFO     complete training: model ckpt was saved at small_combined_trained_ckpt/model_lwtqag
2023-12-20 13:00:36 INFO     ## 1st RUN: Configuration 1/12 ##
2023-12-20 13:00:36 INFO     initialize model trainer
2023-12-20 13:00:36 INFO     initialize checkpoint at small_combined_trained_ckpt/model_eszyci
2023-12-20 13:00:37 INFO     hyperparameters
2023-12-20 13:00:37 INFO     	 * dataset_path: StellarMilk/squad_newsqa
2023-12-20 13:00:37 INFO     	 * dataset_name: default
2023-12-20 13:00:37 INFO     	 * input_types: ['paragraph']
2023-12-20 13:00:37 INFO     	 * output_types: ['questions_answers']
2023-12-20 13:00:37 INFO     	 * prefix_types: ['qag']
2023-12-20 13:00:37 INFO     	 * model: t5-small
2023-12-20 13:00:37 INFO     	 * max_length: 512
2023-12-20 13:00:37 INFO     	 * max_length_output: 512
2023-12-20 13:00:37 INFO     	 * epoch: 15
2023-12-20 13:00:37 INFO     	 * batch: 2
2023-12-20 13:00:37 INFO     	 * lr: 0.0001
2023-12-20 13:00:37 INFO     	 * fp16: False
2023-12-20 13:00:37 INFO     	 * random_seed: 1
2023-12-20 13:00:37 INFO     	 * gradient_accumulation_steps: 2
2023-12-20 13:00:37 INFO     	 * label_smoothing: 0.15
2023-12-20 13:00:37 INFO     initialize checkpoint with t5-small
2023-12-20 13:00:38 INFO     use spaCy answer extraction model: positionrank
2023-12-20 13:00:38 INFO     Model `t5-small`
2023-12-20 13:00:38 INFO     	 * Num of GPU in use: 1
2023-12-20 13:00:38 INFO     	 * Prefix: True
2023-12-20 13:00:38 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 13:00:38 INFO     dataset preprocessing
2023-12-20 13:00:39 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/squad_newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-20 13:00:42 INFO     start model training
2023-12-20 13:00:50 INFO     	 * (global step 50: loss: 0.8271616697311401, lr: 0.0001
2023-12-20 13:00:58 INFO     	 * (global step 100: loss: 0.6524037718772888, lr: 0.0001
2023-12-20 13:01:06 INFO     	 * (global step 150: loss: 0.6664508879184723, lr: 0.0001
2023-12-20 13:01:14 INFO     	 * (global step 200: loss: 0.5491462796926498, lr: 0.0001
2023-12-20 13:01:22 INFO     	 * (global step 250: loss: 0.43371284008026123, lr: 0.0001
2023-12-20 13:01:30 INFO     	 * (global step 300: loss: 0.46526917815208435, lr: 0.0001
2023-12-20 13:01:38 INFO     	 * (global step 350: loss: 0.4478193074464798, lr: 0.0001
2023-12-20 13:01:46 INFO     	 * (global step 400: loss: 0.3979707509279251, lr: 0.0001
2023-12-20 13:01:54 INFO     	 * (global step 450: loss: 0.4297632575035095, lr: 0.0001
2023-12-20 13:02:02 INFO     	 * (global step 500: loss: 0.48071466386318207, lr: 0.0001
2023-12-20 13:02:10 INFO     	 * (global step 550: loss: 0.4966909885406494, lr: 0.0001
2023-12-20 13:02:18 INFO     	 * (global step 600: loss: 0.2775631844997406, lr: 0.0001
2023-12-20 13:02:26 INFO     	 * (global step 650: loss: 0.3532160818576813, lr: 0.0001
2023-12-20 13:02:34 INFO     	 * (global step 700: loss: 0.49068139493465424, lr: 0.0001
2023-12-20 13:02:42 INFO     	 * (global step 750: loss: 0.34847238659858704, lr: 0.0001
2023-12-20 13:02:50 INFO     	 * (global step 800: loss: 0.34140749275684357, lr: 0.0001
2023-12-20 13:02:58 INFO     	 * (global step 850: loss: 0.2636881172657013, lr: 0.0001
2023-12-20 13:03:06 INFO     	 * (global step 900: loss: 0.3844577819108963, lr: 0.0001
2023-12-20 13:03:14 INFO     	 * (global step 950: loss: 0.33912548422813416, lr: 0.0001
2023-12-20 13:03:22 INFO     	 * (global step 1000: loss: 0.3110716938972473, lr: 0.0001
2023-12-20 13:03:30 INFO     	 * (global step 1050: loss: 0.3500579744577408, lr: 0.0001
2023-12-20 13:03:38 INFO     	 * (global step 1100: loss: 0.44707708060741425, lr: 0.0001
2023-12-20 13:03:46 INFO     	 * (global step 1150: loss: 0.3369816541671753, lr: 0.0001
2023-12-20 13:03:54 INFO     	 * (global step 1200: loss: 0.4391486793756485, lr: 0.0001
2023-12-20 13:04:02 INFO     	 * (global step 1250: loss: 0.4535181373357773, lr: 0.0001
2023-12-20 13:04:10 INFO     	 * (global step 1300: loss: 0.3385179191827774, lr: 0.0001
2023-12-20 13:04:18 INFO     	 * (global step 1350: loss: 0.4148726314306259, lr: 0.0001
2023-12-20 13:04:26 INFO     	 * (global step 1400: loss: 0.38037510216236115, lr: 0.0001
2023-12-20 13:04:34 INFO     	 * (global step 1450: loss: 0.35716864466667175, lr: 0.0001
2023-12-20 13:04:42 INFO     	 * (global step 1500: loss: 0.35934391617774963, lr: 0.0001
2023-12-20 13:04:49 INFO     	 * (global step 1550: loss: 0.3233787715435028, lr: 0.0001
2023-12-20 13:04:57 INFO     	 * (global step 1600: loss: 0.35465239733457565, lr: 0.0001
2023-12-20 13:05:05 INFO     	 * (global step 1650: loss: 0.33534251153469086, lr: 0.0001
2023-12-20 13:05:13 INFO     	 * (global step 1700: loss: 0.3942243605852127, lr: 0.0001
2023-12-20 13:05:21 INFO     	 * (global step 1750: loss: 0.28824780881404877, lr: 0.0001
2023-12-20 13:05:29 INFO     	 * (global step 1800: loss: 0.3217493146657944, lr: 0.0001
2023-12-20 13:05:37 INFO     	 * (global step 1850: loss: 0.47703753411769867, lr: 0.0001
2023-12-20 13:05:45 INFO     	 * (global step 1900: loss: 0.26451561599969864, lr: 0.0001
2023-12-20 13:05:53 INFO     	 * (global step 1950: loss: 0.32487814128398895, lr: 0.0001
2023-12-20 13:06:01 INFO     	 * (global step 2000: loss: 0.33413295447826385, lr: 0.0001
2023-12-20 13:06:09 INFO     	 * (global step 2050: loss: 0.34771692752838135, lr: 0.0001
2023-12-20 13:06:17 INFO     	 * (global step 2100: loss: 0.3137528598308563, lr: 0.0001
2023-12-20 13:06:25 INFO     	 * (global step 2150: loss: 0.41993166506290436, lr: 0.0001
2023-12-20 13:06:33 INFO     	 * (global step 2200: loss: 0.36110542714595795, lr: 0.0001
2023-12-20 13:06:41 INFO     	 * (global step 2250: loss: 0.3997979909181595, lr: 0.0001
2023-12-20 13:06:49 INFO     	 * (global step 2300: loss: 0.34187929332256317, lr: 0.0001
2023-12-20 13:06:57 INFO     	 * (global step 2350: loss: 0.2998390793800354, lr: 0.0001
2023-12-20 13:07:05 INFO     	 * (global step 2400: loss: 0.38901519775390625, lr: 0.0001
2023-12-20 13:07:13 INFO     	 * (global step 2450: loss: 0.28565219044685364, lr: 0.0001
2023-12-20 13:07:21 INFO     	 * (global step 2500: loss: 0.31550586968660355, lr: 0.0001
2023-12-20 13:07:29 INFO     	 * (global step 2550: loss: 0.6091780364513397, lr: 0.0001
2023-12-20 13:07:37 INFO     	 * (global step 2600: loss: 0.4343915432691574, lr: 0.0001
2023-12-20 13:07:45 INFO     	 * (global step 2650: loss: 0.4264722764492035, lr: 0.0001
2023-12-20 13:07:53 INFO     	 * (global step 2700: loss: 0.2735323905944824, lr: 0.0001
2023-12-20 13:08:01 INFO     	 * (global step 2750: loss: 0.4176067113876343, lr: 0.0001
2023-12-20 13:08:09 INFO     	 * (global step 2800: loss: 0.3612472116947174, lr: 0.0001
2023-12-20 13:08:17 INFO     	 * (global step 2850: loss: 0.23354067653417587, lr: 0.0001
2023-12-20 13:08:25 INFO     	 * (global step 2900: loss: 0.34553441405296326, lr: 0.0001
2023-12-20 13:08:33 INFO     	 * (global step 2950: loss: 0.20275916159152985, lr: 0.0001
2023-12-20 13:08:41 INFO     	 * (global step 3000: loss: 0.5465555042028427, lr: 0.0001
2023-12-20 13:08:49 INFO     	 * (global step 3050: loss: 0.30852995812892914, lr: 0.0001
2023-12-20 13:08:57 INFO     	 * (global step 3100: loss: 0.3207826465368271, lr: 0.0001
2023-12-20 13:09:05 INFO     	 * (global step 3150: loss: 0.32570406794548035, lr: 0.0001
2023-12-20 13:09:13 INFO     	 * (global step 3200: loss: 0.2857859581708908, lr: 0.0001
2023-12-20 13:09:21 INFO     	 * (global step 3250: loss: 0.3404928594827652, lr: 0.0001
2023-12-20 13:09:29 INFO     	 * (global step 3300: loss: 0.47316811978816986, lr: 0.0001
2023-12-20 13:09:37 INFO     	 * (global step 3350: loss: 0.32283230125904083, lr: 0.0001
2023-12-20 13:09:45 INFO     	 * (global step 3400: loss: 0.3340349495410919, lr: 0.0001
2023-12-20 13:09:53 INFO     	 * (global step 3450: loss: 0.3894316405057907, lr: 0.0001
2023-12-20 13:10:01 INFO     	 * (global step 3500: loss: 0.30584122240543365, lr: 0.0001
2023-12-20 13:10:09 INFO     	 * (global step 3550: loss: 0.4423225671052933, lr: 0.0001
2023-12-20 13:10:17 INFO     	 * (global step 3600: loss: 0.3839430958032608, lr: 0.0001
2023-12-20 13:10:25 INFO     	 * (global step 3650: loss: 0.39216695725917816, lr: 0.0001
2023-12-20 13:10:33 INFO     	 * (global step 3700: loss: 0.3082556128501892, lr: 0.0001
2023-12-20 13:10:41 INFO     	 * (global step 3750: loss: 0.31285491585731506, lr: 0.0001
2023-12-20 13:10:49 INFO     	 * (global step 3800: loss: 0.30929549038410187, lr: 0.0001
2023-12-20 13:10:57 INFO     	 * (global step 3850: loss: 0.19066964089870453, lr: 0.0001
2023-12-20 13:11:05 INFO     	 * (global step 3900: loss: 0.39277657866477966, lr: 0.0001
2023-12-20 13:11:13 INFO     	 * (global step 3950: loss: 0.25756123661994934, lr: 0.0001
2023-12-20 13:11:21 INFO     	 * (global step 4000: loss: 0.22978612035512924, lr: 0.0001
2023-12-20 13:11:29 INFO     	 * (global step 4050: loss: 0.2370132878422737, lr: 0.0001
2023-12-20 13:11:37 INFO     	 * (global step 4100: loss: 0.31075039505958557, lr: 0.0001
2023-12-20 13:11:45 INFO     	 * (global step 4150: loss: 0.2710302472114563, lr: 0.0001
2023-12-20 13:11:53 INFO     	 * (global step 4200: loss: 0.2736429199576378, lr: 0.0001
2023-12-20 13:12:01 INFO     	 * (global step 4250: loss: 0.3568546324968338, lr: 0.0001
2023-12-20 13:12:09 INFO     	 * (global step 4300: loss: 0.4385364204645157, lr: 0.0001
2023-12-20 13:12:17 INFO     	 * (global step 4350: loss: 0.25632504373788834, lr: 0.0001
2023-12-20 13:12:25 INFO     	 * (global step 4400: loss: 0.38654597103595734, lr: 0.0001
2023-12-20 13:12:33 INFO     	 * (global step 4450: loss: 0.4087716341018677, lr: 0.0001
2023-12-20 13:12:41 INFO     	 * (global step 4500: loss: 0.2708672508597374, lr: 0.0001
2023-12-20 13:12:49 INFO     	 * (global step 4550: loss: 0.28480878472328186, lr: 0.0001
2023-12-20 13:12:57 INFO     	 * (global step 4600: loss: 0.32132264971733093, lr: 0.0001
2023-12-20 13:13:05 INFO     	 * (global step 4650: loss: 0.37032797932624817, lr: 0.0001
2023-12-20 13:13:13 INFO     	 * (global step 4700: loss: 0.3147039860486984, lr: 0.0001
2023-12-20 13:13:16 INFO     [epoch 0/15] average loss: 0.4, lr: 0.0001
2023-12-20 13:13:16 INFO     saving model related files
2023-12-20 13:13:16 INFO     saving model
2023-12-20 13:13:17 INFO     saving tokenizer
2023-12-20 13:13:17 INFO     saving optimizer
2023-12-20 13:13:18 INFO     remove old optimizer files
2023-12-20 13:13:22 INFO     	 * (global step 4750: loss: 0.4028175473213196, lr: 0.0001
2023-12-20 13:13:30 INFO     	 * (global step 4800: loss: 0.3011888116598129, lr: 0.0001
2023-12-20 13:13:38 INFO     	 * (global step 4850: loss: 0.26301459223032, lr: 0.0001
2023-12-20 13:13:46 INFO     	 * (global step 4900: loss: 0.3752302825450897, lr: 0.0001
2023-12-20 13:13:54 INFO     	 * (global step 4950: loss: 0.3141064941883087, lr: 0.0001
2023-12-20 13:14:02 INFO     	 * (global step 5000: loss: 0.21882600337266922, lr: 0.0001
2023-12-20 13:14:10 INFO     	 * (global step 5050: loss: 0.3736335337162018, lr: 0.0001
2023-12-20 13:14:18 INFO     	 * (global step 5100: loss: 0.31620338559150696, lr: 0.0001
2023-12-20 13:14:26 INFO     	 * (global step 5150: loss: 0.24794524163007736, lr: 0.0001
2023-12-20 13:14:34 INFO     	 * (global step 5200: loss: 0.3122042939066887, lr: 0.0001
2023-12-20 13:14:42 INFO     	 * (global step 5250: loss: 0.23535826057195663, lr: 0.0001
2023-12-20 13:14:50 INFO     	 * (global step 5300: loss: 0.32717300951480865, lr: 0.0001
2023-12-20 13:14:58 INFO     	 * (global step 5350: loss: 0.39696095883846283, lr: 0.0001
2023-12-20 13:15:06 INFO     	 * (global step 5400: loss: 0.365206316113472, lr: 0.0001
2023-12-20 13:15:14 INFO     	 * (global step 5450: loss: 0.5872928500175476, lr: 0.0001
2023-12-20 13:15:22 INFO     	 * (global step 5500: loss: 0.2755320221185684, lr: 0.0001
2023-12-20 13:15:30 INFO     	 * (global step 5550: loss: 0.26313717663288116, lr: 0.0001
2023-12-20 13:15:38 INFO     	 * (global step 5600: loss: 0.4250462204217911, lr: 0.0001
2023-12-20 13:15:46 INFO     	 * (global step 5650: loss: 0.22083240747451782, lr: 0.0001
2023-12-20 13:15:54 INFO     	 * (global step 5700: loss: 0.2839819937944412, lr: 0.0001
2023-12-20 13:16:02 INFO     	 * (global step 5750: loss: 0.36670543253421783, lr: 0.0001
2023-12-20 13:16:10 INFO     	 * (global step 5800: loss: 0.2838018983602524, lr: 0.0001
2023-12-20 13:16:18 INFO     	 * (global step 5850: loss: 0.3070010393857956, lr: 0.0001
2023-12-20 13:16:26 INFO     	 * (global step 5900: loss: 0.23722118884325027, lr: 0.0001
2023-12-20 13:16:34 INFO     	 * (global step 5950: loss: 0.40142691135406494, lr: 0.0001
2023-12-20 13:16:42 INFO     	 * (global step 6000: loss: 0.32010845839977264, lr: 0.0001
2023-12-20 13:16:50 INFO     	 * (global step 6050: loss: 0.2853192687034607, lr: 0.0001
2023-12-20 13:16:58 INFO     	 * (global step 6100: loss: 0.26059025526046753, lr: 0.0001
2023-12-20 13:17:06 INFO     	 * (global step 6150: loss: 0.3007309138774872, lr: 0.0001
2023-12-20 13:17:14 INFO     	 * (global step 6200: loss: 0.365091934800148, lr: 0.0001
2023-12-20 13:17:22 INFO     	 * (global step 6250: loss: 0.31507109105587006, lr: 0.0001
2023-12-20 13:17:30 INFO     	 * (global step 6300: loss: 0.6202414780855179, lr: 0.0001
2023-12-20 13:17:38 INFO     	 * (global step 6350: loss: 0.18817392364144325, lr: 0.0001
2023-12-20 13:17:46 INFO     	 * (global step 6400: loss: 0.2960405722260475, lr: 0.0001
2023-12-20 13:17:54 INFO     	 * (global step 6450: loss: 0.2544739246368408, lr: 0.0001
2023-12-20 13:18:02 INFO     	 * (global step 6500: loss: 0.24611817300319672, lr: 0.0001
2023-12-20 13:18:10 INFO     	 * (global step 6550: loss: 0.16521605849266052, lr: 0.0001
2023-12-20 13:18:18 INFO     	 * (global step 6600: loss: 0.32385629415512085, lr: 0.0001
2023-12-20 13:18:26 INFO     	 * (global step 6650: loss: 0.2819456160068512, lr: 0.0001
2023-12-20 13:18:34 INFO     	 * (global step 6700: loss: 0.2947951406240463, lr: 0.0001
2023-12-20 13:18:42 INFO     	 * (global step 6750: loss: 0.41339220106601715, lr: 0.0001
2023-12-20 13:18:50 INFO     	 * (global step 6800: loss: 0.28182704746723175, lr: 0.0001
2023-12-20 13:18:57 INFO     	 * (global step 6850: loss: 0.641890749335289, lr: 0.0001
2023-12-20 13:19:05 INFO     	 * (global step 6900: loss: 0.2871968001127243, lr: 0.0001
2023-12-20 13:19:13 INFO     	 * (global step 6950: loss: 0.24256956577301025, lr: 0.0001
2023-12-20 13:19:22 INFO     	 * (global step 7000: loss: 0.4375937134027481, lr: 0.0001
2023-12-20 13:19:30 INFO     	 * (global step 7050: loss: 0.28623150289058685, lr: 0.0001
2023-12-20 13:19:38 INFO     	 * (global step 7100: loss: 0.32530562579631805, lr: 0.0001
2023-12-20 13:19:45 INFO     	 * (global step 7150: loss: 0.2662817984819412, lr: 0.0001
2023-12-20 13:19:53 INFO     	 * (global step 7200: loss: 0.2959093302488327, lr: 0.0001
2023-12-20 13:20:01 INFO     	 * (global step 7250: loss: 0.3087804615497589, lr: 0.0001
2023-12-20 13:20:09 INFO     	 * (global step 7300: loss: 0.29506850242614746, lr: 0.0001
2023-12-20 13:20:17 INFO     	 * (global step 7350: loss: 0.21285330504179, lr: 0.0001
2023-12-20 13:20:25 INFO     	 * (global step 7400: loss: 0.4435056298971176, lr: 0.0001
2023-12-20 13:20:33 INFO     	 * (global step 7450: loss: 0.45115064829587936, lr: 0.0001
2023-12-20 13:20:41 INFO     	 * (global step 7500: loss: 0.3212864398956299, lr: 0.0001
2023-12-20 13:20:49 INFO     	 * (global step 7550: loss: 0.39446118474006653, lr: 0.0001
2023-12-20 13:20:57 INFO     	 * (global step 7600: loss: 0.2303786724805832, lr: 0.0001
2023-12-20 13:21:05 INFO     	 * (global step 7650: loss: 0.26484057307243347, lr: 0.0001
2023-12-20 13:21:13 INFO     	 * (global step 7700: loss: 0.369942307472229, lr: 0.0001
2023-12-20 13:21:21 INFO     	 * (global step 7750: loss: 0.2837032079696655, lr: 0.0001
2023-12-20 13:21:29 INFO     	 * (global step 7800: loss: 0.28612979501485825, lr: 0.0001
2023-12-20 13:21:37 INFO     	 * (global step 7850: loss: 0.33040210604667664, lr: 0.0001
2023-12-20 13:21:45 INFO     	 * (global step 7900: loss: 0.3640543520450592, lr: 0.0001
2023-12-20 13:21:53 INFO     	 * (global step 7950: loss: 0.3605392277240753, lr: 0.0001
2023-12-20 13:22:01 INFO     	 * (global step 8000: loss: 0.23699674755334854, lr: 0.0001
2023-12-20 13:22:09 INFO     	 * (global step 8050: loss: 0.3604760318994522, lr: 0.0001
2023-12-20 13:22:17 INFO     	 * (global step 8100: loss: 0.4083573669195175, lr: 0.0001
2023-12-20 13:22:25 INFO     	 * (global step 8150: loss: 0.4711400121450424, lr: 0.0001
2023-12-20 13:22:33 INFO     	 * (global step 8200: loss: 0.2292110174894333, lr: 0.0001
2023-12-20 13:22:41 INFO     	 * (global step 8250: loss: 0.43282245099544525, lr: 0.0001
2023-12-20 13:22:49 INFO     	 * (global step 8300: loss: 0.27951765805482864, lr: 0.0001
2023-12-20 13:22:57 INFO     	 * (global step 8350: loss: 0.4145510494709015, lr: 0.0001
2023-12-20 13:23:05 INFO     	 * (global step 8400: loss: 0.274776853621006, lr: 0.0001
2023-12-20 13:23:13 INFO     	 * (global step 8450: loss: 0.35055555403232574, lr: 0.0001
2023-12-20 13:23:21 INFO     	 * (global step 8500: loss: 0.45220324397087097, lr: 0.0001
2023-12-20 13:23:29 INFO     	 * (global step 8550: loss: 0.4677845537662506, lr: 0.0001
2023-12-20 13:23:37 INFO     	 * (global step 8600: loss: 0.19487250596284866, lr: 0.0001
2023-12-20 13:23:45 INFO     	 * (global step 8650: loss: 0.40158508718013763, lr: 0.0001
2023-12-20 13:23:53 INFO     	 * (global step 8700: loss: 0.17107561230659485, lr: 0.0001
2023-12-20 13:24:01 INFO     	 * (global step 8750: loss: 0.22036534547805786, lr: 0.0001
2023-12-20 13:24:09 INFO     	 * (global step 8800: loss: 0.2769481986761093, lr: 0.0001
2023-12-20 13:24:17 INFO     	 * (global step 8850: loss: 0.3845016211271286, lr: 0.0001
2023-12-20 13:24:25 INFO     	 * (global step 8900: loss: 0.2399858981370926, lr: 0.0001
2023-12-20 13:24:33 INFO     	 * (global step 8950: loss: 0.2295173779129982, lr: 0.0001
2023-12-20 13:24:41 INFO     	 * (global step 9000: loss: 0.23560109734535217, lr: 0.0001
2023-12-20 13:24:49 INFO     	 * (global step 9050: loss: 0.4662911295890808, lr: 0.0001
2023-12-20 13:24:57 INFO     	 * (global step 9100: loss: 0.3512527197599411, lr: 0.0001
2023-12-20 13:25:05 INFO     	 * (global step 9150: loss: 0.2570121958851814, lr: 0.0001
2023-12-20 13:25:13 INFO     	 * (global step 9200: loss: 0.23134779185056686, lr: 0.0001
2023-12-20 13:25:21 INFO     	 * (global step 9250: loss: 0.5491283535957336, lr: 0.0001
2023-12-20 13:25:29 INFO     	 * (global step 9300: loss: 0.2880124896764755, lr: 0.0001
2023-12-20 13:25:37 INFO     	 * (global step 9350: loss: 0.393671490252018, lr: 0.0001
2023-12-20 13:25:45 INFO     	 * (global step 9400: loss: 0.3097275495529175, lr: 0.0001
2023-12-20 13:25:52 INFO     [epoch 1/15] average loss: 0.32, lr: 0.0001
2023-12-20 13:25:52 INFO     saving model related files
2023-12-20 13:25:52 INFO     saving model
2023-12-20 13:25:53 INFO     saving tokenizer
2023-12-20 13:25:53 INFO     saving optimizer
2023-12-20 13:25:54 INFO     remove old optimizer files
2023-12-20 13:25:54 INFO     	 * (global step 9450: loss: 0.3872256726026535, lr: 0.0001
2023-12-20 13:26:02 INFO     	 * (global step 9500: loss: 0.26972195506095886, lr: 0.0001
2023-12-20 13:26:10 INFO     	 * (global step 9550: loss: 0.3487430512905121, lr: 0.0001
2023-12-20 13:26:18 INFO     	 * (global step 9600: loss: 0.27516864240169525, lr: 0.0001
2023-12-20 13:26:26 INFO     	 * (global step 9650: loss: 0.23891960084438324, lr: 0.0001
2023-12-20 13:26:34 INFO     	 * (global step 9700: loss: 0.22023504972457886, lr: 0.0001
2023-12-20 13:26:42 INFO     	 * (global step 9750: loss: 0.3958714157342911, lr: 0.0001
2023-12-20 13:26:50 INFO     	 * (global step 9800: loss: 0.24131107330322266, lr: 0.0001
2023-12-20 13:26:58 INFO     	 * (global step 9850: loss: 0.24350683391094208, lr: 0.0001
2023-12-20 13:27:06 INFO     	 * (global step 9900: loss: 0.17997188866138458, lr: 0.0001
2023-12-20 13:27:14 INFO     	 * (global step 9950: loss: 0.28736351430416107, lr: 0.0001
2023-12-20 13:27:22 INFO     	 * (global step 10000: loss: 0.21856629103422165, lr: 0.0001
2023-12-20 13:27:30 INFO     	 * (global step 10050: loss: 0.19880611449480057, lr: 0.0001
2023-12-20 13:27:38 INFO     	 * (global step 10100: loss: 0.27865470945835114, lr: 0.0001
2023-12-20 13:27:46 INFO     	 * (global step 10150: loss: 0.2760651707649231, lr: 0.0001
2023-12-20 13:27:54 INFO     	 * (global step 10200: loss: 0.28255072236061096, lr: 0.0001
2023-12-20 13:28:02 INFO     	 * (global step 10250: loss: 0.44060901552438736, lr: 0.0001
2023-12-20 13:28:10 INFO     	 * (global step 10300: loss: 0.37851081788539886, lr: 0.0001
2023-12-20 13:28:18 INFO     	 * (global step 10350: loss: 0.3691565543413162, lr: 0.0001
2023-12-20 13:28:26 INFO     	 * (global step 10400: loss: 0.2814341187477112, lr: 0.0001
2023-12-20 13:28:34 INFO     	 * (global step 10450: loss: 0.2799670994281769, lr: 0.0001
2023-12-20 13:28:42 INFO     	 * (global step 10500: loss: 0.2563272565603256, lr: 0.0001
2023-12-20 13:28:50 INFO     	 * (global step 10550: loss: 0.3047880530357361, lr: 0.0001
2023-12-20 13:28:58 INFO     	 * (global step 10600: loss: 0.29215557873249054, lr: 0.0001
2023-12-20 13:29:06 INFO     	 * (global step 10650: loss: 0.28850696980953217, lr: 0.0001
2023-12-20 13:29:14 INFO     	 * (global step 10700: loss: 0.25151459127664566, lr: 0.0001
2023-12-20 13:29:22 INFO     	 * (global step 10750: loss: 0.3558840751647949, lr: 0.0001
2023-12-20 13:29:30 INFO     	 * (global step 10800: loss: 0.36173921823501587, lr: 0.0001
2023-12-20 13:29:38 INFO     	 * (global step 10850: loss: 0.2760454975068569, lr: 0.0001
2023-12-20 13:29:46 INFO     	 * (global step 10900: loss: 0.3061661571264267, lr: 0.0001
2023-12-20 13:29:54 INFO     	 * (global step 10950: loss: 0.2819584757089615, lr: 0.0001
2023-12-20 13:30:02 INFO     	 * (global step 11000: loss: 0.5288621187210083, lr: 0.0001
2023-12-20 13:30:10 INFO     	 * (global step 11050: loss: 0.28267353028059006, lr: 0.0001
2023-12-20 13:30:18 INFO     	 * (global step 11100: loss: 0.5040313452482224, lr: 0.0001
2023-12-20 13:30:26 INFO     	 * (global step 11150: loss: 0.31508489698171616, lr: 0.0001
2023-12-20 13:30:34 INFO     	 * (global step 11200: loss: 0.2263030856847763, lr: 0.0001
2023-12-20 13:30:42 INFO     	 * (global step 11250: loss: 0.19294483214616776, lr: 0.0001
2023-12-20 13:30:50 INFO     	 * (global step 11300: loss: 0.3012232333421707, lr: 0.0001
2023-12-20 13:30:58 INFO     	 * (global step 11350: loss: 0.2603948563337326, lr: 0.0001
2023-12-20 13:31:06 INFO     	 * (global step 11400: loss: 0.20812062919139862, lr: 0.0001
2023-12-20 13:31:14 INFO     	 * (global step 11450: loss: 0.25505317747592926, lr: 0.0001
2023-12-20 13:31:22 INFO     	 * (global step 11500: loss: 0.21381040662527084, lr: 0.0001
2023-12-20 13:31:30 INFO     	 * (global step 11550: loss: 0.44318417459726334, lr: 0.0001
2023-12-20 13:31:38 INFO     	 * (global step 11600: loss: 0.3595367819070816, lr: 0.0001
2023-12-20 13:31:46 INFO     	 * (global step 11650: loss: 0.260381743311882, lr: 0.0001
2023-12-20 13:31:54 INFO     	 * (global step 11700: loss: 0.429203063249588, lr: 0.0001
2023-12-20 13:32:02 INFO     	 * (global step 11750: loss: 0.2432335838675499, lr: 0.0001
2023-12-20 13:32:10 INFO     	 * (global step 11800: loss: 0.1861291565001011, lr: 0.0001
2023-12-20 13:32:18 INFO     	 * (global step 11850: loss: 0.28444355353713036, lr: 0.0001
2023-12-20 13:32:26 INFO     	 * (global step 11900: loss: 0.3409533351659775, lr: 0.0001
2023-12-20 13:32:34 INFO     	 * (global step 11950: loss: 0.2900679409503937, lr: 0.0001
2023-12-20 13:32:42 INFO     	 * (global step 12000: loss: 0.25056371092796326, lr: 0.0001
2023-12-20 13:32:50 INFO     	 * (global step 12050: loss: 0.26095912605524063, lr: 0.0001
2023-12-20 13:32:58 INFO     	 * (global step 12100: loss: 0.33715131133794785, lr: 0.0001
2023-12-20 13:33:06 INFO     	 * (global step 12150: loss: 0.35220296680927277, lr: 0.0001
2023-12-20 13:33:14 INFO     	 * (global step 12200: loss: 0.2986986115574837, lr: 0.0001
2023-12-20 13:33:22 INFO     	 * (global step 12250: loss: 0.2889099270105362, lr: 0.0001
2023-12-20 13:33:30 INFO     	 * (global step 12300: loss: 0.2363070473074913, lr: 0.0001
2023-12-20 13:33:38 INFO     	 * (global step 12350: loss: 0.3657078891992569, lr: 0.0001
2023-12-20 13:33:46 INFO     	 * (global step 12400: loss: 0.59853395819664, lr: 0.0001
2023-12-20 13:33:54 INFO     	 * (global step 12450: loss: 0.18972447514533997, lr: 0.0001
2023-12-20 13:34:02 INFO     	 * (global step 12500: loss: 0.35947373509407043, lr: 0.0001
2023-12-20 13:34:09 INFO     	 * (global step 12550: loss: 0.34456808865070343, lr: 0.0001
2023-12-20 13:34:17 INFO     	 * (global step 12600: loss: 0.43003305047750473, lr: 0.0001
2023-12-20 13:34:25 INFO     	 * (global step 12650: loss: 0.30607983469963074, lr: 0.0001
2023-12-20 13:34:33 INFO     	 * (global step 12700: loss: 0.24571488797664642, lr: 0.0001
2023-12-20 13:34:41 INFO     	 * (global step 12750: loss: 0.335485577583313, lr: 0.0001
2023-12-20 13:34:49 INFO     	 * (global step 12800: loss: 0.2695401534438133, lr: 0.0001
2023-12-20 13:34:57 INFO     	 * (global step 12850: loss: 0.24414778500795364, lr: 0.0001
2023-12-20 13:35:05 INFO     	 * (global step 12900: loss: 0.262064553797245, lr: 0.0001
2023-12-20 13:35:13 INFO     	 * (global step 12950: loss: 0.347895011305809, lr: 0.0001
2023-12-20 13:35:21 INFO     	 * (global step 13000: loss: 0.3968004435300827, lr: 0.0001
2023-12-20 13:35:29 INFO     	 * (global step 13050: loss: 0.22985001653432846, lr: 0.0001
2023-12-20 13:35:37 INFO     	 * (global step 13100: loss: 0.3196030631661415, lr: 0.0001
2023-12-20 13:35:45 INFO     	 * (global step 13150: loss: 0.34057655930519104, lr: 0.0001
2023-12-20 13:35:53 INFO     	 * (global step 13200: loss: 0.2731246203184128, lr: 0.0001
2023-12-20 13:36:01 INFO     	 * (global step 13250: loss: 0.23149719461798668, lr: 0.0001
2023-12-20 13:36:09 INFO     	 * (global step 13300: loss: 0.36779123544692993, lr: 0.0001
2023-12-20 13:36:17 INFO     	 * (global step 13350: loss: 0.366485595703125, lr: 0.0001
2023-12-20 13:36:25 INFO     	 * (global step 13400: loss: 0.23038245737552643, lr: 0.0001
2023-12-20 13:36:33 INFO     	 * (global step 13450: loss: 0.33425888419151306, lr: 0.0001
2023-12-20 13:36:41 INFO     	 * (global step 13500: loss: 0.2588309198617935, lr: 0.0001
2023-12-20 13:36:49 INFO     	 * (global step 13550: loss: 0.20822495222091675, lr: 0.0001
2023-12-20 13:36:57 INFO     	 * (global step 13600: loss: 0.5125405937433243, lr: 0.0001
2023-12-20 13:37:05 INFO     	 * (global step 13650: loss: 0.19889827072620392, lr: 0.0001
2023-12-20 13:37:13 INFO     	 * (global step 13700: loss: 0.3075735792517662, lr: 0.0001
2023-12-20 13:37:21 INFO     	 * (global step 13750: loss: 0.2738979011774063, lr: 0.0001
2023-12-20 13:37:29 INFO     	 * (global step 13800: loss: 0.324254110455513, lr: 0.0001
2023-12-20 13:37:37 INFO     	 * (global step 13850: loss: 0.45659419149160385, lr: 0.0001
2023-12-20 13:37:45 INFO     	 * (global step 13900: loss: 0.21992535889148712, lr: 0.0001
2023-12-20 13:37:53 INFO     	 * (global step 13950: loss: 0.281266912817955, lr: 0.0001
2023-12-20 13:38:01 INFO     	 * (global step 14000: loss: 0.3263564333319664, lr: 0.0001
2023-12-20 13:38:09 INFO     	 * (global step 14050: loss: 0.34042641520500183, lr: 0.0001
2023-12-20 13:38:17 INFO     	 * (global step 14100: loss: 0.22453968599438667, lr: 0.0001
2023-12-20 13:38:25 INFO     	 * (global step 14150: loss: 0.3269149512052536, lr: 0.0001
2023-12-20 13:38:28 INFO     [epoch 2/15] average loss: 0.301, lr: 0.0001
2023-12-20 13:38:28 INFO     saving model related files
2023-12-20 13:38:28 INFO     saving model
2023-12-20 13:38:29 INFO     saving tokenizer
2023-12-20 13:38:29 INFO     saving optimizer
2023-12-20 13:38:30 INFO     remove old optimizer files
2023-12-20 13:38:35 INFO     	 * (global step 14200: loss: 0.3135155662894249, lr: 0.0001
2023-12-20 13:38:43 INFO     	 * (global step 14250: loss: 0.23777157813310623, lr: 0.0001
2023-12-20 13:38:51 INFO     	 * (global step 14300: loss: 0.1487441509962082, lr: 0.0001
2023-12-20 13:38:59 INFO     	 * (global step 14350: loss: 0.32092516124248505, lr: 0.0001
2023-12-20 13:39:07 INFO     	 * (global step 14400: loss: 0.3063064515590668, lr: 0.0001
2023-12-20 13:39:15 INFO     	 * (global step 14450: loss: 0.23200588673353195, lr: 0.0001
2023-12-20 13:39:23 INFO     	 * (global step 14500: loss: 0.46683546900749207, lr: 0.0001
2023-12-20 13:39:31 INFO     	 * (global step 14550: loss: 0.20359806716442108, lr: 0.0001
2023-12-20 13:39:39 INFO     	 * (global step 14600: loss: 0.21132917702198029, lr: 0.0001
2023-12-20 13:39:47 INFO     	 * (global step 14650: loss: 0.26808712631464005, lr: 0.0001
2023-12-20 13:39:55 INFO     	 * (global step 14700: loss: 0.3693437799811363, lr: 0.0001
2023-12-20 13:40:03 INFO     	 * (global step 14750: loss: 0.24642974883317947, lr: 0.0001
2023-12-20 13:40:11 INFO     	 * (global step 14800: loss: 0.322937473654747, lr: 0.0001
2023-12-20 13:40:19 INFO     	 * (global step 14850: loss: 0.2600170597434044, lr: 0.0001
2023-12-20 13:40:27 INFO     	 * (global step 14900: loss: 0.3833831176161766, lr: 0.0001
2023-12-20 13:40:35 INFO     	 * (global step 14950: loss: 0.3037773072719574, lr: 0.0001
2023-12-20 13:40:42 INFO     	 * (global step 15000: loss: 0.2180059626698494, lr: 0.0001
2023-12-20 13:40:50 INFO     	 * (global step 15050: loss: 0.24765221774578094, lr: 0.0001
2023-12-20 13:40:58 INFO     	 * (global step 15100: loss: 0.23608457297086716, lr: 0.0001
2023-12-20 13:41:06 INFO     	 * (global step 15150: loss: 0.3790866881608963, lr: 0.0001
2023-12-20 13:41:14 INFO     	 * (global step 15200: loss: 0.41218265891075134, lr: 0.0001
2023-12-20 13:41:22 INFO     	 * (global step 15250: loss: 0.4479391574859619, lr: 0.0001
2023-12-20 13:41:30 INFO     	 * (global step 15300: loss: 0.3088438808917999, lr: 0.0001
2023-12-20 13:41:38 INFO     	 * (global step 15350: loss: 0.2782549113035202, lr: 0.0001
2023-12-20 13:41:46 INFO     	 * (global step 15400: loss: 0.22189433127641678, lr: 0.0001
2023-12-20 13:41:54 INFO     	 * (global step 15450: loss: 0.27625471353530884, lr: 0.0001
2023-12-20 13:42:02 INFO     	 * (global step 15500: loss: 0.29286637902259827, lr: 0.0001
2023-12-20 13:42:10 INFO     	 * (global step 15550: loss: 0.21971970796585083, lr: 0.0001
2023-12-20 13:42:18 INFO     	 * (global step 15600: loss: 0.3697306364774704, lr: 0.0001
2023-12-20 13:42:26 INFO     	 * (global step 15650: loss: 0.3295300304889679, lr: 0.0001
2023-12-20 13:42:34 INFO     	 * (global step 15700: loss: 0.32684604823589325, lr: 0.0001
2023-12-20 13:42:42 INFO     	 * (global step 15750: loss: 0.3753734529018402, lr: 0.0001
2023-12-20 13:42:50 INFO     	 * (global step 15800: loss: 0.2040494903922081, lr: 0.0001
2023-12-20 13:42:58 INFO     	 * (global step 15850: loss: 0.33553948253393173, lr: 0.0001
2023-12-20 13:43:06 INFO     	 * (global step 15900: loss: 0.20567482709884644, lr: 0.0001
2023-12-20 13:43:14 INFO     	 * (global step 15950: loss: 0.3050018772482872, lr: 0.0001
2023-12-20 13:43:22 INFO     	 * (global step 16000: loss: 0.1594764105975628, lr: 0.0001
2023-12-20 13:43:30 INFO     	 * (global step 16050: loss: 0.4151228219270706, lr: 0.0001
2023-12-20 13:43:38 INFO     	 * (global step 16100: loss: 0.33437222987413406, lr: 0.0001
2023-12-20 13:43:46 INFO     	 * (global step 16150: loss: 0.2523704841732979, lr: 0.0001
2023-12-20 13:43:54 INFO     	 * (global step 16200: loss: 0.2851177304983139, lr: 0.0001
2023-12-20 13:44:02 INFO     	 * (global step 16250: loss: 0.31893379986286163, lr: 0.0001
2023-12-20 13:44:10 INFO     	 * (global step 16300: loss: 0.2439952939748764, lr: 0.0001
2023-12-20 13:44:18 INFO     	 * (global step 16350: loss: 0.2988832890987396, lr: 0.0001
2023-12-20 13:44:26 INFO     	 * (global step 16400: loss: 0.32739734649658203, lr: 0.0001
2023-12-20 13:44:34 INFO     	 * (global step 16450: loss: 0.2798065170645714, lr: 0.0001
2023-12-20 13:44:42 INFO     	 * (global step 16500: loss: 0.28629185259342194, lr: 0.0001
2023-12-20 13:44:50 INFO     	 * (global step 16550: loss: 0.3014482259750366, lr: 0.0001
2023-12-20 13:44:58 INFO     	 * (global step 16600: loss: 0.2463492602109909, lr: 0.0001
2023-12-20 13:45:06 INFO     	 * (global step 16650: loss: 0.2984957695007324, lr: 0.0001
2023-12-20 13:45:14 INFO     	 * (global step 16700: loss: 0.19346945732831955, lr: 0.0001
2023-12-20 13:45:22 INFO     	 * (global step 16750: loss: 0.1289362758398056, lr: 0.0001
2023-12-20 13:45:30 INFO     	 * (global step 16800: loss: 0.29831840097904205, lr: 0.0001
2023-12-20 13:45:38 INFO     	 * (global step 16850: loss: 0.34585946798324585, lr: 0.0001
2023-12-20 13:45:46 INFO     	 * (global step 16900: loss: 0.2693219780921936, lr: 0.0001
2023-12-20 13:45:54 INFO     	 * (global step 16950: loss: 0.3202950656414032, lr: 0.0001
2023-12-20 13:46:02 INFO     	 * (global step 17000: loss: 0.22660671174526215, lr: 0.0001
2023-12-20 13:46:10 INFO     	 * (global step 17050: loss: 0.20092298835515976, lr: 0.0001
2023-12-20 13:46:18 INFO     	 * (global step 17100: loss: 0.3111855164170265, lr: 0.0001
2023-12-20 13:46:26 INFO     	 * (global step 17150: loss: 0.24518763273954391, lr: 0.0001
2023-12-20 13:46:34 INFO     	 * (global step 17200: loss: 0.29003677517175674, lr: 0.0001
2023-12-20 13:46:42 INFO     	 * (global step 17250: loss: 0.2712724804878235, lr: 0.0001
2023-12-20 13:46:50 INFO     	 * (global step 17300: loss: 0.20014577358961105, lr: 0.0001
2023-12-20 13:46:58 INFO     	 * (global step 17350: loss: 0.3528253436088562, lr: 0.0001
2023-12-20 13:47:06 INFO     	 * (global step 17400: loss: 0.18821822851896286, lr: 0.0001
2023-12-20 13:47:14 INFO     	 * (global step 17450: loss: 0.43323802202939987, lr: 0.0001
2023-12-20 13:47:22 INFO     	 * (global step 17500: loss: 0.2996262311935425, lr: 0.0001
2023-12-20 13:47:30 INFO     	 * (global step 17550: loss: 0.26238955557346344, lr: 0.0001
2023-12-20 13:47:38 INFO     	 * (global step 17600: loss: 0.2732754498720169, lr: 0.0001
2023-12-20 13:47:46 INFO     	 * (global step 17650: loss: 0.4000013768672943, lr: 0.0001
2023-12-20 13:47:54 INFO     	 * (global step 17700: loss: 0.2529173269867897, lr: 0.0001
2023-12-20 13:48:02 INFO     	 * (global step 17750: loss: 0.3415864259004593, lr: 0.0001
2023-12-20 13:48:10 INFO     	 * (global step 17800: loss: 0.45168137550354004, lr: 0.0001
2023-12-20 13:48:18 INFO     	 * (global step 17850: loss: 0.23264814168214798, lr: 0.0001
2023-12-20 13:48:26 INFO     	 * (global step 17900: loss: 0.30630406737327576, lr: 0.0001
2023-12-20 13:48:34 INFO     	 * (global step 17950: loss: 0.27310027927160263, lr: 0.0001
2023-12-20 13:48:42 INFO     	 * (global step 18000: loss: 0.281439445912838, lr: 0.0001
2023-12-20 13:48:50 INFO     	 * (global step 18050: loss: 0.19075606018304825, lr: 0.0001
2023-12-20 13:48:57 INFO     	 * (global step 18100: loss: 0.3021008223295212, lr: 0.0001
2023-12-20 13:49:05 INFO     	 * (global step 18150: loss: 0.286302849650383, lr: 0.0001
2023-12-20 13:49:13 INFO     	 * (global step 18200: loss: 0.36040347814559937, lr: 0.0001
2023-12-20 13:49:21 INFO     	 * (global step 18250: loss: 0.23977259546518326, lr: 0.0001
2023-12-20 13:49:29 INFO     	 * (global step 18300: loss: 0.26867639273405075, lr: 0.0001
2023-12-20 13:49:37 INFO     	 * (global step 18350: loss: 0.2989097982645035, lr: 0.0001
2023-12-20 13:49:45 INFO     	 * (global step 18400: loss: 0.28799305111169815, lr: 0.0001
2023-12-20 13:49:53 INFO     	 * (global step 18450: loss: 0.3287470042705536, lr: 0.0001
2023-12-20 13:50:01 INFO     	 * (global step 18500: loss: 0.2792894244194031, lr: 0.0001
2023-12-20 13:50:09 INFO     	 * (global step 18550: loss: 0.3289451524615288, lr: 0.0001
2023-12-20 13:50:17 INFO     	 * (global step 18600: loss: 0.39443646371364594, lr: 0.0001
2023-12-20 13:50:25 INFO     	 * (global step 18650: loss: 0.5060535669326782, lr: 0.0001
2023-12-20 13:50:33 INFO     	 * (global step 18700: loss: 0.35843099653720856, lr: 0.0001
2023-12-20 13:50:41 INFO     	 * (global step 18750: loss: 0.19591008871793747, lr: 0.0001
2023-12-20 13:50:49 INFO     	 * (global step 18800: loss: 0.4871596097946167, lr: 0.0001
2023-12-20 13:50:57 INFO     	 * (global step 18850: loss: 0.24751998484134674, lr: 0.0001
2023-12-20 13:51:04 INFO     [epoch 3/15] average loss: 0.289, lr: 0.0001
2023-12-20 13:51:04 INFO     saving model related files
2023-12-20 13:51:04 INFO     saving model
2023-12-20 13:51:05 INFO     saving tokenizer
2023-12-20 13:51:05 INFO     saving optimizer
2023-12-20 13:51:05 INFO     remove old optimizer files
2023-12-20 13:51:07 INFO     	 * (global step 18900: loss: 0.24126236885786057, lr: 0.0001
2023-12-20 13:51:15 INFO     	 * (global step 18950: loss: 0.26237909495830536, lr: 0.0001
2023-12-20 13:51:23 INFO     	 * (global step 19000: loss: 0.48543646931648254, lr: 0.0001
2023-12-20 13:51:31 INFO     	 * (global step 19050: loss: 0.35536710917949677, lr: 0.0001
2023-12-20 13:51:39 INFO     	 * (global step 19100: loss: 0.21043676882982254, lr: 0.0001
2023-12-20 13:51:47 INFO     	 * (global step 19150: loss: 0.22059305012226105, lr: 0.0001
2023-12-20 13:51:55 INFO     	 * (global step 19200: loss: 0.2970532700419426, lr: 0.0001
2023-12-20 13:52:03 INFO     	 * (global step 19250: loss: 0.20218787342309952, lr: 0.0001
2023-12-20 13:52:11 INFO     	 * (global step 19300: loss: 0.20414281636476517, lr: 0.0001
2023-12-20 13:52:19 INFO     	 * (global step 19350: loss: 0.16897157579660416, lr: 0.0001
2023-12-20 13:52:27 INFO     	 * (global step 19400: loss: 0.2819555103778839, lr: 0.0001
2023-12-20 13:52:35 INFO     	 * (global step 19450: loss: 0.1664678379893303, lr: 0.0001
2023-12-20 13:52:43 INFO     	 * (global step 19500: loss: 0.2921825349330902, lr: 0.0001
2023-12-20 13:52:51 INFO     	 * (global step 19550: loss: 0.29000458121299744, lr: 0.0001
2023-12-20 13:52:59 INFO     	 * (global step 19600: loss: 0.2118455357849598, lr: 0.0001
2023-12-20 13:53:07 INFO     	 * (global step 19650: loss: 0.5165088474750519, lr: 0.0001
2023-12-20 13:53:15 INFO     	 * (global step 19700: loss: 0.2761857211589813, lr: 0.0001
2023-12-20 13:53:23 INFO     	 * (global step 19750: loss: 0.2727895677089691, lr: 0.0001
2023-12-20 13:53:31 INFO     	 * (global step 19800: loss: 0.22040103375911713, lr: 0.0001
2023-12-20 13:53:39 INFO     	 * (global step 19850: loss: 0.18576977774500847, lr: 0.0001
2023-12-20 13:53:47 INFO     	 * (global step 19900: loss: 0.30128854513168335, lr: 0.0001
2023-12-20 13:53:55 INFO     	 * (global step 19950: loss: 0.24889559298753738, lr: 0.0001
2023-12-20 13:54:03 INFO     	 * (global step 20000: loss: 0.18598619103431702, lr: 0.0001
2023-12-20 13:54:10 INFO     	 * (global step 20050: loss: 0.2213660329580307, lr: 0.0001
2023-12-20 13:54:18 INFO     	 * (global step 20100: loss: 0.3373931348323822, lr: 0.0001
2023-12-20 13:54:26 INFO     	 * (global step 20150: loss: 0.24095720797777176, lr: 0.0001
2023-12-20 13:54:34 INFO     	 * (global step 20200: loss: 0.2592647671699524, lr: 0.0001
2023-12-20 13:54:42 INFO     	 * (global step 20250: loss: 0.2625441774725914, lr: 0.0001
2023-12-20 13:54:50 INFO     	 * (global step 20300: loss: 0.29073552787303925, lr: 0.0001
2023-12-20 13:54:59 INFO     	 * (global step 20350: loss: 0.22550077736377716, lr: 0.0001
2023-12-20 13:55:07 INFO     	 * (global step 20400: loss: 0.23513681441545486, lr: 0.0001
2023-12-20 13:55:14 INFO     	 * (global step 20450: loss: 0.20199356973171234, lr: 0.0001
2023-12-20 13:55:22 INFO     	 * (global step 20500: loss: 0.20873768627643585, lr: 0.0001
2023-12-20 13:55:31 INFO     	 * (global step 20550: loss: 0.23123537749052048, lr: 0.0001
2023-12-20 13:55:38 INFO     	 * (global step 20600: loss: 0.2836463525891304, lr: 0.0001
2023-12-20 13:55:46 INFO     	 * (global step 20650: loss: 0.3867341876029968, lr: 0.0001
2023-12-20 13:55:54 INFO     	 * (global step 20700: loss: 0.21925906836986542, lr: 0.0001
2023-12-20 13:56:02 INFO     	 * (global step 20750: loss: 0.22965586185455322, lr: 0.0001
2023-12-20 13:56:10 INFO     	 * (global step 20800: loss: 0.3205198422074318, lr: 0.0001
2023-12-20 13:56:18 INFO     	 * (global step 20850: loss: 0.2823002189397812, lr: 0.0001
2023-12-20 13:56:26 INFO     	 * (global step 20900: loss: 0.30006979405879974, lr: 0.0001
2023-12-20 13:56:34 INFO     	 * (global step 20950: loss: 0.285110741853714, lr: 0.0001
2023-12-20 13:56:42 INFO     	 * (global step 21000: loss: 0.4229329600930214, lr: 0.0001
2023-12-20 13:56:50 INFO     	 * (global step 21050: loss: 0.24991875141859055, lr: 0.0001
2023-12-20 13:56:58 INFO     	 * (global step 21100: loss: 0.22678419947624207, lr: 0.0001
2023-12-20 13:57:06 INFO     	 * (global step 21150: loss: 0.25463200360536575, lr: 0.0001
2023-12-20 13:57:14 INFO     	 * (global step 21200: loss: 0.3177981376647949, lr: 0.0001
2023-12-20 13:57:22 INFO     	 * (global step 21250: loss: 0.23819900304079056, lr: 0.0001
2023-12-20 13:57:30 INFO     	 * (global step 21300: loss: 0.2843959107995033, lr: 0.0001
2023-12-20 13:57:38 INFO     	 * (global step 21350: loss: 0.2245904505252838, lr: 0.0001
2023-12-20 13:57:46 INFO     	 * (global step 21400: loss: 0.1870378777384758, lr: 0.0001
2023-12-20 13:57:54 INFO     	 * (global step 21450: loss: 0.46051034331321716, lr: 0.0001
2023-12-20 13:58:02 INFO     	 * (global step 21500: loss: 0.25666916370391846, lr: 0.0001
2023-12-20 13:58:10 INFO     	 * (global step 21550: loss: 0.2717542052268982, lr: 0.0001
2023-12-20 13:58:18 INFO     	 * (global step 21600: loss: 0.32972266525030136, lr: 0.0001
2023-12-20 13:58:26 INFO     	 * (global step 21650: loss: 0.23807231336832047, lr: 0.0001
2023-12-20 13:58:34 INFO     	 * (global step 21700: loss: 0.1668468341231346, lr: 0.0001
2023-12-20 13:58:42 INFO     	 * (global step 21750: loss: 0.33381541818380356, lr: 0.0001
2023-12-20 13:58:50 INFO     	 * (global step 21800: loss: 0.29275304824113846, lr: 0.0001
2023-12-20 13:58:58 INFO     	 * (global step 21850: loss: 0.35127121210098267, lr: 0.0001
2023-12-20 13:59:06 INFO     	 * (global step 21900: loss: 0.25643055886030197, lr: 0.0001
2023-12-20 13:59:14 INFO     	 * (global step 21950: loss: 0.28650201857089996, lr: 0.0001
2023-12-20 13:59:22 INFO     	 * (global step 22000: loss: 0.28250371664762497, lr: 0.0001
2023-12-20 13:59:30 INFO     	 * (global step 22050: loss: 0.32394639402627945, lr: 0.0001
2023-12-20 13:59:38 INFO     	 * (global step 22100: loss: 0.3870999366044998, lr: 0.0001
2023-12-20 13:59:46 INFO     	 * (global step 22150: loss: 0.42811043560504913, lr: 0.0001
2023-12-20 13:59:54 INFO     	 * (global step 22200: loss: 0.19084323942661285, lr: 0.0001
2023-12-20 14:00:02 INFO     	 * (global step 22250: loss: 0.2416144758462906, lr: 0.0001
2023-12-20 14:00:10 INFO     	 * (global step 22300: loss: 0.28080688416957855, lr: 0.0001
2023-12-20 14:00:18 INFO     	 * (global step 22350: loss: 0.3480326235294342, lr: 0.0001
2023-12-20 14:00:26 INFO     	 * (global step 22400: loss: 0.18859810382127762, lr: 0.0001
2023-12-20 14:00:34 INFO     	 * (global step 22450: loss: 0.3836170732975006, lr: 0.0001
2023-12-20 14:00:42 INFO     	 * (global step 22500: loss: 0.28373152017593384, lr: 0.0001
2023-12-20 14:00:50 INFO     	 * (global step 22550: loss: 0.2723863571882248, lr: 0.0001
2023-12-20 14:00:58 INFO     	 * (global step 22600: loss: 0.2857648581266403, lr: 0.0001
2023-12-20 14:01:06 INFO     	 * (global step 22650: loss: 0.15350624546408653, lr: 0.0001
2023-12-20 14:01:14 INFO     	 * (global step 22700: loss: 0.3744552433490753, lr: 0.0001
2023-12-20 14:01:22 INFO     	 * (global step 22750: loss: 0.3001786917448044, lr: 0.0001
2023-12-20 14:01:30 INFO     	 * (global step 22800: loss: 0.37189166247844696, lr: 0.0001
2023-12-20 14:01:38 INFO     	 * (global step 22850: loss: 0.2852146625518799, lr: 0.0001
2023-12-20 14:01:46 INFO     	 * (global step 22900: loss: 0.2975253611803055, lr: 0.0001
2023-12-20 14:01:54 INFO     	 * (global step 22950: loss: 0.20938292145729065, lr: 0.0001
2023-12-20 14:02:02 INFO     	 * (global step 23000: loss: 0.3392571210861206, lr: 0.0001
2023-12-20 14:02:10 INFO     	 * (global step 23050: loss: 0.4409485161304474, lr: 0.0001
2023-12-20 14:02:18 INFO     	 * (global step 23100: loss: 0.316413052380085, lr: 0.0001
2023-12-20 14:02:26 INFO     	 * (global step 23150: loss: 0.25836367905139923, lr: 0.0001
2023-12-20 14:02:34 INFO     	 * (global step 23200: loss: 0.26430879533290863, lr: 0.0001
2023-12-20 14:02:42 INFO     	 * (global step 23250: loss: 0.141017634421587, lr: 0.0001
2023-12-20 14:02:50 INFO     	 * (global step 23300: loss: 0.24921534210443497, lr: 0.0001
2023-12-20 14:02:58 INFO     	 * (global step 23350: loss: 0.2611812502145767, lr: 0.0001
2023-12-20 14:03:06 INFO     	 * (global step 23400: loss: 0.33587557077407837, lr: 0.0001
2023-12-20 14:03:14 INFO     	 * (global step 23450: loss: 0.3543877378106117, lr: 0.0001
2023-12-20 14:03:22 INFO     	 * (global step 23500: loss: 0.22074054181575775, lr: 0.0001
2023-12-20 14:03:29 INFO     	 * (global step 23550: loss: 0.2740589454770088, lr: 0.0001
2023-12-20 14:03:37 INFO     	 * (global step 23600: loss: 0.3396383821964264, lr: 0.0001
2023-12-20 14:03:40 INFO     [epoch 4/15] average loss: 0.279, lr: 0.0001
2023-12-20 14:03:40 INFO     saving model related files
2023-12-20 14:03:40 INFO     saving model
2023-12-20 14:03:40 INFO     saving tokenizer
2023-12-20 14:03:40 INFO     saving optimizer
2023-12-20 14:03:41 INFO     remove old optimizer files
2023-12-20 14:03:47 INFO     	 * (global step 23650: loss: 0.39981672167778015, lr: 0.0001
2023-12-20 14:03:55 INFO     	 * (global step 23700: loss: 0.22026581317186356, lr: 0.0001
2023-12-20 14:04:03 INFO     	 * (global step 23750: loss: 0.2852151691913605, lr: 0.0001
2023-12-20 14:04:11 INFO     	 * (global step 23800: loss: 0.21686579659581184, lr: 0.0001
2023-12-20 14:04:19 INFO     	 * (global step 23850: loss: 0.3118506222963333, lr: 0.0001
2023-12-20 14:04:27 INFO     	 * (global step 23900: loss: 0.2416154444217682, lr: 0.0001
2023-12-20 14:04:35 INFO     	 * (global step 23950: loss: 0.21642886847257614, lr: 0.0001
2023-12-20 14:04:43 INFO     	 * (global step 24000: loss: 0.3231402486562729, lr: 0.0001
2023-12-20 14:04:51 INFO     	 * (global step 24050: loss: 0.41516707092523575, lr: 0.0001
2023-12-20 14:04:59 INFO     	 * (global step 24100: loss: 0.23989125341176987, lr: 0.0001
2023-12-20 14:05:07 INFO     	 * (global step 24150: loss: 0.3320179581642151, lr: 0.0001
2023-12-20 14:05:15 INFO     	 * (global step 24200: loss: 0.3246507942676544, lr: 0.0001
2023-12-20 14:05:23 INFO     	 * (global step 24250: loss: 0.39527249336242676, lr: 0.0001
2023-12-20 14:05:31 INFO     	 * (global step 24300: loss: 0.25887712091207504, lr: 0.0001
2023-12-20 14:05:39 INFO     	 * (global step 24350: loss: 0.34158919751644135, lr: 0.0001
2023-12-20 14:05:47 INFO     	 * (global step 24400: loss: 0.26784496009349823, lr: 0.0001
2023-12-20 14:05:55 INFO     	 * (global step 24450: loss: 0.3514682352542877, lr: 0.0001
2023-12-20 14:06:03 INFO     	 * (global step 24500: loss: 0.21868792921304703, lr: 0.0001
2023-12-20 14:06:11 INFO     	 * (global step 24550: loss: 0.3109421730041504, lr: 0.0001
2023-12-20 14:06:19 INFO     	 * (global step 24600: loss: 0.30236805230379105, lr: 0.0001
2023-12-20 14:06:27 INFO     	 * (global step 24650: loss: 0.19993148744106293, lr: 0.0001
2023-12-20 14:06:35 INFO     	 * (global step 24700: loss: 0.3079506531357765, lr: 0.0001
2023-12-20 14:06:43 INFO     	 * (global step 24750: loss: 0.25363386422395706, lr: 0.0001
2023-12-20 14:06:51 INFO     	 * (global step 24800: loss: 0.27459007501602173, lr: 0.0001
2023-12-20 14:06:59 INFO     	 * (global step 24850: loss: 0.324444979429245, lr: 0.0001
2023-12-20 14:07:06 INFO     	 * (global step 24900: loss: 0.31631772220134735, lr: 0.0001
2023-12-20 14:07:14 INFO     	 * (global step 24950: loss: 0.2634746581315994, lr: 0.0001
2023-12-20 14:07:22 INFO     	 * (global step 25000: loss: 0.3017366975545883, lr: 0.0001
2023-12-20 14:07:30 INFO     	 * (global step 25050: loss: 0.15697142481803894, lr: 0.0001
2023-12-20 14:07:38 INFO     	 * (global step 25100: loss: 0.3159841224551201, lr: 0.0001
2023-12-20 14:07:46 INFO     	 * (global step 25150: loss: 0.7295415103435516, lr: 0.0001
2023-12-20 14:07:54 INFO     	 * (global step 25200: loss: 0.28538039326667786, lr: 0.0001
2023-12-20 14:08:02 INFO     	 * (global step 25250: loss: 0.25393108278512955, lr: 0.0001
2023-12-20 14:08:10 INFO     	 * (global step 25300: loss: 0.30462557822465897, lr: 0.0001
2023-12-20 14:08:18 INFO     	 * (global step 25350: loss: 0.2599557191133499, lr: 0.0001
2023-12-20 14:08:26 INFO     	 * (global step 25400: loss: 0.24385125190019608, lr: 0.0001
2023-12-20 14:08:34 INFO     	 * (global step 25450: loss: 0.24537105858325958, lr: 0.0001
2023-12-20 14:08:42 INFO     	 * (global step 25500: loss: 0.29170694947242737, lr: 0.0001
2023-12-20 14:08:50 INFO     	 * (global step 25550: loss: 0.33766238391399384, lr: 0.0001
2023-12-20 14:08:58 INFO     	 * (global step 25600: loss: 0.2443147599697113, lr: 0.0001
2023-12-20 14:09:06 INFO     	 * (global step 25650: loss: 0.20914360135793686, lr: 0.0001
2023-12-20 14:09:14 INFO     	 * (global step 25700: loss: 0.3200403302907944, lr: 0.0001
2023-12-20 14:09:22 INFO     	 * (global step 25750: loss: 0.23306190222501755, lr: 0.0001
2023-12-20 14:09:30 INFO     	 * (global step 25800: loss: 0.2705727368593216, lr: 0.0001
2023-12-20 14:09:38 INFO     	 * (global step 25850: loss: 0.2558381035923958, lr: 0.0001
2023-12-20 14:09:46 INFO     	 * (global step 25900: loss: 0.5021561086177826, lr: 0.0001
2023-12-20 14:09:54 INFO     	 * (global step 25950: loss: 0.18160232156515121, lr: 0.0001
2023-12-20 14:10:02 INFO     	 * (global step 26000: loss: 0.32582077383995056, lr: 0.0001
2023-12-20 14:10:10 INFO     	 * (global step 26050: loss: 0.3937885910272598, lr: 0.0001
2023-12-20 14:10:18 INFO     	 * (global step 26100: loss: 0.30337242037057877, lr: 0.0001
2023-12-20 14:10:26 INFO     	 * (global step 26150: loss: 0.2540189027786255, lr: 0.0001
2023-12-20 14:10:34 INFO     	 * (global step 26200: loss: 0.4291854798793793, lr: 0.0001
2023-12-20 14:10:42 INFO     	 * (global step 26250: loss: 0.2430640384554863, lr: 0.0001
2023-12-20 14:10:50 INFO     	 * (global step 26300: loss: 0.365701362490654, lr: 0.0001
2023-12-20 14:10:58 INFO     	 * (global step 26350: loss: 0.21537934243679047, lr: 0.0001
2023-12-20 14:11:06 INFO     	 * (global step 26400: loss: 0.35451942682266235, lr: 0.0001
2023-12-20 14:11:14 INFO     	 * (global step 26450: loss: 0.23590098321437836, lr: 0.0001
2023-12-20 14:11:22 INFO     	 * (global step 26500: loss: 0.23336812108755112, lr: 0.0001
2023-12-20 14:11:30 INFO     	 * (global step 26550: loss: 0.2519474923610687, lr: 0.0001
2023-12-20 14:11:38 INFO     	 * (global step 26600: loss: 0.2050919309258461, lr: 0.0001
2023-12-20 14:11:46 INFO     	 * (global step 26650: loss: 0.3234940618276596, lr: 0.0001
2023-12-20 14:11:54 INFO     	 * (global step 26700: loss: 0.14117463678121567, lr: 0.0001
2023-12-20 14:12:02 INFO     	 * (global step 26750: loss: 0.20912519097328186, lr: 0.0001
2023-12-20 14:12:10 INFO     	 * (global step 26800: loss: 0.2953801453113556, lr: 0.0001
2023-12-20 14:12:18 INFO     	 * (global step 26850: loss: 0.1986789032816887, lr: 0.0001
2023-12-20 14:12:26 INFO     	 * (global step 26900: loss: 0.21453243494033813, lr: 0.0001
2023-12-20 14:12:34 INFO     	 * (global step 26950: loss: 0.2923752963542938, lr: 0.0001
2023-12-20 14:12:42 INFO     	 * (global step 27000: loss: 0.28754376620054245, lr: 0.0001
2023-12-20 14:12:50 INFO     	 * (global step 27050: loss: 0.183517724275589, lr: 0.0001
2023-12-20 14:12:58 INFO     	 * (global step 27100: loss: 0.2855837196111679, lr: 0.0001
2023-12-20 14:13:05 INFO     	 * (global step 27150: loss: 0.30053889751434326, lr: 0.0001
2023-12-20 14:13:13 INFO     	 * (global step 27200: loss: 0.4407432824373245, lr: 0.0001
2023-12-20 14:13:21 INFO     	 * (global step 27250: loss: 0.232603307813406, lr: 0.0001
2023-12-20 14:13:29 INFO     	 * (global step 27300: loss: 0.272098146378994, lr: 0.0001
2023-12-20 14:13:37 INFO     	 * (global step 27350: loss: 0.18129034340381622, lr: 0.0001
2023-12-20 14:13:45 INFO     	 * (global step 27400: loss: 0.2510508820414543, lr: 0.0001
2023-12-20 14:13:53 INFO     	 * (global step 27450: loss: 0.3104972690343857, lr: 0.0001
2023-12-20 14:14:01 INFO     	 * (global step 27500: loss: 0.2831896245479584, lr: 0.0001
2023-12-20 14:14:09 INFO     	 * (global step 27550: loss: 0.3829435259103775, lr: 0.0001
2023-12-20 14:14:17 INFO     	 * (global step 27600: loss: 0.25227194279432297, lr: 0.0001
2023-12-20 14:14:25 INFO     	 * (global step 27650: loss: 0.27106859534978867, lr: 0.0001
2023-12-20 14:14:33 INFO     	 * (global step 27700: loss: 0.2859691008925438, lr: 0.0001
2023-12-20 14:14:41 INFO     	 * (global step 27750: loss: 0.2144283875823021, lr: 0.0001
2023-12-20 14:14:49 INFO     	 * (global step 27800: loss: 0.2610223740339279, lr: 0.0001
2023-12-20 14:14:57 INFO     	 * (global step 27850: loss: 0.3346105217933655, lr: 0.0001
2023-12-20 14:15:05 INFO     	 * (global step 27900: loss: 0.2714061439037323, lr: 0.0001
2023-12-20 14:15:13 INFO     	 * (global step 27950: loss: 0.38865330815315247, lr: 0.0001
2023-12-20 14:15:21 INFO     	 * (global step 28000: loss: 0.30375704914331436, lr: 0.0001
2023-12-20 14:15:29 INFO     	 * (global step 28050: loss: 0.2646310031414032, lr: 0.0001
2023-12-20 14:15:37 INFO     	 * (global step 28100: loss: 0.3736805021762848, lr: 0.0001
2023-12-20 14:15:45 INFO     	 * (global step 28150: loss: 0.32493438571691513, lr: 0.0001
2023-12-20 14:15:53 INFO     	 * (global step 28200: loss: 0.4779650419950485, lr: 0.0001
2023-12-20 14:16:01 INFO     	 * (global step 28250: loss: 0.1876078024506569, lr: 0.0001
2023-12-20 14:16:09 INFO     	 * (global step 28300: loss: 0.13225648924708366, lr: 0.0001
2023-12-20 14:16:15 INFO     [epoch 5/15] average loss: 0.271, lr: 0.0001
2023-12-20 14:16:15 INFO     saving model related files
2023-12-20 14:16:15 INFO     saving model
2023-12-20 14:16:16 INFO     saving tokenizer
2023-12-20 14:16:16 INFO     saving optimizer
2023-12-20 14:16:17 INFO     remove old optimizer files
2023-12-20 14:16:18 INFO     	 * (global step 28350: loss: 0.30494435876607895, lr: 0.0001
2023-12-20 14:16:26 INFO     	 * (global step 28400: loss: 0.2864616811275482, lr: 0.0001
2023-12-20 14:16:34 INFO     	 * (global step 28450: loss: 0.14023136347532272, lr: 0.0001
2023-12-20 14:16:42 INFO     	 * (global step 28500: loss: 0.3065795600414276, lr: 0.0001
2023-12-20 14:16:50 INFO     	 * (global step 28550: loss: 0.14823468774557114, lr: 0.0001
2023-12-20 14:16:58 INFO     	 * (global step 28600: loss: 0.22434978932142258, lr: 0.0001
2023-12-20 14:17:06 INFO     	 * (global step 28650: loss: 0.3150828182697296, lr: 0.0001
2023-12-20 14:17:14 INFO     	 * (global step 28700: loss: 0.21916808933019638, lr: 0.0001
2023-12-20 14:17:22 INFO     	 * (global step 28750: loss: 0.17865049093961716, lr: 0.0001
2023-12-20 14:17:30 INFO     	 * (global step 28800: loss: 0.18358857929706573, lr: 0.0001
2023-12-20 14:17:38 INFO     	 * (global step 28850: loss: 0.2598281502723694, lr: 0.0001
2023-12-20 14:17:46 INFO     	 * (global step 28900: loss: 0.33871524035930634, lr: 0.0001
2023-12-20 14:17:54 INFO     	 * (global step 28950: loss: 0.17082183063030243, lr: 0.0001
2023-12-20 14:18:02 INFO     	 * (global step 29000: loss: 0.22138355672359467, lr: 0.0001
2023-12-20 14:18:10 INFO     	 * (global step 29050: loss: 0.19403044134378433, lr: 0.0001
2023-12-20 14:18:18 INFO     	 * (global step 29100: loss: 0.2629057914018631, lr: 0.0001
2023-12-20 14:18:26 INFO     	 * (global step 29150: loss: 0.29399508237838745, lr: 0.0001
2023-12-20 14:18:34 INFO     	 * (global step 29200: loss: 0.3674326092004776, lr: 0.0001
2023-12-20 14:18:42 INFO     	 * (global step 29250: loss: 0.21977247297763824, lr: 0.0001
2023-12-20 14:18:50 INFO     	 * (global step 29300: loss: 0.2058594822883606, lr: 0.0001
2023-12-20 14:18:58 INFO     	 * (global step 29350: loss: 0.26997070759534836, lr: 0.0001
2023-12-20 14:19:06 INFO     	 * (global step 29400: loss: 0.18322035297751427, lr: 0.0001
2023-12-20 14:19:14 INFO     	 * (global step 29450: loss: 0.28532078862190247, lr: 0.0001
2023-12-20 14:19:22 INFO     	 * (global step 29500: loss: 0.20141446590423584, lr: 0.0001
2023-12-20 14:19:30 INFO     	 * (global step 29550: loss: 0.23521729558706284, lr: 0.0001
2023-12-20 14:19:38 INFO     	 * (global step 29600: loss: 0.283493235707283, lr: 0.0001
2023-12-20 14:19:45 INFO     	 * (global step 29650: loss: 0.20434072613716125, lr: 0.0001
2023-12-20 14:19:53 INFO     	 * (global step 29700: loss: 0.23227432370185852, lr: 0.0001
2023-12-20 14:20:01 INFO     	 * (global step 29750: loss: 0.21953046321868896, lr: 0.0001
2023-12-20 14:20:09 INFO     	 * (global step 29800: loss: 0.5453434139490128, lr: 0.0001
2023-12-20 14:20:17 INFO     	 * (global step 29850: loss: 0.2047569714486599, lr: 0.0001
2023-12-20 14:20:25 INFO     	 * (global step 29900: loss: 0.3288632929325104, lr: 0.0001
2023-12-20 14:20:33 INFO     	 * (global step 29950: loss: 0.22645893692970276, lr: 0.0001
2023-12-20 14:20:41 INFO     	 * (global step 30000: loss: 0.1436513438820839, lr: 0.0001
2023-12-20 14:20:49 INFO     	 * (global step 30050: loss: 0.18178905546665192, lr: 0.0001
2023-12-20 14:20:57 INFO     	 * (global step 30100: loss: 0.2531168609857559, lr: 0.0001
2023-12-20 14:21:05 INFO     	 * (global step 30150: loss: 0.270131953060627, lr: 0.0001
2023-12-20 14:21:13 INFO     	 * (global step 30200: loss: 0.16913698986172676, lr: 0.0001
2023-12-20 14:21:21 INFO     	 * (global step 30250: loss: 0.27326008677482605, lr: 0.0001
2023-12-20 14:21:29 INFO     	 * (global step 30300: loss: 0.19358789175748825, lr: 0.0001
2023-12-20 14:21:37 INFO     	 * (global step 30350: loss: 0.23539484292268753, lr: 0.0001
2023-12-20 14:21:45 INFO     	 * (global step 30400: loss: 0.23971929401159286, lr: 0.0001
2023-12-20 14:21:53 INFO     	 * (global step 30450: loss: 0.27801502496004105, lr: 0.0001
2023-12-20 14:22:01 INFO     	 * (global step 30500: loss: 0.18274928629398346, lr: 0.0001
2023-12-20 14:22:09 INFO     	 * (global step 30550: loss: 0.2303961217403412, lr: 0.0001
2023-12-20 14:22:17 INFO     	 * (global step 30600: loss: 0.4613683670759201, lr: 0.0001
2023-12-20 14:22:25 INFO     	 * (global step 30650: loss: 0.21204660087823868, lr: 0.0001
2023-12-20 14:22:33 INFO     	 * (global step 30700: loss: 0.2545270472764969, lr: 0.0001
2023-12-20 14:22:41 INFO     	 * (global step 30750: loss: 0.24567219614982605, lr: 0.0001
2023-12-20 14:22:49 INFO     	 * (global step 30800: loss: 0.1651785522699356, lr: 0.0001
2023-12-20 14:22:57 INFO     	 * (global step 30850: loss: 0.19726008921861649, lr: 0.0001
2023-12-20 14:23:05 INFO     	 * (global step 30900: loss: 0.23434922844171524, lr: 0.0001
2023-12-20 14:23:13 INFO     	 * (global step 30950: loss: 0.3256928622722626, lr: 0.0001
2023-12-20 14:23:20 INFO     	 * (global step 31000: loss: 0.2623453512787819, lr: 0.0001
2023-12-20 14:23:28 INFO     	 * (global step 31050: loss: 0.24626397341489792, lr: 0.0001
2023-12-20 14:23:36 INFO     	 * (global step 31100: loss: 0.2433420568704605, lr: 0.0001
2023-12-20 14:23:44 INFO     	 * (global step 31150: loss: 0.20831311494112015, lr: 0.0001
2023-12-20 14:23:52 INFO     	 * (global step 31200: loss: 0.2870246022939682, lr: 0.0001
2023-12-20 14:24:00 INFO     	 * (global step 31250: loss: 0.23621252179145813, lr: 0.0001
2023-12-20 14:24:08 INFO     	 * (global step 31300: loss: 0.21913212537765503, lr: 0.0001
2023-12-20 14:24:16 INFO     	 * (global step 31350: loss: 0.32564152777194977, lr: 0.0001
2023-12-20 14:24:24 INFO     	 * (global step 31400: loss: 0.2368583083152771, lr: 0.0001
2023-12-20 14:24:32 INFO     	 * (global step 31450: loss: 0.2627502977848053, lr: 0.0001
2023-12-20 14:24:40 INFO     	 * (global step 31500: loss: 0.443256251513958, lr: 0.0001
2023-12-20 14:24:48 INFO     	 * (global step 31550: loss: 0.2127804458141327, lr: 0.0001
2023-12-20 14:24:56 INFO     	 * (global step 31600: loss: 0.22443663328886032, lr: 0.0001
2023-12-20 14:25:04 INFO     	 * (global step 31650: loss: 0.17479102313518524, lr: 0.0001
2023-12-20 14:25:12 INFO     	 * (global step 31700: loss: 0.275846928358078, lr: 0.0001
2023-12-20 14:25:20 INFO     	 * (global step 31750: loss: 0.380905419588089, lr: 0.0001
2023-12-20 14:25:28 INFO     	 * (global step 31800: loss: 0.1960434392094612, lr: 0.0001
2023-12-20 14:25:36 INFO     	 * (global step 31850: loss: 0.25135670602321625, lr: 0.0001
2023-12-20 14:25:44 INFO     	 * (global step 31900: loss: 0.2596695199608803, lr: 0.0001
2023-12-20 14:25:52 INFO     	 * (global step 31950: loss: 0.20357824116945267, lr: 0.0001
2023-12-20 14:26:00 INFO     	 * (global step 32000: loss: 0.20382293313741684, lr: 0.0001
2023-12-20 14:26:08 INFO     	 * (global step 32050: loss: 0.3102835863828659, lr: 0.0001
2023-12-20 14:26:16 INFO     	 * (global step 32100: loss: 0.2804695814847946, lr: 0.0001
2023-12-20 14:26:24 INFO     	 * (global step 32150: loss: 0.2767818346619606, lr: 0.0001
2023-12-20 14:26:32 INFO     	 * (global step 32200: loss: 0.2806825563311577, lr: 0.0001
2023-12-20 14:26:39 INFO     	 * (global step 32250: loss: 0.18658556789159775, lr: 0.0001
2023-12-20 14:26:47 INFO     	 * (global step 32300: loss: 0.2592295631766319, lr: 0.0001
2023-12-20 14:26:55 INFO     	 * (global step 32350: loss: 0.2402830794453621, lr: 0.0001
2023-12-20 14:27:03 INFO     	 * (global step 32400: loss: 0.17301412671804428, lr: 0.0001
2023-12-20 14:27:11 INFO     	 * (global step 32450: loss: 0.21896426379680634, lr: 0.0001
2023-12-20 14:27:19 INFO     	 * (global step 32500: loss: 0.2797045111656189, lr: 0.0001
2023-12-20 14:27:27 INFO     	 * (global step 32550: loss: 0.2802124544978142, lr: 0.0001
2023-12-20 14:27:35 INFO     	 * (global step 32600: loss: 0.15207930654287338, lr: 0.0001
2023-12-20 14:27:43 INFO     	 * (global step 32650: loss: 0.41725628077983856, lr: 0.0001
2023-12-20 14:27:51 INFO     	 * (global step 32700: loss: 0.23180902749300003, lr: 0.0001
2023-12-20 14:27:59 INFO     	 * (global step 32750: loss: 0.3127550110220909, lr: 0.0001
2023-12-20 14:28:07 INFO     	 * (global step 32800: loss: 0.18206530064344406, lr: 0.0001
2023-12-20 14:28:15 INFO     	 * (global step 32850: loss: 0.19812211394309998, lr: 0.0001
2023-12-20 14:28:23 INFO     	 * (global step 32900: loss: 0.3642219305038452, lr: 0.0001
2023-12-20 14:28:31 INFO     	 * (global step 32950: loss: 0.36062435805797577, lr: 0.0001
2023-12-20 14:28:39 INFO     	 * (global step 33000: loss: 0.21500489115715027, lr: 0.0001
2023-12-20 14:28:47 INFO     	 * (global step 33050: loss: 0.23407375812530518, lr: 0.0001
2023-12-20 14:28:49 INFO     [epoch 6/15] average loss: 0.264, lr: 0.0001
2023-12-20 14:28:49 INFO     saving model related files
2023-12-20 14:28:49 INFO     saving model
2023-12-20 14:28:49 INFO     saving tokenizer
2023-12-20 14:28:49 INFO     saving optimizer
2023-12-20 14:28:50 INFO     remove old optimizer files
2023-12-20 14:28:56 INFO     	 * (global step 33100: loss: 0.25825853645801544, lr: 0.0001
2023-12-20 14:29:04 INFO     	 * (global step 33150: loss: 0.2972260043025017, lr: 0.0001
2023-12-20 14:29:12 INFO     	 * (global step 33200: loss: 0.31830140948295593, lr: 0.0001
2023-12-20 14:29:20 INFO     	 * (global step 33250: loss: 0.2592499926686287, lr: 0.0001
2023-12-20 14:29:28 INFO     	 * (global step 33300: loss: 0.2190180942416191, lr: 0.0001
2023-12-20 14:29:36 INFO     	 * (global step 33350: loss: 0.2606041580438614, lr: 0.0001
2023-12-20 14:29:44 INFO     	 * (global step 33400: loss: 0.20109526813030243, lr: 0.0001
2023-12-20 14:29:52 INFO     	 * (global step 33450: loss: 0.24840154498815536, lr: 0.0001
2023-12-20 14:30:00 INFO     	 * (global step 33500: loss: 0.204464852809906, lr: 0.0001
2023-12-20 14:30:08 INFO     	 * (global step 33550: loss: 0.27518463134765625, lr: 0.0001
2023-12-20 14:30:16 INFO     	 * (global step 33600: loss: 0.24082612991333008, lr: 0.0001
2023-12-20 14:30:24 INFO     	 * (global step 33650: loss: 0.3287433832883835, lr: 0.0001
2023-12-20 14:30:32 INFO     	 * (global step 33700: loss: 0.1871819645166397, lr: 0.0001
2023-12-20 14:30:40 INFO     	 * (global step 33750: loss: 0.36553855240345, lr: 0.0001
2023-12-20 14:30:48 INFO     	 * (global step 33800: loss: 0.30957721918821335, lr: 0.0001
2023-12-20 14:30:56 INFO     	 * (global step 33850: loss: 0.26286935806274414, lr: 0.0001
2023-12-20 14:31:04 INFO     	 * (global step 33900: loss: 0.2646247148513794, lr: 0.0001
2023-12-20 14:31:12 INFO     	 * (global step 33950: loss: 0.25870149582624435, lr: 0.0001
2023-12-20 14:31:20 INFO     	 * (global step 34000: loss: 0.1973947435617447, lr: 0.0001
2023-12-20 14:31:28 INFO     	 * (global step 34050: loss: 0.2984907403588295, lr: 0.0001
2023-12-20 14:31:36 INFO     	 * (global step 34100: loss: 0.24506506323814392, lr: 0.0001
2023-12-20 14:31:44 INFO     	 * (global step 34150: loss: 0.4255058467388153, lr: 0.0001
2023-12-20 14:31:52 INFO     	 * (global step 34200: loss: 0.3267756998538971, lr: 0.0001
2023-12-20 14:32:00 INFO     	 * (global step 34250: loss: 0.25274277478456497, lr: 0.0001
2023-12-20 14:32:08 INFO     	 * (global step 34300: loss: 0.21789265424013138, lr: 0.0001
2023-12-20 14:32:16 INFO     	 * (global step 34350: loss: 0.15494507551193237, lr: 0.0001
2023-12-20 14:32:24 INFO     	 * (global step 34400: loss: 0.21779567748308182, lr: 0.0001
2023-12-20 14:32:32 INFO     	 * (global step 34450: loss: 0.30559349060058594, lr: 0.0001
2023-12-20 14:32:40 INFO     	 * (global step 34500: loss: 0.24552972614765167, lr: 0.0001
2023-12-20 14:32:48 INFO     	 * (global step 34550: loss: 0.26151642203330994, lr: 0.0001
2023-12-20 14:32:56 INFO     	 * (global step 34600: loss: 0.19935089349746704, lr: 0.0001
2023-12-20 14:33:04 INFO     	 * (global step 34650: loss: 0.20553383976221085, lr: 0.0001
2023-12-20 14:33:12 INFO     	 * (global step 34700: loss: 0.3125576004385948, lr: 0.0001
2023-12-20 14:33:19 INFO     	 * (global step 34750: loss: 0.2123267501592636, lr: 0.0001
2023-12-20 14:33:27 INFO     	 * (global step 34800: loss: 0.17701367288827896, lr: 0.0001
2023-12-20 14:33:35 INFO     	 * (global step 34850: loss: 0.23042617738246918, lr: 0.0001
2023-12-20 14:33:43 INFO     	 * (global step 34900: loss: 0.1847316101193428, lr: 0.0001
2023-12-20 14:33:51 INFO     	 * (global step 34950: loss: 0.2640499845147133, lr: 0.0001
2023-12-20 14:33:59 INFO     	 * (global step 35000: loss: 0.2587156370282173, lr: 0.0001
2023-12-20 14:34:07 INFO     	 * (global step 35050: loss: 0.38734786212444305, lr: 0.0001
2023-12-20 14:34:15 INFO     	 * (global step 35100: loss: 0.32193176448345184, lr: 0.0001
2023-12-20 14:34:23 INFO     	 * (global step 35150: loss: 0.2961084023118019, lr: 0.0001
2023-12-20 14:34:31 INFO     	 * (global step 35200: loss: 0.2613377422094345, lr: 0.0001
2023-12-20 14:34:39 INFO     	 * (global step 35250: loss: 0.28056828677654266, lr: 0.0001
2023-12-20 14:34:47 INFO     	 * (global step 35300: loss: 0.307781346142292, lr: 0.0001
2023-12-20 14:34:55 INFO     	 * (global step 35350: loss: 0.19481248781085014, lr: 0.0001
2023-12-20 14:35:03 INFO     	 * (global step 35400: loss: 0.18951642513275146, lr: 0.0001
2023-12-20 14:35:11 INFO     	 * (global step 35450: loss: 0.33184848725795746, lr: 0.0001
2023-12-20 14:35:19 INFO     	 * (global step 35500: loss: 0.27972362190485, lr: 0.0001
2023-12-20 14:35:27 INFO     	 * (global step 35550: loss: 0.15988001227378845, lr: 0.0001
2023-12-20 14:35:35 INFO     	 * (global step 35600: loss: 0.26314492523670197, lr: 0.0001
2023-12-20 14:35:43 INFO     	 * (global step 35650: loss: 0.288486547768116, lr: 0.0001
2023-12-20 14:35:51 INFO     	 * (global step 35700: loss: 0.30675017833709717, lr: 0.0001
2023-12-20 14:35:59 INFO     	 * (global step 35750: loss: 0.20771024748682976, lr: 0.0001
2023-12-20 14:36:07 INFO     	 * (global step 35800: loss: 0.2027532011270523, lr: 0.0001
2023-12-20 14:36:15 INFO     	 * (global step 35850: loss: 0.22551464289426804, lr: 0.0001
2023-12-20 14:36:23 INFO     	 * (global step 35900: loss: 0.24697339534759521, lr: 0.0001
2023-12-20 14:36:31 INFO     	 * (global step 35950: loss: 0.3557923659682274, lr: 0.0001
2023-12-20 14:36:39 INFO     	 * (global step 36000: loss: 0.20091427117586136, lr: 0.0001
2023-12-20 14:36:47 INFO     	 * (global step 36050: loss: 0.0885862335562706, lr: 0.0001
2023-12-20 14:36:55 INFO     	 * (global step 36100: loss: 0.3010246120393276, lr: 0.0001
2023-12-20 14:37:03 INFO     	 * (global step 36150: loss: 0.4355921745300293, lr: 0.0001
2023-12-20 14:37:11 INFO     	 * (global step 36200: loss: 0.31690292060375214, lr: 0.0001
2023-12-20 14:37:19 INFO     	 * (global step 36250: loss: 0.21203555166721344, lr: 0.0001
2023-12-20 14:37:27 INFO     	 * (global step 36300: loss: 0.2405427247285843, lr: 0.0001
2023-12-20 14:37:35 INFO     	 * (global step 36350: loss: 0.33375073224306107, lr: 0.0001
2023-12-20 14:37:43 INFO     	 * (global step 36400: loss: 0.2188713476061821, lr: 0.0001
2023-12-20 14:37:51 INFO     	 * (global step 36450: loss: 0.2102493792772293, lr: 0.0001
2023-12-20 14:37:59 INFO     	 * (global step 36500: loss: 0.39200247824192047, lr: 0.0001
2023-12-20 14:38:07 INFO     	 * (global step 36550: loss: 0.1803743615746498, lr: 0.0001
2023-12-20 14:38:15 INFO     	 * (global step 36600: loss: 0.43302659690380096, lr: 0.0001
2023-12-20 14:38:23 INFO     	 * (global step 36650: loss: 0.25640614330768585, lr: 0.0001
2023-12-20 14:38:31 INFO     	 * (global step 36700: loss: 0.2278258502483368, lr: 0.0001
2023-12-20 14:38:39 INFO     	 * (global step 36750: loss: 0.34290911257267, lr: 0.0001
2023-12-20 14:38:47 INFO     	 * (global step 36800: loss: 0.19701585173606873, lr: 0.0001
2023-12-20 14:38:55 INFO     	 * (global step 36850: loss: 0.18702863156795502, lr: 0.0001
2023-12-20 14:39:03 INFO     	 * (global step 36900: loss: 0.28385181725025177, lr: 0.0001
2023-12-20 14:39:11 INFO     	 * (global step 36950: loss: 0.2671339735388756, lr: 0.0001
2023-12-20 14:39:19 INFO     	 * (global step 37000: loss: 0.2689373940229416, lr: 0.0001
2023-12-20 14:39:27 INFO     	 * (global step 37050: loss: 0.19798557087779045, lr: 0.0001
2023-12-20 14:39:35 INFO     	 * (global step 37100: loss: 0.2756906896829605, lr: 0.0001
2023-12-20 14:39:43 INFO     	 * (global step 37150: loss: 0.1974811628460884, lr: 0.0001
2023-12-20 14:39:51 INFO     	 * (global step 37200: loss: 0.4363517612218857, lr: 0.0001
2023-12-20 14:39:59 INFO     	 * (global step 37250: loss: 0.19838453084230423, lr: 0.0001
2023-12-20 14:40:07 INFO     	 * (global step 37300: loss: 0.2621178552508354, lr: 0.0001
2023-12-20 14:40:15 INFO     	 * (global step 37350: loss: 0.12549539655447006, lr: 0.0001
2023-12-20 14:40:22 INFO     	 * (global step 37400: loss: 0.22267138212919235, lr: 0.0001
2023-12-20 14:40:30 INFO     	 * (global step 37450: loss: 0.31581179052591324, lr: 0.0001
2023-12-20 14:40:38 INFO     	 * (global step 37500: loss: 0.26118117570877075, lr: 0.0001
2023-12-20 14:40:46 INFO     	 * (global step 37550: loss: 0.20728909969329834, lr: 0.0001
2023-12-20 14:40:54 INFO     	 * (global step 37600: loss: 0.2078755870461464, lr: 0.0001
2023-12-20 14:41:02 INFO     	 * (global step 37650: loss: 0.14113473519682884, lr: 0.0001
2023-12-20 14:41:10 INFO     	 * (global step 37700: loss: 0.24531766027212143, lr: 0.0001
2023-12-20 14:41:18 INFO     	 * (global step 37750: loss: 0.16172730177640915, lr: 0.0001
2023-12-20 14:41:24 INFO     [epoch 7/15] average loss: 0.258, lr: 0.0001
2023-12-20 14:41:24 INFO     saving model related files
2023-12-20 14:41:24 INFO     saving model
2023-12-20 14:41:24 INFO     saving tokenizer
2023-12-20 14:41:24 INFO     saving optimizer
2023-12-20 14:41:25 INFO     remove old optimizer files
2023-12-20 14:41:28 INFO     	 * (global step 37800: loss: 0.34113098680973053, lr: 0.0001
2023-12-20 14:41:36 INFO     	 * (global step 37850: loss: 0.23376404494047165, lr: 0.0001
2023-12-20 14:41:44 INFO     	 * (global step 37900: loss: 0.24337977916002274, lr: 0.0001
2023-12-20 14:41:52 INFO     	 * (global step 37950: loss: 0.2513437867164612, lr: 0.0001
2023-12-20 14:42:00 INFO     	 * (global step 38000: loss: 0.2850319594144821, lr: 0.0001
2023-12-20 14:42:08 INFO     	 * (global step 38050: loss: 0.22999738156795502, lr: 0.0001
2023-12-20 14:42:16 INFO     	 * (global step 38100: loss: 0.2605184465646744, lr: 0.0001
2023-12-20 14:42:24 INFO     	 * (global step 38150: loss: 0.23751495778560638, lr: 0.0001
2023-12-20 14:42:31 INFO     	 * (global step 38200: loss: 0.3296380043029785, lr: 0.0001
2023-12-20 14:42:39 INFO     	 * (global step 38250: loss: 0.31787627190351486, lr: 0.0001
2023-12-20 14:42:47 INFO     	 * (global step 38300: loss: 0.16899258643388748, lr: 0.0001
2023-12-20 14:42:55 INFO     	 * (global step 38350: loss: 0.28564177453517914, lr: 0.0001
2023-12-20 14:43:03 INFO     	 * (global step 38400: loss: 0.29236239194869995, lr: 0.0001
2023-12-20 14:43:11 INFO     	 * (global step 38450: loss: 0.2905453145503998, lr: 0.0001
2023-12-20 14:43:19 INFO     	 * (global step 38500: loss: 0.21163499355316162, lr: 0.0001
2023-12-20 14:43:27 INFO     	 * (global step 38550: loss: 0.2755730152130127, lr: 0.0001
2023-12-20 14:43:35 INFO     	 * (global step 38600: loss: 0.29704269766807556, lr: 0.0001
2023-12-20 14:43:43 INFO     	 * (global step 38650: loss: 0.1297411471605301, lr: 0.0001
2023-12-20 14:43:51 INFO     	 * (global step 38700: loss: 0.22973952442407608, lr: 0.0001
2023-12-20 14:43:59 INFO     	 * (global step 38750: loss: 0.2451966255903244, lr: 0.0001
2023-12-20 14:44:07 INFO     	 * (global step 38800: loss: 0.1814197599887848, lr: 0.0001
2023-12-20 14:44:15 INFO     	 * (global step 38850: loss: 0.3809175491333008, lr: 0.0001
2023-12-20 14:44:23 INFO     	 * (global step 38900: loss: 0.2027003914117813, lr: 0.0001
2023-12-20 14:44:31 INFO     	 * (global step 38950: loss: 0.21208297461271286, lr: 0.0001
2023-12-20 14:44:39 INFO     	 * (global step 39000: loss: 0.23285043239593506, lr: 0.0001
2023-12-20 14:44:47 INFO     	 * (global step 39050: loss: 0.2582274377346039, lr: 0.0001
2023-12-20 14:44:55 INFO     	 * (global step 39100: loss: 0.19501355290412903, lr: 0.0001
2023-12-20 14:45:03 INFO     	 * (global step 39150: loss: 0.17646099627017975, lr: 0.0001
2023-12-20 14:45:11 INFO     	 * (global step 39200: loss: 0.2624596953392029, lr: 0.0001
2023-12-20 14:45:19 INFO     	 * (global step 39250: loss: 0.21953822672367096, lr: 0.0001
2023-12-20 14:45:27 INFO     	 * (global step 39300: loss: 0.1640595868229866, lr: 0.0001
2023-12-20 14:45:35 INFO     	 * (global step 39350: loss: 0.2111584171652794, lr: 0.0001
2023-12-20 14:45:43 INFO     	 * (global step 39400: loss: 0.18245212733745575, lr: 0.0001
2023-12-20 14:45:50 INFO     	 * (global step 39450: loss: 0.2054043710231781, lr: 0.0001
2023-12-20 14:45:58 INFO     	 * (global step 39500: loss: 0.19663812965154648, lr: 0.0001
2023-12-20 14:46:06 INFO     	 * (global step 39550: loss: 0.18709790706634521, lr: 0.0001
2023-12-20 14:46:14 INFO     	 * (global step 39600: loss: 0.2458503283560276, lr: 0.0001
2023-12-20 14:46:22 INFO     	 * (global step 39650: loss: 0.19410236924886703, lr: 0.0001
2023-12-20 14:46:30 INFO     	 * (global step 39700: loss: 0.19505725800991058, lr: 0.0001
2023-12-20 14:46:38 INFO     	 * (global step 39750: loss: 0.2858321964740753, lr: 0.0001
2023-12-20 14:46:46 INFO     	 * (global step 39800: loss: 0.3379174768924713, lr: 0.0001
2023-12-20 14:46:54 INFO     	 * (global step 39850: loss: 0.3278750032186508, lr: 0.0001
2023-12-20 14:47:02 INFO     	 * (global step 39900: loss: 0.3129340633749962, lr: 0.0001
2023-12-20 14:47:10 INFO     	 * (global step 39950: loss: 0.3230953961610794, lr: 0.0001
2023-12-20 14:47:18 INFO     	 * (global step 40000: loss: 0.18574495613574982, lr: 0.0001
2023-12-20 14:47:26 INFO     	 * (global step 40050: loss: 0.21673612296581268, lr: 0.0001
2023-12-20 14:47:34 INFO     	 * (global step 40100: loss: 0.24834495037794113, lr: 0.0001
2023-12-20 14:47:42 INFO     	 * (global step 40150: loss: 0.20899178087711334, lr: 0.0001
2023-12-20 14:47:50 INFO     	 * (global step 40200: loss: 0.27118464559316635, lr: 0.0001
2023-12-20 14:47:58 INFO     	 * (global step 40250: loss: 0.29890284687280655, lr: 0.0001
2023-12-20 14:48:06 INFO     	 * (global step 40300: loss: 0.24494519084692, lr: 0.0001
2023-12-20 14:48:14 INFO     	 * (global step 40350: loss: 0.24209559708833694, lr: 0.0001
2023-12-20 14:48:22 INFO     	 * (global step 40400: loss: 0.20382296293973923, lr: 0.0001
2023-12-20 14:48:30 INFO     	 * (global step 40450: loss: 0.1853606253862381, lr: 0.0001
2023-12-20 14:48:38 INFO     	 * (global step 40500: loss: 0.17334818840026855, lr: 0.0001
2023-12-20 14:48:46 INFO     	 * (global step 40550: loss: 0.20167915523052216, lr: 0.0001
2023-12-20 14:48:54 INFO     	 * (global step 40600: loss: 0.18641408532857895, lr: 0.0001
2023-12-20 14:49:02 INFO     	 * (global step 40650: loss: 0.27520890533924103, lr: 0.0001
2023-12-20 14:49:10 INFO     	 * (global step 40700: loss: 0.34853920340538025, lr: 0.0001
2023-12-20 14:49:18 INFO     	 * (global step 40750: loss: 0.22266389429569244, lr: 0.0001
2023-12-20 14:49:26 INFO     	 * (global step 40800: loss: 0.3066474050283432, lr: 0.0001
2023-12-20 14:49:34 INFO     	 * (global step 40850: loss: 0.2584924027323723, lr: 0.0001
2023-12-20 14:49:42 INFO     	 * (global step 40900: loss: 0.25423918664455414, lr: 0.0001
2023-12-20 14:49:50 INFO     	 * (global step 40950: loss: 0.17849933356046677, lr: 0.0001
2023-12-20 14:49:57 INFO     	 * (global step 41000: loss: 0.3328578993678093, lr: 0.0001
2023-12-20 14:50:05 INFO     	 * (global step 41050: loss: 0.22258329391479492, lr: 0.0001
2023-12-20 14:50:13 INFO     	 * (global step 41100: loss: 0.2224612683057785, lr: 0.0001
2023-12-20 14:50:21 INFO     	 * (global step 41150: loss: 0.2374282106757164, lr: 0.0001
2023-12-20 14:50:29 INFO     	 * (global step 41200: loss: 0.22934924066066742, lr: 0.0001
2023-12-20 14:50:37 INFO     	 * (global step 41250: loss: 0.1620149090886116, lr: 0.0001
2023-12-20 14:50:45 INFO     	 * (global step 41300: loss: 0.3058035746216774, lr: 0.0001
2023-12-20 14:50:53 INFO     	 * (global step 41350: loss: 0.20098061114549637, lr: 0.0001
2023-12-20 14:51:01 INFO     	 * (global step 41400: loss: 0.17258652299642563, lr: 0.0001
2023-12-20 14:51:09 INFO     	 * (global step 41450: loss: 0.3294881582260132, lr: 0.0001
2023-12-20 14:51:17 INFO     	 * (global step 41500: loss: 0.22568487375974655, lr: 0.0001
2023-12-20 14:51:25 INFO     	 * (global step 41550: loss: 0.258240707218647, lr: 0.0001
2023-12-20 14:51:33 INFO     	 * (global step 41600: loss: 0.3226526379585266, lr: 0.0001
2023-12-20 14:51:41 INFO     	 * (global step 41650: loss: 0.2303977906703949, lr: 0.0001
2023-12-20 14:51:49 INFO     	 * (global step 41700: loss: 0.3059329465031624, lr: 0.0001
2023-12-20 14:51:57 INFO     	 * (global step 41750: loss: 0.15320104360580444, lr: 0.0001
2023-12-20 14:52:05 INFO     	 * (global step 41800: loss: 0.2616029679775238, lr: 0.0001
2023-12-20 14:52:13 INFO     	 * (global step 41850: loss: 0.2777627110481262, lr: 0.0001
2023-12-20 14:52:21 INFO     	 * (global step 41900: loss: 0.2892392873764038, lr: 0.0001
2023-12-20 14:52:29 INFO     	 * (global step 41950: loss: 0.25794408470392227, lr: 0.0001
2023-12-20 14:52:37 INFO     	 * (global step 42000: loss: 0.17643895745277405, lr: 0.0001
2023-12-20 14:52:45 INFO     	 * (global step 42050: loss: 0.17653611302375793, lr: 0.0001
2023-12-20 14:52:53 INFO     	 * (global step 42100: loss: 0.27644288539886475, lr: 0.0001
2023-12-20 14:53:01 INFO     	 * (global step 42150: loss: 0.17639916390180588, lr: 0.0001
2023-12-20 14:53:09 INFO     	 * (global step 42200: loss: 0.2370661422610283, lr: 0.0001
2023-12-20 14:53:17 INFO     	 * (global step 42250: loss: 0.2754311263561249, lr: 0.0001
2023-12-20 14:53:25 INFO     	 * (global step 42300: loss: 0.22962404042482376, lr: 0.0001
2023-12-20 14:53:33 INFO     	 * (global step 42350: loss: 0.16081152856349945, lr: 0.0001
2023-12-20 14:53:41 INFO     	 * (global step 42400: loss: 0.272986501455307, lr: 0.0001
2023-12-20 14:53:49 INFO     	 * (global step 42450: loss: 0.3460436388850212, lr: 0.0001
2023-12-20 14:53:57 INFO     	 * (global step 42500: loss: 0.2008546143770218, lr: 0.0001
2023-12-20 14:53:58 INFO     [epoch 8/15] average loss: 0.252, lr: 0.0001
2023-12-20 14:53:58 INFO     saving model related files
2023-12-20 14:53:58 INFO     saving model
2023-12-20 14:53:58 INFO     saving tokenizer
2023-12-20 14:53:58 INFO     saving optimizer
2023-12-20 14:53:59 INFO     remove old optimizer files
2023-12-20 14:54:06 INFO     	 * (global step 42550: loss: 0.2535373643040657, lr: 0.0001
2023-12-20 14:54:14 INFO     	 * (global step 42600: loss: 0.2754538804292679, lr: 0.0001
2023-12-20 14:54:22 INFO     	 * (global step 42650: loss: 0.2994484603404999, lr: 0.0001
2023-12-20 14:54:30 INFO     	 * (global step 42700: loss: 0.17205040901899338, lr: 0.0001
2023-12-20 14:54:38 INFO     	 * (global step 42750: loss: 0.2563624158501625, lr: 0.0001
2023-12-20 14:54:46 INFO     	 * (global step 42800: loss: 0.45896948873996735, lr: 0.0001
2023-12-20 14:54:54 INFO     	 * (global step 42850: loss: 0.20467963814735413, lr: 0.0001
2023-12-20 14:55:02 INFO     	 * (global step 42900: loss: 0.258229598402977, lr: 0.0001
2023-12-20 14:55:10 INFO     	 * (global step 42950: loss: 0.25023020058870316, lr: 0.0001
2023-12-20 14:55:18 INFO     	 * (global step 43000: loss: 0.3090815916657448, lr: 0.0001
2023-12-20 14:55:26 INFO     	 * (global step 43050: loss: 0.24192117154598236, lr: 0.0001
2023-12-20 14:55:34 INFO     	 * (global step 43100: loss: 0.28268565237522125, lr: 0.0001
2023-12-20 14:55:42 INFO     	 * (global step 43150: loss: 0.23504668474197388, lr: 0.0001
2023-12-20 14:55:50 INFO     	 * (global step 43200: loss: 0.23361309617757797, lr: 0.0001
2023-12-20 14:55:58 INFO     	 * (global step 43250: loss: 0.19926414638757706, lr: 0.0001
2023-12-20 14:56:06 INFO     	 * (global step 43300: loss: 0.42574314773082733, lr: 0.0001
2023-12-20 14:56:14 INFO     	 * (global step 43350: loss: 0.2808857560157776, lr: 0.0001
2023-12-20 14:56:22 INFO     	 * (global step 43400: loss: 0.18422269821166992, lr: 0.0001
2023-12-20 14:56:30 INFO     	 * (global step 43450: loss: 0.223678357899189, lr: 0.0001
2023-12-20 14:56:38 INFO     	 * (global step 43500: loss: 0.35313764214515686, lr: 0.0001
2023-12-20 14:56:46 INFO     	 * (global step 43550: loss: 0.27177123725414276, lr: 0.0001
2023-12-20 14:56:54 INFO     	 * (global step 43600: loss: 0.2869601473212242, lr: 0.0001
2023-12-20 14:57:02 INFO     	 * (global step 43650: loss: 0.4325338304042816, lr: 0.0001
2023-12-20 14:57:10 INFO     	 * (global step 43700: loss: 0.23427635431289673, lr: 0.0001
2023-12-20 14:57:18 INFO     	 * (global step 43750: loss: 0.2790532037615776, lr: 0.0001
2023-12-20 14:57:26 INFO     	 * (global step 43800: loss: 0.20834416896104813, lr: 0.0001
2023-12-20 14:57:33 INFO     	 * (global step 43850: loss: 0.2517258748412132, lr: 0.0001
2023-12-20 14:57:41 INFO     	 * (global step 43900: loss: 0.3029553145170212, lr: 0.0001
2023-12-20 14:57:49 INFO     	 * (global step 43950: loss: 0.1903456300497055, lr: 0.0001
2023-12-20 14:57:57 INFO     	 * (global step 44000: loss: 0.29764725267887115, lr: 0.0001
2023-12-20 14:58:05 INFO     	 * (global step 44050: loss: 0.2205166220664978, lr: 0.0001
2023-12-20 14:58:13 INFO     	 * (global step 44100: loss: 0.18900496512651443, lr: 0.0001
2023-12-20 14:58:21 INFO     	 * (global step 44150: loss: 0.35424867272377014, lr: 0.0001
2023-12-20 14:58:29 INFO     	 * (global step 44200: loss: 0.1670159548521042, lr: 0.0001
2023-12-20 14:58:37 INFO     	 * (global step 44250: loss: 0.2793109267950058, lr: 0.0001
2023-12-20 14:58:45 INFO     	 * (global step 44300: loss: 0.5849491208791733, lr: 0.0001
2023-12-20 14:58:53 INFO     	 * (global step 44350: loss: 0.2138366401195526, lr: 0.0001
2023-12-20 14:59:01 INFO     	 * (global step 44400: loss: 0.16802917048335075, lr: 0.0001
2023-12-20 14:59:09 INFO     	 * (global step 44450: loss: 0.290192075073719, lr: 0.0001
2023-12-20 14:59:17 INFO     	 * (global step 44500: loss: 0.34121233969926834, lr: 0.0001
2023-12-20 14:59:25 INFO     	 * (global step 44550: loss: 0.14585710316896439, lr: 0.0001
2023-12-20 14:59:33 INFO     	 * (global step 44600: loss: 0.1790093034505844, lr: 0.0001
2023-12-20 14:59:41 INFO     	 * (global step 44650: loss: 0.3003520667552948, lr: 0.0001
2023-12-20 14:59:49 INFO     	 * (global step 44700: loss: 0.20902444422245026, lr: 0.0001
2023-12-20 14:59:57 INFO     	 * (global step 44750: loss: 0.16738679260015488, lr: 0.0001
2023-12-20 15:00:05 INFO     	 * (global step 44800: loss: 0.22841238230466843, lr: 0.0001
2023-12-20 15:00:13 INFO     	 * (global step 44850: loss: 0.2321968749165535, lr: 0.0001
2023-12-20 15:00:21 INFO     	 * (global step 44900: loss: 0.1904221922159195, lr: 0.0001
2023-12-20 15:00:29 INFO     	 * (global step 44950: loss: 0.2431403324007988, lr: 0.0001
2023-12-20 15:00:37 INFO     	 * (global step 45000: loss: 0.21034863591194153, lr: 0.0001
2023-12-20 15:00:45 INFO     	 * (global step 45050: loss: 0.23561633378267288, lr: 0.0001
2023-12-20 15:00:53 INFO     	 * (global step 45100: loss: 0.284078024327755, lr: 0.0001
2023-12-20 15:01:01 INFO     	 * (global step 45150: loss: 0.30364859104156494, lr: 0.0001
2023-12-20 15:01:09 INFO     	 * (global step 45200: loss: 0.17080378532409668, lr: 0.0001
2023-12-20 15:01:17 INFO     	 * (global step 45250: loss: 0.2632893770933151, lr: 0.0001
2023-12-20 15:01:25 INFO     	 * (global step 45300: loss: 0.18094078451395035, lr: 0.0001
2023-12-20 15:01:33 INFO     	 * (global step 45350: loss: 0.19085142016410828, lr: 0.0001
2023-12-20 15:01:41 INFO     	 * (global step 45400: loss: 0.2166171818971634, lr: 0.0001
2023-12-20 15:01:49 INFO     	 * (global step 45450: loss: 0.17678354680538177, lr: 0.0001
2023-12-20 15:01:57 INFO     	 * (global step 45500: loss: 0.17663685977458954, lr: 0.0001
2023-12-20 15:02:05 INFO     	 * (global step 45550: loss: 0.24048008769750595, lr: 0.0001
2023-12-20 15:02:13 INFO     	 * (global step 45600: loss: 0.23418285697698593, lr: 0.0001
2023-12-20 15:02:21 INFO     	 * (global step 45650: loss: 0.27968572825193405, lr: 0.0001
2023-12-20 15:02:29 INFO     	 * (global step 45700: loss: 0.20449700951576233, lr: 0.0001
2023-12-20 15:02:37 INFO     	 * (global step 45750: loss: 0.3928813934326172, lr: 0.0001
2023-12-20 15:02:45 INFO     	 * (global step 45800: loss: 0.2516218572854996, lr: 0.0001
2023-12-20 15:02:53 INFO     	 * (global step 45850: loss: 0.24357876181602478, lr: 0.0001
2023-12-20 15:03:01 INFO     	 * (global step 45900: loss: 0.39765147864818573, lr: 0.0001
2023-12-20 15:03:09 INFO     	 * (global step 45950: loss: 0.307277075946331, lr: 0.0001
2023-12-20 15:03:17 INFO     	 * (global step 46000: loss: 0.19429487735033035, lr: 0.0001
2023-12-20 15:03:25 INFO     	 * (global step 46050: loss: 0.20704755932092667, lr: 0.0001
2023-12-20 15:03:33 INFO     	 * (global step 46100: loss: 0.33811821043491364, lr: 0.0001
2023-12-20 15:03:41 INFO     	 * (global step 46150: loss: 0.1571236327290535, lr: 0.0001
2023-12-20 15:03:49 INFO     	 * (global step 46200: loss: 0.16942625120282173, lr: 0.0001
2023-12-20 15:03:57 INFO     	 * (global step 46250: loss: 0.21379832178354263, lr: 0.0001
2023-12-20 15:04:05 INFO     	 * (global step 46300: loss: 0.18118232488632202, lr: 0.0001
2023-12-20 15:04:12 INFO     	 * (global step 46350: loss: 0.24538473784923553, lr: 0.0001
2023-12-20 15:04:20 INFO     	 * (global step 46400: loss: 0.2554761990904808, lr: 0.0001
2023-12-20 15:04:28 INFO     	 * (global step 46450: loss: 0.32017982006073, lr: 0.0001
2023-12-20 15:04:36 INFO     	 * (global step 46500: loss: 0.14876124635338783, lr: 0.0001
2023-12-20 15:04:44 INFO     	 * (global step 46550: loss: 0.5179525762796402, lr: 0.0001
2023-12-20 15:04:52 INFO     	 * (global step 46600: loss: 0.2706458270549774, lr: 0.0001
2023-12-20 15:05:00 INFO     	 * (global step 46650: loss: 0.340962216258049, lr: 0.0001
2023-12-20 15:05:08 INFO     	 * (global step 46700: loss: 0.2865235507488251, lr: 0.0001
2023-12-20 15:05:16 INFO     	 * (global step 46750: loss: 0.25658778101205826, lr: 0.0001
2023-12-20 15:05:24 INFO     	 * (global step 46800: loss: 0.16164449602365494, lr: 0.0001
2023-12-20 15:05:32 INFO     	 * (global step 46850: loss: 0.387458935379982, lr: 0.0001
2023-12-20 15:05:40 INFO     	 * (global step 46900: loss: 0.16614501178264618, lr: 0.0001
2023-12-20 15:05:48 INFO     	 * (global step 46950: loss: 0.1878531500697136, lr: 0.0001
2023-12-20 15:05:56 INFO     	 * (global step 47000: loss: 0.3343411758542061, lr: 0.0001
2023-12-20 15:06:04 INFO     	 * (global step 47050: loss: 0.23720521479845047, lr: 0.0001
2023-12-20 15:06:12 INFO     	 * (global step 47100: loss: 0.2027878277003765, lr: 0.0001
2023-12-20 15:06:20 INFO     	 * (global step 47150: loss: 0.2064366713166237, lr: 0.0001
2023-12-20 15:06:28 INFO     	 * (global step 47200: loss: 0.20314102619886398, lr: 0.0001
2023-12-20 15:06:33 INFO     [epoch 9/15] average loss: 0.247, lr: 0.0001
2023-12-20 15:06:33 INFO     saving model related files
2023-12-20 15:06:33 INFO     saving model
2023-12-20 15:06:34 INFO     saving tokenizer
2023-12-20 15:06:34 INFO     saving optimizer
2023-12-20 15:06:35 INFO     remove old optimizer files
2023-12-20 15:06:35 INFO     complete training: model ckpt was saved at small_combined_trained_ckpt/model_eszyci
2023-12-20 15:06:35 INFO     ## 1st RUN: Configuration 2/12 ##
2023-12-20 15:06:35 INFO     initialize model trainer
2023-12-20 15:06:35 INFO     initialize checkpoint at small_combined_trained_ckpt/model_dpyopu
2023-12-20 15:06:35 INFO     hyperparameters
2023-12-20 15:06:35 INFO     	 * dataset_path: StellarMilk/squad_newsqa
2023-12-20 15:06:35 INFO     	 * dataset_name: default
2023-12-20 15:06:35 INFO     	 * input_types: ['paragraph']
2023-12-20 15:06:35 INFO     	 * output_types: ['questions_answers']
2023-12-20 15:06:35 INFO     	 * prefix_types: ['qag']
2023-12-20 15:06:35 INFO     	 * model: t5-small
2023-12-20 15:06:35 INFO     	 * max_length: 512
2023-12-20 15:06:35 INFO     	 * max_length_output: 512
2023-12-20 15:06:35 INFO     	 * epoch: 15
2023-12-20 15:06:35 INFO     	 * batch: 2
2023-12-20 15:06:35 INFO     	 * lr: 0.0001
2023-12-20 15:06:35 INFO     	 * fp16: False
2023-12-20 15:06:35 INFO     	 * random_seed: 1
2023-12-20 15:06:35 INFO     	 * gradient_accumulation_steps: 4
2023-12-20 15:06:35 INFO     	 * label_smoothing: 0.0
2023-12-20 15:06:35 INFO     initialize checkpoint with t5-small
2023-12-20 15:06:36 INFO     use spaCy answer extraction model: positionrank
2023-12-20 15:06:36 INFO     Model `t5-small`
2023-12-20 15:06:36 INFO     	 * Num of GPU in use: 1
2023-12-20 15:06:36 INFO     	 * Prefix: True
2023-12-20 15:06:36 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 15:06:36 INFO     dataset preprocessing
2023-12-20 15:06:38 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/squad_newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-20 15:06:40 INFO     start model training
2023-12-20 15:06:55 INFO     	 * (global step 50: loss: 0.794698640704155, lr: 0.0001
2023-12-20 15:07:11 INFO     	 * (global step 100: loss: 0.8158146739006042, lr: 0.0001
2023-12-20 15:07:26 INFO     	 * (global step 150: loss: 0.5092844516038895, lr: 0.0001
2023-12-20 15:07:41 INFO     	 * (global step 200: loss: 0.48985590040683746, lr: 0.0001
2023-12-20 15:07:57 INFO     	 * (global step 250: loss: 0.5685335397720337, lr: 0.0001
2023-12-20 15:08:13 INFO     	 * (global step 300: loss: 0.4591766595840454, lr: 0.0001
2023-12-20 15:08:28 INFO     	 * (global step 350: loss: 0.49949783831834793, lr: 0.0001
2023-12-20 15:08:43 INFO     	 * (global step 400: loss: 0.32772936671972275, lr: 0.0001
2023-12-20 15:08:59 INFO     	 * (global step 450: loss: 0.38464515656232834, lr: 0.0001
2023-12-20 15:09:14 INFO     	 * (global step 500: loss: 0.3578670769929886, lr: 0.0001
2023-12-20 15:09:30 INFO     	 * (global step 550: loss: 0.42995817959308624, lr: 0.0001
2023-12-20 15:09:45 INFO     	 * (global step 600: loss: 0.4401578903198242, lr: 0.0001
2023-12-20 15:10:01 INFO     	 * (global step 650: loss: 0.3297661617398262, lr: 0.0001
2023-12-20 15:10:16 INFO     	 * (global step 700: loss: 0.3470190726220608, lr: 0.0001
2023-12-20 15:10:32 INFO     	 * (global step 750: loss: 0.46080951392650604, lr: 0.0001
2023-12-20 15:10:47 INFO     	 * (global step 800: loss: 0.36384502053260803, lr: 0.0001
2023-12-20 15:11:03 INFO     	 * (global step 850: loss: 0.3340333253145218, lr: 0.0001
2023-12-20 15:11:19 INFO     	 * (global step 900: loss: 0.34148379415273666, lr: 0.0001
2023-12-20 15:11:34 INFO     	 * (global step 950: loss: 0.3397870473563671, lr: 0.0001
2023-12-20 15:11:50 INFO     	 * (global step 1000: loss: 0.362209677696228, lr: 0.0001
2023-12-20 15:12:05 INFO     	 * (global step 1050: loss: 0.3599195182323456, lr: 0.0001
2023-12-20 15:12:21 INFO     	 * (global step 1100: loss: 0.38159240037202835, lr: 0.0001
2023-12-20 15:12:36 INFO     	 * (global step 1150: loss: 0.5759630724787712, lr: 0.0001
2023-12-20 15:12:52 INFO     	 * (global step 1200: loss: 0.3516113795340061, lr: 0.0001
2023-12-20 15:13:07 INFO     	 * (global step 1250: loss: 0.30679260566830635, lr: 0.0001
2023-12-20 15:13:23 INFO     	 * (global step 1300: loss: 0.3484619930386543, lr: 0.0001
2023-12-20 15:13:38 INFO     	 * (global step 1350: loss: 0.30273113772273064, lr: 0.0001
2023-12-20 15:13:54 INFO     	 * (global step 1400: loss: 0.3991699293255806, lr: 0.0001
2023-12-20 15:14:09 INFO     	 * (global step 1450: loss: 0.3522299453616142, lr: 0.0001
2023-12-20 15:14:25 INFO     	 * (global step 1500: loss: 0.44122854992747307, lr: 0.0001
2023-12-20 15:14:40 INFO     	 * (global step 1550: loss: 0.2924797087907791, lr: 0.0001
2023-12-20 15:14:56 INFO     	 * (global step 1600: loss: 0.29544708877801895, lr: 0.0001
2023-12-20 15:15:11 INFO     	 * (global step 1650: loss: 0.4055129364132881, lr: 0.0001
2023-12-20 15:15:27 INFO     	 * (global step 1700: loss: 0.3162122517824173, lr: 0.0001
2023-12-20 15:15:43 INFO     	 * (global step 1750: loss: 0.3017149865627289, lr: 0.0001
2023-12-20 15:15:58 INFO     	 * (global step 1800: loss: 0.30947345495224, lr: 0.0001
2023-12-20 15:16:14 INFO     	 * (global step 1850: loss: 0.24946700781583786, lr: 0.0001
2023-12-20 15:16:29 INFO     	 * (global step 1900: loss: 0.3348980024456978, lr: 0.0001
2023-12-20 15:16:45 INFO     	 * (global step 1950: loss: 0.30491098016500473, lr: 0.0001
2023-12-20 15:17:00 INFO     	 * (global step 2000: loss: 0.25571754574775696, lr: 0.0001
2023-12-20 15:17:16 INFO     	 * (global step 2050: loss: 0.2758674770593643, lr: 0.0001
2023-12-20 15:17:31 INFO     	 * (global step 2100: loss: 0.2771972827613354, lr: 0.0001
2023-12-20 15:17:47 INFO     	 * (global step 2150: loss: 0.419622078537941, lr: 0.0001
2023-12-20 15:18:02 INFO     	 * (global step 2200: loss: 0.4423096254467964, lr: 0.0001
2023-12-20 15:18:18 INFO     	 * (global step 2250: loss: 0.28373606503009796, lr: 0.0001
2023-12-20 15:18:33 INFO     	 * (global step 2300: loss: 0.3657923713326454, lr: 0.0001
2023-12-20 15:18:49 INFO     	 * (global step 2350: loss: 0.34129631146788597, lr: 0.0001
2023-12-20 15:18:53 INFO     [epoch 0/15] average loss: 0.436, lr: 0.0001
2023-12-20 15:18:53 INFO     saving model related files
2023-12-20 15:18:53 INFO     saving model
2023-12-20 15:18:53 INFO     saving tokenizer
2023-12-20 15:18:53 INFO     saving optimizer
2023-12-20 15:18:54 INFO     remove old optimizer files
2023-12-20 15:19:06 INFO     	 * (global step 2400: loss: 0.32220079004764557, lr: 0.0001
2023-12-20 15:19:21 INFO     	 * (global step 2450: loss: 0.3519979864358902, lr: 0.0001
2023-12-20 15:19:37 INFO     	 * (global step 2500: loss: 0.33408186212182045, lr: 0.0001
2023-12-20 15:19:52 INFO     	 * (global step 2550: loss: 0.31032033264636993, lr: 0.0001
2023-12-20 15:20:08 INFO     	 * (global step 2600: loss: 0.3125217743217945, lr: 0.0001
2023-12-20 15:20:23 INFO     	 * (global step 2650: loss: 0.5425296351313591, lr: 0.0001
2023-12-20 15:20:39 INFO     	 * (global step 2700: loss: 0.3850136175751686, lr: 0.0001
2023-12-20 15:20:54 INFO     	 * (global step 2750: loss: 0.2798687443137169, lr: 0.0001
2023-12-20 15:21:10 INFO     	 * (global step 2800: loss: 0.42221230268478394, lr: 0.0001
2023-12-20 15:21:25 INFO     	 * (global step 2850: loss: 0.4185946062207222, lr: 0.0001
2023-12-20 15:21:41 INFO     	 * (global step 2900: loss: 0.40989673137664795, lr: 0.0001
2023-12-20 15:21:56 INFO     	 * (global step 2950: loss: 0.3224378377199173, lr: 0.0001
2023-12-20 15:22:12 INFO     	 * (global step 3000: loss: 0.3858010545372963, lr: 0.0001
2023-12-20 15:22:27 INFO     	 * (global step 3050: loss: 0.35008156672120094, lr: 0.0001
2023-12-20 15:22:42 INFO     	 * (global step 3100: loss: 0.33248430490493774, lr: 0.0001
2023-12-20 15:22:58 INFO     	 * (global step 3150: loss: 0.47973456233739853, lr: 0.0001
2023-12-20 15:23:13 INFO     	 * (global step 3200: loss: 0.3016412705183029, lr: 0.0001
2023-12-20 15:23:29 INFO     	 * (global step 3250: loss: 0.3782372921705246, lr: 0.0001
2023-12-20 15:23:44 INFO     	 * (global step 3300: loss: 0.3296907916665077, lr: 0.0001
2023-12-20 15:24:00 INFO     	 * (global step 3350: loss: 0.2881235219538212, lr: 0.0001
2023-12-20 15:24:15 INFO     	 * (global step 3400: loss: 0.2685377597808838, lr: 0.0001
2023-12-20 15:24:31 INFO     	 * (global step 3450: loss: 0.3674388974905014, lr: 0.0001
2023-12-20 15:24:46 INFO     	 * (global step 3500: loss: 0.48867956548929214, lr: 0.0001
2023-12-20 15:25:02 INFO     	 * (global step 3550: loss: 0.31830860674381256, lr: 0.0001
2023-12-20 15:25:17 INFO     	 * (global step 3600: loss: 0.35436491668224335, lr: 0.0001
2023-12-20 15:25:33 INFO     	 * (global step 3650: loss: 0.31908390671014786, lr: 0.0001
2023-12-20 15:25:48 INFO     	 * (global step 3700: loss: 0.3549884781241417, lr: 0.0001
2023-12-20 15:26:04 INFO     	 * (global step 3750: loss: 0.33602409437298775, lr: 0.0001
2023-12-20 15:26:19 INFO     	 * (global step 3800: loss: 0.25316664949059486, lr: 0.0001
2023-12-20 15:26:34 INFO     	 * (global step 3850: loss: 0.34346091747283936, lr: 0.0001
2023-12-20 15:26:50 INFO     	 * (global step 3900: loss: 0.27555112913250923, lr: 0.0001
2023-12-20 15:27:05 INFO     	 * (global step 3950: loss: 0.34766818583011627, lr: 0.0001
2023-12-20 15:27:21 INFO     	 * (global step 4000: loss: 0.2862633913755417, lr: 0.0001
2023-12-20 15:27:36 INFO     	 * (global step 4050: loss: 0.39706095308065414, lr: 0.0001
2023-12-20 15:27:52 INFO     	 * (global step 4100: loss: 0.2631145939230919, lr: 0.0001
2023-12-20 15:28:07 INFO     	 * (global step 4150: loss: 0.2628025487065315, lr: 0.0001
2023-12-20 15:28:23 INFO     	 * (global step 4200: loss: 0.35479169338941574, lr: 0.0001
2023-12-20 15:28:38 INFO     	 * (global step 4250: loss: 0.3913527838885784, lr: 0.0001
2023-12-20 15:28:54 INFO     	 * (global step 4300: loss: 0.28558509051799774, lr: 0.0001
2023-12-20 15:29:09 INFO     	 * (global step 4350: loss: 0.26326051726937294, lr: 0.0001
2023-12-20 15:29:25 INFO     	 * (global step 4400: loss: 0.29415087401866913, lr: 0.0001
2023-12-20 15:29:40 INFO     	 * (global step 4450: loss: 0.262018371373415, lr: 0.0001
2023-12-20 15:29:56 INFO     	 * (global step 4500: loss: 0.264301136136055, lr: 0.0001
2023-12-20 15:30:11 INFO     	 * (global step 4550: loss: 0.32391639053821564, lr: 0.0001
2023-12-20 15:30:27 INFO     	 * (global step 4600: loss: 0.21828724816441536, lr: 0.0001
2023-12-20 15:30:42 INFO     	 * (global step 4650: loss: 0.3331867344677448, lr: 0.0001
2023-12-20 15:30:58 INFO     	 * (global step 4700: loss: 0.3040569983422756, lr: 0.0001
2023-12-20 15:31:05 INFO     [epoch 1/15] average loss: 0.333, lr: 0.0001
2023-12-20 15:31:05 INFO     saving model related files
2023-12-20 15:31:05 INFO     saving model
2023-12-20 15:31:05 INFO     saving tokenizer
2023-12-20 15:31:06 INFO     saving optimizer
2023-12-20 15:31:06 INFO     remove old optimizer files
2023-12-20 15:31:15 INFO     	 * (global step 4750: loss: 0.4743799492716789, lr: 0.0001
2023-12-20 15:31:31 INFO     	 * (global step 4800: loss: 0.28612641245126724, lr: 0.0001
2023-12-20 15:31:46 INFO     	 * (global step 4850: loss: 0.242323137819767, lr: 0.0001
2023-12-20 15:32:02 INFO     	 * (global step 4900: loss: 0.2775350734591484, lr: 0.0001
2023-12-20 15:32:17 INFO     	 * (global step 4950: loss: 0.28253724053502083, lr: 0.0001
2023-12-20 15:32:33 INFO     	 * (global step 5000: loss: 0.4605114310979843, lr: 0.0001
2023-12-20 15:32:48 INFO     	 * (global step 5050: loss: 0.3534008339047432, lr: 0.0001
2023-12-20 15:33:04 INFO     	 * (global step 5100: loss: 0.25451648607850075, lr: 0.0001
2023-12-20 15:33:19 INFO     	 * (global step 5150: loss: 0.30367235839366913, lr: 0.0001
2023-12-20 15:33:35 INFO     	 * (global step 5200: loss: 0.36998898535966873, lr: 0.0001
2023-12-20 15:33:50 INFO     	 * (global step 5250: loss: 0.2979753576219082, lr: 0.0001
2023-12-20 15:34:06 INFO     	 * (global step 5300: loss: 0.322780542075634, lr: 0.0001
2023-12-20 15:34:21 INFO     	 * (global step 5350: loss: 0.2674420177936554, lr: 0.0001
2023-12-20 15:34:37 INFO     	 * (global step 5400: loss: 0.21772386506199837, lr: 0.0001
2023-12-20 15:34:52 INFO     	 * (global step 5450: loss: 0.4315117672085762, lr: 0.0001
2023-12-20 15:35:08 INFO     	 * (global step 5500: loss: 0.41965071856975555, lr: 0.0001
2023-12-20 15:35:23 INFO     	 * (global step 5550: loss: 0.2781887762248516, lr: 0.0001
2023-12-20 15:35:39 INFO     	 * (global step 5600: loss: 0.24984923005104065, lr: 0.0001
2023-12-20 15:35:54 INFO     	 * (global step 5650: loss: 0.2629379481077194, lr: 0.0001
2023-12-20 15:36:10 INFO     	 * (global step 5700: loss: 0.4250052943825722, lr: 0.0001
2023-12-20 15:36:25 INFO     	 * (global step 5750: loss: 0.35296111553907394, lr: 0.0001
2023-12-20 15:36:41 INFO     	 * (global step 5800: loss: 0.3252169191837311, lr: 0.0001
2023-12-20 15:36:57 INFO     	 * (global step 5850: loss: 0.37009841203689575, lr: 0.0001
2023-12-20 15:37:12 INFO     	 * (global step 5900: loss: 0.316234327852726, lr: 0.0001
2023-12-20 15:37:28 INFO     	 * (global step 5950: loss: 0.37720517069101334, lr: 0.0001
2023-12-20 15:37:43 INFO     	 * (global step 6000: loss: 0.36206046119332314, lr: 0.0001
2023-12-20 15:37:59 INFO     	 * (global step 6050: loss: 0.2820034362375736, lr: 0.0001
2023-12-20 15:38:14 INFO     	 * (global step 6100: loss: 0.3602571561932564, lr: 0.0001
2023-12-20 15:38:30 INFO     	 * (global step 6150: loss: 0.3613818734884262, lr: 0.0001
2023-12-20 15:38:45 INFO     	 * (global step 6200: loss: 0.28925982490181923, lr: 0.0001
2023-12-20 15:39:01 INFO     	 * (global step 6250: loss: 0.25868910551071167, lr: 0.0001
2023-12-20 15:39:16 INFO     	 * (global step 6300: loss: 0.357315294444561, lr: 0.0001
2023-12-20 15:39:32 INFO     	 * (global step 6350: loss: 0.38496094197034836, lr: 0.0001
2023-12-20 15:39:47 INFO     	 * (global step 6400: loss: 0.2727784737944603, lr: 0.0001
2023-12-20 15:40:03 INFO     	 * (global step 6450: loss: 0.30949050188064575, lr: 0.0001
2023-12-20 15:40:18 INFO     	 * (global step 6500: loss: 0.2983803078532219, lr: 0.0001
2023-12-20 15:40:34 INFO     	 * (global step 6550: loss: 0.41341155767440796, lr: 0.0001
2023-12-20 15:40:49 INFO     	 * (global step 6600: loss: 0.3885060325264931, lr: 0.0001
2023-12-20 15:41:05 INFO     	 * (global step 6650: loss: 0.29920003563165665, lr: 0.0001
2023-12-20 15:41:20 INFO     	 * (global step 6700: loss: 0.303361427038908, lr: 0.0001
2023-12-20 15:41:36 INFO     	 * (global step 6750: loss: 0.36094309017062187, lr: 0.0001
2023-12-20 15:41:51 INFO     	 * (global step 6800: loss: 0.30268706008791924, lr: 0.0001
2023-12-20 15:42:07 INFO     	 * (global step 6850: loss: 0.2471390888094902, lr: 0.0001
2023-12-20 15:42:23 INFO     	 * (global step 6900: loss: 0.38052524253726006, lr: 0.0001
2023-12-20 15:42:38 INFO     	 * (global step 6950: loss: 0.28337113931775093, lr: 0.0001
2023-12-20 15:42:54 INFO     	 * (global step 7000: loss: 0.28006818890571594, lr: 0.0001
2023-12-20 15:43:09 INFO     	 * (global step 7050: loss: 0.29546940326690674, lr: 0.0001
2023-12-20 15:43:20 INFO     [epoch 2/15] average loss: 0.314, lr: 0.0001
2023-12-20 15:43:20 INFO     saving model related files
2023-12-20 15:43:20 INFO     saving model
2023-12-20 15:43:20 INFO     saving tokenizer
2023-12-20 15:43:20 INFO     saving optimizer
2023-12-20 15:43:22 INFO     remove old optimizer files
2023-12-20 15:43:27 INFO     	 * (global step 7100: loss: 0.26735008135437965, lr: 0.0001
2023-12-20 15:43:42 INFO     	 * (global step 7150: loss: 0.42639103904366493, lr: 0.0001
2023-12-20 15:43:58 INFO     	 * (global step 7200: loss: 0.3046858534216881, lr: 0.0001
2023-12-20 15:44:13 INFO     	 * (global step 7250: loss: 0.23654332011938095, lr: 0.0001
2023-12-20 15:44:29 INFO     	 * (global step 7300: loss: 0.4119284451007843, lr: 0.0001
2023-12-20 15:44:45 INFO     	 * (global step 7350: loss: 0.26185332983732224, lr: 0.0001
2023-12-20 15:45:00 INFO     	 * (global step 7400: loss: 0.3289632089436054, lr: 0.0001
2023-12-20 15:45:16 INFO     	 * (global step 7450: loss: 0.256744135171175, lr: 0.0001
2023-12-20 15:45:31 INFO     	 * (global step 7500: loss: 0.2654450982809067, lr: 0.0001
2023-12-20 15:45:47 INFO     	 * (global step 7550: loss: 0.31848692893981934, lr: 0.0001
2023-12-20 15:46:02 INFO     	 * (global step 7600: loss: 0.3086092323064804, lr: 0.0001
2023-12-20 15:46:18 INFO     	 * (global step 7650: loss: 0.26864271610975266, lr: 0.0001
2023-12-20 15:46:33 INFO     	 * (global step 7700: loss: 0.3414244130253792, lr: 0.0001
2023-12-20 15:46:49 INFO     	 * (global step 7750: loss: 0.3125448524951935, lr: 0.0001
2023-12-20 15:47:04 INFO     	 * (global step 7800: loss: 0.2596990503370762, lr: 0.0001
2023-12-20 15:47:20 INFO     	 * (global step 7850: loss: 0.23763268440961838, lr: 0.0001
2023-12-20 15:47:35 INFO     	 * (global step 7900: loss: 0.24257349595427513, lr: 0.0001
2023-12-20 15:47:51 INFO     	 * (global step 7950: loss: 0.23198523372411728, lr: 0.0001
2023-12-20 15:48:06 INFO     	 * (global step 8000: loss: 0.32308872044086456, lr: 0.0001
2023-12-20 15:48:22 INFO     	 * (global step 8050: loss: 0.2656877897679806, lr: 0.0001
2023-12-20 15:48:37 INFO     	 * (global step 8100: loss: 0.4199715442955494, lr: 0.0001
2023-12-20 15:48:53 INFO     	 * (global step 8150: loss: 0.2506383880972862, lr: 0.0001
2023-12-20 15:49:08 INFO     	 * (global step 8200: loss: 0.27415624260902405, lr: 0.0001
2023-12-20 15:49:24 INFO     	 * (global step 8250: loss: 0.2923671007156372, lr: 0.0001
2023-12-20 15:49:40 INFO     	 * (global step 8300: loss: 0.2640255056321621, lr: 0.0001
2023-12-20 15:49:55 INFO     	 * (global step 8350: loss: 0.34015386551618576, lr: 0.0001
2023-12-20 15:50:11 INFO     	 * (global step 8400: loss: 0.2899046540260315, lr: 0.0001
2023-12-20 15:50:26 INFO     	 * (global step 8450: loss: 0.291726965457201, lr: 0.0001
2023-12-20 15:50:42 INFO     	 * (global step 8500: loss: 0.29617438837885857, lr: 0.0001
2023-12-20 15:50:57 INFO     	 * (global step 8550: loss: 0.23272128775715828, lr: 0.0001
2023-12-20 15:51:13 INFO     	 * (global step 8600: loss: 0.3375900164246559, lr: 0.0001
2023-12-20 15:51:28 INFO     	 * (global step 8650: loss: 0.4255778379738331, lr: 0.0001
2023-12-20 15:51:44 INFO     	 * (global step 8700: loss: 0.34871598705649376, lr: 0.0001
2023-12-20 15:52:00 INFO     	 * (global step 8750: loss: 0.2308386228978634, lr: 0.0001
2023-12-20 15:52:15 INFO     	 * (global step 8800: loss: 0.30984029918909073, lr: 0.0001
2023-12-20 15:52:31 INFO     	 * (global step 8850: loss: 0.2931140214204788, lr: 0.0001
2023-12-20 15:52:46 INFO     	 * (global step 8900: loss: 0.2331311460584402, lr: 0.0001
2023-12-20 15:53:02 INFO     	 * (global step 8950: loss: 0.2816232368350029, lr: 0.0001
2023-12-20 15:53:17 INFO     	 * (global step 9000: loss: 0.32486873865127563, lr: 0.0001
2023-12-20 15:53:33 INFO     	 * (global step 9050: loss: 0.27955763041973114, lr: 0.0001
2023-12-20 15:53:48 INFO     	 * (global step 9100: loss: 0.2993909865617752, lr: 0.0001
2023-12-20 15:54:04 INFO     	 * (global step 9150: loss: 0.2571081444621086, lr: 0.0001
2023-12-20 15:54:19 INFO     	 * (global step 9200: loss: 0.2419983223080635, lr: 0.0001
2023-12-20 15:54:35 INFO     	 * (global step 9250: loss: 0.20178828574717045, lr: 0.0001
2023-12-20 15:54:50 INFO     	 * (global step 9300: loss: 0.24090223386883736, lr: 0.0001
2023-12-20 15:55:06 INFO     	 * (global step 9350: loss: 0.30274274200201035, lr: 0.0001
2023-12-20 15:55:21 INFO     	 * (global step 9400: loss: 0.33377840369939804, lr: 0.0001
2023-12-20 15:55:35 INFO     [epoch 3/15] average loss: 0.301, lr: 0.0001
2023-12-20 15:55:35 INFO     saving model related files
2023-12-20 15:55:35 INFO     saving model
2023-12-20 15:55:36 INFO     saving tokenizer
2023-12-20 15:55:36 INFO     saving optimizer
2023-12-20 15:55:37 INFO     remove old optimizer files
2023-12-20 15:55:38 INFO     	 * (global step 9450: loss: 0.2306361123919487, lr: 0.0001
2023-12-20 15:55:54 INFO     	 * (global step 9500: loss: 0.368644155561924, lr: 0.0001
2023-12-20 15:56:09 INFO     	 * (global step 9550: loss: 0.29723208025097847, lr: 0.0001
2023-12-20 15:56:25 INFO     	 * (global step 9600: loss: 0.3680744469165802, lr: 0.0001
2023-12-20 15:56:41 INFO     	 * (global step 9650: loss: 0.29484302550554276, lr: 0.0001
2023-12-20 15:56:56 INFO     	 * (global step 9700: loss: 0.27184395119547844, lr: 0.0001
2023-12-20 15:57:12 INFO     	 * (global step 9750: loss: 0.2659461833536625, lr: 0.0001
2023-12-20 15:57:27 INFO     	 * (global step 9800: loss: 0.2607880160212517, lr: 0.0001
2023-12-20 15:57:43 INFO     	 * (global step 9850: loss: 0.24633964896202087, lr: 0.0001
2023-12-20 15:57:58 INFO     	 * (global step 9900: loss: 0.3415997177362442, lr: 0.0001
2023-12-20 15:58:14 INFO     	 * (global step 9950: loss: 0.3708193376660347, lr: 0.0001
2023-12-20 15:58:29 INFO     	 * (global step 10000: loss: 0.2465841993689537, lr: 0.0001
2023-12-20 15:58:45 INFO     	 * (global step 10050: loss: 0.41812875866889954, lr: 0.0001
2023-12-20 15:59:00 INFO     	 * (global step 10100: loss: 0.21324950829148293, lr: 0.0001
2023-12-20 15:59:16 INFO     	 * (global step 10150: loss: 0.40309975296258926, lr: 0.0001
2023-12-20 15:59:31 INFO     	 * (global step 10200: loss: 0.3538661040365696, lr: 0.0001
2023-12-20 15:59:47 INFO     	 * (global step 10250: loss: 0.19044896587729454, lr: 0.0001
2023-12-20 16:00:02 INFO     	 * (global step 10300: loss: 0.24244306981563568, lr: 0.0001
2023-12-20 16:00:18 INFO     	 * (global step 10350: loss: 0.34641698375344276, lr: 0.0001
2023-12-20 16:00:33 INFO     	 * (global step 10400: loss: 0.21581478044390678, lr: 0.0001
2023-12-20 16:00:49 INFO     	 * (global step 10450: loss: 0.37193620949983597, lr: 0.0001
2023-12-20 16:01:04 INFO     	 * (global step 10500: loss: 0.32558927685022354, lr: 0.0001
2023-12-20 16:01:20 INFO     	 * (global step 10550: loss: 0.2667055279016495, lr: 0.0001
2023-12-20 16:01:35 INFO     	 * (global step 10600: loss: 0.316746324300766, lr: 0.0001
2023-12-20 16:01:51 INFO     	 * (global step 10650: loss: 0.30861596390604973, lr: 0.0001
2023-12-20 16:02:06 INFO     	 * (global step 10700: loss: 0.42267896980047226, lr: 0.0001
2023-12-20 16:02:22 INFO     	 * (global step 10750: loss: 0.4446251168847084, lr: 0.0001
2023-12-20 16:02:37 INFO     	 * (global step 10800: loss: 0.26104601100087166, lr: 0.0001
2023-12-20 16:02:53 INFO     	 * (global step 10850: loss: 0.27269813790917397, lr: 0.0001
2023-12-20 16:03:08 INFO     	 * (global step 10900: loss: 0.2943039536476135, lr: 0.0001
2023-12-20 16:03:24 INFO     	 * (global step 10950: loss: 0.17876771837472916, lr: 0.0001
2023-12-20 16:03:39 INFO     	 * (global step 11000: loss: 0.28435633331537247, lr: 0.0001
2023-12-20 16:03:55 INFO     	 * (global step 11050: loss: 0.316412840038538, lr: 0.0001
2023-12-20 16:04:10 INFO     	 * (global step 11100: loss: 0.23709474876523018, lr: 0.0001
2023-12-20 16:04:26 INFO     	 * (global step 11150: loss: 0.251188050955534, lr: 0.0001
2023-12-20 16:04:41 INFO     	 * (global step 11200: loss: 0.3402763418853283, lr: 0.0001
2023-12-20 16:04:57 INFO     	 * (global step 11250: loss: 0.35046662762761116, lr: 0.0001
2023-12-20 16:05:12 INFO     	 * (global step 11300: loss: 0.2774139642715454, lr: 0.0001
2023-12-20 16:05:28 INFO     	 * (global step 11350: loss: 0.28885577619075775, lr: 0.0001
2023-12-20 16:05:43 INFO     	 * (global step 11400: loss: 0.24490999802947044, lr: 0.0001
2023-12-20 16:05:59 INFO     	 * (global step 11450: loss: 0.2750055268406868, lr: 0.0001
2023-12-20 16:06:14 INFO     	 * (global step 11500: loss: 0.21591392904520035, lr: 0.0001
2023-12-20 16:06:30 INFO     	 * (global step 11550: loss: 0.29298005998134613, lr: 0.0001
2023-12-20 16:06:45 INFO     	 * (global step 11600: loss: 0.32462434098124504, lr: 0.0001
2023-12-20 16:07:01 INFO     	 * (global step 11650: loss: 0.3111067973077297, lr: 0.0001
2023-12-20 16:07:16 INFO     	 * (global step 11700: loss: 0.32593970745801926, lr: 0.0001
2023-12-20 16:07:32 INFO     	 * (global step 11750: loss: 0.27469387650489807, lr: 0.0001
2023-12-20 16:07:47 INFO     	 * (global step 11800: loss: 0.35244496911764145, lr: 0.0001
2023-12-20 16:07:49 INFO     [epoch 4/15] average loss: 0.291, lr: 0.0001
2023-12-20 16:07:49 INFO     saving model related files
2023-12-20 16:07:49 INFO     saving model
2023-12-20 16:07:50 INFO     saving tokenizer
2023-12-20 16:07:50 INFO     saving optimizer
2023-12-20 16:07:51 INFO     remove old optimizer files
2023-12-20 16:08:05 INFO     	 * (global step 11850: loss: 0.397550992667675, lr: 0.0001
2023-12-20 16:08:20 INFO     	 * (global step 11900: loss: 0.3010600805282593, lr: 0.0001
2023-12-20 16:08:36 INFO     	 * (global step 11950: loss: 0.2657768651843071, lr: 0.0001
2023-12-20 16:08:51 INFO     	 * (global step 12000: loss: 0.2984388880431652, lr: 0.0001
2023-12-20 16:09:07 INFO     	 * (global step 12050: loss: 0.2874509394168854, lr: 0.0001
2023-12-20 16:09:22 INFO     	 * (global step 12100: loss: 0.3442595899105072, lr: 0.0001
2023-12-20 16:09:38 INFO     	 * (global step 12150: loss: 0.34044282883405685, lr: 0.0001
2023-12-20 16:09:53 INFO     	 * (global step 12200: loss: 0.34013159573078156, lr: 0.0001
2023-12-20 16:10:09 INFO     	 * (global step 12250: loss: 0.22736457362771034, lr: 0.0001
2023-12-20 16:10:24 INFO     	 * (global step 12300: loss: 0.299767404794693, lr: 0.0001
2023-12-20 16:10:40 INFO     	 * (global step 12350: loss: 0.23718631640076637, lr: 0.0001
2023-12-20 16:10:55 INFO     	 * (global step 12400: loss: 0.3067733906209469, lr: 0.0001
2023-12-20 16:11:11 INFO     	 * (global step 12450: loss: 0.2812284305691719, lr: 0.0001
2023-12-20 16:11:26 INFO     	 * (global step 12500: loss: 0.22798864357173443, lr: 0.0001
2023-12-20 16:11:42 INFO     	 * (global step 12550: loss: 0.3094985857605934, lr: 0.0001
2023-12-20 16:11:57 INFO     	 * (global step 12600: loss: 0.3309333994984627, lr: 0.0001
2023-12-20 16:12:13 INFO     	 * (global step 12650: loss: 0.2833055928349495, lr: 0.0001
2023-12-20 16:12:28 INFO     	 * (global step 12700: loss: 0.29347480461001396, lr: 0.0001
2023-12-20 16:12:44 INFO     	 * (global step 12750: loss: 0.27481988817453384, lr: 0.0001
2023-12-20 16:12:59 INFO     	 * (global step 12800: loss: 0.3099144771695137, lr: 0.0001
2023-12-20 16:13:15 INFO     	 * (global step 12850: loss: 0.2031956948339939, lr: 0.0001
2023-12-20 16:13:30 INFO     	 * (global step 12900: loss: 0.28555772453546524, lr: 0.0001
2023-12-20 16:13:46 INFO     	 * (global step 12950: loss: 0.22975089401006699, lr: 0.0001
2023-12-20 16:14:01 INFO     	 * (global step 13000: loss: 0.3479439839720726, lr: 0.0001
2023-12-20 16:14:17 INFO     	 * (global step 13050: loss: 0.2158633917570114, lr: 0.0001
2023-12-20 16:14:32 INFO     	 * (global step 13100: loss: 0.2748282589018345, lr: 0.0001
2023-12-20 16:14:48 INFO     	 * (global step 13150: loss: 0.26097943633794785, lr: 0.0001
2023-12-20 16:15:03 INFO     	 * (global step 13200: loss: 0.2268790863454342, lr: 0.0001
2023-12-20 16:15:19 INFO     	 * (global step 13250: loss: 0.2758730798959732, lr: 0.0001
2023-12-20 16:15:34 INFO     	 * (global step 13300: loss: 0.34892909601330757, lr: 0.0001
2023-12-20 16:15:50 INFO     	 * (global step 13350: loss: 0.28610897436738014, lr: 0.0001
2023-12-20 16:16:05 INFO     	 * (global step 13400: loss: 0.29486948251724243, lr: 0.0001
2023-12-20 16:16:21 INFO     	 * (global step 13450: loss: 0.26001979783177376, lr: 0.0001
2023-12-20 16:16:36 INFO     	 * (global step 13500: loss: 0.22170386090874672, lr: 0.0001
2023-12-20 16:16:52 INFO     	 * (global step 13550: loss: 0.3186596632003784, lr: 0.0001
2023-12-20 16:17:07 INFO     	 * (global step 13600: loss: 0.3214953690767288, lr: 0.0001
2023-12-20 16:17:23 INFO     	 * (global step 13650: loss: 0.2872379384934902, lr: 0.0001
2023-12-20 16:17:38 INFO     	 * (global step 13700: loss: 0.3301205337047577, lr: 0.0001
2023-12-20 16:17:54 INFO     	 * (global step 13750: loss: 0.19089411944150925, lr: 0.0001
2023-12-20 16:18:09 INFO     	 * (global step 13800: loss: 0.29973988980054855, lr: 0.0001
2023-12-20 16:18:25 INFO     	 * (global step 13850: loss: 0.3123796693980694, lr: 0.0001
2023-12-20 16:18:41 INFO     	 * (global step 13900: loss: 0.3156430013477802, lr: 0.0001
2023-12-20 16:18:56 INFO     	 * (global step 13950: loss: 0.36395326629281044, lr: 0.0001
2023-12-20 16:19:12 INFO     	 * (global step 14000: loss: 0.2860645242035389, lr: 0.0001
2023-12-20 16:19:27 INFO     	 * (global step 14050: loss: 0.26120808720588684, lr: 0.0001
2023-12-20 16:19:43 INFO     	 * (global step 14100: loss: 0.35797078162431717, lr: 0.0001
2023-12-20 16:19:58 INFO     	 * (global step 14150: loss: 0.30805156752467155, lr: 0.0001
2023-12-20 16:20:03 INFO     [epoch 5/15] average loss: 0.284, lr: 0.0001
2023-12-20 16:20:03 INFO     saving model related files
2023-12-20 16:20:03 INFO     saving model
2023-12-20 16:20:04 INFO     saving tokenizer
2023-12-20 16:20:04 INFO     saving optimizer
2023-12-20 16:20:05 INFO     remove old optimizer files
2023-12-20 16:20:15 INFO     	 * (global step 14200: loss: 0.3243229500949383, lr: 0.0001
2023-12-20 16:20:31 INFO     	 * (global step 14250: loss: 0.32747844606637955, lr: 0.0001
2023-12-20 16:20:46 INFO     	 * (global step 14300: loss: 0.28909366950392723, lr: 0.0001
2023-12-20 16:21:02 INFO     	 * (global step 14350: loss: 0.2658029831945896, lr: 0.0001
2023-12-20 16:21:17 INFO     	 * (global step 14400: loss: 0.3384033255279064, lr: 0.0001
2023-12-20 16:21:33 INFO     	 * (global step 14450: loss: 0.25982842594385147, lr: 0.0001
2023-12-20 16:21:48 INFO     	 * (global step 14500: loss: 0.35240892693400383, lr: 0.0001
2023-12-20 16:22:04 INFO     	 * (global step 14550: loss: 0.3347499594092369, lr: 0.0001
2023-12-20 16:22:19 INFO     	 * (global step 14600: loss: 0.26602066680788994, lr: 0.0001
2023-12-20 16:22:35 INFO     	 * (global step 14650: loss: 0.2840142846107483, lr: 0.0001
2023-12-20 16:22:50 INFO     	 * (global step 14700: loss: 0.23039864003658295, lr: 0.0001
2023-12-20 16:23:06 INFO     	 * (global step 14750: loss: 0.3099052980542183, lr: 0.0001
2023-12-20 16:23:21 INFO     	 * (global step 14800: loss: 0.3101281560957432, lr: 0.0001
2023-12-20 16:23:37 INFO     	 * (global step 14850: loss: 0.25714149698615074, lr: 0.0001
2023-12-20 16:23:52 INFO     	 * (global step 14900: loss: 0.23760640248656273, lr: 0.0001
2023-12-20 16:24:08 INFO     	 * (global step 14950: loss: 0.35595183819532394, lr: 0.0001
2023-12-20 16:24:23 INFO     	 * (global step 15000: loss: 0.3122466281056404, lr: 0.0001
2023-12-20 16:24:39 INFO     	 * (global step 15050: loss: 0.28230585902929306, lr: 0.0001
2023-12-20 16:24:54 INFO     	 * (global step 15100: loss: 0.2869769148528576, lr: 0.0001
2023-12-20 16:25:10 INFO     	 * (global step 15150: loss: 0.2247777208685875, lr: 0.0001
2023-12-20 16:25:26 INFO     	 * (global step 15200: loss: 0.3381291702389717, lr: 0.0001
2023-12-20 16:25:41 INFO     	 * (global step 15250: loss: 0.1844586282968521, lr: 0.0001
2023-12-20 16:25:57 INFO     	 * (global step 15300: loss: 0.20849664136767387, lr: 0.0001
2023-12-20 16:26:12 INFO     	 * (global step 15350: loss: 0.1982424221932888, lr: 0.0001
2023-12-20 16:26:28 INFO     	 * (global step 15400: loss: 0.2828752398490906, lr: 0.0001
2023-12-20 16:26:43 INFO     	 * (global step 15450: loss: 0.2748861685395241, lr: 0.0001
2023-12-20 16:26:59 INFO     	 * (global step 15500: loss: 0.29030678421258926, lr: 0.0001
2023-12-20 16:27:14 INFO     	 * (global step 15550: loss: 0.23059413209557533, lr: 0.0001
2023-12-20 16:27:30 INFO     	 * (global step 15600: loss: 0.2544834278523922, lr: 0.0001
2023-12-20 16:27:45 INFO     	 * (global step 15650: loss: 0.2850787565112114, lr: 0.0001
2023-12-20 16:28:01 INFO     	 * (global step 15700: loss: 0.25854117423295975, lr: 0.0001
2023-12-20 16:28:16 INFO     	 * (global step 15750: loss: 0.286511093378067, lr: 0.0001
2023-12-20 16:28:32 INFO     	 * (global step 15800: loss: 0.2580495122820139, lr: 0.0001
2023-12-20 16:28:47 INFO     	 * (global step 15850: loss: 0.3291594311594963, lr: 0.0001
2023-12-20 16:29:03 INFO     	 * (global step 15900: loss: 0.2921023294329643, lr: 0.0001
2023-12-20 16:29:18 INFO     	 * (global step 15950: loss: 0.23735458962619305, lr: 0.0001
2023-12-20 16:29:34 INFO     	 * (global step 16000: loss: 0.20947272703051567, lr: 0.0001
2023-12-20 16:29:49 INFO     	 * (global step 16050: loss: 0.22744100913405418, lr: 0.0001
2023-12-20 16:30:05 INFO     	 * (global step 16100: loss: 0.28969164192676544, lr: 0.0001
2023-12-20 16:30:20 INFO     	 * (global step 16150: loss: 0.2648211158812046, lr: 0.0001
2023-12-20 16:30:36 INFO     	 * (global step 16200: loss: 0.22785421833395958, lr: 0.0001
2023-12-20 16:30:51 INFO     	 * (global step 16250: loss: 0.25519610568881035, lr: 0.0001
2023-12-20 16:31:07 INFO     	 * (global step 16300: loss: 0.28295033425092697, lr: 0.0001
2023-12-20 16:31:22 INFO     	 * (global step 16350: loss: 0.29985906556248665, lr: 0.0001
2023-12-20 16:31:38 INFO     	 * (global step 16400: loss: 0.21443424373865128, lr: 0.0001
2023-12-20 16:31:53 INFO     	 * (global step 16450: loss: 0.3260040730237961, lr: 0.0001
2023-12-20 16:32:09 INFO     	 * (global step 16500: loss: 0.2191232368350029, lr: 0.0001
2023-12-20 16:32:17 INFO     [epoch 6/15] average loss: 0.277, lr: 0.0001
2023-12-20 16:32:17 INFO     saving model related files
2023-12-20 16:32:17 INFO     saving model
2023-12-20 16:32:18 INFO     saving tokenizer
2023-12-20 16:32:18 INFO     saving optimizer
2023-12-20 16:32:19 INFO     remove old optimizer files
2023-12-20 16:32:26 INFO     	 * (global step 16550: loss: 0.311431348323822, lr: 0.0001
2023-12-20 16:32:41 INFO     	 * (global step 16600: loss: 0.20643962919712067, lr: 0.0001
2023-12-20 16:32:57 INFO     	 * (global step 16650: loss: 0.24480168521404266, lr: 0.0001
2023-12-20 16:33:12 INFO     	 * (global step 16700: loss: 0.28725095093250275, lr: 0.0001
2023-12-20 16:33:28 INFO     	 * (global step 16750: loss: 0.30810751765966415, lr: 0.0001
2023-12-20 16:33:43 INFO     	 * (global step 16800: loss: 0.30752822756767273, lr: 0.0001
2023-12-20 16:33:59 INFO     	 * (global step 16850: loss: 0.2516601160168648, lr: 0.0001
2023-12-20 16:34:14 INFO     	 * (global step 16900: loss: 0.23863399773836136, lr: 0.0001
2023-12-20 16:34:30 INFO     	 * (global step 16950: loss: 0.3437451086938381, lr: 0.0001
2023-12-20 16:34:45 INFO     	 * (global step 17000: loss: 0.2373247742652893, lr: 0.0001
2023-12-20 16:35:01 INFO     	 * (global step 17050: loss: 0.34652645140886307, lr: 0.0001
2023-12-20 16:35:16 INFO     	 * (global step 17100: loss: 0.3965378925204277, lr: 0.0001
2023-12-20 16:35:32 INFO     	 * (global step 17150: loss: 0.25654174387454987, lr: 0.0001
2023-12-20 16:35:47 INFO     	 * (global step 17200: loss: 0.2117989882826805, lr: 0.0001
2023-12-20 16:36:03 INFO     	 * (global step 17250: loss: 0.3182353600859642, lr: 0.0001
2023-12-20 16:36:18 INFO     	 * (global step 17300: loss: 0.2903923951089382, lr: 0.0001
2023-12-20 16:36:34 INFO     	 * (global step 17350: loss: 0.40127794072031975, lr: 0.0001
2023-12-20 16:36:49 INFO     	 * (global step 17400: loss: 0.2495391145348549, lr: 0.0001
2023-12-20 16:37:05 INFO     	 * (global step 17450: loss: 0.21892603486776352, lr: 0.0001
2023-12-20 16:37:21 INFO     	 * (global step 17500: loss: 0.25060872733592987, lr: 0.0001
2023-12-20 16:37:36 INFO     	 * (global step 17550: loss: 0.255885012447834, lr: 0.0001
2023-12-20 16:37:52 INFO     	 * (global step 17600: loss: 0.2805434390902519, lr: 0.0001
2023-12-20 16:38:07 INFO     	 * (global step 17650: loss: 0.274538267403841, lr: 0.0001
2023-12-20 16:38:23 INFO     	 * (global step 17700: loss: 0.32267458736896515, lr: 0.0001
2023-12-20 16:38:38 INFO     	 * (global step 17750: loss: 0.2127838134765625, lr: 0.0001
2023-12-20 16:38:54 INFO     	 * (global step 17800: loss: 0.2246692106127739, lr: 0.0001
2023-12-20 16:39:09 INFO     	 * (global step 17850: loss: 0.2431018464267254, lr: 0.0001
2023-12-20 16:39:25 INFO     	 * (global step 17900: loss: 0.2245984897017479, lr: 0.0001
2023-12-20 16:39:40 INFO     	 * (global step 17950: loss: 0.27818556129932404, lr: 0.0001
2023-12-20 16:39:56 INFO     	 * (global step 18000: loss: 0.2719857208430767, lr: 0.0001
2023-12-20 16:40:11 INFO     	 * (global step 18050: loss: 0.26991256326436996, lr: 0.0001
2023-12-20 16:40:27 INFO     	 * (global step 18100: loss: 0.2380734160542488, lr: 0.0001
2023-12-20 16:40:42 INFO     	 * (global step 18150: loss: 0.2809135131537914, lr: 0.0001
2023-12-20 16:40:58 INFO     	 * (global step 18200: loss: 0.2695728428661823, lr: 0.0001
2023-12-20 16:41:13 INFO     	 * (global step 18250: loss: 0.166603721678257, lr: 0.0001
2023-12-20 16:41:29 INFO     	 * (global step 18300: loss: 0.3439089059829712, lr: 0.0001
2023-12-20 16:41:44 INFO     	 * (global step 18350: loss: 0.40637853741645813, lr: 0.0001
2023-12-20 16:42:00 INFO     	 * (global step 18400: loss: 0.24314631894230843, lr: 0.0001
2023-12-20 16:42:15 INFO     	 * (global step 18450: loss: 0.18052418157458305, lr: 0.0001
2023-12-20 16:42:31 INFO     	 * (global step 18500: loss: 0.34149548783898354, lr: 0.0001
2023-12-20 16:42:46 INFO     	 * (global step 18550: loss: 0.18449593521654606, lr: 0.0001
2023-12-20 16:43:02 INFO     	 * (global step 18600: loss: 0.24255840107798576, lr: 0.0001
2023-12-20 16:43:17 INFO     	 * (global step 18650: loss: 0.22848133742809296, lr: 0.0001
2023-12-20 16:43:33 INFO     	 * (global step 18700: loss: 0.3663252852857113, lr: 0.0001
2023-12-20 16:43:48 INFO     	 * (global step 18750: loss: 0.40421993657946587, lr: 0.0001
2023-12-20 16:44:04 INFO     	 * (global step 18800: loss: 0.25788069888949394, lr: 0.0001
2023-12-20 16:44:19 INFO     	 * (global step 18850: loss: 0.18800209276378155, lr: 0.0001
2023-12-20 16:44:31 INFO     [epoch 7/15] average loss: 0.271, lr: 0.0001
2023-12-20 16:44:31 INFO     saving model related files
2023-12-20 16:44:31 INFO     saving model
2023-12-20 16:44:32 INFO     saving tokenizer
2023-12-20 16:44:32 INFO     saving optimizer
2023-12-20 16:44:33 INFO     remove old optimizer files
2023-12-20 16:44:36 INFO     	 * (global step 18900: loss: 0.2897914834320545, lr: 0.0001
2023-12-20 16:44:52 INFO     	 * (global step 18950: loss: 0.2933793477714062, lr: 0.0001
2023-12-20 16:45:07 INFO     	 * (global step 19000: loss: 0.23124147206544876, lr: 0.0001
2023-12-20 16:45:23 INFO     	 * (global step 19050: loss: 0.26128723099827766, lr: 0.0001
2023-12-20 16:45:38 INFO     	 * (global step 19100: loss: 0.24848582968115807, lr: 0.0001
2023-12-20 16:45:54 INFO     	 * (global step 19150: loss: 0.25422216206789017, lr: 0.0001
2023-12-20 16:46:09 INFO     	 * (global step 19200: loss: 0.29268041998147964, lr: 0.0001
2023-12-20 16:46:25 INFO     	 * (global step 19250: loss: 0.2420947626233101, lr: 0.0001
2023-12-20 16:46:40 INFO     	 * (global step 19300: loss: 0.22748566791415215, lr: 0.0001
2023-12-20 16:46:56 INFO     	 * (global step 19350: loss: 0.23568953573703766, lr: 0.0001
2023-12-20 16:47:11 INFO     	 * (global step 19400: loss: 0.26348330453038216, lr: 0.0001
2023-12-20 16:47:26 INFO     	 * (global step 19450: loss: 0.2548668123781681, lr: 0.0001
2023-12-20 16:47:42 INFO     	 * (global step 19500: loss: 0.3850899301469326, lr: 0.0001
2023-12-20 16:47:57 INFO     	 * (global step 19550: loss: 0.21553673222661018, lr: 0.0001
2023-12-20 16:48:13 INFO     	 * (global step 19600: loss: 0.23256201297044754, lr: 0.0001
2023-12-20 16:48:28 INFO     	 * (global step 19650: loss: 0.23970747739076614, lr: 0.0001
2023-12-20 16:48:44 INFO     	 * (global step 19700: loss: 0.33320897072553635, lr: 0.0001
2023-12-20 16:48:59 INFO     	 * (global step 19750: loss: 0.20020775869488716, lr: 0.0001
2023-12-20 16:49:15 INFO     	 * (global step 19800: loss: 0.24242104589939117, lr: 0.0001
2023-12-20 16:49:30 INFO     	 * (global step 19850: loss: 0.2589806579053402, lr: 0.0001
2023-12-20 16:49:46 INFO     	 * (global step 19900: loss: 0.24368271231651306, lr: 0.0001
2023-12-20 16:50:01 INFO     	 * (global step 19950: loss: 0.2672891542315483, lr: 0.0001
2023-12-20 16:50:17 INFO     	 * (global step 20000: loss: 0.23918800801038742, lr: 0.0001
2023-12-20 16:50:32 INFO     	 * (global step 20050: loss: 0.22363650240004063, lr: 0.0001
2023-12-20 16:50:48 INFO     	 * (global step 20100: loss: 0.27785252034664154, lr: 0.0001
2023-12-20 16:51:03 INFO     	 * (global step 20150: loss: 0.2577386423945427, lr: 0.0001
2023-12-20 16:51:19 INFO     	 * (global step 20200: loss: 0.4617351330816746, lr: 0.0001
2023-12-20 16:51:34 INFO     	 * (global step 20250: loss: 0.2891774885356426, lr: 0.0001
2023-12-20 16:51:50 INFO     	 * (global step 20300: loss: 0.26684610173106194, lr: 0.0001
2023-12-20 16:52:05 INFO     	 * (global step 20350: loss: 0.2749948650598526, lr: 0.0001
2023-12-20 16:52:21 INFO     	 * (global step 20400: loss: 0.2181677371263504, lr: 0.0001
2023-12-20 16:52:36 INFO     	 * (global step 20450: loss: 0.3336305133998394, lr: 0.0001
2023-12-20 16:52:52 INFO     	 * (global step 20500: loss: 0.19211263582110405, lr: 0.0001
2023-12-20 16:53:07 INFO     	 * (global step 20550: loss: 0.27820904552936554, lr: 0.0001
2023-12-20 16:53:23 INFO     	 * (global step 20600: loss: 0.28792131692171097, lr: 0.0001
2023-12-20 16:53:38 INFO     	 * (global step 20650: loss: 0.23849576711654663, lr: 0.0001
2023-12-20 16:53:54 INFO     	 * (global step 20700: loss: 0.2723420225083828, lr: 0.0001
2023-12-20 16:54:09 INFO     	 * (global step 20750: loss: 0.2679600939154625, lr: 0.0001
2023-12-20 16:54:25 INFO     	 * (global step 20800: loss: 0.2943212352693081, lr: 0.0001
2023-12-20 16:54:40 INFO     	 * (global step 20850: loss: 0.28812069073319435, lr: 0.0001
2023-12-20 16:54:55 INFO     	 * (global step 20900: loss: 0.3080320656299591, lr: 0.0001
2023-12-20 16:55:11 INFO     	 * (global step 20950: loss: 0.28351806104183197, lr: 0.0001
2023-12-20 16:55:26 INFO     	 * (global step 21000: loss: 0.27354574389755726, lr: 0.0001
2023-12-20 16:55:42 INFO     	 * (global step 21050: loss: 0.2059126552194357, lr: 0.0001
2023-12-20 16:55:57 INFO     	 * (global step 21100: loss: 0.21221406757831573, lr: 0.0001
2023-12-20 16:56:13 INFO     	 * (global step 21150: loss: 0.23915284126996994, lr: 0.0001
2023-12-20 16:56:28 INFO     	 * (global step 21200: loss: 0.29714278504252434, lr: 0.0001
2023-12-20 16:56:44 INFO     [epoch 8/15] average loss: 0.265, lr: 0.0001
2023-12-20 16:56:44 INFO     saving model related files
2023-12-20 16:56:44 INFO     saving model
2023-12-20 16:56:44 INFO     saving tokenizer
2023-12-20 16:56:44 INFO     saving optimizer
2023-12-20 16:56:45 INFO     remove old optimizer files
2023-12-20 16:56:46 INFO     	 * (global step 21250: loss: 0.30239661782979965, lr: 0.0001
2023-12-20 16:57:01 INFO     	 * (global step 21300: loss: 0.2207176312804222, lr: 0.0001
2023-12-20 16:57:17 INFO     	 * (global step 21350: loss: 0.25877102464437485, lr: 0.0001
2023-12-20 16:57:32 INFO     	 * (global step 21400: loss: 0.30564454570412636, lr: 0.0001
2023-12-20 16:57:48 INFO     	 * (global step 21450: loss: 0.3158227354288101, lr: 0.0001
2023-12-20 16:58:03 INFO     	 * (global step 21500: loss: 0.24125304445624352, lr: 0.0001
2023-12-20 16:58:19 INFO     	 * (global step 21550: loss: 0.28680386394262314, lr: 0.0001
2023-12-20 16:58:34 INFO     	 * (global step 21600: loss: 0.22200078889727592, lr: 0.0001
2023-12-20 16:58:50 INFO     	 * (global step 21650: loss: 0.2736923284828663, lr: 0.0001
2023-12-20 16:59:05 INFO     	 * (global step 21700: loss: 0.26165393367409706, lr: 0.0001
2023-12-20 16:59:21 INFO     	 * (global step 21750: loss: 0.21833612769842148, lr: 0.0001
2023-12-20 16:59:36 INFO     	 * (global step 21800: loss: 0.37150420621037483, lr: 0.0001
2023-12-20 16:59:52 INFO     	 * (global step 21850: loss: 0.2699969634413719, lr: 0.0001
2023-12-20 17:00:07 INFO     	 * (global step 21900: loss: 0.3560263440012932, lr: 0.0001
2023-12-20 17:00:23 INFO     	 * (global step 21950: loss: 0.3618002310395241, lr: 0.0001
2023-12-20 17:00:38 INFO     	 * (global step 22000: loss: 0.31443318352103233, lr: 0.0001
2023-12-20 17:00:54 INFO     	 * (global step 22050: loss: 0.33166268467903137, lr: 0.0001
2023-12-20 17:01:09 INFO     	 * (global step 22100: loss: 0.23336530476808548, lr: 0.0001
2023-12-20 17:01:25 INFO     	 * (global step 22150: loss: 0.24523918703198433, lr: 0.0001
2023-12-20 17:01:40 INFO     	 * (global step 22200: loss: 0.1797492355108261, lr: 0.0001
2023-12-20 17:01:56 INFO     	 * (global step 22250: loss: 0.2749163843691349, lr: 0.0001
2023-12-20 17:02:11 INFO     	 * (global step 22300: loss: 0.2533942684531212, lr: 0.0001
2023-12-20 17:02:27 INFO     	 * (global step 22350: loss: 0.3049536943435669, lr: 0.0001
2023-12-20 17:02:42 INFO     	 * (global step 22400: loss: 0.28700394555926323, lr: 0.0001
2023-12-20 17:02:58 INFO     	 * (global step 22450: loss: 0.2347981482744217, lr: 0.0001
2023-12-20 17:03:13 INFO     	 * (global step 22500: loss: 0.20324342884123325, lr: 0.0001
2023-12-20 17:03:29 INFO     	 * (global step 22550: loss: 0.2966163530945778, lr: 0.0001
2023-12-20 17:03:45 INFO     	 * (global step 22600: loss: 0.3140881545841694, lr: 0.0001
2023-12-20 17:04:00 INFO     	 * (global step 22650: loss: 0.261786051094532, lr: 0.0001
2023-12-20 17:04:16 INFO     	 * (global step 22700: loss: 0.23507140949368477, lr: 0.0001
2023-12-20 17:04:31 INFO     	 * (global step 22750: loss: 0.2944134194403887, lr: 0.0001
2023-12-20 17:04:47 INFO     	 * (global step 22800: loss: 0.22796881943941116, lr: 0.0001
2023-12-20 17:05:02 INFO     	 * (global step 22850: loss: 0.27746254950761795, lr: 0.0001
2023-12-20 17:05:18 INFO     	 * (global step 22900: loss: 0.23419520258903503, lr: 0.0001
2023-12-20 17:05:33 INFO     	 * (global step 22950: loss: 0.2990921884775162, lr: 0.0001
2023-12-20 17:05:49 INFO     	 * (global step 23000: loss: 0.24701180681586266, lr: 0.0001
2023-12-20 17:06:04 INFO     	 * (global step 23050: loss: 0.22759873792529106, lr: 0.0001
2023-12-20 17:06:20 INFO     	 * (global step 23100: loss: 0.310496736317873, lr: 0.0001
2023-12-20 17:06:35 INFO     	 * (global step 23150: loss: 0.3067304491996765, lr: 0.0001
2023-12-20 17:06:51 INFO     	 * (global step 23200: loss: 0.27253561839461327, lr: 0.0001
2023-12-20 17:07:07 INFO     	 * (global step 23250: loss: 0.243511114269495, lr: 0.0001
2023-12-20 17:07:22 INFO     	 * (global step 23300: loss: 0.21969323605298996, lr: 0.0001
2023-12-20 17:07:38 INFO     	 * (global step 23350: loss: 0.2802576385438442, lr: 0.0001
2023-12-20 17:07:53 INFO     	 * (global step 23400: loss: 0.26982683688402176, lr: 0.0001
2023-12-20 17:08:09 INFO     	 * (global step 23450: loss: 0.24352477118372917, lr: 0.0001
2023-12-20 17:08:24 INFO     	 * (global step 23500: loss: 0.24836770445108414, lr: 0.0001
2023-12-20 17:08:40 INFO     	 * (global step 23550: loss: 0.32831593602895737, lr: 0.0001
2023-12-20 17:08:55 INFO     	 * (global step 23600: loss: 0.21863803640007973, lr: 0.0001
2023-12-20 17:08:58 INFO     [epoch 9/15] average loss: 0.261, lr: 0.0001
2023-12-20 17:08:58 INFO     saving model related files
2023-12-20 17:08:58 INFO     saving model
2023-12-20 17:08:59 INFO     saving tokenizer
2023-12-20 17:08:59 INFO     saving optimizer
2023-12-20 17:09:00 INFO     remove old optimizer files
2023-12-20 17:09:00 INFO     complete training: model ckpt was saved at small_combined_trained_ckpt/model_dpyopu
2023-12-20 17:09:00 INFO     ## 1st RUN: Configuration 3/12 ##
2023-12-20 17:09:00 INFO     initialize model trainer
2023-12-20 17:09:00 INFO     initialize checkpoint at small_combined_trained_ckpt/model_mzgdpa
2023-12-20 17:09:00 INFO     hyperparameters
2023-12-20 17:09:00 INFO     	 * dataset_path: StellarMilk/squad_newsqa
2023-12-20 17:09:00 INFO     	 * dataset_name: default
2023-12-20 17:09:00 INFO     	 * input_types: ['paragraph']
2023-12-20 17:09:00 INFO     	 * output_types: ['questions_answers']
2023-12-20 17:09:00 INFO     	 * prefix_types: ['qag']
2023-12-20 17:09:00 INFO     	 * model: t5-small
2023-12-20 17:09:00 INFO     	 * max_length: 512
2023-12-20 17:09:00 INFO     	 * max_length_output: 512
2023-12-20 17:09:00 INFO     	 * epoch: 15
2023-12-20 17:09:00 INFO     	 * batch: 2
2023-12-20 17:09:00 INFO     	 * lr: 0.0001
2023-12-20 17:09:00 INFO     	 * fp16: False
2023-12-20 17:09:00 INFO     	 * random_seed: 1
2023-12-20 17:09:00 INFO     	 * gradient_accumulation_steps: 2
2023-12-20 17:09:00 INFO     	 * label_smoothing: 0.0
2023-12-20 17:09:00 INFO     initialize checkpoint with t5-small
2023-12-20 17:09:01 INFO     use spaCy answer extraction model: positionrank
2023-12-20 17:09:01 INFO     Model `t5-small`
2023-12-20 17:09:01 INFO     	 * Num of GPU in use: 1
2023-12-20 17:09:01 INFO     	 * Prefix: True
2023-12-20 17:09:01 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 17:09:02 INFO     dataset preprocessing
2023-12-20 17:09:03 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/squad_newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-20 17:09:05 INFO     start model training
2023-12-20 17:09:12 INFO     	 * (global step 50: loss: 0.8271616697311401, lr: 0.0001
2023-12-20 17:09:20 INFO     	 * (global step 100: loss: 0.6524037718772888, lr: 0.0001
2023-12-20 17:09:28 INFO     	 * (global step 150: loss: 0.6664508879184723, lr: 0.0001
2023-12-20 17:09:36 INFO     	 * (global step 200: loss: 0.5491462796926498, lr: 0.0001
2023-12-20 17:09:44 INFO     	 * (global step 250: loss: 0.43371284008026123, lr: 0.0001
2023-12-20 17:09:52 INFO     	 * (global step 300: loss: 0.46526917815208435, lr: 0.0001
2023-12-20 17:10:00 INFO     	 * (global step 350: loss: 0.4478193074464798, lr: 0.0001
2023-12-20 17:10:08 INFO     	 * (global step 400: loss: 0.3979707509279251, lr: 0.0001
2023-12-20 17:10:16 INFO     	 * (global step 450: loss: 0.4297632575035095, lr: 0.0001
2023-12-20 17:10:24 INFO     	 * (global step 500: loss: 0.48071466386318207, lr: 0.0001
2023-12-20 17:10:32 INFO     	 * (global step 550: loss: 0.4966909885406494, lr: 0.0001
2023-12-20 17:10:40 INFO     	 * (global step 600: loss: 0.2775631844997406, lr: 0.0001
2023-12-20 17:10:48 INFO     	 * (global step 650: loss: 0.3532160818576813, lr: 0.0001
2023-12-20 17:10:56 INFO     	 * (global step 700: loss: 0.49068139493465424, lr: 0.0001
2023-12-20 17:11:04 INFO     	 * (global step 750: loss: 0.34847238659858704, lr: 0.0001
2023-12-20 17:11:12 INFO     	 * (global step 800: loss: 0.34140749275684357, lr: 0.0001
2023-12-20 17:11:20 INFO     	 * (global step 850: loss: 0.2636881172657013, lr: 0.0001
2023-12-20 17:11:28 INFO     	 * (global step 900: loss: 0.3844577819108963, lr: 0.0001
2023-12-20 17:11:36 INFO     	 * (global step 950: loss: 0.33912548422813416, lr: 0.0001
2023-12-20 17:11:44 INFO     	 * (global step 1000: loss: 0.3110716938972473, lr: 0.0001
2023-12-20 17:11:52 INFO     	 * (global step 1050: loss: 0.3500579744577408, lr: 0.0001
2023-12-20 17:12:00 INFO     	 * (global step 1100: loss: 0.44707708060741425, lr: 0.0001
2023-12-20 17:12:08 INFO     	 * (global step 1150: loss: 0.3369816541671753, lr: 0.0001
2023-12-20 17:12:16 INFO     	 * (global step 1200: loss: 0.4391486793756485, lr: 0.0001
2023-12-20 17:12:24 INFO     	 * (global step 1250: loss: 0.4535181373357773, lr: 0.0001
2023-12-20 17:12:32 INFO     	 * (global step 1300: loss: 0.3385179191827774, lr: 0.0001
2023-12-20 17:12:40 INFO     	 * (global step 1350: loss: 0.4148726314306259, lr: 0.0001
2023-12-20 17:12:48 INFO     	 * (global step 1400: loss: 0.38037510216236115, lr: 0.0001
2023-12-20 17:12:56 INFO     	 * (global step 1450: loss: 0.35716864466667175, lr: 0.0001
2023-12-20 17:13:05 INFO     	 * (global step 1500: loss: 0.35934391617774963, lr: 0.0001
2023-12-20 17:13:13 INFO     	 * (global step 1550: loss: 0.3233787715435028, lr: 0.0001
2023-12-20 17:13:21 INFO     	 * (global step 1600: loss: 0.35465239733457565, lr: 0.0001
2023-12-20 17:13:29 INFO     	 * (global step 1650: loss: 0.33534251153469086, lr: 0.0001
2023-12-20 17:13:37 INFO     	 * (global step 1700: loss: 0.3942243605852127, lr: 0.0001
2023-12-20 17:13:45 INFO     	 * (global step 1750: loss: 0.28824780881404877, lr: 0.0001
2023-12-20 17:13:53 INFO     	 * (global step 1800: loss: 0.3217493146657944, lr: 0.0001
2023-12-20 17:14:01 INFO     	 * (global step 1850: loss: 0.47703753411769867, lr: 0.0001
2023-12-20 17:14:09 INFO     	 * (global step 1900: loss: 0.26451561599969864, lr: 0.0001
2023-12-20 17:14:17 INFO     	 * (global step 1950: loss: 0.32487814128398895, lr: 0.0001
2023-12-20 17:14:25 INFO     	 * (global step 2000: loss: 0.33413295447826385, lr: 0.0001
2023-12-20 17:14:33 INFO     	 * (global step 2050: loss: 0.34771692752838135, lr: 0.0001
2023-12-20 17:14:41 INFO     	 * (global step 2100: loss: 0.3137528598308563, lr: 0.0001
2023-12-20 17:14:49 INFO     	 * (global step 2150: loss: 0.41993166506290436, lr: 0.0001
2023-12-20 17:14:57 INFO     	 * (global step 2200: loss: 0.36110542714595795, lr: 0.0001
2023-12-20 17:15:05 INFO     	 * (global step 2250: loss: 0.3997979909181595, lr: 0.0001
2023-12-20 17:15:14 INFO     	 * (global step 2300: loss: 0.34187929332256317, lr: 0.0001
2023-12-20 17:15:22 INFO     	 * (global step 2350: loss: 0.2998390793800354, lr: 0.0001
2023-12-20 17:15:30 INFO     	 * (global step 2400: loss: 0.38901519775390625, lr: 0.0001
2023-12-20 17:15:38 INFO     	 * (global step 2450: loss: 0.28565219044685364, lr: 0.0001
2023-12-20 17:15:46 INFO     	 * (global step 2500: loss: 0.31550586968660355, lr: 0.0001
2023-12-20 17:15:54 INFO     	 * (global step 2550: loss: 0.6091780364513397, lr: 0.0001
2023-12-20 17:16:02 INFO     	 * (global step 2600: loss: 0.4343915432691574, lr: 0.0001
2023-12-20 17:16:10 INFO     	 * (global step 2650: loss: 0.4264722764492035, lr: 0.0001
2023-12-20 17:16:18 INFO     	 * (global step 2700: loss: 0.2735323905944824, lr: 0.0001
2023-12-20 17:16:26 INFO     	 * (global step 2750: loss: 0.4176067113876343, lr: 0.0001
2023-12-20 17:16:34 INFO     	 * (global step 2800: loss: 0.3612472116947174, lr: 0.0001
2023-12-20 17:16:42 INFO     	 * (global step 2850: loss: 0.23354067653417587, lr: 0.0001
2023-12-20 17:16:50 INFO     	 * (global step 2900: loss: 0.34553441405296326, lr: 0.0001
2023-12-20 17:16:58 INFO     	 * (global step 2950: loss: 0.20275916159152985, lr: 0.0001
2023-12-20 17:17:06 INFO     	 * (global step 3000: loss: 0.5465555042028427, lr: 0.0001
2023-12-20 17:17:14 INFO     	 * (global step 3050: loss: 0.30852995812892914, lr: 0.0001
2023-12-20 17:17:22 INFO     	 * (global step 3100: loss: 0.3207826465368271, lr: 0.0001
2023-12-20 17:17:30 INFO     	 * (global step 3150: loss: 0.32570406794548035, lr: 0.0001
2023-12-20 17:17:38 INFO     	 * (global step 3200: loss: 0.2857859581708908, lr: 0.0001
2023-12-20 17:17:46 INFO     	 * (global step 3250: loss: 0.3404928594827652, lr: 0.0001
2023-12-20 17:17:54 INFO     	 * (global step 3300: loss: 0.47316811978816986, lr: 0.0001
2023-12-20 17:18:02 INFO     	 * (global step 3350: loss: 0.32283230125904083, lr: 0.0001
2023-12-20 17:18:10 INFO     	 * (global step 3400: loss: 0.3340349495410919, lr: 0.0001
2023-12-20 17:18:18 INFO     	 * (global step 3450: loss: 0.3894316405057907, lr: 0.0001
2023-12-20 17:18:27 INFO     	 * (global step 3500: loss: 0.30584122240543365, lr: 0.0001
2023-12-20 17:18:35 INFO     	 * (global step 3550: loss: 0.4423225671052933, lr: 0.0001
2023-12-20 17:18:43 INFO     	 * (global step 3600: loss: 0.3839430958032608, lr: 0.0001
2023-12-20 17:18:51 INFO     	 * (global step 3650: loss: 0.39216695725917816, lr: 0.0001
2023-12-20 17:18:59 INFO     	 * (global step 3700: loss: 0.3082556128501892, lr: 0.0001
2023-12-20 17:19:07 INFO     	 * (global step 3750: loss: 0.31285491585731506, lr: 0.0001
2023-12-20 17:19:15 INFO     	 * (global step 3800: loss: 0.30929549038410187, lr: 0.0001
2023-12-20 17:19:23 INFO     	 * (global step 3850: loss: 0.19066964089870453, lr: 0.0001
2023-12-20 17:19:31 INFO     	 * (global step 3900: loss: 0.39277657866477966, lr: 0.0001
2023-12-20 17:19:39 INFO     	 * (global step 3950: loss: 0.25756123661994934, lr: 0.0001
2023-12-20 17:19:47 INFO     	 * (global step 4000: loss: 0.22978612035512924, lr: 0.0001
2023-12-20 17:19:55 INFO     	 * (global step 4050: loss: 0.2370132878422737, lr: 0.0001
2023-12-20 17:20:03 INFO     	 * (global step 4100: loss: 0.31075039505958557, lr: 0.0001
2023-12-20 17:20:12 INFO     	 * (global step 4150: loss: 0.2710302472114563, lr: 0.0001
2023-12-20 17:20:20 INFO     	 * (global step 4200: loss: 0.2736429199576378, lr: 0.0001
2023-12-20 17:20:28 INFO     	 * (global step 4250: loss: 0.3568546324968338, lr: 0.0001
2023-12-20 17:20:36 INFO     	 * (global step 4300: loss: 0.4385364204645157, lr: 0.0001
2023-12-20 17:20:44 INFO     	 * (global step 4350: loss: 0.25632504373788834, lr: 0.0001
2023-12-20 17:20:52 INFO     	 * (global step 4400: loss: 0.38654597103595734, lr: 0.0001
2023-12-20 17:21:00 INFO     	 * (global step 4450: loss: 0.4087716341018677, lr: 0.0001
2023-12-20 17:21:08 INFO     	 * (global step 4500: loss: 0.2708672508597374, lr: 0.0001
2023-12-20 17:21:16 INFO     	 * (global step 4550: loss: 0.28480878472328186, lr: 0.0001
2023-12-20 17:21:24 INFO     	 * (global step 4600: loss: 0.32132264971733093, lr: 0.0001
2023-12-20 17:21:32 INFO     	 * (global step 4650: loss: 0.37032797932624817, lr: 0.0001
2023-12-20 17:21:41 INFO     	 * (global step 4700: loss: 0.3147039860486984, lr: 0.0001
2023-12-20 17:21:44 INFO     [epoch 0/15] average loss: 0.4, lr: 0.0001
2023-12-20 17:21:44 INFO     saving model related files
2023-12-20 17:21:44 INFO     saving model
2023-12-20 17:21:45 INFO     saving tokenizer
2023-12-20 17:21:45 INFO     saving optimizer
2023-12-20 17:21:46 INFO     remove old optimizer files
2023-12-20 17:21:50 INFO     	 * (global step 4750: loss: 0.4028175473213196, lr: 0.0001
2023-12-20 17:21:58 INFO     	 * (global step 4800: loss: 0.3011888116598129, lr: 0.0001
2023-12-20 17:22:06 INFO     	 * (global step 4850: loss: 0.26301459223032, lr: 0.0001
2023-12-20 17:22:14 INFO     	 * (global step 4900: loss: 0.3752302825450897, lr: 0.0001
2023-12-20 17:22:22 INFO     	 * (global step 4950: loss: 0.3141064941883087, lr: 0.0001
2023-12-20 17:22:30 INFO     	 * (global step 5000: loss: 0.21882600337266922, lr: 0.0001
2023-12-20 17:22:39 INFO     	 * (global step 5050: loss: 0.3736335337162018, lr: 0.0001
2023-12-20 17:22:47 INFO     	 * (global step 5100: loss: 0.31620338559150696, lr: 0.0001
2023-12-20 17:22:55 INFO     	 * (global step 5150: loss: 0.24794524163007736, lr: 0.0001
2023-12-20 17:23:03 INFO     	 * (global step 5200: loss: 0.3122042939066887, lr: 0.0001
2023-12-20 17:23:11 INFO     	 * (global step 5250: loss: 0.23535826057195663, lr: 0.0001
2023-12-20 17:23:19 INFO     	 * (global step 5300: loss: 0.32717300951480865, lr: 0.0001
2023-12-20 17:23:27 INFO     	 * (global step 5350: loss: 0.39696095883846283, lr: 0.0001
2023-12-20 17:23:35 INFO     	 * (global step 5400: loss: 0.365206316113472, lr: 0.0001
2023-12-20 17:23:43 INFO     	 * (global step 5450: loss: 0.5872928500175476, lr: 0.0001
2023-12-20 17:23:51 INFO     	 * (global step 5500: loss: 0.2755320221185684, lr: 0.0001
2023-12-20 17:23:59 INFO     	 * (global step 5550: loss: 0.26313717663288116, lr: 0.0001
2023-12-20 17:24:07 INFO     	 * (global step 5600: loss: 0.4250462204217911, lr: 0.0001
2023-12-20 17:24:15 INFO     	 * (global step 5650: loss: 0.22083240747451782, lr: 0.0001
2023-12-20 17:24:23 INFO     	 * (global step 5700: loss: 0.2839819937944412, lr: 0.0001
2023-12-20 17:24:31 INFO     	 * (global step 5750: loss: 0.36670543253421783, lr: 0.0001
2023-12-20 17:24:39 INFO     	 * (global step 5800: loss: 0.2838018983602524, lr: 0.0001
2023-12-20 17:24:48 INFO     	 * (global step 5850: loss: 0.3070010393857956, lr: 0.0001
2023-12-20 17:24:56 INFO     	 * (global step 5900: loss: 0.23722118884325027, lr: 0.0001
2023-12-20 17:25:04 INFO     	 * (global step 5950: loss: 0.40142691135406494, lr: 0.0001
2023-12-20 17:25:12 INFO     	 * (global step 6000: loss: 0.32010845839977264, lr: 0.0001
2023-12-20 17:25:20 INFO     	 * (global step 6050: loss: 0.2853192687034607, lr: 0.0001
2023-12-20 17:25:28 INFO     	 * (global step 6100: loss: 0.26059025526046753, lr: 0.0001
2023-12-20 17:25:36 INFO     	 * (global step 6150: loss: 0.3007309138774872, lr: 0.0001
2023-12-20 17:25:44 INFO     	 * (global step 6200: loss: 0.365091934800148, lr: 0.0001
2023-12-20 17:25:52 INFO     	 * (global step 6250: loss: 0.31507109105587006, lr: 0.0001
2023-12-20 17:26:00 INFO     	 * (global step 6300: loss: 0.6202414780855179, lr: 0.0001
2023-12-20 17:26:08 INFO     	 * (global step 6350: loss: 0.18817392364144325, lr: 0.0001
2023-12-20 17:26:16 INFO     	 * (global step 6400: loss: 0.2960405722260475, lr: 0.0001
2023-12-20 17:26:24 INFO     	 * (global step 6450: loss: 0.2544739246368408, lr: 0.0001
2023-12-20 17:26:32 INFO     	 * (global step 6500: loss: 0.24611817300319672, lr: 0.0001
2023-12-20 17:26:40 INFO     	 * (global step 6550: loss: 0.16521605849266052, lr: 0.0001
2023-12-20 17:26:48 INFO     	 * (global step 6600: loss: 0.32385629415512085, lr: 0.0001
2023-12-20 17:26:56 INFO     	 * (global step 6650: loss: 0.2819456160068512, lr: 0.0001
2023-12-20 17:27:05 INFO     	 * (global step 6700: loss: 0.2947951406240463, lr: 0.0001
2023-12-20 17:27:13 INFO     	 * (global step 6750: loss: 0.41339220106601715, lr: 0.0001
2023-12-20 17:27:21 INFO     	 * (global step 6800: loss: 0.28182704746723175, lr: 0.0001
2023-12-20 17:27:29 INFO     	 * (global step 6850: loss: 0.641890749335289, lr: 0.0001
2023-12-20 17:27:37 INFO     	 * (global step 6900: loss: 0.2871968001127243, lr: 0.0001
2023-12-20 17:27:45 INFO     	 * (global step 6950: loss: 0.24256956577301025, lr: 0.0001
2023-12-20 17:27:53 INFO     	 * (global step 7000: loss: 0.4375937134027481, lr: 0.0001
2023-12-20 17:28:01 INFO     	 * (global step 7050: loss: 0.28623150289058685, lr: 0.0001
2023-12-20 17:28:09 INFO     	 * (global step 7100: loss: 0.32530562579631805, lr: 0.0001
2023-12-20 17:28:17 INFO     	 * (global step 7150: loss: 0.2662817984819412, lr: 0.0001
2023-12-20 17:28:25 INFO     	 * (global step 7200: loss: 0.2959093302488327, lr: 0.0001
2023-12-20 17:28:33 INFO     	 * (global step 7250: loss: 0.3087804615497589, lr: 0.0001
2023-12-20 17:28:41 INFO     	 * (global step 7300: loss: 0.29506850242614746, lr: 0.0001
2023-12-20 17:28:49 INFO     	 * (global step 7350: loss: 0.21285330504179, lr: 0.0001
2023-12-20 17:28:58 INFO     	 * (global step 7400: loss: 0.4435056298971176, lr: 0.0001
2023-12-20 17:29:06 INFO     	 * (global step 7450: loss: 0.45115064829587936, lr: 0.0001
2023-12-20 17:29:14 INFO     	 * (global step 7500: loss: 0.3212864398956299, lr: 0.0001
2023-12-20 17:29:22 INFO     	 * (global step 7550: loss: 0.39446118474006653, lr: 0.0001
2023-12-20 17:29:30 INFO     	 * (global step 7600: loss: 0.2303786724805832, lr: 0.0001
2023-12-20 17:29:38 INFO     	 * (global step 7650: loss: 0.26484057307243347, lr: 0.0001
2023-12-20 17:29:46 INFO     	 * (global step 7700: loss: 0.369942307472229, lr: 0.0001
2023-12-20 17:29:54 INFO     	 * (global step 7750: loss: 0.2837032079696655, lr: 0.0001
2023-12-20 17:30:02 INFO     	 * (global step 7800: loss: 0.28612979501485825, lr: 0.0001
2023-12-20 17:30:10 INFO     	 * (global step 7850: loss: 0.33040210604667664, lr: 0.0001
2023-12-20 17:30:18 INFO     	 * (global step 7900: loss: 0.3640543520450592, lr: 0.0001
2023-12-20 17:30:26 INFO     	 * (global step 7950: loss: 0.3605392277240753, lr: 0.0001
2023-12-20 17:30:34 INFO     	 * (global step 8000: loss: 0.23699674755334854, lr: 0.0001
2023-12-20 17:30:42 INFO     	 * (global step 8050: loss: 0.3604760318994522, lr: 0.0001
2023-12-20 17:30:50 INFO     	 * (global step 8100: loss: 0.4083573669195175, lr: 0.0001
2023-12-20 17:30:58 INFO     	 * (global step 8150: loss: 0.4711400121450424, lr: 0.0001
2023-12-20 17:31:06 INFO     	 * (global step 8200: loss: 0.2292110174894333, lr: 0.0001
2023-12-20 17:31:14 INFO     	 * (global step 8250: loss: 0.43282245099544525, lr: 0.0001
2023-12-20 17:31:22 INFO     	 * (global step 8300: loss: 0.27951765805482864, lr: 0.0001
2023-12-20 17:31:30 INFO     	 * (global step 8350: loss: 0.4145510494709015, lr: 0.0001
2023-12-20 17:31:38 INFO     	 * (global step 8400: loss: 0.274776853621006, lr: 0.0001
2023-12-20 17:31:46 INFO     	 * (global step 8450: loss: 0.35055555403232574, lr: 0.0001
2023-12-20 17:31:54 INFO     	 * (global step 8500: loss: 0.45220324397087097, lr: 0.0001
2023-12-20 17:32:02 INFO     	 * (global step 8550: loss: 0.4677845537662506, lr: 0.0001
2023-12-20 17:32:11 INFO     	 * (global step 8600: loss: 0.19487250596284866, lr: 0.0001
2023-12-20 17:32:19 INFO     	 * (global step 8650: loss: 0.40158508718013763, lr: 0.0001
2023-12-20 17:32:27 INFO     	 * (global step 8700: loss: 0.17107561230659485, lr: 0.0001
2023-12-20 17:32:35 INFO     	 * (global step 8750: loss: 0.22036534547805786, lr: 0.0001
2023-12-20 17:32:43 INFO     	 * (global step 8800: loss: 0.2769481986761093, lr: 0.0001
2023-12-20 17:32:51 INFO     	 * (global step 8850: loss: 0.3845016211271286, lr: 0.0001
2023-12-20 17:32:59 INFO     	 * (global step 8900: loss: 0.2399858981370926, lr: 0.0001
2023-12-20 17:33:07 INFO     	 * (global step 8950: loss: 0.2295173779129982, lr: 0.0001
2023-12-20 17:33:15 INFO     	 * (global step 9000: loss: 0.23560109734535217, lr: 0.0001
2023-12-20 17:33:23 INFO     	 * (global step 9050: loss: 0.4662911295890808, lr: 0.0001
2023-12-20 17:33:31 INFO     	 * (global step 9100: loss: 0.3512527197599411, lr: 0.0001
2023-12-20 17:33:39 INFO     	 * (global step 9150: loss: 0.2570121958851814, lr: 0.0001
2023-12-20 17:33:47 INFO     	 * (global step 9200: loss: 0.23134779185056686, lr: 0.0001
2023-12-20 17:33:55 INFO     	 * (global step 9250: loss: 0.5491283535957336, lr: 0.0001
2023-12-20 17:34:03 INFO     	 * (global step 9300: loss: 0.2880124896764755, lr: 0.0001
2023-12-20 17:34:11 INFO     	 * (global step 9350: loss: 0.393671490252018, lr: 0.0001
2023-12-20 17:34:20 INFO     	 * (global step 9400: loss: 0.3097275495529175, lr: 0.0001
2023-12-20 17:34:27 INFO     [epoch 1/15] average loss: 0.32, lr: 0.0001
2023-12-20 17:34:27 INFO     saving model related files
2023-12-20 17:34:27 INFO     saving model
2023-12-20 17:34:27 INFO     saving tokenizer
2023-12-20 17:34:28 INFO     saving optimizer
2023-12-20 17:34:28 INFO     remove old optimizer files
2023-12-20 17:34:29 INFO     	 * (global step 9450: loss: 0.3872256726026535, lr: 0.0001
2023-12-20 17:34:37 INFO     	 * (global step 9500: loss: 0.26972195506095886, lr: 0.0001
2023-12-20 17:34:45 INFO     	 * (global step 9550: loss: 0.3487430512905121, lr: 0.0001
2023-12-20 17:34:53 INFO     	 * (global step 9600: loss: 0.27516864240169525, lr: 0.0001
2023-12-20 17:35:01 INFO     	 * (global step 9650: loss: 0.23891960084438324, lr: 0.0001
2023-12-20 17:35:09 INFO     	 * (global step 9700: loss: 0.22023504972457886, lr: 0.0001
2023-12-20 17:35:17 INFO     	 * (global step 9750: loss: 0.3958714157342911, lr: 0.0001
2023-12-20 17:35:25 INFO     	 * (global step 9800: loss: 0.24131107330322266, lr: 0.0001
2023-12-20 17:35:33 INFO     	 * (global step 9850: loss: 0.24350683391094208, lr: 0.0001
2023-12-20 17:35:42 INFO     	 * (global step 9900: loss: 0.17997188866138458, lr: 0.0001
2023-12-20 17:35:50 INFO     	 * (global step 9950: loss: 0.28736351430416107, lr: 0.0001
2023-12-20 17:35:58 INFO     	 * (global step 10000: loss: 0.21856629103422165, lr: 0.0001
2023-12-20 17:36:06 INFO     	 * (global step 10050: loss: 0.19880611449480057, lr: 0.0001
2023-12-20 17:36:14 INFO     	 * (global step 10100: loss: 0.27865470945835114, lr: 0.0001
2023-12-20 17:36:22 INFO     	 * (global step 10150: loss: 0.2760651707649231, lr: 0.0001
2023-12-20 17:36:30 INFO     	 * (global step 10200: loss: 0.28255072236061096, lr: 0.0001
2023-12-20 17:36:38 INFO     	 * (global step 10250: loss: 0.44060901552438736, lr: 0.0001
2023-12-20 17:36:46 INFO     	 * (global step 10300: loss: 0.37851081788539886, lr: 0.0001
2023-12-20 17:36:54 INFO     	 * (global step 10350: loss: 0.3691565543413162, lr: 0.0001
2023-12-20 17:37:02 INFO     	 * (global step 10400: loss: 0.2814341187477112, lr: 0.0001
2023-12-20 17:37:10 INFO     	 * (global step 10450: loss: 0.2799670994281769, lr: 0.0001
2023-12-20 17:37:18 INFO     	 * (global step 10500: loss: 0.2563272565603256, lr: 0.0001
2023-12-20 17:37:26 INFO     	 * (global step 10550: loss: 0.3047880530357361, lr: 0.0001
2023-12-20 17:37:34 INFO     	 * (global step 10600: loss: 0.29215557873249054, lr: 0.0001
2023-12-20 17:37:42 INFO     	 * (global step 10650: loss: 0.28850696980953217, lr: 0.0001
2023-12-20 17:37:50 INFO     	 * (global step 10700: loss: 0.25151459127664566, lr: 0.0001
2023-12-20 17:37:58 INFO     	 * (global step 10750: loss: 0.3558840751647949, lr: 0.0001
2023-12-20 17:38:06 INFO     	 * (global step 10800: loss: 0.36173921823501587, lr: 0.0001
2023-12-20 17:38:14 INFO     	 * (global step 10850: loss: 0.2760454975068569, lr: 0.0001
2023-12-20 17:38:22 INFO     	 * (global step 10900: loss: 0.3061661571264267, lr: 0.0001
2023-12-20 17:38:30 INFO     	 * (global step 10950: loss: 0.2819584757089615, lr: 0.0001
2023-12-20 17:38:38 INFO     	 * (global step 11000: loss: 0.5288621187210083, lr: 0.0001
2023-12-20 17:38:46 INFO     	 * (global step 11050: loss: 0.28267353028059006, lr: 0.0001
2023-12-20 17:38:54 INFO     	 * (global step 11100: loss: 0.5040313452482224, lr: 0.0001
2023-12-20 17:39:02 INFO     	 * (global step 11150: loss: 0.31508489698171616, lr: 0.0001
2023-12-20 17:39:11 INFO     	 * (global step 11200: loss: 0.2263030856847763, lr: 0.0001
2023-12-20 17:39:19 INFO     	 * (global step 11250: loss: 0.19294483214616776, lr: 0.0001
2023-12-20 17:39:27 INFO     	 * (global step 11300: loss: 0.3012232333421707, lr: 0.0001
2023-12-20 17:39:35 INFO     	 * (global step 11350: loss: 0.2603948563337326, lr: 0.0001
2023-12-20 17:39:43 INFO     	 * (global step 11400: loss: 0.20812062919139862, lr: 0.0001
2023-12-20 17:39:51 INFO     	 * (global step 11450: loss: 0.25505317747592926, lr: 0.0001
2023-12-20 17:39:59 INFO     	 * (global step 11500: loss: 0.21381040662527084, lr: 0.0001
2023-12-20 17:40:07 INFO     	 * (global step 11550: loss: 0.44318417459726334, lr: 0.0001
2023-12-20 17:40:15 INFO     	 * (global step 11600: loss: 0.3595367819070816, lr: 0.0001
2023-12-20 17:40:23 INFO     	 * (global step 11650: loss: 0.260381743311882, lr: 0.0001
2023-12-20 17:40:31 INFO     	 * (global step 11700: loss: 0.429203063249588, lr: 0.0001
2023-12-20 17:40:39 INFO     	 * (global step 11750: loss: 0.2432335838675499, lr: 0.0001
2023-12-20 17:40:47 INFO     	 * (global step 11800: loss: 0.1861291565001011, lr: 0.0001
2023-12-20 17:40:55 INFO     	 * (global step 11850: loss: 0.28444355353713036, lr: 0.0001
2023-12-20 17:41:03 INFO     	 * (global step 11900: loss: 0.3409533351659775, lr: 0.0001
2023-12-20 17:41:11 INFO     	 * (global step 11950: loss: 0.2900679409503937, lr: 0.0001
2023-12-20 17:41:19 INFO     	 * (global step 12000: loss: 0.25056371092796326, lr: 0.0001
2023-12-20 17:41:28 INFO     	 * (global step 12050: loss: 0.26095912605524063, lr: 0.0001
2023-12-20 17:41:36 INFO     	 * (global step 12100: loss: 0.33715131133794785, lr: 0.0001
2023-12-20 17:41:44 INFO     	 * (global step 12150: loss: 0.35220296680927277, lr: 0.0001
2023-12-20 17:41:52 INFO     	 * (global step 12200: loss: 0.2986986115574837, lr: 0.0001
2023-12-20 17:42:00 INFO     	 * (global step 12250: loss: 0.2889099270105362, lr: 0.0001
2023-12-20 17:42:08 INFO     	 * (global step 12300: loss: 0.2363070473074913, lr: 0.0001
2023-12-20 17:42:16 INFO     	 * (global step 12350: loss: 0.3657078891992569, lr: 0.0001
2023-12-20 17:42:24 INFO     	 * (global step 12400: loss: 0.59853395819664, lr: 0.0001
2023-12-20 17:42:32 INFO     	 * (global step 12450: loss: 0.18972447514533997, lr: 0.0001
2023-12-20 17:42:40 INFO     	 * (global step 12500: loss: 0.35947373509407043, lr: 0.0001
2023-12-20 17:42:49 INFO     	 * (global step 12550: loss: 0.34456808865070343, lr: 0.0001
2023-12-20 17:42:57 INFO     	 * (global step 12600: loss: 0.43003305047750473, lr: 0.0001
2023-12-20 17:43:05 INFO     	 * (global step 12650: loss: 0.30607983469963074, lr: 0.0001
2023-12-20 17:43:13 INFO     	 * (global step 12700: loss: 0.24571488797664642, lr: 0.0001
2023-12-20 17:43:21 INFO     	 * (global step 12750: loss: 0.335485577583313, lr: 0.0001
2023-12-20 17:43:29 INFO     	 * (global step 12800: loss: 0.2695401534438133, lr: 0.0001
2023-12-20 17:43:37 INFO     	 * (global step 12850: loss: 0.24414778500795364, lr: 0.0001
2023-12-20 17:43:45 INFO     	 * (global step 12900: loss: 0.262064553797245, lr: 0.0001
2023-12-20 17:43:53 INFO     	 * (global step 12950: loss: 0.347895011305809, lr: 0.0001
2023-12-20 17:44:01 INFO     	 * (global step 13000: loss: 0.3968004435300827, lr: 0.0001
2023-12-20 17:44:09 INFO     	 * (global step 13050: loss: 0.22985001653432846, lr: 0.0001
2023-12-20 17:44:17 INFO     	 * (global step 13100: loss: 0.3196030631661415, lr: 0.0001
2023-12-20 17:44:25 INFO     	 * (global step 13150: loss: 0.34057655930519104, lr: 0.0001
2023-12-20 17:44:34 INFO     	 * (global step 13200: loss: 0.2731246203184128, lr: 0.0001
2023-12-20 17:44:42 INFO     	 * (global step 13250: loss: 0.23149719461798668, lr: 0.0001
2023-12-20 17:44:50 INFO     	 * (global step 13300: loss: 0.36779123544692993, lr: 0.0001
2023-12-20 17:44:58 INFO     	 * (global step 13350: loss: 0.366485595703125, lr: 0.0001
2023-12-20 17:45:06 INFO     	 * (global step 13400: loss: 0.23038245737552643, lr: 0.0001
2023-12-20 17:45:14 INFO     	 * (global step 13450: loss: 0.33425888419151306, lr: 0.0001
2023-12-20 17:45:22 INFO     	 * (global step 13500: loss: 0.2588309198617935, lr: 0.0001
2023-12-20 17:45:30 INFO     	 * (global step 13550: loss: 0.20822495222091675, lr: 0.0001
2023-12-20 17:45:38 INFO     	 * (global step 13600: loss: 0.5125405937433243, lr: 0.0001
2023-12-20 17:45:46 INFO     	 * (global step 13650: loss: 0.19889827072620392, lr: 0.0001
2023-12-20 17:45:54 INFO     	 * (global step 13700: loss: 0.3075735792517662, lr: 0.0001
2023-12-20 17:46:02 INFO     	 * (global step 13750: loss: 0.2738979011774063, lr: 0.0001
2023-12-20 17:46:10 INFO     	 * (global step 13800: loss: 0.324254110455513, lr: 0.0001
2023-12-20 17:46:18 INFO     	 * (global step 13850: loss: 0.45659419149160385, lr: 0.0001
2023-12-20 17:46:26 INFO     	 * (global step 13900: loss: 0.21992535889148712, lr: 0.0001
2023-12-20 17:46:34 INFO     	 * (global step 13950: loss: 0.281266912817955, lr: 0.0001
2023-12-20 17:46:42 INFO     	 * (global step 14000: loss: 0.3263564333319664, lr: 0.0001
2023-12-20 17:46:50 INFO     	 * (global step 14050: loss: 0.34042641520500183, lr: 0.0001
2023-12-20 17:46:58 INFO     	 * (global step 14100: loss: 0.22453968599438667, lr: 0.0001
2023-12-20 17:47:06 INFO     	 * (global step 14150: loss: 0.3269149512052536, lr: 0.0001
2023-12-20 17:47:10 INFO     [epoch 2/15] average loss: 0.301, lr: 0.0001
2023-12-20 17:47:10 INFO     saving model related files
2023-12-20 17:47:10 INFO     saving model
2023-12-20 17:47:10 INFO     saving tokenizer
2023-12-20 17:47:10 INFO     saving optimizer
2023-12-20 17:47:11 INFO     remove old optimizer files
2023-12-20 17:47:16 INFO     	 * (global step 14200: loss: 0.3135155662894249, lr: 0.0001
2023-12-20 17:47:25 INFO     	 * (global step 14250: loss: 0.23777157813310623, lr: 0.0001
2023-12-20 17:47:33 INFO     	 * (global step 14300: loss: 0.1487441509962082, lr: 0.0001
2023-12-20 17:47:41 INFO     	 * (global step 14350: loss: 0.32092516124248505, lr: 0.0001
2023-12-20 17:47:49 INFO     	 * (global step 14400: loss: 0.3063064515590668, lr: 0.0001
2023-12-20 17:47:57 INFO     	 * (global step 14450: loss: 0.23200588673353195, lr: 0.0001
2023-12-20 17:48:05 INFO     	 * (global step 14500: loss: 0.46683546900749207, lr: 0.0001
2023-12-20 17:48:13 INFO     	 * (global step 14550: loss: 0.20359806716442108, lr: 0.0001
2023-12-20 17:48:21 INFO     	 * (global step 14600: loss: 0.21132917702198029, lr: 0.0001
2023-12-20 17:48:29 INFO     	 * (global step 14650: loss: 0.26808712631464005, lr: 0.0001
2023-12-20 17:48:37 INFO     	 * (global step 14700: loss: 0.3693437799811363, lr: 0.0001
2023-12-20 17:48:45 INFO     	 * (global step 14750: loss: 0.24642974883317947, lr: 0.0001
2023-12-20 17:48:53 INFO     	 * (global step 14800: loss: 0.322937473654747, lr: 0.0001
2023-12-20 17:49:01 INFO     	 * (global step 14850: loss: 0.2600170597434044, lr: 0.0001
2023-12-20 17:49:10 INFO     	 * (global step 14900: loss: 0.3833831176161766, lr: 0.0001
2023-12-20 17:49:18 INFO     	 * (global step 14950: loss: 0.3037773072719574, lr: 0.0001
2023-12-20 17:49:26 INFO     	 * (global step 15000: loss: 0.2180059626698494, lr: 0.0001
2023-12-20 17:49:34 INFO     	 * (global step 15050: loss: 0.24765221774578094, lr: 0.0001
2023-12-20 17:49:42 INFO     	 * (global step 15100: loss: 0.23608457297086716, lr: 0.0001
2023-12-20 17:49:50 INFO     	 * (global step 15150: loss: 0.3790866881608963, lr: 0.0001
2023-12-20 17:49:58 INFO     	 * (global step 15200: loss: 0.41218265891075134, lr: 0.0001
2023-12-20 17:50:06 INFO     	 * (global step 15250: loss: 0.4479391574859619, lr: 0.0001
2023-12-20 17:50:14 INFO     	 * (global step 15300: loss: 0.3088438808917999, lr: 0.0001
2023-12-20 17:50:22 INFO     	 * (global step 15350: loss: 0.2782549113035202, lr: 0.0001
2023-12-20 17:50:30 INFO     	 * (global step 15400: loss: 0.22189433127641678, lr: 0.0001
2023-12-20 17:50:38 INFO     	 * (global step 15450: loss: 0.27625471353530884, lr: 0.0001
2023-12-20 17:50:46 INFO     	 * (global step 15500: loss: 0.29286637902259827, lr: 0.0001
2023-12-20 17:50:54 INFO     	 * (global step 15550: loss: 0.21971970796585083, lr: 0.0001
2023-12-20 17:51:02 INFO     	 * (global step 15600: loss: 0.3697306364774704, lr: 0.0001
2023-12-20 17:51:10 INFO     	 * (global step 15650: loss: 0.3295300304889679, lr: 0.0001
2023-12-20 17:51:18 INFO     	 * (global step 15700: loss: 0.32684604823589325, lr: 0.0001
2023-12-20 17:51:26 INFO     	 * (global step 15750: loss: 0.3753734529018402, lr: 0.0001
2023-12-20 17:51:34 INFO     	 * (global step 15800: loss: 0.2040494903922081, lr: 0.0001
2023-12-20 17:51:42 INFO     	 * (global step 15850: loss: 0.33553948253393173, lr: 0.0001
2023-12-20 17:51:50 INFO     	 * (global step 15900: loss: 0.20567482709884644, lr: 0.0001
2023-12-20 17:51:58 INFO     	 * (global step 15950: loss: 0.3050018772482872, lr: 0.0001
2023-12-20 17:52:07 INFO     	 * (global step 16000: loss: 0.1594764105975628, lr: 0.0001
2023-12-20 17:52:15 INFO     	 * (global step 16050: loss: 0.4151228219270706, lr: 0.0001
2023-12-20 17:52:23 INFO     	 * (global step 16100: loss: 0.33437222987413406, lr: 0.0001
2023-12-20 17:52:31 INFO     	 * (global step 16150: loss: 0.2523704841732979, lr: 0.0001
2023-12-20 17:52:39 INFO     	 * (global step 16200: loss: 0.2851177304983139, lr: 0.0001
2023-12-20 17:52:47 INFO     	 * (global step 16250: loss: 0.31893379986286163, lr: 0.0001
2023-12-20 17:52:55 INFO     	 * (global step 16300: loss: 0.2439952939748764, lr: 0.0001
2023-12-20 17:53:03 INFO     	 * (global step 16350: loss: 0.2988832890987396, lr: 0.0001
2023-12-20 17:53:11 INFO     	 * (global step 16400: loss: 0.32739734649658203, lr: 0.0001
2023-12-20 17:53:19 INFO     	 * (global step 16450: loss: 0.2798065170645714, lr: 0.0001
2023-12-20 17:53:27 INFO     	 * (global step 16500: loss: 0.28629185259342194, lr: 0.0001
2023-12-20 17:53:35 INFO     	 * (global step 16550: loss: 0.3014482259750366, lr: 0.0001
2023-12-20 17:53:43 INFO     	 * (global step 16600: loss: 0.2463492602109909, lr: 0.0001
2023-12-20 17:53:51 INFO     	 * (global step 16650: loss: 0.2984957695007324, lr: 0.0001
2023-12-20 17:53:59 INFO     	 * (global step 16700: loss: 0.19346945732831955, lr: 0.0001
2023-12-20 17:54:07 INFO     	 * (global step 16750: loss: 0.1289362758398056, lr: 0.0001
2023-12-20 17:54:15 INFO     	 * (global step 16800: loss: 0.29831840097904205, lr: 0.0001
2023-12-20 17:54:23 INFO     	 * (global step 16850: loss: 0.34585946798324585, lr: 0.0001
2023-12-20 17:54:31 INFO     	 * (global step 16900: loss: 0.2693219780921936, lr: 0.0001
2023-12-20 17:54:39 INFO     	 * (global step 16950: loss: 0.3202950656414032, lr: 0.0001
2023-12-20 17:54:47 INFO     	 * (global step 17000: loss: 0.22660671174526215, lr: 0.0001
2023-12-20 17:54:55 INFO     	 * (global step 17050: loss: 0.20092298835515976, lr: 0.0001
2023-12-20 17:55:03 INFO     	 * (global step 17100: loss: 0.3111855164170265, lr: 0.0001
2023-12-20 17:55:11 INFO     	 * (global step 17150: loss: 0.24518763273954391, lr: 0.0001
2023-12-20 17:55:19 INFO     	 * (global step 17200: loss: 0.29003677517175674, lr: 0.0001
2023-12-20 17:55:27 INFO     	 * (global step 17250: loss: 0.2712724804878235, lr: 0.0001
2023-12-20 17:55:35 INFO     	 * (global step 17300: loss: 0.20014577358961105, lr: 0.0001
2023-12-20 17:55:43 INFO     	 * (global step 17350: loss: 0.3528253436088562, lr: 0.0001
2023-12-20 17:55:51 INFO     	 * (global step 17400: loss: 0.18821822851896286, lr: 0.0001
2023-12-20 17:55:59 INFO     	 * (global step 17450: loss: 0.43323802202939987, lr: 0.0001
2023-12-20 17:56:07 INFO     	 * (global step 17500: loss: 0.2996262311935425, lr: 0.0001
2023-12-20 17:56:15 INFO     	 * (global step 17550: loss: 0.26238955557346344, lr: 0.0001
2023-12-20 17:56:23 INFO     	 * (global step 17600: loss: 0.2732754498720169, lr: 0.0001
2023-12-20 17:56:31 INFO     	 * (global step 17650: loss: 0.4000013768672943, lr: 0.0001
2023-12-20 17:56:40 INFO     	 * (global step 17700: loss: 0.2529173269867897, lr: 0.0001
2023-12-20 17:56:48 INFO     	 * (global step 17750: loss: 0.3415864259004593, lr: 0.0001
2023-12-20 17:56:56 INFO     	 * (global step 17800: loss: 0.45168137550354004, lr: 0.0001
2023-12-20 17:57:04 INFO     	 * (global step 17850: loss: 0.23264814168214798, lr: 0.0001
2023-12-20 17:57:12 INFO     	 * (global step 17900: loss: 0.30630406737327576, lr: 0.0001
2023-12-20 17:57:20 INFO     	 * (global step 17950: loss: 0.27310027927160263, lr: 0.0001
2023-12-20 17:57:28 INFO     	 * (global step 18000: loss: 0.281439445912838, lr: 0.0001
2023-12-20 17:57:36 INFO     	 * (global step 18050: loss: 0.19075606018304825, lr: 0.0001
2023-12-20 17:57:44 INFO     	 * (global step 18100: loss: 0.3021008223295212, lr: 0.0001
2023-12-20 17:57:52 INFO     	 * (global step 18150: loss: 0.286302849650383, lr: 0.0001
2023-12-20 17:58:00 INFO     	 * (global step 18200: loss: 0.36040347814559937, lr: 0.0001
2023-12-20 17:58:08 INFO     	 * (global step 18250: loss: 0.23977259546518326, lr: 0.0001
2023-12-20 17:58:16 INFO     	 * (global step 18300: loss: 0.26867639273405075, lr: 0.0001
2023-12-20 17:58:24 INFO     	 * (global step 18350: loss: 0.2989097982645035, lr: 0.0001
2023-12-20 17:58:32 INFO     	 * (global step 18400: loss: 0.28799305111169815, lr: 0.0001
2023-12-20 17:58:40 INFO     	 * (global step 18450: loss: 0.3287470042705536, lr: 0.0001
2023-12-20 17:58:48 INFO     	 * (global step 18500: loss: 0.2792894244194031, lr: 0.0001
2023-12-20 17:58:56 INFO     	 * (global step 18550: loss: 0.3289451524615288, lr: 0.0001
2023-12-20 17:59:04 INFO     	 * (global step 18600: loss: 0.39443646371364594, lr: 0.0001
2023-12-20 17:59:12 INFO     	 * (global step 18650: loss: 0.5060535669326782, lr: 0.0001
2023-12-20 17:59:20 INFO     	 * (global step 18700: loss: 0.35843099653720856, lr: 0.0001
2023-12-20 17:59:28 INFO     	 * (global step 18750: loss: 0.19591008871793747, lr: 0.0001
2023-12-20 17:59:36 INFO     	 * (global step 18800: loss: 0.4871596097946167, lr: 0.0001
2023-12-20 17:59:44 INFO     	 * (global step 18850: loss: 0.24751998484134674, lr: 0.0001
2023-12-20 17:59:51 INFO     [epoch 3/15] average loss: 0.289, lr: 0.0001
2023-12-20 17:59:51 INFO     saving model related files
2023-12-20 17:59:51 INFO     saving model
2023-12-20 17:59:52 INFO     saving tokenizer
2023-12-20 17:59:52 INFO     saving optimizer
2023-12-20 17:59:52 INFO     remove old optimizer files
2023-12-20 17:59:54 INFO     	 * (global step 18900: loss: 0.24126236885786057, lr: 0.0001
2023-12-20 18:00:02 INFO     	 * (global step 18950: loss: 0.26237909495830536, lr: 0.0001
2023-12-20 18:00:10 INFO     	 * (global step 19000: loss: 0.48543646931648254, lr: 0.0001
2023-12-20 18:00:18 INFO     	 * (global step 19050: loss: 0.35536710917949677, lr: 0.0001
2023-12-20 18:00:26 INFO     	 * (global step 19100: loss: 0.21043676882982254, lr: 0.0001
2023-12-20 18:00:34 INFO     	 * (global step 19150: loss: 0.22059305012226105, lr: 0.0001
2023-12-20 18:00:42 INFO     	 * (global step 19200: loss: 0.2970532700419426, lr: 0.0001
2023-12-20 18:00:50 INFO     	 * (global step 19250: loss: 0.20218787342309952, lr: 0.0001
2023-12-20 18:00:58 INFO     	 * (global step 19300: loss: 0.20414281636476517, lr: 0.0001
2023-12-20 18:01:07 INFO     	 * (global step 19350: loss: 0.16897157579660416, lr: 0.0001
2023-12-20 18:01:15 INFO     	 * (global step 19400: loss: 0.2819555103778839, lr: 0.0001
2023-12-20 18:01:23 INFO     	 * (global step 19450: loss: 0.1664678379893303, lr: 0.0001
2023-12-20 18:01:31 INFO     	 * (global step 19500: loss: 0.2921825349330902, lr: 0.0001
2023-12-20 18:01:39 INFO     	 * (global step 19550: loss: 0.29000458121299744, lr: 0.0001
2023-12-20 18:01:47 INFO     	 * (global step 19600: loss: 0.2118455357849598, lr: 0.0001
2023-12-20 18:01:55 INFO     	 * (global step 19650: loss: 0.5165088474750519, lr: 0.0001
2023-12-20 18:02:03 INFO     	 * (global step 19700: loss: 0.2761857211589813, lr: 0.0001
2023-12-20 18:02:11 INFO     	 * (global step 19750: loss: 0.2727895677089691, lr: 0.0001
2023-12-20 18:02:19 INFO     	 * (global step 19800: loss: 0.22040103375911713, lr: 0.0001
2023-12-20 18:02:27 INFO     	 * (global step 19850: loss: 0.18576977774500847, lr: 0.0001
2023-12-20 18:02:35 INFO     	 * (global step 19900: loss: 0.30128854513168335, lr: 0.0001
2023-12-20 18:02:43 INFO     	 * (global step 19950: loss: 0.24889559298753738, lr: 0.0001
2023-12-20 18:02:52 INFO     	 * (global step 20000: loss: 0.18598619103431702, lr: 0.0001
2023-12-20 18:03:00 INFO     	 * (global step 20050: loss: 0.2213660329580307, lr: 0.0001
2023-12-20 18:03:08 INFO     	 * (global step 20100: loss: 0.3373931348323822, lr: 0.0001
2023-12-20 18:03:16 INFO     	 * (global step 20150: loss: 0.24095720797777176, lr: 0.0001
2023-12-20 18:03:24 INFO     	 * (global step 20200: loss: 0.2592647671699524, lr: 0.0001
2023-12-20 18:03:32 INFO     	 * (global step 20250: loss: 0.2625441774725914, lr: 0.0001
2023-12-20 18:03:40 INFO     	 * (global step 20300: loss: 0.29073552787303925, lr: 0.0001
2023-12-20 18:03:48 INFO     	 * (global step 20350: loss: 0.22550077736377716, lr: 0.0001
2023-12-20 18:03:56 INFO     	 * (global step 20400: loss: 0.23513681441545486, lr: 0.0001
2023-12-20 18:04:04 INFO     	 * (global step 20450: loss: 0.20199356973171234, lr: 0.0001
2023-12-20 18:04:12 INFO     	 * (global step 20500: loss: 0.20873768627643585, lr: 0.0001
2023-12-20 18:04:20 INFO     	 * (global step 20550: loss: 0.23123537749052048, lr: 0.0001
2023-12-20 18:04:28 INFO     	 * (global step 20600: loss: 0.2836463525891304, lr: 0.0001
2023-12-20 18:04:36 INFO     	 * (global step 20650: loss: 0.3867341876029968, lr: 0.0001
2023-12-20 18:04:44 INFO     	 * (global step 20700: loss: 0.21925906836986542, lr: 0.0001
2023-12-20 18:04:52 INFO     	 * (global step 20750: loss: 0.22965586185455322, lr: 0.0001
2023-12-20 18:05:00 INFO     	 * (global step 20800: loss: 0.3205198422074318, lr: 0.0001
2023-12-20 18:05:08 INFO     	 * (global step 20850: loss: 0.2823002189397812, lr: 0.0001
2023-12-20 18:05:16 INFO     	 * (global step 20900: loss: 0.30006979405879974, lr: 0.0001
2023-12-20 18:05:24 INFO     	 * (global step 20950: loss: 0.285110741853714, lr: 0.0001
2023-12-20 18:05:32 INFO     	 * (global step 21000: loss: 0.4229329600930214, lr: 0.0001
2023-12-20 18:05:41 INFO     	 * (global step 21050: loss: 0.24991875141859055, lr: 0.0001
2023-12-20 18:05:49 INFO     	 * (global step 21100: loss: 0.22678419947624207, lr: 0.0001
2023-12-20 18:05:57 INFO     	 * (global step 21150: loss: 0.25463200360536575, lr: 0.0001
2023-12-20 18:06:05 INFO     	 * (global step 21200: loss: 0.3177981376647949, lr: 0.0001
2023-12-20 18:06:13 INFO     	 * (global step 21250: loss: 0.23819900304079056, lr: 0.0001
2023-12-20 18:06:21 INFO     	 * (global step 21300: loss: 0.2843959107995033, lr: 0.0001
2023-12-20 18:06:29 INFO     	 * (global step 21350: loss: 0.2245904505252838, lr: 0.0001
2023-12-20 18:06:37 INFO     	 * (global step 21400: loss: 0.1870378777384758, lr: 0.0001
2023-12-20 18:06:45 INFO     	 * (global step 21450: loss: 0.46051034331321716, lr: 0.0001
2023-12-20 18:06:53 INFO     	 * (global step 21500: loss: 0.25666916370391846, lr: 0.0001
2023-12-20 18:07:01 INFO     	 * (global step 21550: loss: 0.2717542052268982, lr: 0.0001
2023-12-20 18:07:09 INFO     	 * (global step 21600: loss: 0.32972266525030136, lr: 0.0001
2023-12-20 18:07:17 INFO     	 * (global step 21650: loss: 0.23807231336832047, lr: 0.0001
2023-12-20 18:07:25 INFO     	 * (global step 21700: loss: 0.1668468341231346, lr: 0.0001
2023-12-20 18:07:33 INFO     	 * (global step 21750: loss: 0.33381541818380356, lr: 0.0001
2023-12-20 18:07:42 INFO     	 * (global step 21800: loss: 0.29275304824113846, lr: 0.0001
2023-12-20 18:07:50 INFO     	 * (global step 21850: loss: 0.35127121210098267, lr: 0.0001
2023-12-20 18:07:58 INFO     	 * (global step 21900: loss: 0.25643055886030197, lr: 0.0001
2023-12-20 18:08:06 INFO     	 * (global step 21950: loss: 0.28650201857089996, lr: 0.0001
2023-12-20 18:08:14 INFO     	 * (global step 22000: loss: 0.28250371664762497, lr: 0.0001
2023-12-20 18:08:22 INFO     	 * (global step 22050: loss: 0.32394639402627945, lr: 0.0001
2023-12-20 18:08:30 INFO     	 * (global step 22100: loss: 0.3870999366044998, lr: 0.0001
2023-12-20 18:08:38 INFO     	 * (global step 22150: loss: 0.42811043560504913, lr: 0.0001
2023-12-20 18:08:46 INFO     	 * (global step 22200: loss: 0.19084323942661285, lr: 0.0001
2023-12-20 18:08:55 INFO     	 * (global step 22250: loss: 0.2416144758462906, lr: 0.0001
2023-12-20 18:09:03 INFO     	 * (global step 22300: loss: 0.28080688416957855, lr: 0.0001
2023-12-20 18:09:11 INFO     	 * (global step 22350: loss: 0.3480326235294342, lr: 0.0001
2023-12-20 18:09:19 INFO     	 * (global step 22400: loss: 0.18859810382127762, lr: 0.0001
2023-12-20 18:09:27 INFO     	 * (global step 22450: loss: 0.3836170732975006, lr: 0.0001
2023-12-20 18:09:35 INFO     	 * (global step 22500: loss: 0.28373152017593384, lr: 0.0001
2023-12-20 18:09:43 INFO     	 * (global step 22550: loss: 0.2723863571882248, lr: 0.0001
2023-12-20 18:09:51 INFO     	 * (global step 22600: loss: 0.2857648581266403, lr: 0.0001
2023-12-20 18:09:59 INFO     	 * (global step 22650: loss: 0.15350624546408653, lr: 0.0001
2023-12-20 18:10:07 INFO     	 * (global step 22700: loss: 0.3744552433490753, lr: 0.0001
2023-12-20 18:10:15 INFO     	 * (global step 22750: loss: 0.3001786917448044, lr: 0.0001
2023-12-20 18:10:23 INFO     	 * (global step 22800: loss: 0.37189166247844696, lr: 0.0001
2023-12-20 18:10:31 INFO     	 * (global step 22850: loss: 0.2852146625518799, lr: 0.0001
2023-12-20 18:10:39 INFO     	 * (global step 22900: loss: 0.2975253611803055, lr: 0.0001
2023-12-20 18:10:47 INFO     	 * (global step 22950: loss: 0.20938292145729065, lr: 0.0001
2023-12-20 18:10:55 INFO     	 * (global step 23000: loss: 0.3392571210861206, lr: 0.0001
2023-12-20 18:11:04 INFO     	 * (global step 23050: loss: 0.4409485161304474, lr: 0.0001
2023-12-20 18:11:12 INFO     	 * (global step 23100: loss: 0.316413052380085, lr: 0.0001
2023-12-20 18:11:20 INFO     	 * (global step 23150: loss: 0.25836367905139923, lr: 0.0001
2023-12-20 18:11:28 INFO     	 * (global step 23200: loss: 0.26430879533290863, lr: 0.0001
2023-12-20 18:11:36 INFO     	 * (global step 23250: loss: 0.141017634421587, lr: 0.0001
2023-12-20 18:11:44 INFO     	 * (global step 23300: loss: 0.24921534210443497, lr: 0.0001
2023-12-20 18:11:52 INFO     	 * (global step 23350: loss: 0.2611812502145767, lr: 0.0001
2023-12-20 18:12:00 INFO     	 * (global step 23400: loss: 0.33587557077407837, lr: 0.0001
2023-12-20 18:12:09 INFO     	 * (global step 23450: loss: 0.3543877378106117, lr: 0.0001
2023-12-20 18:12:17 INFO     	 * (global step 23500: loss: 0.22074054181575775, lr: 0.0001
2023-12-20 18:12:25 INFO     	 * (global step 23550: loss: 0.2740589454770088, lr: 0.0001
2023-12-20 18:12:33 INFO     	 * (global step 23600: loss: 0.3396383821964264, lr: 0.0001
2023-12-20 18:12:35 INFO     [epoch 4/15] average loss: 0.279, lr: 0.0001
2023-12-20 18:12:35 INFO     saving model related files
2023-12-20 18:12:35 INFO     saving model
2023-12-20 18:12:36 INFO     saving tokenizer
2023-12-20 18:12:36 INFO     saving optimizer
2023-12-20 18:12:37 INFO     remove old optimizer files
2023-12-20 18:12:42 INFO     	 * (global step 23650: loss: 0.39981672167778015, lr: 0.0001
2023-12-20 18:12:50 INFO     	 * (global step 23700: loss: 0.22026581317186356, lr: 0.0001
2023-12-20 18:12:58 INFO     	 * (global step 23750: loss: 0.2852151691913605, lr: 0.0001
2023-12-20 18:13:07 INFO     	 * (global step 23800: loss: 0.21686579659581184, lr: 0.0001
2023-12-20 18:13:15 INFO     	 * (global step 23850: loss: 0.3118506222963333, lr: 0.0001
2023-12-20 18:13:23 INFO     	 * (global step 23900: loss: 0.2416154444217682, lr: 0.0001
2023-12-20 18:13:31 INFO     	 * (global step 23950: loss: 0.21642886847257614, lr: 0.0001
2023-12-20 18:13:39 INFO     	 * (global step 24000: loss: 0.3231402486562729, lr: 0.0001
2023-12-20 18:13:47 INFO     	 * (global step 24050: loss: 0.41516707092523575, lr: 0.0001
2023-12-20 18:13:55 INFO     	 * (global step 24100: loss: 0.23989125341176987, lr: 0.0001
2023-12-20 18:14:03 INFO     	 * (global step 24150: loss: 0.3320179581642151, lr: 0.0001
2023-12-20 18:14:11 INFO     	 * (global step 24200: loss: 0.3246507942676544, lr: 0.0001
2023-12-20 18:14:19 INFO     	 * (global step 24250: loss: 0.39527249336242676, lr: 0.0001
2023-12-20 18:14:27 INFO     	 * (global step 24300: loss: 0.25887712091207504, lr: 0.0001
2023-12-20 18:14:35 INFO     	 * (global step 24350: loss: 0.34158919751644135, lr: 0.0001
2023-12-20 18:14:43 INFO     	 * (global step 24400: loss: 0.26784496009349823, lr: 0.0001
2023-12-20 18:14:51 INFO     	 * (global step 24450: loss: 0.3514682352542877, lr: 0.0001
2023-12-20 18:14:59 INFO     	 * (global step 24500: loss: 0.21868792921304703, lr: 0.0001
2023-12-20 18:15:07 INFO     	 * (global step 24550: loss: 0.3109421730041504, lr: 0.0001
2023-12-20 18:15:15 INFO     	 * (global step 24600: loss: 0.30236805230379105, lr: 0.0001
2023-12-20 18:15:24 INFO     	 * (global step 24650: loss: 0.19993148744106293, lr: 0.0001
2023-12-20 18:15:32 INFO     	 * (global step 24700: loss: 0.3079506531357765, lr: 0.0001
2023-12-20 18:15:40 INFO     	 * (global step 24750: loss: 0.25363386422395706, lr: 0.0001
2023-12-20 18:15:48 INFO     	 * (global step 24800: loss: 0.27459007501602173, lr: 0.0001
2023-12-20 18:15:56 INFO     	 * (global step 24850: loss: 0.324444979429245, lr: 0.0001
2023-12-20 18:16:04 INFO     	 * (global step 24900: loss: 0.31631772220134735, lr: 0.0001
2023-12-20 18:16:12 INFO     	 * (global step 24950: loss: 0.2634746581315994, lr: 0.0001
2023-12-20 18:16:20 INFO     	 * (global step 25000: loss: 0.3017366975545883, lr: 0.0001
2023-12-20 18:16:28 INFO     	 * (global step 25050: loss: 0.15697142481803894, lr: 0.0001
2023-12-20 18:16:36 INFO     	 * (global step 25100: loss: 0.3159841224551201, lr: 0.0001
2023-12-20 18:16:44 INFO     	 * (global step 25150: loss: 0.7295415103435516, lr: 0.0001
2023-12-20 18:16:52 INFO     	 * (global step 25200: loss: 0.28538039326667786, lr: 0.0001
2023-12-20 18:17:00 INFO     	 * (global step 25250: loss: 0.25393108278512955, lr: 0.0001
2023-12-20 18:17:08 INFO     	 * (global step 25300: loss: 0.30462557822465897, lr: 0.0001
2023-12-20 18:17:16 INFO     	 * (global step 25350: loss: 0.2599557191133499, lr: 0.0001
2023-12-20 18:17:24 INFO     	 * (global step 25400: loss: 0.24385125190019608, lr: 0.0001
2023-12-20 18:17:32 INFO     	 * (global step 25450: loss: 0.24537105858325958, lr: 0.0001
2023-12-20 18:17:40 INFO     	 * (global step 25500: loss: 0.29170694947242737, lr: 0.0001
2023-12-20 18:17:48 INFO     	 * (global step 25550: loss: 0.33766238391399384, lr: 0.0001
2023-12-20 18:17:56 INFO     	 * (global step 25600: loss: 0.2443147599697113, lr: 0.0001
2023-12-20 18:18:05 INFO     	 * (global step 25650: loss: 0.20914360135793686, lr: 0.0001
2023-12-20 18:18:13 INFO     	 * (global step 25700: loss: 0.3200403302907944, lr: 0.0001
2023-12-20 18:18:21 INFO     	 * (global step 25750: loss: 0.23306190222501755, lr: 0.0001
2023-12-20 18:18:29 INFO     	 * (global step 25800: loss: 0.2705727368593216, lr: 0.0001
2023-12-20 18:18:37 INFO     	 * (global step 25850: loss: 0.2558381035923958, lr: 0.0001
2023-12-20 18:18:45 INFO     	 * (global step 25900: loss: 0.5021561086177826, lr: 0.0001
2023-12-20 18:18:53 INFO     	 * (global step 25950: loss: 0.18160232156515121, lr: 0.0001
2023-12-20 18:19:01 INFO     	 * (global step 26000: loss: 0.32582077383995056, lr: 0.0001
2023-12-20 18:19:09 INFO     	 * (global step 26050: loss: 0.3937885910272598, lr: 0.0001
2023-12-20 18:19:17 INFO     	 * (global step 26100: loss: 0.30337242037057877, lr: 0.0001
2023-12-20 18:19:25 INFO     	 * (global step 26150: loss: 0.2540189027786255, lr: 0.0001
2023-12-20 18:19:33 INFO     	 * (global step 26200: loss: 0.4291854798793793, lr: 0.0001
2023-12-20 18:19:41 INFO     	 * (global step 26250: loss: 0.2430640384554863, lr: 0.0001
2023-12-20 18:19:49 INFO     	 * (global step 26300: loss: 0.365701362490654, lr: 0.0001
2023-12-20 18:19:57 INFO     	 * (global step 26350: loss: 0.21537934243679047, lr: 0.0001
2023-12-20 18:20:05 INFO     	 * (global step 26400: loss: 0.35451942682266235, lr: 0.0001
2023-12-20 18:20:13 INFO     	 * (global step 26450: loss: 0.23590098321437836, lr: 0.0001
2023-12-20 18:20:21 INFO     	 * (global step 26500: loss: 0.23336812108755112, lr: 0.0001
2023-12-20 18:20:29 INFO     	 * (global step 26550: loss: 0.2519474923610687, lr: 0.0001
2023-12-20 18:20:37 INFO     	 * (global step 26600: loss: 0.2050919309258461, lr: 0.0001
2023-12-20 18:20:46 INFO     	 * (global step 26650: loss: 0.3234940618276596, lr: 0.0001
2023-12-20 18:20:54 INFO     	 * (global step 26700: loss: 0.14117463678121567, lr: 0.0001
2023-12-20 18:21:02 INFO     	 * (global step 26750: loss: 0.20912519097328186, lr: 0.0001
2023-12-20 18:21:10 INFO     	 * (global step 26800: loss: 0.2953801453113556, lr: 0.0001
2023-12-20 18:21:18 INFO     	 * (global step 26850: loss: 0.1986789032816887, lr: 0.0001
2023-12-20 18:21:26 INFO     	 * (global step 26900: loss: 0.21453243494033813, lr: 0.0001
2023-12-20 18:21:34 INFO     	 * (global step 26950: loss: 0.2923752963542938, lr: 0.0001
2023-12-20 18:21:42 INFO     	 * (global step 27000: loss: 0.28754376620054245, lr: 0.0001
2023-12-20 18:21:50 INFO     	 * (global step 27050: loss: 0.183517724275589, lr: 0.0001
2023-12-20 18:21:58 INFO     	 * (global step 27100: loss: 0.2855837196111679, lr: 0.0001
2023-12-20 18:22:06 INFO     	 * (global step 27150: loss: 0.30053889751434326, lr: 0.0001
2023-12-20 18:22:14 INFO     	 * (global step 27200: loss: 0.4407432824373245, lr: 0.0001
2023-12-20 18:22:22 INFO     	 * (global step 27250: loss: 0.232603307813406, lr: 0.0001
2023-12-20 18:22:31 INFO     	 * (global step 27300: loss: 0.272098146378994, lr: 0.0001
2023-12-20 18:22:39 INFO     	 * (global step 27350: loss: 0.18129034340381622, lr: 0.0001
2023-12-20 18:22:47 INFO     	 * (global step 27400: loss: 0.2510508820414543, lr: 0.0001
2023-12-20 18:22:55 INFO     	 * (global step 27450: loss: 0.3104972690343857, lr: 0.0001
2023-12-20 18:23:03 INFO     	 * (global step 27500: loss: 0.2831896245479584, lr: 0.0001
2023-12-20 18:23:11 INFO     	 * (global step 27550: loss: 0.3829435259103775, lr: 0.0001
2023-12-20 18:23:19 INFO     	 * (global step 27600: loss: 0.25227194279432297, lr: 0.0001
2023-12-20 18:23:27 INFO     	 * (global step 27650: loss: 0.27106859534978867, lr: 0.0001
2023-12-20 18:23:35 INFO     	 * (global step 27700: loss: 0.2859691008925438, lr: 0.0001
2023-12-20 18:23:43 INFO     	 * (global step 27750: loss: 0.2144283875823021, lr: 0.0001
2023-12-20 18:23:51 INFO     	 * (global step 27800: loss: 0.2610223740339279, lr: 0.0001
2023-12-20 18:23:59 INFO     	 * (global step 27850: loss: 0.3346105217933655, lr: 0.0001
2023-12-20 18:24:07 INFO     	 * (global step 27900: loss: 0.2714061439037323, lr: 0.0001
2023-12-20 18:24:15 INFO     	 * (global step 27950: loss: 0.38865330815315247, lr: 0.0001
2023-12-20 18:24:23 INFO     	 * (global step 28000: loss: 0.30375704914331436, lr: 0.0001
2023-12-20 18:24:31 INFO     	 * (global step 28050: loss: 0.2646310031414032, lr: 0.0001
2023-12-20 18:24:40 INFO     	 * (global step 28100: loss: 0.3736805021762848, lr: 0.0001
2023-12-20 18:24:48 INFO     	 * (global step 28150: loss: 0.32493438571691513, lr: 0.0001
2023-12-20 18:24:56 INFO     	 * (global step 28200: loss: 0.4779650419950485, lr: 0.0001
2023-12-20 18:25:04 INFO     	 * (global step 28250: loss: 0.1876078024506569, lr: 0.0001
2023-12-20 18:25:12 INFO     	 * (global step 28300: loss: 0.13225648924708366, lr: 0.0001
2023-12-20 18:25:18 INFO     [epoch 5/15] average loss: 0.271, lr: 0.0001
2023-12-20 18:25:18 INFO     saving model related files
2023-12-20 18:25:18 INFO     saving model
2023-12-20 18:25:18 INFO     saving tokenizer
2023-12-20 18:25:18 INFO     saving optimizer
2023-12-20 18:25:19 INFO     remove old optimizer files
2023-12-20 18:25:21 INFO     	 * (global step 28350: loss: 0.30494435876607895, lr: 0.0001
2023-12-20 18:25:29 INFO     	 * (global step 28400: loss: 0.2864616811275482, lr: 0.0001
2023-12-20 18:25:37 INFO     	 * (global step 28450: loss: 0.14023136347532272, lr: 0.0001
2023-12-20 18:25:45 INFO     	 * (global step 28500: loss: 0.3065795600414276, lr: 0.0001
2023-12-20 18:25:53 INFO     	 * (global step 28550: loss: 0.14823468774557114, lr: 0.0001
2023-12-20 18:26:01 INFO     	 * (global step 28600: loss: 0.22434978932142258, lr: 0.0001
2023-12-20 18:26:10 INFO     	 * (global step 28650: loss: 0.3150828182697296, lr: 0.0001
2023-12-20 18:26:18 INFO     	 * (global step 28700: loss: 0.21916808933019638, lr: 0.0001
2023-12-20 18:26:26 INFO     	 * (global step 28750: loss: 0.17865049093961716, lr: 0.0001
2023-12-20 18:26:34 INFO     	 * (global step 28800: loss: 0.18358857929706573, lr: 0.0001
2023-12-20 18:26:42 INFO     	 * (global step 28850: loss: 0.2598281502723694, lr: 0.0001
2023-12-20 18:26:50 INFO     	 * (global step 28900: loss: 0.33871524035930634, lr: 0.0001
2023-12-20 18:26:58 INFO     	 * (global step 28950: loss: 0.17082183063030243, lr: 0.0001
2023-12-20 18:27:06 INFO     	 * (global step 29000: loss: 0.22138355672359467, lr: 0.0001
2023-12-20 18:27:14 INFO     	 * (global step 29050: loss: 0.19403044134378433, lr: 0.0001
2023-12-20 18:27:22 INFO     	 * (global step 29100: loss: 0.2629057914018631, lr: 0.0001
2023-12-20 18:27:30 INFO     	 * (global step 29150: loss: 0.29399508237838745, lr: 0.0001
2023-12-20 18:27:38 INFO     	 * (global step 29200: loss: 0.3674326092004776, lr: 0.0001
2023-12-20 18:27:47 INFO     	 * (global step 29250: loss: 0.21977247297763824, lr: 0.0001
2023-12-20 18:27:55 INFO     	 * (global step 29300: loss: 0.2058594822883606, lr: 0.0001
2023-12-20 18:28:03 INFO     	 * (global step 29350: loss: 0.26997070759534836, lr: 0.0001
2023-12-20 18:28:11 INFO     	 * (global step 29400: loss: 0.18322035297751427, lr: 0.0001
2023-12-20 18:28:19 INFO     	 * (global step 29450: loss: 0.28532078862190247, lr: 0.0001
2023-12-20 18:28:27 INFO     	 * (global step 29500: loss: 0.20141446590423584, lr: 0.0001
2023-12-20 18:28:35 INFO     	 * (global step 29550: loss: 0.23521729558706284, lr: 0.0001
2023-12-20 18:28:43 INFO     	 * (global step 29600: loss: 0.283493235707283, lr: 0.0001
2023-12-20 18:28:51 INFO     	 * (global step 29650: loss: 0.20434072613716125, lr: 0.0001
2023-12-20 18:28:59 INFO     	 * (global step 29700: loss: 0.23227432370185852, lr: 0.0001
2023-12-20 18:29:07 INFO     	 * (global step 29750: loss: 0.21953046321868896, lr: 0.0001
2023-12-20 18:29:15 INFO     	 * (global step 29800: loss: 0.5453434139490128, lr: 0.0001
2023-12-20 18:29:23 INFO     	 * (global step 29850: loss: 0.2047569714486599, lr: 0.0001
2023-12-20 18:29:31 INFO     	 * (global step 29900: loss: 0.3288632929325104, lr: 0.0001
2023-12-20 18:29:39 INFO     	 * (global step 29950: loss: 0.22645893692970276, lr: 0.0001
2023-12-20 18:29:47 INFO     	 * (global step 30000: loss: 0.1436513438820839, lr: 0.0001
2023-12-20 18:29:56 INFO     	 * (global step 30050: loss: 0.18178905546665192, lr: 0.0001
2023-12-20 18:30:04 INFO     	 * (global step 30100: loss: 0.2531168609857559, lr: 0.0001
2023-12-20 18:30:12 INFO     	 * (global step 30150: loss: 0.270131953060627, lr: 0.0001
2023-12-20 18:30:20 INFO     	 * (global step 30200: loss: 0.16913698986172676, lr: 0.0001
2023-12-20 18:30:28 INFO     	 * (global step 30250: loss: 0.27326008677482605, lr: 0.0001
2023-12-20 18:30:36 INFO     	 * (global step 30300: loss: 0.19358789175748825, lr: 0.0001
2023-12-20 18:30:44 INFO     	 * (global step 30350: loss: 0.23539484292268753, lr: 0.0001
2023-12-20 18:30:52 INFO     	 * (global step 30400: loss: 0.23971929401159286, lr: 0.0001
2023-12-20 18:31:00 INFO     	 * (global step 30450: loss: 0.27801502496004105, lr: 0.0001
2023-12-20 18:31:08 INFO     	 * (global step 30500: loss: 0.18274928629398346, lr: 0.0001
2023-12-20 18:31:16 INFO     	 * (global step 30550: loss: 0.2303961217403412, lr: 0.0001
2023-12-20 18:31:25 INFO     	 * (global step 30600: loss: 0.4613683670759201, lr: 0.0001
2023-12-20 18:31:33 INFO     	 * (global step 30650: loss: 0.21204660087823868, lr: 0.0001
2023-12-20 18:31:41 INFO     	 * (global step 30700: loss: 0.2545270472764969, lr: 0.0001
2023-12-20 18:31:49 INFO     	 * (global step 30750: loss: 0.24567219614982605, lr: 0.0001
2023-12-20 18:31:57 INFO     	 * (global step 30800: loss: 0.1651785522699356, lr: 0.0001
2023-12-20 18:32:05 INFO     	 * (global step 30850: loss: 0.19726008921861649, lr: 0.0001
2023-12-20 18:32:13 INFO     	 * (global step 30900: loss: 0.23434922844171524, lr: 0.0001
2023-12-20 18:32:21 INFO     	 * (global step 30950: loss: 0.3256928622722626, lr: 0.0001
2023-12-20 18:32:29 INFO     	 * (global step 31000: loss: 0.2623453512787819, lr: 0.0001
2023-12-20 18:32:37 INFO     	 * (global step 31050: loss: 0.24626397341489792, lr: 0.0001
2023-12-20 18:32:46 INFO     	 * (global step 31100: loss: 0.2433420568704605, lr: 0.0001
2023-12-20 18:32:54 INFO     	 * (global step 31150: loss: 0.20831311494112015, lr: 0.0001
2023-12-20 18:33:02 INFO     	 * (global step 31200: loss: 0.2870246022939682, lr: 0.0001
2023-12-20 18:33:10 INFO     	 * (global step 31250: loss: 0.23621252179145813, lr: 0.0001
2023-12-20 18:33:18 INFO     	 * (global step 31300: loss: 0.21913212537765503, lr: 0.0001
2023-12-20 18:33:26 INFO     	 * (global step 31350: loss: 0.32564152777194977, lr: 0.0001
2023-12-20 18:33:34 INFO     	 * (global step 31400: loss: 0.2368583083152771, lr: 0.0001
2023-12-20 18:33:42 INFO     	 * (global step 31450: loss: 0.2627502977848053, lr: 0.0001
2023-12-20 18:33:50 INFO     	 * (global step 31500: loss: 0.443256251513958, lr: 0.0001
2023-12-20 18:33:58 INFO     	 * (global step 31550: loss: 0.2127804458141327, lr: 0.0001
2023-12-20 18:34:06 INFO     	 * (global step 31600: loss: 0.22443663328886032, lr: 0.0001
2023-12-20 18:34:15 INFO     	 * (global step 31650: loss: 0.17479102313518524, lr: 0.0001
2023-12-20 18:34:23 INFO     	 * (global step 31700: loss: 0.275846928358078, lr: 0.0001
2023-12-20 18:34:31 INFO     	 * (global step 31750: loss: 0.380905419588089, lr: 0.0001
2023-12-20 18:34:39 INFO     	 * (global step 31800: loss: 0.1960434392094612, lr: 0.0001
2023-12-20 18:34:47 INFO     	 * (global step 31850: loss: 0.25135670602321625, lr: 0.0001
2023-12-20 18:34:55 INFO     	 * (global step 31900: loss: 0.2596695199608803, lr: 0.0001
2023-12-20 18:35:03 INFO     	 * (global step 31950: loss: 0.20357824116945267, lr: 0.0001
2023-12-20 18:35:11 INFO     	 * (global step 32000: loss: 0.20382293313741684, lr: 0.0001
2023-12-20 18:35:19 INFO     	 * (global step 32050: loss: 0.3102835863828659, lr: 0.0001
2023-12-20 18:35:27 INFO     	 * (global step 32100: loss: 0.2804695814847946, lr: 0.0001
2023-12-20 18:35:35 INFO     	 * (global step 32150: loss: 0.2767818346619606, lr: 0.0001
2023-12-20 18:35:43 INFO     	 * (global step 32200: loss: 0.2806825563311577, lr: 0.0001
2023-12-20 18:35:51 INFO     	 * (global step 32250: loss: 0.18658556789159775, lr: 0.0001
2023-12-20 18:35:59 INFO     	 * (global step 32300: loss: 0.2592295631766319, lr: 0.0001
2023-12-20 18:36:07 INFO     	 * (global step 32350: loss: 0.2402830794453621, lr: 0.0001
2023-12-20 18:36:16 INFO     	 * (global step 32400: loss: 0.17301412671804428, lr: 0.0001
2023-12-20 18:36:24 INFO     	 * (global step 32450: loss: 0.21896426379680634, lr: 0.0001
2023-12-20 18:36:32 INFO     	 * (global step 32500: loss: 0.2797045111656189, lr: 0.0001
2023-12-20 18:36:40 INFO     	 * (global step 32550: loss: 0.2802124544978142, lr: 0.0001
2023-12-20 18:36:48 INFO     	 * (global step 32600: loss: 0.15207930654287338, lr: 0.0001
2023-12-20 18:36:56 INFO     	 * (global step 32650: loss: 0.41725628077983856, lr: 0.0001
2023-12-20 18:37:04 INFO     	 * (global step 32700: loss: 0.23180902749300003, lr: 0.0001
2023-12-20 18:37:12 INFO     	 * (global step 32750: loss: 0.3127550110220909, lr: 0.0001
2023-12-20 18:37:20 INFO     	 * (global step 32800: loss: 0.18206530064344406, lr: 0.0001
2023-12-20 18:37:28 INFO     	 * (global step 32850: loss: 0.19812211394309998, lr: 0.0001
2023-12-20 18:37:36 INFO     	 * (global step 32900: loss: 0.3642219305038452, lr: 0.0001
2023-12-20 18:37:44 INFO     	 * (global step 32950: loss: 0.36062435805797577, lr: 0.0001
2023-12-20 18:37:52 INFO     	 * (global step 33000: loss: 0.21500489115715027, lr: 0.0001
2023-12-20 18:38:00 INFO     	 * (global step 33050: loss: 0.23407375812530518, lr: 0.0001
2023-12-20 18:38:02 INFO     [epoch 6/15] average loss: 0.264, lr: 0.0001
2023-12-20 18:38:02 INFO     saving model related files
2023-12-20 18:38:02 INFO     saving model
2023-12-20 18:38:02 INFO     saving tokenizer
2023-12-20 18:38:02 INFO     saving optimizer
2023-12-20 18:38:03 INFO     remove old optimizer files
2023-12-20 18:38:10 INFO     	 * (global step 33100: loss: 0.25825853645801544, lr: 0.0001
2023-12-20 18:38:18 INFO     	 * (global step 33150: loss: 0.2972260043025017, lr: 0.0001
2023-12-20 18:38:26 INFO     	 * (global step 33200: loss: 0.31830140948295593, lr: 0.0001
2023-12-20 18:38:34 INFO     	 * (global step 33250: loss: 0.2592499926686287, lr: 0.0001
2023-12-20 18:38:42 INFO     	 * (global step 33300: loss: 0.2190180942416191, lr: 0.0001
2023-12-20 18:38:50 INFO     	 * (global step 33350: loss: 0.2606041580438614, lr: 0.0001
2023-12-20 18:38:58 INFO     	 * (global step 33400: loss: 0.20109526813030243, lr: 0.0001
2023-12-20 18:39:06 INFO     	 * (global step 33450: loss: 0.24840154498815536, lr: 0.0001
2023-12-20 18:39:14 INFO     	 * (global step 33500: loss: 0.204464852809906, lr: 0.0001
2023-12-20 18:39:22 INFO     	 * (global step 33550: loss: 0.27518463134765625, lr: 0.0001
2023-12-20 18:39:30 INFO     	 * (global step 33600: loss: 0.24082612991333008, lr: 0.0001
2023-12-20 18:39:38 INFO     	 * (global step 33650: loss: 0.3287433832883835, lr: 0.0001
2023-12-20 18:39:46 INFO     	 * (global step 33700: loss: 0.1871819645166397, lr: 0.0001
2023-12-20 18:39:54 INFO     	 * (global step 33750: loss: 0.36553855240345, lr: 0.0001
2023-12-20 18:40:02 INFO     	 * (global step 33800: loss: 0.30957721918821335, lr: 0.0001
2023-12-20 18:40:10 INFO     	 * (global step 33850: loss: 0.26286935806274414, lr: 0.0001
2023-12-20 18:40:18 INFO     	 * (global step 33900: loss: 0.2646247148513794, lr: 0.0001
2023-12-20 18:40:26 INFO     	 * (global step 33950: loss: 0.25870149582624435, lr: 0.0001
2023-12-20 18:40:34 INFO     	 * (global step 34000: loss: 0.1973947435617447, lr: 0.0001
2023-12-20 18:40:42 INFO     	 * (global step 34050: loss: 0.2984907403588295, lr: 0.0001
2023-12-20 18:40:50 INFO     	 * (global step 34100: loss: 0.24506506323814392, lr: 0.0001
2023-12-20 18:40:58 INFO     	 * (global step 34150: loss: 0.4255058467388153, lr: 0.0001
2023-12-20 18:41:06 INFO     	 * (global step 34200: loss: 0.3267756998538971, lr: 0.0001
2023-12-20 18:41:14 INFO     	 * (global step 34250: loss: 0.25274277478456497, lr: 0.0001
2023-12-20 18:41:22 INFO     	 * (global step 34300: loss: 0.21789265424013138, lr: 0.0001
2023-12-20 18:41:30 INFO     	 * (global step 34350: loss: 0.15494507551193237, lr: 0.0001
2023-12-20 18:41:38 INFO     	 * (global step 34400: loss: 0.21779567748308182, lr: 0.0001
2023-12-20 18:41:46 INFO     	 * (global step 34450: loss: 0.30559349060058594, lr: 0.0001
2023-12-20 18:41:54 INFO     	 * (global step 34500: loss: 0.24552972614765167, lr: 0.0001
2023-12-20 18:42:02 INFO     	 * (global step 34550: loss: 0.26151642203330994, lr: 0.0001
2023-12-20 18:42:10 INFO     	 * (global step 34600: loss: 0.19935089349746704, lr: 0.0001
2023-12-20 18:42:18 INFO     	 * (global step 34650: loss: 0.20553383976221085, lr: 0.0001
2023-12-20 18:42:26 INFO     	 * (global step 34700: loss: 0.3125576004385948, lr: 0.0001
2023-12-20 18:42:34 INFO     	 * (global step 34750: loss: 0.2123267501592636, lr: 0.0001
2023-12-20 18:42:42 INFO     	 * (global step 34800: loss: 0.17701367288827896, lr: 0.0001
2023-12-20 18:42:50 INFO     	 * (global step 34850: loss: 0.23042617738246918, lr: 0.0001
2023-12-20 18:42:58 INFO     	 * (global step 34900: loss: 0.1847316101193428, lr: 0.0001
2023-12-20 18:43:06 INFO     	 * (global step 34950: loss: 0.2640499845147133, lr: 0.0001
2023-12-20 18:43:14 INFO     	 * (global step 35000: loss: 0.2587156370282173, lr: 0.0001
2023-12-20 18:43:22 INFO     	 * (global step 35050: loss: 0.38734786212444305, lr: 0.0001
2023-12-20 18:43:30 INFO     	 * (global step 35100: loss: 0.32193176448345184, lr: 0.0001
2023-12-20 18:43:38 INFO     	 * (global step 35150: loss: 0.2961084023118019, lr: 0.0001
2023-12-20 18:43:46 INFO     	 * (global step 35200: loss: 0.2613377422094345, lr: 0.0001
2023-12-20 18:43:54 INFO     	 * (global step 35250: loss: 0.28056828677654266, lr: 0.0001
2023-12-20 18:44:02 INFO     	 * (global step 35300: loss: 0.307781346142292, lr: 0.0001
2023-12-20 18:44:10 INFO     	 * (global step 35350: loss: 0.19481248781085014, lr: 0.0001
2023-12-20 18:44:18 INFO     	 * (global step 35400: loss: 0.18951642513275146, lr: 0.0001
2023-12-20 18:44:26 INFO     	 * (global step 35450: loss: 0.33184848725795746, lr: 0.0001
2023-12-20 18:44:34 INFO     	 * (global step 35500: loss: 0.27972362190485, lr: 0.0001
2023-12-20 18:44:42 INFO     	 * (global step 35550: loss: 0.15988001227378845, lr: 0.0001
2023-12-20 18:44:50 INFO     	 * (global step 35600: loss: 0.26314492523670197, lr: 0.0001
2023-12-20 18:44:58 INFO     	 * (global step 35650: loss: 0.288486547768116, lr: 0.0001
2023-12-20 18:45:06 INFO     	 * (global step 35700: loss: 0.30675017833709717, lr: 0.0001
2023-12-20 18:45:14 INFO     	 * (global step 35750: loss: 0.20771024748682976, lr: 0.0001
2023-12-20 18:45:22 INFO     	 * (global step 35800: loss: 0.2027532011270523, lr: 0.0001
2023-12-20 18:45:30 INFO     	 * (global step 35850: loss: 0.22551464289426804, lr: 0.0001
2023-12-20 18:45:38 INFO     	 * (global step 35900: loss: 0.24697339534759521, lr: 0.0001
2023-12-20 18:45:46 INFO     	 * (global step 35950: loss: 0.3557923659682274, lr: 0.0001
2023-12-20 18:45:54 INFO     	 * (global step 36000: loss: 0.20091427117586136, lr: 0.0001
2023-12-20 18:46:02 INFO     	 * (global step 36050: loss: 0.0885862335562706, lr: 0.0001
2023-12-20 18:46:10 INFO     	 * (global step 36100: loss: 0.3010246120393276, lr: 0.0001
2023-12-20 18:46:18 INFO     	 * (global step 36150: loss: 0.4355921745300293, lr: 0.0001
2023-12-20 18:46:26 INFO     	 * (global step 36200: loss: 0.31690292060375214, lr: 0.0001
2023-12-20 18:46:34 INFO     	 * (global step 36250: loss: 0.21203555166721344, lr: 0.0001
2023-12-20 18:46:42 INFO     	 * (global step 36300: loss: 0.2405427247285843, lr: 0.0001
2023-12-20 18:46:50 INFO     	 * (global step 36350: loss: 0.33375073224306107, lr: 0.0001
2023-12-20 18:46:58 INFO     	 * (global step 36400: loss: 0.2188713476061821, lr: 0.0001
2023-12-20 18:47:06 INFO     	 * (global step 36450: loss: 0.2102493792772293, lr: 0.0001
2023-12-20 18:47:14 INFO     	 * (global step 36500: loss: 0.39200247824192047, lr: 0.0001
2023-12-20 18:47:22 INFO     	 * (global step 36550: loss: 0.1803743615746498, lr: 0.0001
2023-12-20 18:47:30 INFO     	 * (global step 36600: loss: 0.43302659690380096, lr: 0.0001
2023-12-20 18:47:38 INFO     	 * (global step 36650: loss: 0.25640614330768585, lr: 0.0001
2023-12-20 18:47:46 INFO     	 * (global step 36700: loss: 0.2278258502483368, lr: 0.0001
2023-12-20 18:47:54 INFO     	 * (global step 36750: loss: 0.34290911257267, lr: 0.0001
2023-12-20 18:48:02 INFO     	 * (global step 36800: loss: 0.19701585173606873, lr: 0.0001
2023-12-20 18:48:10 INFO     	 * (global step 36850: loss: 0.18702863156795502, lr: 0.0001
2023-12-20 18:48:18 INFO     	 * (global step 36900: loss: 0.28385181725025177, lr: 0.0001
2023-12-20 18:48:26 INFO     	 * (global step 36950: loss: 0.2671339735388756, lr: 0.0001
2023-12-20 18:48:34 INFO     	 * (global step 37000: loss: 0.2689373940229416, lr: 0.0001
2023-12-20 18:48:42 INFO     	 * (global step 37050: loss: 0.19798557087779045, lr: 0.0001
2023-12-20 18:48:50 INFO     	 * (global step 37100: loss: 0.2756906896829605, lr: 0.0001
2023-12-20 18:48:58 INFO     	 * (global step 37150: loss: 0.1974811628460884, lr: 0.0001
2023-12-20 18:49:06 INFO     	 * (global step 37200: loss: 0.4363517612218857, lr: 0.0001
2023-12-20 18:49:14 INFO     	 * (global step 37250: loss: 0.19838453084230423, lr: 0.0001
2023-12-20 18:49:22 INFO     	 * (global step 37300: loss: 0.2621178552508354, lr: 0.0001
2023-12-20 18:49:30 INFO     	 * (global step 37350: loss: 0.12549539655447006, lr: 0.0001
2023-12-20 18:49:39 INFO     	 * (global step 37400: loss: 0.22267138212919235, lr: 0.0001
2023-12-20 18:49:47 INFO     	 * (global step 37450: loss: 0.31581179052591324, lr: 0.0001
2023-12-20 18:49:55 INFO     	 * (global step 37500: loss: 0.26118117570877075, lr: 0.0001
2023-12-20 18:50:03 INFO     	 * (global step 37550: loss: 0.20728909969329834, lr: 0.0001
2023-12-20 18:50:11 INFO     	 * (global step 37600: loss: 0.2078755870461464, lr: 0.0001
2023-12-20 18:50:19 INFO     	 * (global step 37650: loss: 0.14113473519682884, lr: 0.0001
2023-12-20 18:50:27 INFO     	 * (global step 37700: loss: 0.24531766027212143, lr: 0.0001
2023-12-20 18:50:35 INFO     	 * (global step 37750: loss: 0.16172730177640915, lr: 0.0001
2023-12-20 18:50:40 INFO     [epoch 7/15] average loss: 0.258, lr: 0.0001
2023-12-20 18:50:40 INFO     saving model related files
2023-12-20 18:50:40 INFO     saving model
2023-12-20 18:50:41 INFO     saving tokenizer
2023-12-20 18:50:41 INFO     saving optimizer
2023-12-20 18:50:42 INFO     remove old optimizer files
2023-12-20 18:50:44 INFO     	 * (global step 37800: loss: 0.34113098680973053, lr: 0.0001
2023-12-20 18:50:52 INFO     	 * (global step 37850: loss: 0.23376404494047165, lr: 0.0001
2023-12-20 18:51:00 INFO     	 * (global step 37900: loss: 0.24337977916002274, lr: 0.0001
2023-12-20 18:51:08 INFO     	 * (global step 37950: loss: 0.2513437867164612, lr: 0.0001
2023-12-20 18:51:16 INFO     	 * (global step 38000: loss: 0.2850319594144821, lr: 0.0001
2023-12-20 18:51:24 INFO     	 * (global step 38050: loss: 0.22999738156795502, lr: 0.0001
2023-12-20 18:51:32 INFO     	 * (global step 38100: loss: 0.2605184465646744, lr: 0.0001
2023-12-20 18:51:40 INFO     	 * (global step 38150: loss: 0.23751495778560638, lr: 0.0001
2023-12-20 18:51:48 INFO     	 * (global step 38200: loss: 0.3296380043029785, lr: 0.0001
2023-12-20 18:51:56 INFO     	 * (global step 38250: loss: 0.31787627190351486, lr: 0.0001
2023-12-20 18:52:04 INFO     	 * (global step 38300: loss: 0.16899258643388748, lr: 0.0001
2023-12-20 18:52:12 INFO     	 * (global step 38350: loss: 0.28564177453517914, lr: 0.0001
2023-12-20 18:52:20 INFO     	 * (global step 38400: loss: 0.29236239194869995, lr: 0.0001
2023-12-20 18:52:28 INFO     	 * (global step 38450: loss: 0.2905453145503998, lr: 0.0001
2023-12-20 18:52:36 INFO     	 * (global step 38500: loss: 0.21163499355316162, lr: 0.0001
2023-12-20 18:52:44 INFO     	 * (global step 38550: loss: 0.2755730152130127, lr: 0.0001
2023-12-20 18:52:52 INFO     	 * (global step 38600: loss: 0.29704269766807556, lr: 0.0001
2023-12-20 18:53:00 INFO     	 * (global step 38650: loss: 0.1297411471605301, lr: 0.0001
2023-12-20 18:53:08 INFO     	 * (global step 38700: loss: 0.22973952442407608, lr: 0.0001
2023-12-20 18:53:16 INFO     	 * (global step 38750: loss: 0.2451966255903244, lr: 0.0001
2023-12-20 18:53:24 INFO     	 * (global step 38800: loss: 0.1814197599887848, lr: 0.0001
2023-12-20 18:53:32 INFO     	 * (global step 38850: loss: 0.3809175491333008, lr: 0.0001
2023-12-20 18:53:40 INFO     	 * (global step 38900: loss: 0.2027003914117813, lr: 0.0001
2023-12-20 18:53:48 INFO     	 * (global step 38950: loss: 0.21208297461271286, lr: 0.0001
2023-12-20 18:53:56 INFO     	 * (global step 39000: loss: 0.23285043239593506, lr: 0.0001
2023-12-20 18:54:04 INFO     	 * (global step 39050: loss: 0.2582274377346039, lr: 0.0001
2023-12-20 18:54:12 INFO     	 * (global step 39100: loss: 0.19501355290412903, lr: 0.0001
2023-12-20 18:54:20 INFO     	 * (global step 39150: loss: 0.17646099627017975, lr: 0.0001
2023-12-20 18:54:28 INFO     	 * (global step 39200: loss: 0.2624596953392029, lr: 0.0001
2023-12-20 18:54:36 INFO     	 * (global step 39250: loss: 0.21953822672367096, lr: 0.0001
2023-12-20 18:54:44 INFO     	 * (global step 39300: loss: 0.1640595868229866, lr: 0.0001
2023-12-20 18:54:52 INFO     	 * (global step 39350: loss: 0.2111584171652794, lr: 0.0001
2023-12-20 18:55:00 INFO     	 * (global step 39400: loss: 0.18245212733745575, lr: 0.0001
2023-12-20 18:55:08 INFO     	 * (global step 39450: loss: 0.2054043710231781, lr: 0.0001
2023-12-20 18:55:16 INFO     	 * (global step 39500: loss: 0.19663812965154648, lr: 0.0001
2023-12-20 18:55:24 INFO     	 * (global step 39550: loss: 0.18709790706634521, lr: 0.0001
2023-12-20 18:55:32 INFO     	 * (global step 39600: loss: 0.2458503283560276, lr: 0.0001
2023-12-20 18:55:40 INFO     	 * (global step 39650: loss: 0.19410236924886703, lr: 0.0001
2023-12-20 18:55:48 INFO     	 * (global step 39700: loss: 0.19505725800991058, lr: 0.0001
2023-12-20 18:55:56 INFO     	 * (global step 39750: loss: 0.2858321964740753, lr: 0.0001
2023-12-20 18:56:04 INFO     	 * (global step 39800: loss: 0.3379174768924713, lr: 0.0001
2023-12-20 18:56:12 INFO     	 * (global step 39850: loss: 0.3278750032186508, lr: 0.0001
2023-12-20 18:56:21 INFO     	 * (global step 39900: loss: 0.3129340633749962, lr: 0.0001
2023-12-20 18:56:29 INFO     	 * (global step 39950: loss: 0.3230953961610794, lr: 0.0001
2023-12-20 18:56:37 INFO     	 * (global step 40000: loss: 0.18574495613574982, lr: 0.0001
2023-12-20 18:56:45 INFO     	 * (global step 40050: loss: 0.21673612296581268, lr: 0.0001
2023-12-20 18:56:53 INFO     	 * (global step 40100: loss: 0.24834495037794113, lr: 0.0001
2023-12-20 18:57:01 INFO     	 * (global step 40150: loss: 0.20899178087711334, lr: 0.0001
2023-12-20 18:57:09 INFO     	 * (global step 40200: loss: 0.27118464559316635, lr: 0.0001
2023-12-20 18:57:17 INFO     	 * (global step 40250: loss: 0.29890284687280655, lr: 0.0001
2023-12-20 18:57:25 INFO     	 * (global step 40300: loss: 0.24494519084692, lr: 0.0001
2023-12-20 18:57:33 INFO     	 * (global step 40350: loss: 0.24209559708833694, lr: 0.0001
2023-12-20 18:57:41 INFO     	 * (global step 40400: loss: 0.20382296293973923, lr: 0.0001
2023-12-20 18:57:49 INFO     	 * (global step 40450: loss: 0.1853606253862381, lr: 0.0001
2023-12-20 18:57:57 INFO     	 * (global step 40500: loss: 0.17334818840026855, lr: 0.0001
2023-12-20 18:58:05 INFO     	 * (global step 40550: loss: 0.20167915523052216, lr: 0.0001
2023-12-20 18:58:13 INFO     	 * (global step 40600: loss: 0.18641408532857895, lr: 0.0001
2023-12-20 18:58:21 INFO     	 * (global step 40650: loss: 0.27520890533924103, lr: 0.0001
2023-12-20 18:58:29 INFO     	 * (global step 40700: loss: 0.34853920340538025, lr: 0.0001
2023-12-20 18:58:37 INFO     	 * (global step 40750: loss: 0.22266389429569244, lr: 0.0001
2023-12-20 18:58:45 INFO     	 * (global step 40800: loss: 0.3066474050283432, lr: 0.0001
2023-12-20 18:58:53 INFO     	 * (global step 40850: loss: 0.2584924027323723, lr: 0.0001
2023-12-20 18:59:01 INFO     	 * (global step 40900: loss: 0.25423918664455414, lr: 0.0001
2023-12-20 18:59:09 INFO     	 * (global step 40950: loss: 0.17849933356046677, lr: 0.0001
2023-12-20 18:59:17 INFO     	 * (global step 41000: loss: 0.3328578993678093, lr: 0.0001
2023-12-20 18:59:25 INFO     	 * (global step 41050: loss: 0.22258329391479492, lr: 0.0001
2023-12-20 18:59:33 INFO     	 * (global step 41100: loss: 0.2224612683057785, lr: 0.0001
2023-12-20 18:59:41 INFO     	 * (global step 41150: loss: 0.2374282106757164, lr: 0.0001
2023-12-20 18:59:49 INFO     	 * (global step 41200: loss: 0.22934924066066742, lr: 0.0001
2023-12-20 18:59:57 INFO     	 * (global step 41250: loss: 0.1620149090886116, lr: 0.0001
2023-12-20 19:00:05 INFO     	 * (global step 41300: loss: 0.3058035746216774, lr: 0.0001
2023-12-20 19:00:13 INFO     	 * (global step 41350: loss: 0.20098061114549637, lr: 0.0001
2023-12-20 19:00:21 INFO     	 * (global step 41400: loss: 0.17258652299642563, lr: 0.0001
2023-12-20 19:00:29 INFO     	 * (global step 41450: loss: 0.3294881582260132, lr: 0.0001
2023-12-20 19:00:37 INFO     	 * (global step 41500: loss: 0.22568487375974655, lr: 0.0001
2023-12-20 19:00:45 INFO     	 * (global step 41550: loss: 0.258240707218647, lr: 0.0001
2023-12-20 19:00:53 INFO     	 * (global step 41600: loss: 0.3226526379585266, lr: 0.0001
2023-12-20 19:01:01 INFO     	 * (global step 41650: loss: 0.2303977906703949, lr: 0.0001
2023-12-20 19:01:09 INFO     	 * (global step 41700: loss: 0.3059329465031624, lr: 0.0001
2023-12-20 19:01:17 INFO     	 * (global step 41750: loss: 0.15320104360580444, lr: 0.0001
2023-12-20 19:01:25 INFO     	 * (global step 41800: loss: 0.2616029679775238, lr: 0.0001
2023-12-20 19:01:33 INFO     	 * (global step 41850: loss: 0.2777627110481262, lr: 0.0001
2023-12-20 19:01:41 INFO     	 * (global step 41900: loss: 0.2892392873764038, lr: 0.0001
2023-12-20 19:01:49 INFO     	 * (global step 41950: loss: 0.25794408470392227, lr: 0.0001
2023-12-20 19:01:57 INFO     	 * (global step 42000: loss: 0.17643895745277405, lr: 0.0001
2023-12-20 19:02:05 INFO     	 * (global step 42050: loss: 0.17653611302375793, lr: 0.0001
2023-12-20 19:02:13 INFO     	 * (global step 42100: loss: 0.27644288539886475, lr: 0.0001
2023-12-20 19:02:21 INFO     	 * (global step 42150: loss: 0.17639916390180588, lr: 0.0001
2023-12-20 19:02:29 INFO     	 * (global step 42200: loss: 0.2370661422610283, lr: 0.0001
2023-12-20 19:02:37 INFO     	 * (global step 42250: loss: 0.2754311263561249, lr: 0.0001
2023-12-20 19:02:45 INFO     	 * (global step 42300: loss: 0.22962404042482376, lr: 0.0001
2023-12-20 19:02:53 INFO     	 * (global step 42350: loss: 0.16081152856349945, lr: 0.0001
2023-12-20 19:03:01 INFO     	 * (global step 42400: loss: 0.272986501455307, lr: 0.0001
2023-12-20 19:03:09 INFO     	 * (global step 42450: loss: 0.3460436388850212, lr: 0.0001
2023-12-20 19:03:17 INFO     	 * (global step 42500: loss: 0.2008546143770218, lr: 0.0001
2023-12-20 19:03:18 INFO     [epoch 8/15] average loss: 0.252, lr: 0.0001
2023-12-20 19:03:18 INFO     saving model related files
2023-12-20 19:03:18 INFO     saving model
2023-12-20 19:03:18 INFO     saving tokenizer
2023-12-20 19:03:18 INFO     saving optimizer
2023-12-20 19:03:19 INFO     remove old optimizer files
2023-12-20 19:03:27 INFO     	 * (global step 42550: loss: 0.2535373643040657, lr: 0.0001
2023-12-20 19:03:35 INFO     	 * (global step 42600: loss: 0.2754538804292679, lr: 0.0001
2023-12-20 19:03:43 INFO     	 * (global step 42650: loss: 0.2994484603404999, lr: 0.0001
2023-12-20 19:03:51 INFO     	 * (global step 42700: loss: 0.17205040901899338, lr: 0.0001
2023-12-20 19:03:59 INFO     	 * (global step 42750: loss: 0.2563624158501625, lr: 0.0001
2023-12-20 19:04:07 INFO     	 * (global step 42800: loss: 0.45896948873996735, lr: 0.0001
2023-12-20 19:04:15 INFO     	 * (global step 42850: loss: 0.20467963814735413, lr: 0.0001
2023-12-20 19:04:23 INFO     	 * (global step 42900: loss: 0.258229598402977, lr: 0.0001
2023-12-20 19:04:31 INFO     	 * (global step 42950: loss: 0.25023020058870316, lr: 0.0001
2023-12-20 19:04:39 INFO     	 * (global step 43000: loss: 0.3090815916657448, lr: 0.0001
2023-12-20 19:04:47 INFO     	 * (global step 43050: loss: 0.24192117154598236, lr: 0.0001
2023-12-20 19:04:55 INFO     	 * (global step 43100: loss: 0.28268565237522125, lr: 0.0001
2023-12-20 19:05:03 INFO     	 * (global step 43150: loss: 0.23504668474197388, lr: 0.0001
2023-12-20 19:05:11 INFO     	 * (global step 43200: loss: 0.23361309617757797, lr: 0.0001
2023-12-20 19:05:19 INFO     	 * (global step 43250: loss: 0.19926414638757706, lr: 0.0001
2023-12-20 19:05:27 INFO     	 * (global step 43300: loss: 0.42574314773082733, lr: 0.0001
2023-12-20 19:05:35 INFO     	 * (global step 43350: loss: 0.2808857560157776, lr: 0.0001
2023-12-20 19:05:43 INFO     	 * (global step 43400: loss: 0.18422269821166992, lr: 0.0001
2023-12-20 19:05:51 INFO     	 * (global step 43450: loss: 0.223678357899189, lr: 0.0001
2023-12-20 19:05:59 INFO     	 * (global step 43500: loss: 0.35313764214515686, lr: 0.0001
2023-12-20 19:06:07 INFO     	 * (global step 43550: loss: 0.27177123725414276, lr: 0.0001
2023-12-20 19:06:15 INFO     	 * (global step 43600: loss: 0.2869601473212242, lr: 0.0001
2023-12-20 19:06:23 INFO     	 * (global step 43650: loss: 0.4325338304042816, lr: 0.0001
2023-12-20 19:06:31 INFO     	 * (global step 43700: loss: 0.23427635431289673, lr: 0.0001
2023-12-20 19:06:39 INFO     	 * (global step 43750: loss: 0.2790532037615776, lr: 0.0001
2023-12-20 19:06:47 INFO     	 * (global step 43800: loss: 0.20834416896104813, lr: 0.0001
2023-12-20 19:06:55 INFO     	 * (global step 43850: loss: 0.2517258748412132, lr: 0.0001
2023-12-20 19:07:03 INFO     	 * (global step 43900: loss: 0.3029553145170212, lr: 0.0001
2023-12-20 19:07:11 INFO     	 * (global step 43950: loss: 0.1903456300497055, lr: 0.0001
2023-12-20 19:07:19 INFO     	 * (global step 44000: loss: 0.29764725267887115, lr: 0.0001
2023-12-20 19:07:27 INFO     	 * (global step 44050: loss: 0.2205166220664978, lr: 0.0001
2023-12-20 19:07:35 INFO     	 * (global step 44100: loss: 0.18900496512651443, lr: 0.0001
2023-12-20 19:07:43 INFO     	 * (global step 44150: loss: 0.35424867272377014, lr: 0.0001
2023-12-20 19:07:51 INFO     	 * (global step 44200: loss: 0.1670159548521042, lr: 0.0001
2023-12-20 19:07:59 INFO     	 * (global step 44250: loss: 0.2793109267950058, lr: 0.0001
2023-12-20 19:08:07 INFO     	 * (global step 44300: loss: 0.5849491208791733, lr: 0.0001
2023-12-20 19:08:15 INFO     	 * (global step 44350: loss: 0.2138366401195526, lr: 0.0001
2023-12-20 19:08:23 INFO     	 * (global step 44400: loss: 0.16802917048335075, lr: 0.0001
2023-12-20 19:08:31 INFO     	 * (global step 44450: loss: 0.290192075073719, lr: 0.0001
2023-12-20 19:08:39 INFO     	 * (global step 44500: loss: 0.34121233969926834, lr: 0.0001
2023-12-20 19:08:47 INFO     	 * (global step 44550: loss: 0.14585710316896439, lr: 0.0001
2023-12-20 19:08:55 INFO     	 * (global step 44600: loss: 0.1790093034505844, lr: 0.0001
2023-12-20 19:09:03 INFO     	 * (global step 44650: loss: 0.3003520667552948, lr: 0.0001
2023-12-20 19:09:11 INFO     	 * (global step 44700: loss: 0.20902444422245026, lr: 0.0001
2023-12-20 19:09:19 INFO     	 * (global step 44750: loss: 0.16738679260015488, lr: 0.0001
2023-12-20 19:09:27 INFO     	 * (global step 44800: loss: 0.22841238230466843, lr: 0.0001
2023-12-20 19:09:35 INFO     	 * (global step 44850: loss: 0.2321968749165535, lr: 0.0001
2023-12-20 19:09:43 INFO     	 * (global step 44900: loss: 0.1904221922159195, lr: 0.0001
2023-12-20 19:09:51 INFO     	 * (global step 44950: loss: 0.2431403324007988, lr: 0.0001
2023-12-20 19:09:59 INFO     	 * (global step 45000: loss: 0.21034863591194153, lr: 0.0001
2023-12-20 19:10:07 INFO     	 * (global step 45050: loss: 0.23561633378267288, lr: 0.0001
2023-12-20 19:10:15 INFO     	 * (global step 45100: loss: 0.284078024327755, lr: 0.0001
2023-12-20 19:10:23 INFO     	 * (global step 45150: loss: 0.30364859104156494, lr: 0.0001
2023-12-20 19:10:31 INFO     	 * (global step 45200: loss: 0.17080378532409668, lr: 0.0001
2023-12-20 19:10:39 INFO     	 * (global step 45250: loss: 0.2632893770933151, lr: 0.0001
2023-12-20 19:10:47 INFO     	 * (global step 45300: loss: 0.18094078451395035, lr: 0.0001
2023-12-20 19:10:55 INFO     	 * (global step 45350: loss: 0.19085142016410828, lr: 0.0001
2023-12-20 19:11:04 INFO     	 * (global step 45400: loss: 0.2166171818971634, lr: 0.0001
2023-12-20 19:11:12 INFO     	 * (global step 45450: loss: 0.17678354680538177, lr: 0.0001
2023-12-20 19:11:20 INFO     	 * (global step 45500: loss: 0.17663685977458954, lr: 0.0001
2023-12-20 19:11:28 INFO     	 * (global step 45550: loss: 0.24048008769750595, lr: 0.0001
2023-12-20 19:11:36 INFO     	 * (global step 45600: loss: 0.23418285697698593, lr: 0.0001
2023-12-20 19:11:44 INFO     	 * (global step 45650: loss: 0.27968572825193405, lr: 0.0001
2023-12-20 19:11:52 INFO     	 * (global step 45700: loss: 0.20449700951576233, lr: 0.0001
2023-12-20 19:12:00 INFO     	 * (global step 45750: loss: 0.3928813934326172, lr: 0.0001
2023-12-20 19:12:08 INFO     	 * (global step 45800: loss: 0.2516218572854996, lr: 0.0001
2023-12-20 19:12:16 INFO     	 * (global step 45850: loss: 0.24357876181602478, lr: 0.0001
2023-12-20 19:12:24 INFO     	 * (global step 45900: loss: 0.39765147864818573, lr: 0.0001
2023-12-20 19:12:32 INFO     	 * (global step 45950: loss: 0.307277075946331, lr: 0.0001
2023-12-20 19:12:40 INFO     	 * (global step 46000: loss: 0.19429487735033035, lr: 0.0001
2023-12-20 19:12:48 INFO     	 * (global step 46050: loss: 0.20704755932092667, lr: 0.0001
2023-12-20 19:12:56 INFO     	 * (global step 46100: loss: 0.33811821043491364, lr: 0.0001
2023-12-20 19:13:04 INFO     	 * (global step 46150: loss: 0.1571236327290535, lr: 0.0001
2023-12-20 19:13:12 INFO     	 * (global step 46200: loss: 0.16942625120282173, lr: 0.0001
2023-12-20 19:13:20 INFO     	 * (global step 46250: loss: 0.21379832178354263, lr: 0.0001
2023-12-20 19:13:28 INFO     	 * (global step 46300: loss: 0.18118232488632202, lr: 0.0001
2023-12-20 19:13:36 INFO     	 * (global step 46350: loss: 0.24538473784923553, lr: 0.0001
2023-12-20 19:13:44 INFO     	 * (global step 46400: loss: 0.2554761990904808, lr: 0.0001
2023-12-20 19:13:53 INFO     	 * (global step 46450: loss: 0.32017982006073, lr: 0.0001
2023-12-20 19:14:01 INFO     	 * (global step 46500: loss: 0.14876124635338783, lr: 0.0001
2023-12-20 19:14:09 INFO     	 * (global step 46550: loss: 0.5179525762796402, lr: 0.0001
2023-12-20 19:14:17 INFO     	 * (global step 46600: loss: 0.2706458270549774, lr: 0.0001
2023-12-20 19:14:25 INFO     	 * (global step 46650: loss: 0.340962216258049, lr: 0.0001
2023-12-20 19:14:33 INFO     	 * (global step 46700: loss: 0.2865235507488251, lr: 0.0001
2023-12-20 19:14:41 INFO     	 * (global step 46750: loss: 0.25658778101205826, lr: 0.0001
2023-12-20 19:14:49 INFO     	 * (global step 46800: loss: 0.16164449602365494, lr: 0.0001
2023-12-20 19:14:57 INFO     	 * (global step 46850: loss: 0.387458935379982, lr: 0.0001
2023-12-20 19:15:05 INFO     	 * (global step 46900: loss: 0.16614501178264618, lr: 0.0001
2023-12-20 19:15:13 INFO     	 * (global step 46950: loss: 0.1878531500697136, lr: 0.0001
2023-12-20 19:15:21 INFO     	 * (global step 47000: loss: 0.3343411758542061, lr: 0.0001
2023-12-20 19:15:29 INFO     	 * (global step 47050: loss: 0.23720521479845047, lr: 0.0001
2023-12-20 19:15:37 INFO     	 * (global step 47100: loss: 0.2027878277003765, lr: 0.0001
2023-12-20 19:15:45 INFO     	 * (global step 47150: loss: 0.2064366713166237, lr: 0.0001
2023-12-20 19:15:53 INFO     	 * (global step 47200: loss: 0.20314102619886398, lr: 0.0001
2023-12-20 19:15:58 INFO     [epoch 9/15] average loss: 0.247, lr: 0.0001
2023-12-20 19:15:58 INFO     saving model related files
2023-12-20 19:15:58 INFO     saving model
2023-12-20 19:15:58 INFO     saving tokenizer
2023-12-20 19:15:58 INFO     saving optimizer
2023-12-20 19:15:59 INFO     remove old optimizer files
2023-12-20 19:15:59 INFO     complete training: model ckpt was saved at small_combined_trained_ckpt/model_mzgdpa
2023-12-20 19:15:59 INFO     ## 1st RUN: Configuration 4/12 ##
2023-12-20 19:15:59 INFO     initialize model trainer
2023-12-20 19:15:59 INFO     initialize checkpoint at small_combined_trained_ckpt/model_mntyya
2023-12-20 19:15:59 INFO     hyperparameters
2023-12-20 19:15:59 INFO     	 * dataset_path: StellarMilk/squad_newsqa
2023-12-20 19:15:59 INFO     	 * dataset_name: default
2023-12-20 19:15:59 INFO     	 * input_types: ['paragraph']
2023-12-20 19:15:59 INFO     	 * output_types: ['questions_answers']
2023-12-20 19:15:59 INFO     	 * prefix_types: ['qag']
2023-12-20 19:15:59 INFO     	 * model: t5-small
2023-12-20 19:15:59 INFO     	 * max_length: 512
2023-12-20 19:15:59 INFO     	 * max_length_output: 512
2023-12-20 19:15:59 INFO     	 * epoch: 15
2023-12-20 19:15:59 INFO     	 * batch: 2
2023-12-20 19:15:59 INFO     	 * lr: 5e-05
2023-12-20 19:15:59 INFO     	 * fp16: False
2023-12-20 19:15:59 INFO     	 * random_seed: 1
2023-12-20 19:15:59 INFO     	 * gradient_accumulation_steps: 4
2023-12-20 19:15:59 INFO     	 * label_smoothing: 0.15
2023-12-20 19:15:59 INFO     initialize checkpoint with t5-small
2023-12-20 19:16:00 INFO     use spaCy answer extraction model: positionrank
2023-12-20 19:16:01 INFO     Model `t5-small`
2023-12-20 19:16:01 INFO     	 * Num of GPU in use: 1
2023-12-20 19:16:01 INFO     	 * Prefix: True
2023-12-20 19:16:01 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 19:16:01 INFO     dataset preprocessing
2023-12-20 19:16:02 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/squad_newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-20 19:16:04 INFO     start model training
2023-12-20 19:16:20 INFO     	 * (global step 50: loss: 1.2514507919549942, lr: 5e-05
2023-12-20 19:16:35 INFO     	 * (global step 100: loss: 1.031420722603798, lr: 5e-05
2023-12-20 19:16:51 INFO     	 * (global step 150: loss: 0.6309006661176682, lr: 5e-05
2023-12-20 19:17:06 INFO     	 * (global step 200: loss: 0.6078341603279114, lr: 5e-05
2023-12-20 19:17:22 INFO     	 * (global step 250: loss: 0.6571569293737411, lr: 5e-05
2023-12-20 19:17:37 INFO     	 * (global step 300: loss: 0.5342007353901863, lr: 5e-05
2023-12-20 19:17:53 INFO     	 * (global step 350: loss: 0.5863303542137146, lr: 5e-05
2023-12-20 19:18:08 INFO     	 * (global step 400: loss: 0.3965480178594589, lr: 5e-05
2023-12-20 19:18:24 INFO     	 * (global step 450: loss: 0.46170470118522644, lr: 5e-05
2023-12-20 19:18:40 INFO     	 * (global step 500: loss: 0.41489125043153763, lr: 5e-05
2023-12-20 19:18:55 INFO     	 * (global step 550: loss: 0.4932679757475853, lr: 5e-05
2023-12-20 19:19:11 INFO     	 * (global step 600: loss: 0.5063530504703522, lr: 5e-05
2023-12-20 19:19:26 INFO     	 * (global step 650: loss: 0.3773292228579521, lr: 5e-05
2023-12-20 19:19:42 INFO     	 * (global step 700: loss: 0.389016754925251, lr: 5e-05
2023-12-20 19:19:58 INFO     	 * (global step 750: loss: 0.502255342900753, lr: 5e-05
2023-12-20 19:20:13 INFO     	 * (global step 800: loss: 0.40103085339069366, lr: 5e-05
2023-12-20 19:20:29 INFO     	 * (global step 850: loss: 0.3633226156234741, lr: 5e-05
2023-12-20 19:20:45 INFO     	 * (global step 900: loss: 0.37505991756916046, lr: 5e-05
2023-12-20 19:21:00 INFO     	 * (global step 950: loss: 0.36701276898384094, lr: 5e-05
2023-12-20 19:21:16 INFO     	 * (global step 1000: loss: 0.3881572261452675, lr: 5e-05
2023-12-20 19:21:31 INFO     	 * (global step 1050: loss: 0.3922189697623253, lr: 5e-05
2023-12-20 19:21:47 INFO     	 * (global step 1100: loss: 0.4134680777788162, lr: 5e-05
2023-12-20 19:22:02 INFO     	 * (global step 1150: loss: 0.6286459788680077, lr: 5e-05
2023-12-20 19:22:18 INFO     	 * (global step 1200: loss: 0.36941061168909073, lr: 5e-05
2023-12-20 19:22:33 INFO     	 * (global step 1250: loss: 0.324233278632164, lr: 5e-05
2023-12-20 19:22:49 INFO     	 * (global step 1300: loss: 0.37653743475675583, lr: 5e-05
2023-12-20 19:23:05 INFO     	 * (global step 1350: loss: 0.3270960673689842, lr: 5e-05
2023-12-20 19:23:20 INFO     	 * (global step 1400: loss: 0.42872346192598343, lr: 5e-05
2023-12-20 19:23:36 INFO     	 * (global step 1450: loss: 0.37908757477998734, lr: 5e-05
2023-12-20 19:23:51 INFO     	 * (global step 1500: loss: 0.4681720696389675, lr: 5e-05
2023-12-20 19:24:07 INFO     	 * (global step 1550: loss: 0.31873980164527893, lr: 5e-05
2023-12-20 19:24:22 INFO     	 * (global step 1600: loss: 0.31488513201475143, lr: 5e-05
2023-12-20 19:24:38 INFO     	 * (global step 1650: loss: 0.42861074954271317, lr: 5e-05
2023-12-20 19:24:54 INFO     	 * (global step 1700: loss: 0.33426015079021454, lr: 5e-05
2023-12-20 19:25:09 INFO     	 * (global step 1750: loss: 0.3223353326320648, lr: 5e-05
2023-12-20 19:25:25 INFO     	 * (global step 1800: loss: 0.33300214633345604, lr: 5e-05
2023-12-20 19:25:40 INFO     	 * (global step 1850: loss: 0.26847608387470245, lr: 5e-05
2023-12-20 19:25:56 INFO     	 * (global step 1900: loss: 0.3567707762122154, lr: 5e-05
2023-12-20 19:26:11 INFO     	 * (global step 1950: loss: 0.3229430839419365, lr: 5e-05
2023-12-20 19:26:27 INFO     	 * (global step 2000: loss: 0.27716489881277084, lr: 5e-05
2023-12-20 19:26:43 INFO     	 * (global step 2050: loss: 0.29819265380501747, lr: 5e-05
2023-12-20 19:26:58 INFO     	 * (global step 2100: loss: 0.2992737926542759, lr: 5e-05
2023-12-20 19:27:14 INFO     	 * (global step 2150: loss: 0.43449316918849945, lr: 5e-05
2023-12-20 19:27:29 INFO     	 * (global step 2200: loss: 0.46852172911167145, lr: 5e-05
2023-12-20 19:27:45 INFO     	 * (global step 2250: loss: 0.30048681423068047, lr: 5e-05
2023-12-20 19:28:00 INFO     	 * (global step 2300: loss: 0.3949219211935997, lr: 5e-05
2023-12-20 19:28:16 INFO     	 * (global step 2350: loss: 0.36387690529227257, lr: 5e-05
2023-12-20 19:28:20 INFO     [epoch 0/15] average loss: 0.506, lr: 5e-05
2023-12-20 19:28:20 INFO     saving model related files
2023-12-20 19:28:20 INFO     saving model
2023-12-20 19:28:20 INFO     saving tokenizer
2023-12-20 19:28:20 INFO     saving optimizer
2023-12-20 19:28:21 INFO     remove old optimizer files
2023-12-20 19:28:33 INFO     	 * (global step 2400: loss: 0.33917830511927605, lr: 5e-05
2023-12-20 19:28:49 INFO     	 * (global step 2450: loss: 0.3770027831196785, lr: 5e-05
2023-12-20 19:29:04 INFO     	 * (global step 2500: loss: 0.35855506360530853, lr: 5e-05
2023-12-20 19:29:20 INFO     	 * (global step 2550: loss: 0.33545688539743423, lr: 5e-05
2023-12-20 19:29:35 INFO     	 * (global step 2600: loss: 0.3343246653676033, lr: 5e-05
2023-12-20 19:29:51 INFO     	 * (global step 2650: loss: 0.5834588930010796, lr: 5e-05
2023-12-20 19:30:06 INFO     	 * (global step 2700: loss: 0.4141761139035225, lr: 5e-05
2023-12-20 19:30:22 INFO     	 * (global step 2750: loss: 0.2968132346868515, lr: 5e-05
2023-12-20 19:30:37 INFO     	 * (global step 2800: loss: 0.4524480327963829, lr: 5e-05
2023-12-20 19:30:53 INFO     	 * (global step 2850: loss: 0.44873588532209396, lr: 5e-05
2023-12-20 19:31:08 INFO     	 * (global step 2900: loss: 0.43517279624938965, lr: 5e-05
2023-12-20 19:31:24 INFO     	 * (global step 2950: loss: 0.350632481276989, lr: 5e-05
2023-12-20 19:31:40 INFO     	 * (global step 3000: loss: 0.4087050333619118, lr: 5e-05
2023-12-20 19:31:55 INFO     	 * (global step 3050: loss: 0.36553366854786873, lr: 5e-05
2023-12-20 19:32:11 INFO     	 * (global step 3100: loss: 0.3539527878165245, lr: 5e-05
2023-12-20 19:32:26 INFO     	 * (global step 3150: loss: 0.5079644173383713, lr: 5e-05
2023-12-20 19:32:42 INFO     	 * (global step 3200: loss: 0.3175157532095909, lr: 5e-05
2023-12-20 19:32:57 INFO     	 * (global step 3250: loss: 0.4015091806650162, lr: 5e-05
2023-12-20 19:33:13 INFO     	 * (global step 3300: loss: 0.3427926301956177, lr: 5e-05
2023-12-20 19:33:29 INFO     	 * (global step 3350: loss: 0.314695555716753, lr: 5e-05
2023-12-20 19:33:44 INFO     	 * (global step 3400: loss: 0.2890702225267887, lr: 5e-05
2023-12-20 19:34:00 INFO     	 * (global step 3450: loss: 0.4007195308804512, lr: 5e-05
2023-12-20 19:34:15 INFO     	 * (global step 3500: loss: 0.5199279636144638, lr: 5e-05
2023-12-20 19:34:31 INFO     	 * (global step 3550: loss: 0.33684296160936356, lr: 5e-05
2023-12-20 19:34:46 INFO     	 * (global step 3600: loss: 0.3741217702627182, lr: 5e-05
2023-12-20 19:35:02 INFO     	 * (global step 3650: loss: 0.332588504999876, lr: 5e-05
2023-12-20 19:35:17 INFO     	 * (global step 3700: loss: 0.3688104674220085, lr: 5e-05
2023-12-20 19:35:33 INFO     	 * (global step 3750: loss: 0.3515837639570236, lr: 5e-05
2023-12-20 19:35:48 INFO     	 * (global step 3800: loss: 0.2723842151463032, lr: 5e-05
2023-12-20 19:36:04 INFO     	 * (global step 3850: loss: 0.3553410917520523, lr: 5e-05
2023-12-20 19:36:19 INFO     	 * (global step 3900: loss: 0.28981858864426613, lr: 5e-05
2023-12-20 19:36:35 INFO     	 * (global step 3950: loss: 0.3680156320333481, lr: 5e-05
2023-12-20 19:36:50 INFO     	 * (global step 4000: loss: 0.30535856634378433, lr: 5e-05
2023-12-20 19:37:06 INFO     	 * (global step 4050: loss: 0.4208297058939934, lr: 5e-05
2023-12-20 19:37:21 INFO     	 * (global step 4100: loss: 0.2793261967599392, lr: 5e-05
2023-12-20 19:37:37 INFO     	 * (global step 4150: loss: 0.2765987180173397, lr: 5e-05
2023-12-20 19:37:53 INFO     	 * (global step 4200: loss: 0.3768826872110367, lr: 5e-05
2023-12-20 19:38:08 INFO     	 * (global step 4250: loss: 0.4123363345861435, lr: 5e-05
2023-12-20 19:38:24 INFO     	 * (global step 4300: loss: 0.30036476626992226, lr: 5e-05
2023-12-20 19:38:39 INFO     	 * (global step 4350: loss: 0.2777355872094631, lr: 5e-05
2023-12-20 19:38:55 INFO     	 * (global step 4400: loss: 0.31297317147254944, lr: 5e-05
2023-12-20 19:39:10 INFO     	 * (global step 4450: loss: 0.280557282269001, lr: 5e-05
2023-12-20 19:39:26 INFO     	 * (global step 4500: loss: 0.2898745872080326, lr: 5e-05
2023-12-20 19:39:41 INFO     	 * (global step 4550: loss: 0.34747878462076187, lr: 5e-05
2023-12-20 19:39:57 INFO     	 * (global step 4600: loss: 0.2293122224509716, lr: 5e-05
2023-12-20 19:40:12 INFO     	 * (global step 4650: loss: 0.3560496121644974, lr: 5e-05
2023-12-20 19:40:28 INFO     	 * (global step 4700: loss: 0.3216923549771309, lr: 5e-05
2023-12-20 19:40:35 INFO     [epoch 1/15] average loss: 0.354, lr: 5e-05
2023-12-20 19:40:35 INFO     saving model related files
2023-12-20 19:40:35 INFO     saving model
2023-12-20 19:40:35 INFO     saving tokenizer
2023-12-20 19:40:35 INFO     saving optimizer
2023-12-20 19:40:36 INFO     remove old optimizer files
2023-12-20 19:40:45 INFO     	 * (global step 4750: loss: 0.49299515783786774, lr: 5e-05
2023-12-20 19:41:01 INFO     	 * (global step 4800: loss: 0.3018070198595524, lr: 5e-05
2023-12-20 19:41:16 INFO     	 * (global step 4850: loss: 0.26204423420131207, lr: 5e-05
2023-12-20 19:41:32 INFO     	 * (global step 4900: loss: 0.3013293296098709, lr: 5e-05
2023-12-20 19:41:47 INFO     	 * (global step 4950: loss: 0.3055648133158684, lr: 5e-05
2023-12-20 19:42:03 INFO     	 * (global step 5000: loss: 0.49503380060195923, lr: 5e-05
2023-12-20 19:42:18 INFO     	 * (global step 5050: loss: 0.3810172565281391, lr: 5e-05
2023-12-20 19:42:34 INFO     	 * (global step 5100: loss: 0.2751794904470444, lr: 5e-05
2023-12-20 19:42:50 INFO     	 * (global step 5150: loss: 0.314722016453743, lr: 5e-05
2023-12-20 19:43:05 INFO     	 * (global step 5200: loss: 0.38899099826812744, lr: 5e-05
2023-12-20 19:43:21 INFO     	 * (global step 5250: loss: 0.3177236244082451, lr: 5e-05
2023-12-20 19:43:36 INFO     	 * (global step 5300: loss: 0.3389419727027416, lr: 5e-05
2023-12-20 19:43:52 INFO     	 * (global step 5350: loss: 0.28357503935694695, lr: 5e-05
2023-12-20 19:44:07 INFO     	 * (global step 5400: loss: 0.23152142763137817, lr: 5e-05
2023-12-20 19:44:23 INFO     	 * (global step 5450: loss: 0.45426154881715775, lr: 5e-05
2023-12-20 19:44:38 INFO     	 * (global step 5500: loss: 0.44332563877105713, lr: 5e-05
2023-12-20 19:44:54 INFO     	 * (global step 5550: loss: 0.2970256358385086, lr: 5e-05
2023-12-20 19:45:10 INFO     	 * (global step 5600: loss: 0.26291001960635185, lr: 5e-05
2023-12-20 19:45:25 INFO     	 * (global step 5650: loss: 0.27674170210957527, lr: 5e-05
2023-12-20 19:45:41 INFO     	 * (global step 5700: loss: 0.44791267812252045, lr: 5e-05
2023-12-20 19:45:56 INFO     	 * (global step 5750: loss: 0.37054232135415077, lr: 5e-05
2023-12-20 19:46:12 INFO     	 * (global step 5800: loss: 0.34380362927913666, lr: 5e-05
2023-12-20 19:46:27 INFO     	 * (global step 5850: loss: 0.3928806036710739, lr: 5e-05
2023-12-20 19:46:43 INFO     	 * (global step 5900: loss: 0.333132304251194, lr: 5e-05
2023-12-20 19:46:58 INFO     	 * (global step 5950: loss: 0.39979881048202515, lr: 5e-05
2023-12-20 19:47:14 INFO     	 * (global step 6000: loss: 0.37520166486501694, lr: 5e-05
2023-12-20 19:47:30 INFO     	 * (global step 6050: loss: 0.2963172197341919, lr: 5e-05
2023-12-20 19:47:45 INFO     	 * (global step 6100: loss: 0.3794361650943756, lr: 5e-05
2023-12-20 19:48:01 INFO     	 * (global step 6150: loss: 0.3818548172712326, lr: 5e-05
2023-12-20 19:48:16 INFO     	 * (global step 6200: loss: 0.30733639374375343, lr: 5e-05
2023-12-20 19:48:32 INFO     	 * (global step 6250: loss: 0.278874009847641, lr: 5e-05
2023-12-20 19:48:47 INFO     	 * (global step 6300: loss: 0.3773486688733101, lr: 5e-05
2023-12-20 19:49:03 INFO     	 * (global step 6350: loss: 0.40024247765541077, lr: 5e-05
2023-12-20 19:49:19 INFO     	 * (global step 6400: loss: 0.2851819023489952, lr: 5e-05
2023-12-20 19:49:34 INFO     	 * (global step 6450: loss: 0.3264225758612156, lr: 5e-05
2023-12-20 19:49:50 INFO     	 * (global step 6500: loss: 0.3168735131621361, lr: 5e-05
2023-12-20 19:50:06 INFO     	 * (global step 6550: loss: 0.4381834641098976, lr: 5e-05
2023-12-20 19:50:22 INFO     	 * (global step 6600: loss: 0.4087904542684555, lr: 5e-05
2023-12-20 19:50:38 INFO     	 * (global step 6650: loss: 0.31696968525648117, lr: 5e-05
2023-12-20 19:50:54 INFO     	 * (global step 6700: loss: 0.3130033388733864, lr: 5e-05
2023-12-20 19:51:10 INFO     	 * (global step 6750: loss: 0.37888240441679955, lr: 5e-05
2023-12-20 19:51:26 INFO     	 * (global step 6800: loss: 0.32255005836486816, lr: 5e-05
2023-12-20 19:51:42 INFO     	 * (global step 6850: loss: 0.25864553824067116, lr: 5e-05
2023-12-20 19:51:58 INFO     	 * (global step 6900: loss: 0.3979403153061867, lr: 5e-05
2023-12-20 19:52:14 INFO     	 * (global step 6950: loss: 0.30062083527445793, lr: 5e-05
2023-12-20 19:52:29 INFO     	 * (global step 7000: loss: 0.2978886589407921, lr: 5e-05
2023-12-20 19:52:45 INFO     	 * (global step 7050: loss: 0.30651721358299255, lr: 5e-05
2023-12-20 19:52:56 INFO     [epoch 2/15] average loss: 0.332, lr: 5e-05
2023-12-20 19:52:56 INFO     saving model related files
2023-12-20 19:52:56 INFO     saving model
2023-12-20 19:52:56 INFO     saving tokenizer
2023-12-20 19:52:56 INFO     saving optimizer
2023-12-20 19:52:57 INFO     remove old optimizer files
2023-12-20 19:53:03 INFO     	 * (global step 7100: loss: 0.28404540941119194, lr: 5e-05
2023-12-20 19:53:18 INFO     	 * (global step 7150: loss: 0.4477771446108818, lr: 5e-05
2023-12-20 19:53:34 INFO     	 * (global step 7200: loss: 0.3205093964934349, lr: 5e-05
2023-12-20 19:53:50 INFO     	 * (global step 7250: loss: 0.24396327883005142, lr: 5e-05
2023-12-20 19:54:06 INFO     	 * (global step 7300: loss: 0.44154638051986694, lr: 5e-05
2023-12-20 19:54:22 INFO     	 * (global step 7350: loss: 0.2821950130164623, lr: 5e-05
2023-12-20 19:54:38 INFO     	 * (global step 7400: loss: 0.3560139574110508, lr: 5e-05
2023-12-20 19:54:54 INFO     	 * (global step 7450: loss: 0.2751721106469631, lr: 5e-05
2023-12-20 19:55:10 INFO     	 * (global step 7500: loss: 0.2827438600361347, lr: 5e-05
2023-12-20 19:55:25 INFO     	 * (global step 7550: loss: 0.3392236940562725, lr: 5e-05
2023-12-20 19:55:41 INFO     	 * (global step 7600: loss: 0.33128805458545685, lr: 5e-05
2023-12-20 19:55:57 INFO     	 * (global step 7650: loss: 0.28407691791653633, lr: 5e-05
2023-12-20 19:56:13 INFO     	 * (global step 7700: loss: 0.3521042764186859, lr: 5e-05
2023-12-20 19:56:29 INFO     	 * (global step 7750: loss: 0.3326069712638855, lr: 5e-05
2023-12-20 19:56:45 INFO     	 * (global step 7800: loss: 0.27179888635873795, lr: 5e-05
2023-12-20 19:57:00 INFO     	 * (global step 7850: loss: 0.24890724569559097, lr: 5e-05
2023-12-20 19:57:16 INFO     	 * (global step 7900: loss: 0.25745052099227905, lr: 5e-05
2023-12-20 19:57:32 INFO     	 * (global step 7950: loss: 0.2448427714407444, lr: 5e-05
2023-12-20 19:57:48 INFO     	 * (global step 8000: loss: 0.3414418026804924, lr: 5e-05
2023-12-20 19:58:04 INFO     	 * (global step 8050: loss: 0.27846378460526466, lr: 5e-05
2023-12-20 19:58:20 INFO     	 * (global step 8100: loss: 0.44990237429738045, lr: 5e-05
2023-12-20 19:58:36 INFO     	 * (global step 8150: loss: 0.26651937142014503, lr: 5e-05
2023-12-20 19:58:51 INFO     	 * (global step 8200: loss: 0.293557845056057, lr: 5e-05
2023-12-20 19:59:07 INFO     	 * (global step 8250: loss: 0.3068651482462883, lr: 5e-05
2023-12-20 19:59:23 INFO     	 * (global step 8300: loss: 0.281809214502573, lr: 5e-05
2023-12-20 19:59:39 INFO     	 * (global step 8350: loss: 0.3593912199139595, lr: 5e-05
2023-12-20 19:59:55 INFO     	 * (global step 8400: loss: 0.3031705990433693, lr: 5e-05
2023-12-20 20:00:11 INFO     	 * (global step 8450: loss: 0.307629831135273, lr: 5e-05
2023-12-20 20:00:27 INFO     	 * (global step 8500: loss: 0.31913189589977264, lr: 5e-05
2023-12-20 20:00:43 INFO     	 * (global step 8550: loss: 0.24630553275346756, lr: 5e-05
2023-12-20 20:00:59 INFO     	 * (global step 8600: loss: 0.3608175739645958, lr: 5e-05
2023-12-20 20:01:14 INFO     	 * (global step 8650: loss: 0.44472159817814827, lr: 5e-05
2023-12-20 20:01:30 INFO     	 * (global step 8700: loss: 0.3660810627043247, lr: 5e-05
2023-12-20 20:01:46 INFO     	 * (global step 8750: loss: 0.24399851262569427, lr: 5e-05
2023-12-20 20:02:02 INFO     	 * (global step 8800: loss: 0.32913271337747574, lr: 5e-05
2023-12-20 20:02:18 INFO     	 * (global step 8850: loss: 0.30927328392863274, lr: 5e-05
2023-12-20 20:02:34 INFO     	 * (global step 8900: loss: 0.24646228179335594, lr: 5e-05
2023-12-20 20:02:50 INFO     	 * (global step 8950: loss: 0.3003811873495579, lr: 5e-05
2023-12-20 20:03:05 INFO     	 * (global step 9000: loss: 0.3401215448975563, lr: 5e-05
2023-12-20 20:03:21 INFO     	 * (global step 9050: loss: 0.29867013543844223, lr: 5e-05
2023-12-20 20:03:37 INFO     	 * (global step 9100: loss: 0.3192230500280857, lr: 5e-05
2023-12-20 20:03:53 INFO     	 * (global step 9150: loss: 0.2690129652619362, lr: 5e-05
2023-12-20 20:04:09 INFO     	 * (global step 9200: loss: 0.2514899671077728, lr: 5e-05
2023-12-20 20:04:25 INFO     	 * (global step 9250: loss: 0.21302274614572525, lr: 5e-05
2023-12-20 20:04:41 INFO     	 * (global step 9300: loss: 0.2519058361649513, lr: 5e-05
2023-12-20 20:04:57 INFO     	 * (global step 9350: loss: 0.31657395884394646, lr: 5e-05
2023-12-20 20:05:12 INFO     	 * (global step 9400: loss: 0.3523371294140816, lr: 5e-05
2023-12-20 20:05:27 INFO     [epoch 3/15] average loss: 0.319, lr: 5e-05
2023-12-20 20:05:27 INFO     saving model related files
2023-12-20 20:05:27 INFO     saving model
2023-12-20 20:05:27 INFO     saving tokenizer
2023-12-20 20:05:27 INFO     saving optimizer
2023-12-20 20:05:28 INFO     remove old optimizer files
2023-12-20 20:05:30 INFO     	 * (global step 9450: loss: 0.24532660469412804, lr: 5e-05
2023-12-20 20:05:46 INFO     	 * (global step 9500: loss: 0.3909818306565285, lr: 5e-05
2023-12-20 20:06:02 INFO     	 * (global step 9550: loss: 0.31722912564873695, lr: 5e-05
2023-12-20 20:06:18 INFO     	 * (global step 9600: loss: 0.39062323421239853, lr: 5e-05
2023-12-20 20:06:34 INFO     	 * (global step 9650: loss: 0.3075711037963629, lr: 5e-05
2023-12-20 20:06:50 INFO     	 * (global step 9700: loss: 0.28312962874770164, lr: 5e-05
2023-12-20 20:07:06 INFO     	 * (global step 9750: loss: 0.2833046540617943, lr: 5e-05
2023-12-20 20:07:22 INFO     	 * (global step 9800: loss: 0.2738660238683224, lr: 5e-05
2023-12-20 20:07:38 INFO     	 * (global step 9850: loss: 0.2613283693790436, lr: 5e-05
2023-12-20 20:07:54 INFO     	 * (global step 9900: loss: 0.3598838672041893, lr: 5e-05
2023-12-20 20:08:10 INFO     	 * (global step 9950: loss: 0.3969932720065117, lr: 5e-05
2023-12-20 20:08:26 INFO     	 * (global step 10000: loss: 0.2654412854462862, lr: 5e-05
2023-12-20 20:08:41 INFO     	 * (global step 10050: loss: 0.4495128467679024, lr: 5e-05
2023-12-20 20:08:57 INFO     	 * (global step 10100: loss: 0.2265174761414528, lr: 5e-05
2023-12-20 20:09:13 INFO     	 * (global step 10150: loss: 0.42743659019470215, lr: 5e-05
2023-12-20 20:09:29 INFO     	 * (global step 10200: loss: 0.3732101172208786, lr: 5e-05
2023-12-20 20:09:45 INFO     	 * (global step 10250: loss: 0.2090887911617756, lr: 5e-05
2023-12-20 20:10:01 INFO     	 * (global step 10300: loss: 0.25828272104263306, lr: 5e-05
2023-12-20 20:10:17 INFO     	 * (global step 10350: loss: 0.37129757553339005, lr: 5e-05
2023-12-20 20:10:33 INFO     	 * (global step 10400: loss: 0.23353398963809013, lr: 5e-05
2023-12-20 20:10:49 INFO     	 * (global step 10450: loss: 0.396123506128788, lr: 5e-05
2023-12-20 20:11:05 INFO     	 * (global step 10500: loss: 0.3467966541647911, lr: 5e-05
2023-12-20 20:11:21 INFO     	 * (global step 10550: loss: 0.2866619825363159, lr: 5e-05
2023-12-20 20:11:37 INFO     	 * (global step 10600: loss: 0.3283865302801132, lr: 5e-05
2023-12-20 20:11:52 INFO     	 * (global step 10650: loss: 0.3271193876862526, lr: 5e-05
2023-12-20 20:12:09 INFO     	 * (global step 10700: loss: 0.45398610085248947, lr: 5e-05
2023-12-20 20:12:24 INFO     	 * (global step 10750: loss: 0.4815448373556137, lr: 5e-05
2023-12-20 20:12:40 INFO     	 * (global step 10800: loss: 0.2785295732319355, lr: 5e-05
2023-12-20 20:12:56 INFO     	 * (global step 10850: loss: 0.28767021000385284, lr: 5e-05
2023-12-20 20:13:12 INFO     	 * (global step 10900: loss: 0.3095037639141083, lr: 5e-05
2023-12-20 20:13:28 INFO     	 * (global step 10950: loss: 0.19252237305045128, lr: 5e-05
2023-12-20 20:13:44 INFO     	 * (global step 11000: loss: 0.294195506721735, lr: 5e-05
2023-12-20 20:13:59 INFO     	 * (global step 11050: loss: 0.33553575351834297, lr: 5e-05
2023-12-20 20:14:15 INFO     	 * (global step 11100: loss: 0.24841664358973503, lr: 5e-05
2023-12-20 20:14:31 INFO     	 * (global step 11150: loss: 0.27065359801054, lr: 5e-05
2023-12-20 20:14:47 INFO     	 * (global step 11200: loss: 0.35781172662973404, lr: 5e-05
2023-12-20 20:15:03 INFO     	 * (global step 11250: loss: 0.3760751336812973, lr: 5e-05
2023-12-20 20:15:19 INFO     	 * (global step 11300: loss: 0.28993021696805954, lr: 5e-05
2023-12-20 20:15:35 INFO     	 * (global step 11350: loss: 0.30830442160367966, lr: 5e-05
2023-12-20 20:15:51 INFO     	 * (global step 11400: loss: 0.2578957751393318, lr: 5e-05
2023-12-20 20:16:06 INFO     	 * (global step 11450: loss: 0.2885126881301403, lr: 5e-05
2023-12-20 20:16:22 INFO     	 * (global step 11500: loss: 0.23274170979857445, lr: 5e-05
2023-12-20 20:16:38 INFO     	 * (global step 11550: loss: 0.3103047087788582, lr: 5e-05
2023-12-20 20:16:54 INFO     	 * (global step 11600: loss: 0.34550629928708076, lr: 5e-05
2023-12-20 20:17:10 INFO     	 * (global step 11650: loss: 0.3270331919193268, lr: 5e-05
2023-12-20 20:17:26 INFO     	 * (global step 11700: loss: 0.33892974629998207, lr: 5e-05
2023-12-20 20:17:42 INFO     	 * (global step 11750: loss: 0.29167998768389225, lr: 5e-05
2023-12-20 20:17:58 INFO     	 * (global step 11800: loss: 0.37399471551179886, lr: 5e-05
2023-12-20 20:18:00 INFO     [epoch 4/15] average loss: 0.309, lr: 5e-05
2023-12-20 20:18:00 INFO     saving model related files
2023-12-20 20:18:00 INFO     saving model
2023-12-20 20:18:00 INFO     saving tokenizer
2023-12-20 20:18:00 INFO     saving optimizer
2023-12-20 20:18:01 INFO     remove old optimizer files
2023-12-20 20:18:15 INFO     	 * (global step 11850: loss: 0.4198937714099884, lr: 5e-05
2023-12-20 20:18:31 INFO     	 * (global step 11900: loss: 0.32284823805093765, lr: 5e-05
2023-12-20 20:18:47 INFO     	 * (global step 11950: loss: 0.28115546330809593, lr: 5e-05
2023-12-20 20:19:03 INFO     	 * (global step 12000: loss: 0.31850820407271385, lr: 5e-05
2023-12-20 20:19:19 INFO     	 * (global step 12050: loss: 0.3059108406305313, lr: 5e-05
2023-12-20 20:19:35 INFO     	 * (global step 12100: loss: 0.3633512705564499, lr: 5e-05
2023-12-20 20:19:51 INFO     	 * (global step 12150: loss: 0.3675271198153496, lr: 5e-05
2023-12-20 20:20:07 INFO     	 * (global step 12200: loss: 0.3650164529681206, lr: 5e-05
2023-12-20 20:20:23 INFO     	 * (global step 12250: loss: 0.24695316329598427, lr: 5e-05
2023-12-20 20:20:39 INFO     	 * (global step 12300: loss: 0.3211098313331604, lr: 5e-05
2023-12-20 20:20:54 INFO     	 * (global step 12350: loss: 0.2533741854131222, lr: 5e-05
2023-12-20 20:21:10 INFO     	 * (global step 12400: loss: 0.32240036502480507, lr: 5e-05
2023-12-20 20:21:26 INFO     	 * (global step 12450: loss: 0.30683714896440506, lr: 5e-05
2023-12-20 20:21:42 INFO     	 * (global step 12500: loss: 0.23790408298373222, lr: 5e-05
2023-12-20 20:21:58 INFO     	 * (global step 12550: loss: 0.33307458460330963, lr: 5e-05
2023-12-20 20:22:14 INFO     	 * (global step 12600: loss: 0.3507738411426544, lr: 5e-05
2023-12-20 20:22:30 INFO     	 * (global step 12650: loss: 0.30041829869151115, lr: 5e-05
2023-12-20 20:22:45 INFO     	 * (global step 12700: loss: 0.3112109862267971, lr: 5e-05
2023-12-20 20:23:01 INFO     	 * (global step 12750: loss: 0.29021355882287025, lr: 5e-05
2023-12-20 20:23:17 INFO     	 * (global step 12800: loss: 0.33143506199121475, lr: 5e-05
2023-12-20 20:23:32 INFO     	 * (global step 12850: loss: 0.21908563748002052, lr: 5e-05
2023-12-20 20:23:48 INFO     	 * (global step 12900: loss: 0.30495844781398773, lr: 5e-05
2023-12-20 20:24:04 INFO     	 * (global step 12950: loss: 0.23991383984684944, lr: 5e-05
2023-12-20 20:24:19 INFO     	 * (global step 13000: loss: 0.36461129039525986, lr: 5e-05
2023-12-20 20:24:35 INFO     	 * (global step 13050: loss: 0.22625906392931938, lr: 5e-05
2023-12-20 20:24:51 INFO     	 * (global step 13100: loss: 0.29309912398457527, lr: 5e-05
2023-12-20 20:25:06 INFO     	 * (global step 13150: loss: 0.2731994092464447, lr: 5e-05
2023-12-20 20:25:22 INFO     	 * (global step 13200: loss: 0.23907078430056572, lr: 5e-05
2023-12-20 20:25:37 INFO     	 * (global step 13250: loss: 0.2948162965476513, lr: 5e-05
2023-12-20 20:25:53 INFO     	 * (global step 13300: loss: 0.37154215946793556, lr: 5e-05
2023-12-20 20:26:09 INFO     	 * (global step 13350: loss: 0.3064553774893284, lr: 5e-05
2023-12-20 20:26:24 INFO     	 * (global step 13400: loss: 0.3168192505836487, lr: 5e-05
2023-12-20 20:26:40 INFO     	 * (global step 13450: loss: 0.27452488243579865, lr: 5e-05
2023-12-20 20:26:55 INFO     	 * (global step 13500: loss: 0.23467260971665382, lr: 5e-05
2023-12-20 20:27:11 INFO     	 * (global step 13550: loss: 0.33797791600227356, lr: 5e-05
2023-12-20 20:27:26 INFO     	 * (global step 13600: loss: 0.3429400138556957, lr: 5e-05
2023-12-20 20:27:42 INFO     	 * (global step 13650: loss: 0.30835140869021416, lr: 5e-05
2023-12-20 20:27:58 INFO     	 * (global step 13700: loss: 0.3555232100188732, lr: 5e-05
2023-12-20 20:28:13 INFO     	 * (global step 13750: loss: 0.2004925273358822, lr: 5e-05
2023-12-20 20:28:29 INFO     	 * (global step 13800: loss: 0.3080192059278488, lr: 5e-05
2023-12-20 20:28:44 INFO     	 * (global step 13850: loss: 0.3306274637579918, lr: 5e-05
2023-12-20 20:29:00 INFO     	 * (global step 13900: loss: 0.33012203127145767, lr: 5e-05
2023-12-20 20:29:16 INFO     	 * (global step 13950: loss: 0.3836202882230282, lr: 5e-05
2023-12-20 20:29:31 INFO     	 * (global step 14000: loss: 0.3058270588517189, lr: 5e-05
2023-12-20 20:29:47 INFO     	 * (global step 14050: loss: 0.27988239005208015, lr: 5e-05
2023-12-20 20:30:02 INFO     	 * (global step 14100: loss: 0.3792308419942856, lr: 5e-05
2023-12-20 20:30:18 INFO     	 * (global step 14150: loss: 0.32991211116313934, lr: 5e-05
2023-12-20 20:30:23 INFO     [epoch 5/15] average loss: 0.302, lr: 5e-05
2023-12-20 20:30:23 INFO     saving model related files
2023-12-20 20:30:23 INFO     saving model
2023-12-20 20:30:23 INFO     saving tokenizer
2023-12-20 20:30:24 INFO     saving optimizer
2023-12-20 20:30:24 INFO     remove old optimizer files
2023-12-20 20:30:35 INFO     	 * (global step 14200: loss: 0.34708620607852936, lr: 5e-05
2023-12-20 20:30:51 INFO     	 * (global step 14250: loss: 0.355902723968029, lr: 5e-05
2023-12-20 20:31:06 INFO     	 * (global step 14300: loss: 0.3094586208462715, lr: 5e-05
2023-12-20 20:31:22 INFO     	 * (global step 14350: loss: 0.2845844626426697, lr: 5e-05
2023-12-20 20:31:37 INFO     	 * (global step 14400: loss: 0.361501794308424, lr: 5e-05
2023-12-20 20:31:53 INFO     	 * (global step 14450: loss: 0.2699129991233349, lr: 5e-05
2023-12-20 20:32:08 INFO     	 * (global step 14500: loss: 0.37498415634036064, lr: 5e-05
2023-12-20 20:32:24 INFO     	 * (global step 14550: loss: 0.3555586151778698, lr: 5e-05
2023-12-20 20:32:39 INFO     	 * (global step 14600: loss: 0.2837633416056633, lr: 5e-05
2023-12-20 20:32:55 INFO     	 * (global step 14650: loss: 0.3062523864209652, lr: 5e-05
2023-12-20 20:33:10 INFO     	 * (global step 14700: loss: 0.2506966069340706, lr: 5e-05
2023-12-20 20:33:26 INFO     	 * (global step 14750: loss: 0.3282434269785881, lr: 5e-05
2023-12-20 20:33:41 INFO     	 * (global step 14800: loss: 0.3292386308312416, lr: 5e-05
2023-12-20 20:33:57 INFO     	 * (global step 14850: loss: 0.26985981687903404, lr: 5e-05
2023-12-20 20:34:13 INFO     	 * (global step 14900: loss: 0.2484848089516163, lr: 5e-05
2023-12-20 20:34:28 INFO     	 * (global step 14950: loss: 0.378544881939888, lr: 5e-05
2023-12-20 20:34:44 INFO     	 * (global step 15000: loss: 0.3354713171720505, lr: 5e-05
2023-12-20 20:34:59 INFO     	 * (global step 15050: loss: 0.3079729564487934, lr: 5e-05
2023-12-20 20:35:15 INFO     	 * (global step 15100: loss: 0.30447205156087875, lr: 5e-05
2023-12-20 20:35:30 INFO     	 * (global step 15150: loss: 0.23880759999155998, lr: 5e-05
2023-12-20 20:35:46 INFO     	 * (global step 15200: loss: 0.36606520786881447, lr: 5e-05
2023-12-20 20:36:01 INFO     	 * (global step 15250: loss: 0.19925419986248016, lr: 5e-05
2023-12-20 20:36:17 INFO     	 * (global step 15300: loss: 0.22594623267650604, lr: 5e-05
2023-12-20 20:36:32 INFO     	 * (global step 15350: loss: 0.21407213807106018, lr: 5e-05
2023-12-20 20:36:48 INFO     	 * (global step 15400: loss: 0.30033913999795914, lr: 5e-05
2023-12-20 20:37:03 INFO     	 * (global step 15450: loss: 0.2889576554298401, lr: 5e-05
2023-12-20 20:37:19 INFO     	 * (global step 15500: loss: 0.3175494894385338, lr: 5e-05
2023-12-20 20:37:35 INFO     	 * (global step 15550: loss: 0.2433236576616764, lr: 5e-05
2023-12-20 20:37:50 INFO     	 * (global step 15600: loss: 0.27153845876455307, lr: 5e-05
2023-12-20 20:38:06 INFO     	 * (global step 15650: loss: 0.30535734444856644, lr: 5e-05
2023-12-20 20:38:21 INFO     	 * (global step 15700: loss: 0.2716434858739376, lr: 5e-05
2023-12-20 20:38:37 INFO     	 * (global step 15750: loss: 0.297977551817894, lr: 5e-05
2023-12-20 20:38:52 INFO     	 * (global step 15800: loss: 0.2768876701593399, lr: 5e-05
2023-12-20 20:39:08 INFO     	 * (global step 15850: loss: 0.352714866399765, lr: 5e-05
2023-12-20 20:39:23 INFO     	 * (global step 15900: loss: 0.3090793564915657, lr: 5e-05
2023-12-20 20:39:39 INFO     	 * (global step 15950: loss: 0.2544538900256157, lr: 5e-05
2023-12-20 20:39:54 INFO     	 * (global step 16000: loss: 0.2205713428556919, lr: 5e-05
2023-12-20 20:40:10 INFO     	 * (global step 16050: loss: 0.2487938404083252, lr: 5e-05
2023-12-20 20:40:26 INFO     	 * (global step 16100: loss: 0.3069967105984688, lr: 5e-05
2023-12-20 20:40:41 INFO     	 * (global step 16150: loss: 0.2852795720100403, lr: 5e-05
2023-12-20 20:40:57 INFO     	 * (global step 16200: loss: 0.24248698353767395, lr: 5e-05
2023-12-20 20:41:12 INFO     	 * (global step 16250: loss: 0.26915414445102215, lr: 5e-05
2023-12-20 20:41:28 INFO     	 * (global step 16300: loss: 0.29229558259248734, lr: 5e-05
2023-12-20 20:41:44 INFO     	 * (global step 16350: loss: 0.3236349746584892, lr: 5e-05
2023-12-20 20:41:59 INFO     	 * (global step 16400: loss: 0.22505223751068115, lr: 5e-05
2023-12-20 20:42:15 INFO     	 * (global step 16450: loss: 0.34188756346702576, lr: 5e-05
2023-12-20 20:42:31 INFO     	 * (global step 16500: loss: 0.22881494835019112, lr: 5e-05
2023-12-20 20:42:39 INFO     [epoch 6/15] average loss: 0.296, lr: 5e-05
2023-12-20 20:42:39 INFO     saving model related files
2023-12-20 20:42:39 INFO     saving model
2023-12-20 20:42:40 INFO     saving tokenizer
2023-12-20 20:42:40 INFO     saving optimizer
2023-12-20 20:42:41 INFO     remove old optimizer files
2023-12-20 20:42:48 INFO     	 * (global step 16550: loss: 0.3332788273692131, lr: 5e-05
2023-12-20 20:43:04 INFO     	 * (global step 16600: loss: 0.21914240717887878, lr: 5e-05
2023-12-20 20:43:20 INFO     	 * (global step 16650: loss: 0.26401279121637344, lr: 5e-05
2023-12-20 20:43:35 INFO     	 * (global step 16700: loss: 0.305047944188118, lr: 5e-05
2023-12-20 20:43:51 INFO     	 * (global step 16750: loss: 0.32921259105205536, lr: 5e-05
2023-12-20 20:44:07 INFO     	 * (global step 16800: loss: 0.3341184929013252, lr: 5e-05
2023-12-20 20:44:22 INFO     	 * (global step 16850: loss: 0.27244726568460464, lr: 5e-05
2023-12-20 20:44:38 INFO     	 * (global step 16900: loss: 0.2586113251745701, lr: 5e-05
2023-12-20 20:44:54 INFO     	 * (global step 16950: loss: 0.3693969026207924, lr: 5e-05
2023-12-20 20:45:10 INFO     	 * (global step 17000: loss: 0.25422706082463264, lr: 5e-05
2023-12-20 20:45:25 INFO     	 * (global step 17050: loss: 0.3769301176071167, lr: 5e-05
2023-12-20 20:45:41 INFO     	 * (global step 17100: loss: 0.43110817670822144, lr: 5e-05
2023-12-20 20:45:57 INFO     	 * (global step 17150: loss: 0.2782127521932125, lr: 5e-05
2023-12-20 20:46:12 INFO     	 * (global step 17200: loss: 0.22771378606557846, lr: 5e-05
2023-12-20 20:46:28 INFO     	 * (global step 17250: loss: 0.349971242249012, lr: 5e-05
2023-12-20 20:46:44 INFO     	 * (global step 17300: loss: 0.3075163662433624, lr: 5e-05
2023-12-20 20:47:00 INFO     	 * (global step 17350: loss: 0.4348018653690815, lr: 5e-05
2023-12-20 20:47:15 INFO     	 * (global step 17400: loss: 0.27077094092965126, lr: 5e-05
2023-12-20 20:47:31 INFO     	 * (global step 17450: loss: 0.2371828928589821, lr: 5e-05
2023-12-20 20:47:47 INFO     	 * (global step 17500: loss: 0.27151164785027504, lr: 5e-05
2023-12-20 20:48:02 INFO     	 * (global step 17550: loss: 0.27553149685263634, lr: 5e-05
2023-12-20 20:48:18 INFO     	 * (global step 17600: loss: 0.298464123159647, lr: 5e-05
2023-12-20 20:48:34 INFO     	 * (global step 17650: loss: 0.2938249818980694, lr: 5e-05
2023-12-20 20:48:50 INFO     	 * (global step 17700: loss: 0.33981630951166153, lr: 5e-05
2023-12-20 20:49:05 INFO     	 * (global step 17750: loss: 0.22556887939572334, lr: 5e-05
2023-12-20 20:49:21 INFO     	 * (global step 17800: loss: 0.24098018929362297, lr: 5e-05
2023-12-20 20:49:37 INFO     	 * (global step 17850: loss: 0.2657959833741188, lr: 5e-05
2023-12-20 20:49:53 INFO     	 * (global step 17900: loss: 0.2483506202697754, lr: 5e-05
2023-12-20 20:50:08 INFO     	 * (global step 17950: loss: 0.2932905741035938, lr: 5e-05
2023-12-20 20:50:24 INFO     	 * (global step 18000: loss: 0.294330470263958, lr: 5e-05
2023-12-20 20:50:40 INFO     	 * (global step 18050: loss: 0.29036662727594376, lr: 5e-05
2023-12-20 20:50:55 INFO     	 * (global step 18100: loss: 0.25094788521528244, lr: 5e-05
2023-12-20 20:51:11 INFO     	 * (global step 18150: loss: 0.30205874145030975, lr: 5e-05
2023-12-20 20:51:27 INFO     	 * (global step 18200: loss: 0.28848841041326523, lr: 5e-05
2023-12-20 20:51:43 INFO     	 * (global step 18250: loss: 0.18127499893307686, lr: 5e-05
2023-12-20 20:51:58 INFO     	 * (global step 18300: loss: 0.36036186665296555, lr: 5e-05
2023-12-20 20:52:14 INFO     	 * (global step 18350: loss: 0.42917755991220474, lr: 5e-05
2023-12-20 20:52:30 INFO     	 * (global step 18400: loss: 0.26024962589144707, lr: 5e-05
2023-12-20 20:52:46 INFO     	 * (global step 18450: loss: 0.19666902348399162, lr: 5e-05
2023-12-20 20:53:01 INFO     	 * (global step 18500: loss: 0.3647560141980648, lr: 5e-05
2023-12-20 20:53:17 INFO     	 * (global step 18550: loss: 0.19484606571495533, lr: 5e-05
2023-12-20 20:53:33 INFO     	 * (global step 18600: loss: 0.259154312312603, lr: 5e-05
2023-12-20 20:53:49 INFO     	 * (global step 18650: loss: 0.2436097338795662, lr: 5e-05
2023-12-20 20:54:04 INFO     	 * (global step 18700: loss: 0.3912576250731945, lr: 5e-05
2023-12-20 20:54:20 INFO     	 * (global step 18750: loss: 0.4306974485516548, lr: 5e-05
2023-12-20 20:54:36 INFO     	 * (global step 18800: loss: 0.27712975814938545, lr: 5e-05
2023-12-20 20:54:51 INFO     	 * (global step 18850: loss: 0.2012652289122343, lr: 5e-05
2023-12-20 20:55:04 INFO     [epoch 7/15] average loss: 0.29, lr: 5e-05
2023-12-20 20:55:04 INFO     saving model related files
2023-12-20 20:55:04 INFO     saving model
2023-12-20 20:55:04 INFO     saving tokenizer
2023-12-20 20:55:04 INFO     saving optimizer
2023-12-20 20:55:05 INFO     remove old optimizer files
2023-12-20 20:55:09 INFO     	 * (global step 18900: loss: 0.31522858887910843, lr: 5e-05
2023-12-20 20:55:25 INFO     	 * (global step 18950: loss: 0.31784674897789955, lr: 5e-05
2023-12-20 20:55:41 INFO     	 * (global step 19000: loss: 0.25442985445261, lr: 5e-05
2023-12-20 20:55:56 INFO     	 * (global step 19050: loss: 0.2863067500293255, lr: 5e-05
2023-12-20 20:56:12 INFO     	 * (global step 19100: loss: 0.26225122809410095, lr: 5e-05
2023-12-20 20:56:28 INFO     	 * (global step 19150: loss: 0.2771754749119282, lr: 5e-05
2023-12-20 20:56:44 INFO     	 * (global step 19200: loss: 0.31742892041802406, lr: 5e-05
2023-12-20 20:56:59 INFO     	 * (global step 19250: loss: 0.26099810004234314, lr: 5e-05
2023-12-20 20:57:15 INFO     	 * (global step 19300: loss: 0.2445296123623848, lr: 5e-05
2023-12-20 20:57:31 INFO     	 * (global step 19350: loss: 0.25448114797472954, lr: 5e-05
2023-12-20 20:57:46 INFO     	 * (global step 19400: loss: 0.2822481133043766, lr: 5e-05
2023-12-20 20:58:02 INFO     	 * (global step 19450: loss: 0.2867617830634117, lr: 5e-05
2023-12-20 20:58:18 INFO     	 * (global step 19500: loss: 0.4169023707509041, lr: 5e-05
2023-12-20 20:58:33 INFO     	 * (global step 19550: loss: 0.22852012142539024, lr: 5e-05
2023-12-20 20:58:49 INFO     	 * (global step 19600: loss: 0.2527734488248825, lr: 5e-05
2023-12-20 20:59:05 INFO     	 * (global step 19650: loss: 0.2542137913405895, lr: 5e-05
2023-12-20 20:59:21 INFO     	 * (global step 19700: loss: 0.3552641160786152, lr: 5e-05
2023-12-20 20:59:36 INFO     	 * (global step 19750: loss: 0.21637441217899323, lr: 5e-05
2023-12-20 20:59:52 INFO     	 * (global step 19800: loss: 0.261707354336977, lr: 5e-05
2023-12-20 21:00:08 INFO     	 * (global step 19850: loss: 0.28256040066480637, lr: 5e-05
2023-12-20 21:00:24 INFO     	 * (global step 19900: loss: 0.25934872031211853, lr: 5e-05
2023-12-20 21:00:39 INFO     	 * (global step 19950: loss: 0.2923159673810005, lr: 5e-05
2023-12-20 21:00:55 INFO     	 * (global step 20000: loss: 0.2590944580733776, lr: 5e-05
2023-12-20 21:01:11 INFO     	 * (global step 20050: loss: 0.2439232524484396, lr: 5e-05
2023-12-20 21:01:26 INFO     	 * (global step 20100: loss: 0.30088450759649277, lr: 5e-05
2023-12-20 21:01:42 INFO     	 * (global step 20150: loss: 0.2813235744833946, lr: 5e-05
2023-12-20 21:01:58 INFO     	 * (global step 20200: loss: 0.4888905081897974, lr: 5e-05
2023-12-20 21:02:14 INFO     	 * (global step 20250: loss: 0.3121199868619442, lr: 5e-05
2023-12-20 21:02:29 INFO     	 * (global step 20300: loss: 0.29503031447529793, lr: 5e-05
2023-12-20 21:02:45 INFO     	 * (global step 20350: loss: 0.29116661474108696, lr: 5e-05
2023-12-20 21:03:01 INFO     	 * (global step 20400: loss: 0.23796845972537994, lr: 5e-05
2023-12-20 21:03:16 INFO     	 * (global step 20450: loss: 0.3552364185452461, lr: 5e-05
2023-12-20 21:03:32 INFO     	 * (global step 20500: loss: 0.2093338705599308, lr: 5e-05
2023-12-20 21:03:48 INFO     	 * (global step 20550: loss: 0.2990562133491039, lr: 5e-05
2023-12-20 21:04:04 INFO     	 * (global step 20600: loss: 0.3138859122991562, lr: 5e-05
2023-12-20 21:04:19 INFO     	 * (global step 20650: loss: 0.25705957412719727, lr: 5e-05
2023-12-20 21:04:35 INFO     	 * (global step 20700: loss: 0.2916560433804989, lr: 5e-05
2023-12-20 21:04:51 INFO     	 * (global step 20750: loss: 0.2877148613333702, lr: 5e-05
2023-12-20 21:05:07 INFO     	 * (global step 20800: loss: 0.3189797066152096, lr: 5e-05
2023-12-20 21:05:22 INFO     	 * (global step 20850: loss: 0.30670609697699547, lr: 5e-05
2023-12-20 21:05:38 INFO     	 * (global step 20900: loss: 0.3294382765889168, lr: 5e-05
2023-12-20 21:05:54 INFO     	 * (global step 20950: loss: 0.29823708161711693, lr: 5e-05
2023-12-20 21:06:10 INFO     	 * (global step 21000: loss: 0.2910183221101761, lr: 5e-05
2023-12-20 21:06:25 INFO     	 * (global step 21050: loss: 0.21929924376308918, lr: 5e-05
2023-12-20 21:06:41 INFO     	 * (global step 21100: loss: 0.2310165911912918, lr: 5e-05
2023-12-20 21:06:57 INFO     	 * (global step 21150: loss: 0.2571440078318119, lr: 5e-05
2023-12-20 21:07:13 INFO     	 * (global step 21200: loss: 0.31921274960041046, lr: 5e-05
2023-12-20 21:07:28 INFO     [epoch 8/15] average loss: 0.286, lr: 5e-05
2023-12-20 21:07:28 INFO     saving model related files
2023-12-20 21:07:28 INFO     saving model
2023-12-20 21:07:29 INFO     saving tokenizer
2023-12-20 21:07:29 INFO     saving optimizer
2023-12-20 21:07:30 INFO     remove old optimizer files
2023-12-20 21:07:30 INFO     	 * (global step 21250: loss: 0.33395687490701675, lr: 5e-05
2023-12-20 21:07:46 INFO     	 * (global step 21300: loss: 0.23711403831839561, lr: 5e-05
2023-12-20 21:08:01 INFO     	 * (global step 21350: loss: 0.27659550681710243, lr: 5e-05
2023-12-20 21:08:17 INFO     	 * (global step 21400: loss: 0.33254310861229897, lr: 5e-05
2023-12-20 21:08:33 INFO     	 * (global step 21450: loss: 0.3417467996478081, lr: 5e-05
2023-12-20 21:08:49 INFO     	 * (global step 21500: loss: 0.26344985514879227, lr: 5e-05
2023-12-20 21:09:04 INFO     	 * (global step 21550: loss: 0.30875182524323463, lr: 5e-05
2023-12-20 21:09:20 INFO     	 * (global step 21600: loss: 0.24385585263371468, lr: 5e-05
2023-12-20 21:09:36 INFO     	 * (global step 21650: loss: 0.2945130094885826, lr: 5e-05
2023-12-20 21:09:52 INFO     	 * (global step 21700: loss: 0.2770574279129505, lr: 5e-05
2023-12-20 21:10:07 INFO     	 * (global step 21750: loss: 0.23033160343766212, lr: 5e-05
2023-12-20 21:10:23 INFO     	 * (global step 21800: loss: 0.40226369723677635, lr: 5e-05
2023-12-20 21:10:39 INFO     	 * (global step 21850: loss: 0.28591494634747505, lr: 5e-05
2023-12-20 21:10:55 INFO     	 * (global step 21900: loss: 0.38155315816402435, lr: 5e-05
2023-12-20 21:11:10 INFO     	 * (global step 21950: loss: 0.39782511070370674, lr: 5e-05
2023-12-20 21:11:26 INFO     	 * (global step 22000: loss: 0.3343198783695698, lr: 5e-05
2023-12-20 21:11:42 INFO     	 * (global step 22050: loss: 0.3563004806637764, lr: 5e-05
2023-12-20 21:11:57 INFO     	 * (global step 22100: loss: 0.25434835255146027, lr: 5e-05
2023-12-20 21:12:13 INFO     	 * (global step 22150: loss: 0.2651328183710575, lr: 5e-05
2023-12-20 21:12:29 INFO     	 * (global step 22200: loss: 0.19288185983896255, lr: 5e-05
2023-12-20 21:12:45 INFO     	 * (global step 22250: loss: 0.2965237647294998, lr: 5e-05
2023-12-20 21:13:00 INFO     	 * (global step 22300: loss: 0.27994126081466675, lr: 5e-05
2023-12-20 21:13:16 INFO     	 * (global step 22350: loss: 0.33377545326948166, lr: 5e-05
2023-12-20 21:13:32 INFO     	 * (global step 22400: loss: 0.31209656596183777, lr: 5e-05
2023-12-20 21:13:48 INFO     	 * (global step 22450: loss: 0.25131356343626976, lr: 5e-05
2023-12-20 21:14:03 INFO     	 * (global step 22500: loss: 0.21964603289961815, lr: 5e-05
2023-12-20 21:14:19 INFO     	 * (global step 22550: loss: 0.31837113574147224, lr: 5e-05
2023-12-20 21:14:35 INFO     	 * (global step 22600: loss: 0.34215956553816795, lr: 5e-05
2023-12-20 21:14:51 INFO     	 * (global step 22650: loss: 0.28082234784960747, lr: 5e-05
2023-12-20 21:15:06 INFO     	 * (global step 22700: loss: 0.2511562407016754, lr: 5e-05
2023-12-20 21:15:22 INFO     	 * (global step 22750: loss: 0.31337499618530273, lr: 5e-05
2023-12-20 21:15:38 INFO     	 * (global step 22800: loss: 0.24498648941516876, lr: 5e-05
2023-12-20 21:15:54 INFO     	 * (global step 22850: loss: 0.3019307777285576, lr: 5e-05
2023-12-20 21:16:09 INFO     	 * (global step 22900: loss: 0.250364288687706, lr: 5e-05
2023-12-20 21:16:25 INFO     	 * (global step 22950: loss: 0.32012490183115005, lr: 5e-05
2023-12-20 21:16:41 INFO     	 * (global step 23000: loss: 0.2756831720471382, lr: 5e-05
2023-12-20 21:16:56 INFO     	 * (global step 23050: loss: 0.24425215646624565, lr: 5e-05
2023-12-20 21:17:12 INFO     	 * (global step 23100: loss: 0.3311324939131737, lr: 5e-05
2023-12-20 21:17:28 INFO     	 * (global step 23150: loss: 0.3270963951945305, lr: 5e-05
2023-12-20 21:17:44 INFO     	 * (global step 23200: loss: 0.2952475845813751, lr: 5e-05
2023-12-20 21:17:59 INFO     	 * (global step 23250: loss: 0.26182615384459496, lr: 5e-05
2023-12-20 21:18:15 INFO     	 * (global step 23300: loss: 0.23730820044875145, lr: 5e-05
2023-12-20 21:18:31 INFO     	 * (global step 23350: loss: 0.302082397043705, lr: 5e-05
2023-12-20 21:18:46 INFO     	 * (global step 23400: loss: 0.3047582246363163, lr: 5e-05
2023-12-20 21:19:02 INFO     	 * (global step 23450: loss: 0.2633514478802681, lr: 5e-05
2023-12-20 21:19:18 INFO     	 * (global step 23500: loss: 0.2646372877061367, lr: 5e-05
2023-12-20 21:19:34 INFO     	 * (global step 23550: loss: 0.34476790949702263, lr: 5e-05
2023-12-20 21:19:49 INFO     	 * (global step 23600: loss: 0.23359578102827072, lr: 5e-05
2023-12-20 21:19:53 INFO     [epoch 9/15] average loss: 0.282, lr: 5e-05
2023-12-20 21:19:53 INFO     saving model related files
2023-12-20 21:19:53 INFO     saving model
2023-12-20 21:19:53 INFO     saving tokenizer
2023-12-20 21:19:53 INFO     saving optimizer
2023-12-20 21:19:54 INFO     remove old optimizer files
2023-12-20 21:19:54 INFO     complete training: model ckpt was saved at small_combined_trained_ckpt/model_mntyya
2023-12-20 21:19:55 INFO     ## 1st RUN: Configuration 5/12 ##
2023-12-20 21:19:55 INFO     initialize model trainer
2023-12-20 21:19:55 INFO     initialize checkpoint at small_combined_trained_ckpt/model_woixzh
2023-12-20 21:19:55 INFO     hyperparameters
2023-12-20 21:19:55 INFO     	 * dataset_path: StellarMilk/squad_newsqa
2023-12-20 21:19:55 INFO     	 * dataset_name: default
2023-12-20 21:19:55 INFO     	 * input_types: ['paragraph']
2023-12-20 21:19:55 INFO     	 * output_types: ['questions_answers']
2023-12-20 21:19:55 INFO     	 * prefix_types: ['qag']
2023-12-20 21:19:55 INFO     	 * model: t5-small
2023-12-20 21:19:55 INFO     	 * max_length: 512
2023-12-20 21:19:55 INFO     	 * max_length_output: 512
2023-12-20 21:19:55 INFO     	 * epoch: 15
2023-12-20 21:19:55 INFO     	 * batch: 2
2023-12-20 21:19:55 INFO     	 * lr: 5e-05
2023-12-20 21:19:55 INFO     	 * fp16: False
2023-12-20 21:19:55 INFO     	 * random_seed: 1
2023-12-20 21:19:55 INFO     	 * gradient_accumulation_steps: 2
2023-12-20 21:19:55 INFO     	 * label_smoothing: 0.15
2023-12-20 21:19:55 INFO     initialize checkpoint with t5-small
2023-12-20 21:19:56 INFO     use spaCy answer extraction model: positionrank
2023-12-20 21:19:56 INFO     Model `t5-small`
2023-12-20 21:19:56 INFO     	 * Num of GPU in use: 1
2023-12-20 21:19:56 INFO     	 * Prefix: True
2023-12-20 21:19:56 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 21:19:56 INFO     dataset preprocessing
2023-12-20 21:19:58 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/squad_newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-20 21:19:59 INFO     start model training
2023-12-20 21:20:07 INFO     	 * (global step 50: loss: 1.4694690108299255, lr: 5e-05
2023-12-20 21:20:16 INFO     	 * (global step 100: loss: 0.8707203269004822, lr: 5e-05
2023-12-20 21:20:24 INFO     	 * (global step 150: loss: 0.8047786951065063, lr: 5e-05
2023-12-20 21:20:32 INFO     	 * (global step 200: loss: 0.664919376373291, lr: 5e-05
2023-12-20 21:20:40 INFO     	 * (global step 250: loss: 0.500159814953804, lr: 5e-05
2023-12-20 21:20:48 INFO     	 * (global step 300: loss: 0.5390867739915848, lr: 5e-05
2023-12-20 21:20:56 INFO     	 * (global step 350: loss: 0.5248388051986694, lr: 5e-05
2023-12-20 21:21:04 INFO     	 * (global step 400: loss: 0.5038136690855026, lr: 5e-05
2023-12-20 21:21:12 INFO     	 * (global step 450: loss: 0.4992511421442032, lr: 5e-05
2023-12-20 21:21:20 INFO     	 * (global step 500: loss: 0.5699698626995087, lr: 5e-05
2023-12-20 21:21:28 INFO     	 * (global step 550: loss: 0.5779638290405273, lr: 5e-05
2023-12-20 21:21:36 INFO     	 * (global step 600: loss: 0.3393259048461914, lr: 5e-05
2023-12-20 21:21:44 INFO     	 * (global step 650: loss: 0.4055522084236145, lr: 5e-05
2023-12-20 21:21:52 INFO     	 * (global step 700: loss: 0.5606154501438141, lr: 5e-05
2023-12-20 21:22:00 INFO     	 * (global step 750: loss: 0.3992961049079895, lr: 5e-05
2023-12-20 21:22:08 INFO     	 * (global step 800: loss: 0.378390833735466, lr: 5e-05
2023-12-20 21:22:16 INFO     	 * (global step 850: loss: 0.2946271598339081, lr: 5e-05
2023-12-20 21:22:24 INFO     	 * (global step 900: loss: 0.415529265999794, lr: 5e-05
2023-12-20 21:22:32 INFO     	 * (global step 950: loss: 0.3692760318517685, lr: 5e-05
2023-12-20 21:22:40 INFO     	 * (global step 1000: loss: 0.340553879737854, lr: 5e-05
2023-12-20 21:22:48 INFO     	 * (global step 1050: loss: 0.3775583356618881, lr: 5e-05
2023-12-20 21:22:56 INFO     	 * (global step 1100: loss: 0.48937974870204926, lr: 5e-05
2023-12-20 21:23:04 INFO     	 * (global step 1150: loss: 0.36847984790802, lr: 5e-05
2023-12-20 21:23:13 INFO     	 * (global step 1200: loss: 0.4846610873937607, lr: 5e-05
2023-12-20 21:23:21 INFO     	 * (global step 1250: loss: 0.4859403520822525, lr: 5e-05
2023-12-20 21:23:29 INFO     	 * (global step 1300: loss: 0.35467664897441864, lr: 5e-05
2023-12-20 21:23:37 INFO     	 * (global step 1350: loss: 0.46001939475536346, lr: 5e-05
2023-12-20 21:23:45 INFO     	 * (global step 1400: loss: 0.4172058403491974, lr: 5e-05
2023-12-20 21:23:53 INFO     	 * (global step 1450: loss: 0.4030950963497162, lr: 5e-05
2023-12-20 21:24:01 INFO     	 * (global step 1500: loss: 0.382183775305748, lr: 5e-05
2023-12-20 21:24:09 INFO     	 * (global step 1550: loss: 0.34076617658138275, lr: 5e-05
2023-12-20 21:24:17 INFO     	 * (global step 1600: loss: 0.39296895265579224, lr: 5e-05
2023-12-20 21:24:25 INFO     	 * (global step 1650: loss: 0.3524627387523651, lr: 5e-05
2023-12-20 21:24:33 INFO     	 * (global step 1700: loss: 0.4165772348642349, lr: 5e-05
2023-12-20 21:24:41 INFO     	 * (global step 1750: loss: 0.30743008852005005, lr: 5e-05
2023-12-20 21:24:49 INFO     	 * (global step 1800: loss: 0.34543491899967194, lr: 5e-05
2023-12-20 21:24:57 INFO     	 * (global step 1850: loss: 0.5098693817853928, lr: 5e-05
2023-12-20 21:25:05 INFO     	 * (global step 1900: loss: 0.27542661130428314, lr: 5e-05
2023-12-20 21:25:13 INFO     	 * (global step 1950: loss: 0.36366724967956543, lr: 5e-05
2023-12-20 21:25:21 INFO     	 * (global step 2000: loss: 0.3644634038209915, lr: 5e-05
2023-12-20 21:25:30 INFO     	 * (global step 2050: loss: 0.36361198127269745, lr: 5e-05
2023-12-20 21:25:38 INFO     	 * (global step 2100: loss: 0.32571449875831604, lr: 5e-05
2023-12-20 21:25:46 INFO     	 * (global step 2150: loss: 0.44484764337539673, lr: 5e-05
2023-12-20 21:25:54 INFO     	 * (global step 2200: loss: 0.37773069739341736, lr: 5e-05
2023-12-20 21:26:02 INFO     	 * (global step 2250: loss: 0.418443962931633, lr: 5e-05
2023-12-20 21:26:10 INFO     	 * (global step 2300: loss: 0.3778160363435745, lr: 5e-05
2023-12-20 21:26:18 INFO     	 * (global step 2350: loss: 0.31284213066101074, lr: 5e-05
2023-12-20 21:26:26 INFO     	 * (global step 2400: loss: 0.4032820612192154, lr: 5e-05
2023-12-20 21:26:34 INFO     	 * (global step 2450: loss: 0.29708340764045715, lr: 5e-05
2023-12-20 21:26:42 INFO     	 * (global step 2500: loss: 0.3312201276421547, lr: 5e-05
2023-12-20 21:26:50 INFO     	 * (global step 2550: loss: 0.6520441621541977, lr: 5e-05
2023-12-20 21:26:58 INFO     	 * (global step 2600: loss: 0.4568774998188019, lr: 5e-05
2023-12-20 21:27:06 INFO     	 * (global step 2650: loss: 0.4517098069190979, lr: 5e-05
2023-12-20 21:27:14 INFO     	 * (global step 2700: loss: 0.28790129721164703, lr: 5e-05
2023-12-20 21:27:22 INFO     	 * (global step 2750: loss: 0.43668732047080994, lr: 5e-05
2023-12-20 21:27:30 INFO     	 * (global step 2800: loss: 0.3940373957157135, lr: 5e-05
2023-12-20 21:27:39 INFO     	 * (global step 2850: loss: 0.24999267607927322, lr: 5e-05
2023-12-20 21:27:47 INFO     	 * (global step 2900: loss: 0.3732480853796005, lr: 5e-05
2023-12-20 21:27:55 INFO     	 * (global step 2950: loss: 0.2084834799170494, lr: 5e-05
2023-12-20 21:28:03 INFO     	 * (global step 3000: loss: 0.5849083214998245, lr: 5e-05
2023-12-20 21:28:11 INFO     	 * (global step 3050: loss: 0.34100954234600067, lr: 5e-05
2023-12-20 21:28:19 INFO     	 * (global step 3100: loss: 0.3463921993970871, lr: 5e-05
2023-12-20 21:28:27 INFO     	 * (global step 3150: loss: 0.3381357043981552, lr: 5e-05
2023-12-20 21:28:35 INFO     	 * (global step 3200: loss: 0.30213961005210876, lr: 5e-05
2023-12-20 21:28:43 INFO     	 * (global step 3250: loss: 0.3623880445957184, lr: 5e-05
2023-12-20 21:28:51 INFO     	 * (global step 3300: loss: 0.49386948347091675, lr: 5e-05
2023-12-20 21:28:59 INFO     	 * (global step 3350: loss: 0.3345247209072113, lr: 5e-05
2023-12-20 21:29:07 INFO     	 * (global step 3400: loss: 0.3475605547428131, lr: 5e-05
2023-12-20 21:29:15 INFO     	 * (global step 3450: loss: 0.41429637372493744, lr: 5e-05
2023-12-20 21:29:23 INFO     	 * (global step 3500: loss: 0.32043714821338654, lr: 5e-05
2023-12-20 21:29:32 INFO     	 * (global step 3550: loss: 0.4900576323270798, lr: 5e-05
2023-12-20 21:29:40 INFO     	 * (global step 3600: loss: 0.4014546126127243, lr: 5e-05
2023-12-20 21:29:48 INFO     	 * (global step 3650: loss: 0.4249327927827835, lr: 5e-05
2023-12-20 21:29:56 INFO     	 * (global step 3700: loss: 0.32871130108833313, lr: 5e-05
2023-12-20 21:30:04 INFO     	 * (global step 3750: loss: 0.33325787633657455, lr: 5e-05
2023-12-20 21:30:12 INFO     	 * (global step 3800: loss: 0.32825006544589996, lr: 5e-05
2023-12-20 21:30:20 INFO     	 * (global step 3850: loss: 0.2005608007311821, lr: 5e-05
2023-12-20 21:30:28 INFO     	 * (global step 3900: loss: 0.41273950040340424, lr: 5e-05
2023-12-20 21:30:36 INFO     	 * (global step 3950: loss: 0.2732807621359825, lr: 5e-05
2023-12-20 21:30:44 INFO     	 * (global step 4000: loss: 0.24826553463935852, lr: 5e-05
2023-12-20 21:30:52 INFO     	 * (global step 4050: loss: 0.24983343482017517, lr: 5e-05
2023-12-20 21:31:00 INFO     	 * (global step 4100: loss: 0.33397457003593445, lr: 5e-05
2023-12-20 21:31:08 INFO     	 * (global step 4150: loss: 0.2798596918582916, lr: 5e-05
2023-12-20 21:31:16 INFO     	 * (global step 4200: loss: 0.2940416857600212, lr: 5e-05
2023-12-20 21:31:24 INFO     	 * (global step 4250: loss: 0.37184572219848633, lr: 5e-05
2023-12-20 21:31:33 INFO     	 * (global step 4300: loss: 0.4492526799440384, lr: 5e-05
2023-12-20 21:31:41 INFO     	 * (global step 4350: loss: 0.2766614258289337, lr: 5e-05
2023-12-20 21:31:49 INFO     	 * (global step 4400: loss: 0.4024282395839691, lr: 5e-05
2023-12-20 21:31:57 INFO     	 * (global step 4450: loss: 0.4205382764339447, lr: 5e-05
2023-12-20 21:32:05 INFO     	 * (global step 4500: loss: 0.2828887104988098, lr: 5e-05
2023-12-20 21:32:13 INFO     	 * (global step 4550: loss: 0.30217444896698, lr: 5e-05
2023-12-20 21:32:21 INFO     	 * (global step 4600: loss: 0.3395213335752487, lr: 5e-05
2023-12-20 21:32:29 INFO     	 * (global step 4650: loss: 0.38997337222099304, lr: 5e-05
2023-12-20 21:32:37 INFO     	 * (global step 4700: loss: 0.3370988443493843, lr: 5e-05
2023-12-20 21:32:41 INFO     [epoch 0/15] average loss: 0.449, lr: 5e-05
2023-12-20 21:32:41 INFO     saving model related files
2023-12-20 21:32:41 INFO     saving model
2023-12-20 21:32:41 INFO     saving tokenizer
2023-12-20 21:32:41 INFO     saving optimizer
2023-12-20 21:32:42 INFO     remove old optimizer files
2023-12-20 21:32:47 INFO     	 * (global step 4750: loss: 0.4262048304080963, lr: 5e-05
2023-12-20 21:32:55 INFO     	 * (global step 4800: loss: 0.32191338390111923, lr: 5e-05
2023-12-20 21:33:03 INFO     	 * (global step 4850: loss: 0.2786779850721359, lr: 5e-05
2023-12-20 21:33:11 INFO     	 * (global step 4900: loss: 0.40097402036190033, lr: 5e-05
2023-12-20 21:33:19 INFO     	 * (global step 4950: loss: 0.3491165488958359, lr: 5e-05
2023-12-20 21:33:27 INFO     	 * (global step 5000: loss: 0.23553843051195145, lr: 5e-05
2023-12-20 21:33:35 INFO     	 * (global step 5050: loss: 0.3872740566730499, lr: 5e-05
2023-12-20 21:33:43 INFO     	 * (global step 5100: loss: 0.33547544479370117, lr: 5e-05
2023-12-20 21:33:51 INFO     	 * (global step 5150: loss: 0.25715454667806625, lr: 5e-05
2023-12-20 21:33:59 INFO     	 * (global step 5200: loss: 0.3213687837123871, lr: 5e-05
2023-12-20 21:34:07 INFO     	 * (global step 5250: loss: 0.25105030834674835, lr: 5e-05
2023-12-20 21:34:15 INFO     	 * (global step 5300: loss: 0.3489900827407837, lr: 5e-05
2023-12-20 21:34:23 INFO     	 * (global step 5350: loss: 0.41978152096271515, lr: 5e-05
2023-12-20 21:34:31 INFO     	 * (global step 5400: loss: 0.3931209444999695, lr: 5e-05
2023-12-20 21:34:39 INFO     	 * (global step 5450: loss: 0.6372427344322205, lr: 5e-05
2023-12-20 21:34:47 INFO     	 * (global step 5500: loss: 0.29053647816181183, lr: 5e-05
2023-12-20 21:34:56 INFO     	 * (global step 5550: loss: 0.2802109643816948, lr: 5e-05
2023-12-20 21:35:04 INFO     	 * (global step 5600: loss: 0.4526786357164383, lr: 5e-05
2023-12-20 21:35:12 INFO     	 * (global step 5650: loss: 0.22843992710113525, lr: 5e-05
2023-12-20 21:35:20 INFO     	 * (global step 5700: loss: 0.3037380576133728, lr: 5e-05
2023-12-20 21:35:28 INFO     	 * (global step 5750: loss: 0.38469672203063965, lr: 5e-05
2023-12-20 21:35:36 INFO     	 * (global step 5800: loss: 0.2977406531572342, lr: 5e-05
2023-12-20 21:35:44 INFO     	 * (global step 5850: loss: 0.32861998677253723, lr: 5e-05
2023-12-20 21:35:52 INFO     	 * (global step 5900: loss: 0.2544500082731247, lr: 5e-05
2023-12-20 21:36:00 INFO     	 * (global step 5950: loss: 0.42092469334602356, lr: 5e-05
2023-12-20 21:36:08 INFO     	 * (global step 6000: loss: 0.33996330201625824, lr: 5e-05
2023-12-20 21:36:16 INFO     	 * (global step 6050: loss: 0.30146048963069916, lr: 5e-05
2023-12-20 21:36:24 INFO     	 * (global step 6100: loss: 0.2636791840195656, lr: 5e-05
2023-12-20 21:36:32 INFO     	 * (global step 6150: loss: 0.31652703881263733, lr: 5e-05
2023-12-20 21:36:40 INFO     	 * (global step 6200: loss: 0.39071042835712433, lr: 5e-05
2023-12-20 21:36:48 INFO     	 * (global step 6250: loss: 0.3355560898780823, lr: 5e-05
2023-12-20 21:36:57 INFO     	 * (global step 6300: loss: 0.6583372205495834, lr: 5e-05
2023-12-20 21:37:05 INFO     	 * (global step 6350: loss: 0.19966911524534225, lr: 5e-05
2023-12-20 21:37:13 INFO     	 * (global step 6400: loss: 0.3047928512096405, lr: 5e-05
2023-12-20 21:37:21 INFO     	 * (global step 6450: loss: 0.27576569467782974, lr: 5e-05
2023-12-20 21:37:29 INFO     	 * (global step 6500: loss: 0.26222530007362366, lr: 5e-05
2023-12-20 21:37:37 INFO     	 * (global step 6550: loss: 0.17569176852703094, lr: 5e-05
2023-12-20 21:37:45 INFO     	 * (global step 6600: loss: 0.3404414653778076, lr: 5e-05
2023-12-20 21:37:53 INFO     	 * (global step 6650: loss: 0.31162577867507935, lr: 5e-05
2023-12-20 21:38:01 INFO     	 * (global step 6700: loss: 0.3197499215602875, lr: 5e-05
2023-12-20 21:38:09 INFO     	 * (global step 6750: loss: 0.4353184401988983, lr: 5e-05
2023-12-20 21:38:17 INFO     	 * (global step 6800: loss: 0.29986169934272766, lr: 5e-05
2023-12-20 21:38:25 INFO     	 * (global step 6850: loss: 0.6750082969665527, lr: 5e-05
2023-12-20 21:38:33 INFO     	 * (global step 6900: loss: 0.307513490319252, lr: 5e-05
2023-12-20 21:38:42 INFO     	 * (global step 6950: loss: 0.26398253440856934, lr: 5e-05
2023-12-20 21:38:50 INFO     	 * (global step 7000: loss: 0.4555339515209198, lr: 5e-05
2023-12-20 21:38:58 INFO     	 * (global step 7050: loss: 0.2987794727087021, lr: 5e-05
2023-12-20 21:39:06 INFO     	 * (global step 7100: loss: 0.34532664716243744, lr: 5e-05
2023-12-20 21:39:14 INFO     	 * (global step 7150: loss: 0.2763501852750778, lr: 5e-05
2023-12-20 21:39:22 INFO     	 * (global step 7200: loss: 0.301585890352726, lr: 5e-05
2023-12-20 21:39:30 INFO     	 * (global step 7250: loss: 0.3209274709224701, lr: 5e-05
2023-12-20 21:39:38 INFO     	 * (global step 7300: loss: 0.313582144677639, lr: 5e-05
2023-12-20 21:39:46 INFO     	 * (global step 7350: loss: 0.21756517887115479, lr: 5e-05
2023-12-20 21:39:54 INFO     	 * (global step 7400: loss: 0.4566320776939392, lr: 5e-05
2023-12-20 21:40:02 INFO     	 * (global step 7450: loss: 0.4665980935096741, lr: 5e-05
2023-12-20 21:40:10 INFO     	 * (global step 7500: loss: 0.33430901914834976, lr: 5e-05
2023-12-20 21:40:18 INFO     	 * (global step 7550: loss: 0.41428378224372864, lr: 5e-05
2023-12-20 21:40:27 INFO     	 * (global step 7600: loss: 0.2417927160859108, lr: 5e-05
2023-12-20 21:40:35 INFO     	 * (global step 7650: loss: 0.27838200330734253, lr: 5e-05
2023-12-20 21:40:43 INFO     	 * (global step 7700: loss: 0.381589412689209, lr: 5e-05
2023-12-20 21:40:51 INFO     	 * (global step 7750: loss: 0.2952837273478508, lr: 5e-05
2023-12-20 21:40:59 INFO     	 * (global step 7800: loss: 0.29606587439775467, lr: 5e-05
2023-12-20 21:41:07 INFO     	 * (global step 7850: loss: 0.3505202531814575, lr: 5e-05
2023-12-20 21:41:15 INFO     	 * (global step 7900: loss: 0.38241586089134216, lr: 5e-05
2023-12-20 21:41:23 INFO     	 * (global step 7950: loss: 0.37148579955101013, lr: 5e-05
2023-12-20 21:41:31 INFO     	 * (global step 8000: loss: 0.24643106013536453, lr: 5e-05
2023-12-20 21:41:39 INFO     	 * (global step 8050: loss: 0.38826408982276917, lr: 5e-05
2023-12-20 21:41:47 INFO     	 * (global step 8100: loss: 0.42444874346256256, lr: 5e-05
2023-12-20 21:41:55 INFO     	 * (global step 8150: loss: 0.4999205321073532, lr: 5e-05
2023-12-20 21:42:03 INFO     	 * (global step 8200: loss: 0.23947126418352127, lr: 5e-05
2023-12-20 21:42:11 INFO     	 * (global step 8250: loss: 0.4567016363143921, lr: 5e-05
2023-12-20 21:42:19 INFO     	 * (global step 8300: loss: 0.2919926568865776, lr: 5e-05
2023-12-20 21:42:27 INFO     	 * (global step 8350: loss: 0.4315149337053299, lr: 5e-05
2023-12-20 21:42:35 INFO     	 * (global step 8400: loss: 0.29243339598178864, lr: 5e-05
2023-12-20 21:42:44 INFO     	 * (global step 8450: loss: 0.3633621484041214, lr: 5e-05
2023-12-20 21:42:52 INFO     	 * (global step 8500: loss: 0.47144410014152527, lr: 5e-05
2023-12-20 21:43:00 INFO     	 * (global step 8550: loss: 0.4892416298389435, lr: 5e-05
2023-12-20 21:43:08 INFO     	 * (global step 8600: loss: 0.20206304639577866, lr: 5e-05
2023-12-20 21:43:16 INFO     	 * (global step 8650: loss: 0.4194517433643341, lr: 5e-05
2023-12-20 21:43:24 INFO     	 * (global step 8700: loss: 0.18099746108055115, lr: 5e-05
2023-12-20 21:43:32 INFO     	 * (global step 8750: loss: 0.2268526628613472, lr: 5e-05
2023-12-20 21:43:40 INFO     	 * (global step 8800: loss: 0.28665389120578766, lr: 5e-05
2023-12-20 21:43:48 INFO     	 * (global step 8850: loss: 0.3965510427951813, lr: 5e-05
2023-12-20 21:43:56 INFO     	 * (global step 8900: loss: 0.24944128096103668, lr: 5e-05
2023-12-20 21:44:04 INFO     	 * (global step 8950: loss: 0.24586425721645355, lr: 5e-05
2023-12-20 21:44:13 INFO     	 * (global step 9000: loss: 0.25393659621477127, lr: 5e-05
2023-12-20 21:44:21 INFO     	 * (global step 9050: loss: 0.4834088608622551, lr: 5e-05
2023-12-20 21:44:29 INFO     	 * (global step 9100: loss: 0.3793555200099945, lr: 5e-05
2023-12-20 21:44:37 INFO     	 * (global step 9150: loss: 0.27211447060108185, lr: 5e-05
2023-12-20 21:44:45 INFO     	 * (global step 9200: loss: 0.23592191189527512, lr: 5e-05
2023-12-20 21:44:53 INFO     	 * (global step 9250: loss: 0.5724727287888527, lr: 5e-05
2023-12-20 21:45:01 INFO     	 * (global step 9300: loss: 0.3046146631240845, lr: 5e-05
2023-12-20 21:45:09 INFO     	 * (global step 9350: loss: 0.41254301369190216, lr: 5e-05
2023-12-20 21:45:17 INFO     	 * (global step 9400: loss: 0.3244435489177704, lr: 5e-05
2023-12-20 21:45:25 INFO     [epoch 1/15] average loss: 0.336, lr: 5e-05
2023-12-20 21:45:25 INFO     saving model related files
2023-12-20 21:45:25 INFO     saving model
2023-12-20 21:45:25 INFO     saving tokenizer
2023-12-20 21:45:25 INFO     saving optimizer
2023-12-20 21:45:26 INFO     remove old optimizer files
2023-12-20 21:45:27 INFO     	 * (global step 9450: loss: 0.4080018699169159, lr: 5e-05
2023-12-20 21:45:35 INFO     	 * (global step 9500: loss: 0.29647205024957657, lr: 5e-05
2023-12-20 21:45:43 INFO     	 * (global step 9550: loss: 0.36984654515981674, lr: 5e-05
2023-12-20 21:45:51 INFO     	 * (global step 9600: loss: 0.2885785326361656, lr: 5e-05
2023-12-20 21:45:59 INFO     	 * (global step 9650: loss: 0.25240686535835266, lr: 5e-05
2023-12-20 21:46:07 INFO     	 * (global step 9700: loss: 0.2309986874461174, lr: 5e-05
2023-12-20 21:46:15 INFO     	 * (global step 9750: loss: 0.41165363788604736, lr: 5e-05
2023-12-20 21:46:23 INFO     	 * (global step 9800: loss: 0.2575499787926674, lr: 5e-05
2023-12-20 21:46:32 INFO     	 * (global step 9850: loss: 0.262955866754055, lr: 5e-05
2023-12-20 21:46:40 INFO     	 * (global step 9900: loss: 0.19479398429393768, lr: 5e-05
2023-12-20 21:46:48 INFO     	 * (global step 9950: loss: 0.3065773546695709, lr: 5e-05
2023-12-20 21:46:56 INFO     	 * (global step 10000: loss: 0.23854731023311615, lr: 5e-05
2023-12-20 21:47:04 INFO     	 * (global step 10050: loss: 0.2174072265625, lr: 5e-05
2023-12-20 21:47:12 INFO     	 * (global step 10100: loss: 0.29229340702295303, lr: 5e-05
2023-12-20 21:47:20 INFO     	 * (global step 10150: loss: 0.2965487390756607, lr: 5e-05
2023-12-20 21:47:28 INFO     	 * (global step 10200: loss: 0.29479871690273285, lr: 5e-05
2023-12-20 21:47:36 INFO     	 * (global step 10250: loss: 0.47604554146528244, lr: 5e-05
2023-12-20 21:47:44 INFO     	 * (global step 10300: loss: 0.4041699916124344, lr: 5e-05
2023-12-20 21:47:52 INFO     	 * (global step 10350: loss: 0.4029921740293503, lr: 5e-05
2023-12-20 21:48:00 INFO     	 * (global step 10400: loss: 0.3052178621292114, lr: 5e-05
2023-12-20 21:48:08 INFO     	 * (global step 10450: loss: 0.29419784247875214, lr: 5e-05
2023-12-20 21:48:17 INFO     	 * (global step 10500: loss: 0.2659979537129402, lr: 5e-05
2023-12-20 21:48:25 INFO     	 * (global step 10550: loss: 0.32338573783636093, lr: 5e-05
2023-12-20 21:48:33 INFO     	 * (global step 10600: loss: 0.31411947309970856, lr: 5e-05
2023-12-20 21:48:41 INFO     	 * (global step 10650: loss: 0.31851164996623993, lr: 5e-05
2023-12-20 21:48:49 INFO     	 * (global step 10700: loss: 0.270098939538002, lr: 5e-05
2023-12-20 21:48:57 INFO     	 * (global step 10750: loss: 0.3799494206905365, lr: 5e-05
2023-12-20 21:49:05 INFO     	 * (global step 10800: loss: 0.37898752093315125, lr: 5e-05
2023-12-20 21:49:13 INFO     	 * (global step 10850: loss: 0.28627926111221313, lr: 5e-05
2023-12-20 21:49:21 INFO     	 * (global step 10900: loss: 0.3218190521001816, lr: 5e-05
2023-12-20 21:49:29 INFO     	 * (global step 10950: loss: 0.299844428896904, lr: 5e-05
2023-12-20 21:49:38 INFO     	 * (global step 11000: loss: 0.55124132335186, lr: 5e-05
2023-12-20 21:49:46 INFO     	 * (global step 11050: loss: 0.3022880479693413, lr: 5e-05
2023-12-20 21:49:54 INFO     	 * (global step 11100: loss: 0.5380293875932693, lr: 5e-05
2023-12-20 21:50:02 INFO     	 * (global step 11150: loss: 0.3312738835811615, lr: 5e-05
2023-12-20 21:50:10 INFO     	 * (global step 11200: loss: 0.24391841888427734, lr: 5e-05
2023-12-20 21:50:18 INFO     	 * (global step 11250: loss: 0.20523670315742493, lr: 5e-05
2023-12-20 21:50:26 INFO     	 * (global step 11300: loss: 0.31614647805690765, lr: 5e-05
2023-12-20 21:50:34 INFO     	 * (global step 11350: loss: 0.27652707695961, lr: 5e-05
2023-12-20 21:50:42 INFO     	 * (global step 11400: loss: 0.21694627404212952, lr: 5e-05
2023-12-20 21:50:50 INFO     	 * (global step 11450: loss: 0.26729314774274826, lr: 5e-05
2023-12-20 21:50:58 INFO     	 * (global step 11500: loss: 0.23195292800664902, lr: 5e-05
2023-12-20 21:51:07 INFO     	 * (global step 11550: loss: 0.4551226645708084, lr: 5e-05
2023-12-20 21:51:15 INFO     	 * (global step 11600: loss: 0.37623950839042664, lr: 5e-05
2023-12-20 21:51:23 INFO     	 * (global step 11650: loss: 0.26637449860572815, lr: 5e-05
2023-12-20 21:51:31 INFO     	 * (global step 11700: loss: 0.4652310982346535, lr: 5e-05
2023-12-20 21:51:39 INFO     	 * (global step 11750: loss: 0.25609883666038513, lr: 5e-05
2023-12-20 21:51:47 INFO     	 * (global step 11800: loss: 0.19740831851959229, lr: 5e-05
2023-12-20 21:51:55 INFO     	 * (global step 11850: loss: 0.30517300963401794, lr: 5e-05
2023-12-20 21:52:03 INFO     	 * (global step 11900: loss: 0.3580072447657585, lr: 5e-05
2023-12-20 21:52:11 INFO     	 * (global step 11950: loss: 0.3138323426246643, lr: 5e-05
2023-12-20 21:52:19 INFO     	 * (global step 12000: loss: 0.2644442543387413, lr: 5e-05
2023-12-20 21:52:27 INFO     	 * (global step 12050: loss: 0.2741920202970505, lr: 5e-05
2023-12-20 21:52:36 INFO     	 * (global step 12100: loss: 0.35213638842105865, lr: 5e-05
2023-12-20 21:52:44 INFO     	 * (global step 12150: loss: 0.3688410073518753, lr: 5e-05
2023-12-20 21:52:52 INFO     	 * (global step 12200: loss: 0.3174816966056824, lr: 5e-05
2023-12-20 21:53:00 INFO     	 * (global step 12250: loss: 0.30121685564517975, lr: 5e-05
2023-12-20 21:53:08 INFO     	 * (global step 12300: loss: 0.24626287817955017, lr: 5e-05
2023-12-20 21:53:16 INFO     	 * (global step 12350: loss: 0.3734205514192581, lr: 5e-05
2023-12-20 21:53:24 INFO     	 * (global step 12400: loss: 0.6441972553730011, lr: 5e-05
2023-12-20 21:53:32 INFO     	 * (global step 12450: loss: 0.20445092767477036, lr: 5e-05
2023-12-20 21:53:40 INFO     	 * (global step 12500: loss: 0.37768834829330444, lr: 5e-05
2023-12-20 21:53:48 INFO     	 * (global step 12550: loss: 0.3639635145664215, lr: 5e-05
2023-12-20 21:53:56 INFO     	 * (global step 12600: loss: 0.4568772315979004, lr: 5e-05
2023-12-20 21:54:05 INFO     	 * (global step 12650: loss: 0.31565435230731964, lr: 5e-05
2023-12-20 21:54:13 INFO     	 * (global step 12700: loss: 0.2581126540899277, lr: 5e-05
2023-12-20 21:54:21 INFO     	 * (global step 12750: loss: 0.3496267795562744, lr: 5e-05
2023-12-20 21:54:29 INFO     	 * (global step 12800: loss: 0.2797067388892174, lr: 5e-05
2023-12-20 21:54:37 INFO     	 * (global step 12850: loss: 0.25339779257774353, lr: 5e-05
2023-12-20 21:54:45 INFO     	 * (global step 12900: loss: 0.27517056465148926, lr: 5e-05
2023-12-20 21:54:53 INFO     	 * (global step 12950: loss: 0.356011763215065, lr: 5e-05
2023-12-20 21:55:01 INFO     	 * (global step 13000: loss: 0.40580011904239655, lr: 5e-05
2023-12-20 21:55:09 INFO     	 * (global step 13050: loss: 0.24896392226219177, lr: 5e-05
2023-12-20 21:55:17 INFO     	 * (global step 13100: loss: 0.32809580862522125, lr: 5e-05
2023-12-20 21:55:25 INFO     	 * (global step 13150: loss: 0.35702258348464966, lr: 5e-05
2023-12-20 21:55:33 INFO     	 * (global step 13200: loss: 0.29151828587055206, lr: 5e-05
2023-12-20 21:55:41 INFO     	 * (global step 13250: loss: 0.2432921603322029, lr: 5e-05
2023-12-20 21:55:49 INFO     	 * (global step 13300: loss: 0.3891395330429077, lr: 5e-05
2023-12-20 21:55:58 INFO     	 * (global step 13350: loss: 0.39597587287425995, lr: 5e-05
2023-12-20 21:56:06 INFO     	 * (global step 13400: loss: 0.23459292203187943, lr: 5e-05
2023-12-20 21:56:14 INFO     	 * (global step 13450: loss: 0.3485209345817566, lr: 5e-05
2023-12-20 21:56:22 INFO     	 * (global step 13500: loss: 0.27373313903808594, lr: 5e-05
2023-12-20 21:56:30 INFO     	 * (global step 13550: loss: 0.21504666656255722, lr: 5e-05
2023-12-20 21:56:38 INFO     	 * (global step 13600: loss: 0.5486842095851898, lr: 5e-05
2023-12-20 21:56:46 INFO     	 * (global step 13650: loss: 0.21039129793643951, lr: 5e-05
2023-12-20 21:56:54 INFO     	 * (global step 13700: loss: 0.31167780607938766, lr: 5e-05
2023-12-20 21:57:02 INFO     	 * (global step 13750: loss: 0.2846648842096329, lr: 5e-05
2023-12-20 21:57:10 INFO     	 * (global step 13800: loss: 0.3512328639626503, lr: 5e-05
2023-12-20 21:57:19 INFO     	 * (global step 13850: loss: 0.49861177057027817, lr: 5e-05
2023-12-20 21:57:27 INFO     	 * (global step 13900: loss: 0.23544328659772873, lr: 5e-05
2023-12-20 21:57:35 INFO     	 * (global step 13950: loss: 0.29145608842372894, lr: 5e-05
2023-12-20 21:57:43 INFO     	 * (global step 14000: loss: 0.3470425680279732, lr: 5e-05
2023-12-20 21:57:51 INFO     	 * (global step 14050: loss: 0.3541555404663086, lr: 5e-05
2023-12-20 21:57:59 INFO     	 * (global step 14100: loss: 0.23869816958904266, lr: 5e-05
2023-12-20 21:58:07 INFO     	 * (global step 14150: loss: 0.3370714634656906, lr: 5e-05
2023-12-20 21:58:10 INFO     [epoch 2/15] average loss: 0.317, lr: 5e-05
2023-12-20 21:58:10 INFO     saving model related files
2023-12-20 21:58:10 INFO     saving model
2023-12-20 21:58:11 INFO     saving tokenizer
2023-12-20 21:58:11 INFO     saving optimizer
2023-12-20 21:58:12 INFO     remove old optimizer files
2023-12-20 21:58:17 INFO     	 * (global step 14200: loss: 0.3338601142168045, lr: 5e-05
2023-12-20 21:58:25 INFO     	 * (global step 14250: loss: 0.2629806026816368, lr: 5e-05
2023-12-20 21:58:33 INFO     	 * (global step 14300: loss: 0.16162701696157455, lr: 5e-05
2023-12-20 21:58:41 INFO     	 * (global step 14350: loss: 0.34351149946451187, lr: 5e-05
2023-12-20 21:58:49 INFO     	 * (global step 14400: loss: 0.32036685943603516, lr: 5e-05
2023-12-20 21:58:57 INFO     	 * (global step 14450: loss: 0.24154184013605118, lr: 5e-05
2023-12-20 21:59:05 INFO     	 * (global step 14500: loss: 0.4849095493555069, lr: 5e-05
2023-12-20 21:59:13 INFO     	 * (global step 14550: loss: 0.22155485302209854, lr: 5e-05
2023-12-20 21:59:22 INFO     	 * (global step 14600: loss: 0.22222958505153656, lr: 5e-05
2023-12-20 21:59:30 INFO     	 * (global step 14650: loss: 0.2819180190563202, lr: 5e-05
2023-12-20 21:59:38 INFO     	 * (global step 14700: loss: 0.39949898421764374, lr: 5e-05
2023-12-20 21:59:46 INFO     	 * (global step 14750: loss: 0.2618713155388832, lr: 5e-05
2023-12-20 21:59:54 INFO     	 * (global step 14800: loss: 0.34352031350135803, lr: 5e-05
2023-12-20 22:00:02 INFO     	 * (global step 14850: loss: 0.28560028970241547, lr: 5e-05
2023-12-20 22:00:10 INFO     	 * (global step 14900: loss: 0.4012064039707184, lr: 5e-05
2023-12-20 22:00:18 INFO     	 * (global step 14950: loss: 0.31416335701942444, lr: 5e-05
2023-12-20 22:00:26 INFO     	 * (global step 15000: loss: 0.23281466215848923, lr: 5e-05
2023-12-20 22:00:34 INFO     	 * (global step 15050: loss: 0.25393984466791153, lr: 5e-05
2023-12-20 22:00:42 INFO     	 * (global step 15100: loss: 0.24988969415426254, lr: 5e-05
2023-12-20 22:00:50 INFO     	 * (global step 15150: loss: 0.40884697437286377, lr: 5e-05
2023-12-20 22:00:58 INFO     	 * (global step 15200: loss: 0.4465435743331909, lr: 5e-05
2023-12-20 22:01:06 INFO     	 * (global step 15250: loss: 0.4733869880437851, lr: 5e-05
2023-12-20 22:01:14 INFO     	 * (global step 15300: loss: 0.322667732834816, lr: 5e-05
2023-12-20 22:01:22 INFO     	 * (global step 15350: loss: 0.2897244170308113, lr: 5e-05
2023-12-20 22:01:31 INFO     	 * (global step 15400: loss: 0.23234423249959946, lr: 5e-05
2023-12-20 22:01:39 INFO     	 * (global step 15450: loss: 0.29032668471336365, lr: 5e-05
2023-12-20 22:01:47 INFO     	 * (global step 15500: loss: 0.30735181272029877, lr: 5e-05
2023-12-20 22:01:55 INFO     	 * (global step 15550: loss: 0.23857435584068298, lr: 5e-05
2023-12-20 22:02:03 INFO     	 * (global step 15600: loss: 0.38732776045799255, lr: 5e-05
2023-12-20 22:02:11 INFO     	 * (global step 15650: loss: 0.34734077751636505, lr: 5e-05
2023-12-20 22:02:19 INFO     	 * (global step 15700: loss: 0.33937065303325653, lr: 5e-05
2023-12-20 22:02:27 INFO     	 * (global step 15750: loss: 0.3893700838088989, lr: 5e-05
2023-12-20 22:02:36 INFO     	 * (global step 15800: loss: 0.211372509598732, lr: 5e-05
2023-12-20 22:02:44 INFO     	 * (global step 15850: loss: 0.3586185723543167, lr: 5e-05
2023-12-20 22:02:52 INFO     	 * (global step 15900: loss: 0.22649703174829483, lr: 5e-05
2023-12-20 22:03:00 INFO     	 * (global step 15950: loss: 0.31355322897434235, lr: 5e-05
2023-12-20 22:03:08 INFO     	 * (global step 16000: loss: 0.1636691763997078, lr: 5e-05
2023-12-20 22:03:16 INFO     	 * (global step 16050: loss: 0.4321164935827255, lr: 5e-05
2023-12-20 22:03:24 INFO     	 * (global step 16100: loss: 0.3544512018561363, lr: 5e-05
2023-12-20 22:03:32 INFO     	 * (global step 16150: loss: 0.260684996843338, lr: 5e-05
2023-12-20 22:03:40 INFO     	 * (global step 16200: loss: 0.2925665080547333, lr: 5e-05
2023-12-20 22:03:48 INFO     	 * (global step 16250: loss: 0.33981387317180634, lr: 5e-05
2023-12-20 22:03:56 INFO     	 * (global step 16300: loss: 0.26148951053619385, lr: 5e-05
2023-12-20 22:04:04 INFO     	 * (global step 16350: loss: 0.31902359426021576, lr: 5e-05
2023-12-20 22:04:12 INFO     	 * (global step 16400: loss: 0.33337603509426117, lr: 5e-05
2023-12-20 22:04:20 INFO     	 * (global step 16450: loss: 0.3056844547390938, lr: 5e-05
2023-12-20 22:04:29 INFO     	 * (global step 16500: loss: 0.2990906834602356, lr: 5e-05
2023-12-20 22:04:37 INFO     	 * (global step 16550: loss: 0.31851229816675186, lr: 5e-05
2023-12-20 22:04:45 INFO     	 * (global step 16600: loss: 0.2615521401166916, lr: 5e-05
2023-12-20 22:04:53 INFO     	 * (global step 16650: loss: 0.3120334595441818, lr: 5e-05
2023-12-20 22:05:01 INFO     	 * (global step 16700: loss: 0.20994745939970016, lr: 5e-05
2023-12-20 22:05:09 INFO     	 * (global step 16750: loss: 0.14108256995677948, lr: 5e-05
2023-12-20 22:05:17 INFO     	 * (global step 16800: loss: 0.31999026238918304, lr: 5e-05
2023-12-20 22:05:25 INFO     	 * (global step 16850: loss: 0.36784105002880096, lr: 5e-05
2023-12-20 22:05:33 INFO     	 * (global step 16900: loss: 0.2812661826610565, lr: 5e-05
2023-12-20 22:05:41 INFO     	 * (global step 16950: loss: 0.3438723683357239, lr: 5e-05
2023-12-20 22:05:49 INFO     	 * (global step 17000: loss: 0.2418799102306366, lr: 5e-05
2023-12-20 22:05:57 INFO     	 * (global step 17050: loss: 0.21599678695201874, lr: 5e-05
2023-12-20 22:06:05 INFO     	 * (global step 17100: loss: 0.3293085843324661, lr: 5e-05
2023-12-20 22:06:14 INFO     	 * (global step 17150: loss: 0.2569727450609207, lr: 5e-05
2023-12-20 22:06:22 INFO     	 * (global step 17200: loss: 0.3115086108446121, lr: 5e-05
2023-12-20 22:06:30 INFO     	 * (global step 17250: loss: 0.2855529934167862, lr: 5e-05
2023-12-20 22:06:38 INFO     	 * (global step 17300: loss: 0.21527942642569542, lr: 5e-05
2023-12-20 22:06:46 INFO     	 * (global step 17350: loss: 0.36689551174640656, lr: 5e-05
2023-12-20 22:06:54 INFO     	 * (global step 17400: loss: 0.19750915467739105, lr: 5e-05
2023-12-20 22:07:02 INFO     	 * (global step 17450: loss: 0.45541878044605255, lr: 5e-05
2023-12-20 22:07:10 INFO     	 * (global step 17500: loss: 0.3146605044603348, lr: 5e-05
2023-12-20 22:07:18 INFO     	 * (global step 17550: loss: 0.28206491470336914, lr: 5e-05
2023-12-20 22:07:26 INFO     	 * (global step 17600: loss: 0.2989683151245117, lr: 5e-05
2023-12-20 22:07:34 INFO     	 * (global step 17650: loss: 0.411750853061676, lr: 5e-05
2023-12-20 22:07:42 INFO     	 * (global step 17700: loss: 0.2704358845949173, lr: 5e-05
2023-12-20 22:07:50 INFO     	 * (global step 17750: loss: 0.35037242621183395, lr: 5e-05
2023-12-20 22:07:58 INFO     	 * (global step 17800: loss: 0.4707862585783005, lr: 5e-05
2023-12-20 22:08:07 INFO     	 * (global step 17850: loss: 0.23873066902160645, lr: 5e-05
2023-12-20 22:08:15 INFO     	 * (global step 17900: loss: 0.3206275403499603, lr: 5e-05
2023-12-20 22:08:23 INFO     	 * (global step 17950: loss: 0.2839094400405884, lr: 5e-05
2023-12-20 22:08:31 INFO     	 * (global step 18000: loss: 0.3008187487721443, lr: 5e-05
2023-12-20 22:08:39 INFO     	 * (global step 18050: loss: 0.20254261046648026, lr: 5e-05
2023-12-20 22:08:47 INFO     	 * (global step 18100: loss: 0.32287997007369995, lr: 5e-05
2023-12-20 22:08:55 INFO     	 * (global step 18150: loss: 0.30395588278770447, lr: 5e-05
2023-12-20 22:09:03 INFO     	 * (global step 18200: loss: 0.3827839493751526, lr: 5e-05
2023-12-20 22:09:11 INFO     	 * (global step 18250: loss: 0.25261835008859634, lr: 5e-05
2023-12-20 22:09:19 INFO     	 * (global step 18300: loss: 0.28028687834739685, lr: 5e-05
2023-12-20 22:09:27 INFO     	 * (global step 18350: loss: 0.3164200186729431, lr: 5e-05
2023-12-20 22:09:35 INFO     	 * (global step 18400: loss: 0.2977460101246834, lr: 5e-05
2023-12-20 22:09:43 INFO     	 * (global step 18450: loss: 0.3369392305612564, lr: 5e-05
2023-12-20 22:09:51 INFO     	 * (global step 18500: loss: 0.29375699162483215, lr: 5e-05
2023-12-20 22:10:00 INFO     	 * (global step 18550: loss: 0.3488923907279968, lr: 5e-05
2023-12-20 22:10:08 INFO     	 * (global step 18600: loss: 0.4247339814901352, lr: 5e-05
2023-12-20 22:10:16 INFO     	 * (global step 18650: loss: 0.5368661284446716, lr: 5e-05
2023-12-20 22:10:24 INFO     	 * (global step 18700: loss: 0.3739720284938812, lr: 5e-05
2023-12-20 22:10:32 INFO     	 * (global step 18750: loss: 0.20778482407331467, lr: 5e-05
2023-12-20 22:10:40 INFO     	 * (global step 18800: loss: 0.5127785578370094, lr: 5e-05
2023-12-20 22:10:48 INFO     	 * (global step 18850: loss: 0.2591233104467392, lr: 5e-05
2023-12-20 22:10:55 INFO     [epoch 3/15] average loss: 0.305, lr: 5e-05
2023-12-20 22:10:55 INFO     saving model related files
2023-12-20 22:10:55 INFO     saving model
2023-12-20 22:10:55 INFO     saving tokenizer
2023-12-20 22:10:55 INFO     saving optimizer
2023-12-20 22:10:56 INFO     remove old optimizer files
2023-12-20 22:10:58 INFO     	 * (global step 18900: loss: 0.2597327008843422, lr: 5e-05
2023-12-20 22:11:06 INFO     	 * (global step 18950: loss: 0.28103721141815186, lr: 5e-05
2023-12-20 22:11:14 INFO     	 * (global step 19000: loss: 0.5162516087293625, lr: 5e-05
2023-12-20 22:11:22 INFO     	 * (global step 19050: loss: 0.37475715577602386, lr: 5e-05
2023-12-20 22:11:30 INFO     	 * (global step 19100: loss: 0.23339006304740906, lr: 5e-05
2023-12-20 22:11:38 INFO     	 * (global step 19150: loss: 0.23280133306980133, lr: 5e-05
2023-12-20 22:11:46 INFO     	 * (global step 19200: loss: 0.320724681019783, lr: 5e-05
2023-12-20 22:11:54 INFO     	 * (global step 19250: loss: 0.21257180720567703, lr: 5e-05
2023-12-20 22:12:02 INFO     	 * (global step 19300: loss: 0.21816660463809967, lr: 5e-05
2023-12-20 22:12:10 INFO     	 * (global step 19350: loss: 0.18198733031749725, lr: 5e-05
2023-12-20 22:12:18 INFO     	 * (global step 19400: loss: 0.29898063838481903, lr: 5e-05
2023-12-20 22:12:26 INFO     	 * (global step 19450: loss: 0.18292517215013504, lr: 5e-05
2023-12-20 22:12:35 INFO     	 * (global step 19500: loss: 0.3155505210161209, lr: 5e-05
2023-12-20 22:12:43 INFO     	 * (global step 19550: loss: 0.3062901049852371, lr: 5e-05
2023-12-20 22:12:51 INFO     	 * (global step 19600: loss: 0.22335653752088547, lr: 5e-05
2023-12-20 22:12:59 INFO     	 * (global step 19650: loss: 0.5437907129526138, lr: 5e-05
2023-12-20 22:13:07 INFO     	 * (global step 19700: loss: 0.29417311400175095, lr: 5e-05
2023-12-20 22:13:15 INFO     	 * (global step 19750: loss: 0.29177796840667725, lr: 5e-05
2023-12-20 22:13:23 INFO     	 * (global step 19800: loss: 0.2443215399980545, lr: 5e-05
2023-12-20 22:13:31 INFO     	 * (global step 19850: loss: 0.19885020703077316, lr: 5e-05
2023-12-20 22:13:39 INFO     	 * (global step 19900: loss: 0.3178868144750595, lr: 5e-05
2023-12-20 22:13:47 INFO     	 * (global step 19950: loss: 0.2706698402762413, lr: 5e-05
2023-12-20 22:13:55 INFO     	 * (global step 20000: loss: 0.19959721714258194, lr: 5e-05
2023-12-20 22:14:03 INFO     	 * (global step 20050: loss: 0.22965922951698303, lr: 5e-05
2023-12-20 22:14:11 INFO     	 * (global step 20100: loss: 0.3568739742040634, lr: 5e-05
2023-12-20 22:14:19 INFO     	 * (global step 20150: loss: 0.26205068081617355, lr: 5e-05
2023-12-20 22:14:27 INFO     	 * (global step 20200: loss: 0.2787307798862457, lr: 5e-05
2023-12-20 22:14:35 INFO     	 * (global step 20250: loss: 0.28999002277851105, lr: 5e-05
2023-12-20 22:14:43 INFO     	 * (global step 20300: loss: 0.3050339072942734, lr: 5e-05
2023-12-20 22:14:52 INFO     	 * (global step 20350: loss: 0.23893287777900696, lr: 5e-05
2023-12-20 22:15:00 INFO     	 * (global step 20400: loss: 0.24846801906824112, lr: 5e-05
2023-12-20 22:15:08 INFO     	 * (global step 20450: loss: 0.21583671122789383, lr: 5e-05
2023-12-20 22:15:16 INFO     	 * (global step 20500: loss: 0.23244532942771912, lr: 5e-05
2023-12-20 22:15:24 INFO     	 * (global step 20550: loss: 0.24542687088251114, lr: 5e-05
2023-12-20 22:15:32 INFO     	 * (global step 20600: loss: 0.29340432584285736, lr: 5e-05
2023-12-20 22:15:40 INFO     	 * (global step 20650: loss: 0.41637229919433594, lr: 5e-05
2023-12-20 22:15:48 INFO     	 * (global step 20700: loss: 0.24097245186567307, lr: 5e-05
2023-12-20 22:15:56 INFO     	 * (global step 20750: loss: 0.24637188762426376, lr: 5e-05
2023-12-20 22:16:04 INFO     	 * (global step 20800: loss: 0.3380254879593849, lr: 5e-05
2023-12-20 22:16:13 INFO     	 * (global step 20850: loss: 0.30501043796539307, lr: 5e-05
2023-12-20 22:16:21 INFO     	 * (global step 20900: loss: 0.3170924633741379, lr: 5e-05
2023-12-20 22:16:29 INFO     	 * (global step 20950: loss: 0.29593925178050995, lr: 5e-05
2023-12-20 22:16:37 INFO     	 * (global step 21000: loss: 0.4481329768896103, lr: 5e-05
2023-12-20 22:16:45 INFO     	 * (global step 21050: loss: 0.2641400247812271, lr: 5e-05
2023-12-20 22:16:53 INFO     	 * (global step 21100: loss: 0.23591751605272293, lr: 5e-05
2023-12-20 22:17:01 INFO     	 * (global step 21150: loss: 0.27405497431755066, lr: 5e-05
2023-12-20 22:17:09 INFO     	 * (global step 21200: loss: 0.34128715097904205, lr: 5e-05
2023-12-20 22:17:17 INFO     	 * (global step 21250: loss: 0.2491779550909996, lr: 5e-05
2023-12-20 22:17:25 INFO     	 * (global step 21300: loss: 0.3072229102253914, lr: 5e-05
2023-12-20 22:17:33 INFO     	 * (global step 21350: loss: 0.2404157668352127, lr: 5e-05
2023-12-20 22:17:41 INFO     	 * (global step 21400: loss: 0.20204904675483704, lr: 5e-05
2023-12-20 22:17:49 INFO     	 * (global step 21450: loss: 0.49400730431079865, lr: 5e-05
2023-12-20 22:17:57 INFO     	 * (global step 21500: loss: 0.26865970343351364, lr: 5e-05
2023-12-20 22:18:06 INFO     	 * (global step 21550: loss: 0.29500526189804077, lr: 5e-05
2023-12-20 22:18:14 INFO     	 * (global step 21600: loss: 0.35718830674886703, lr: 5e-05
2023-12-20 22:18:22 INFO     	 * (global step 21650: loss: 0.2535419762134552, lr: 5e-05
2023-12-20 22:18:30 INFO     	 * (global step 21700: loss: 0.18229079246520996, lr: 5e-05
2023-12-20 22:18:38 INFO     	 * (global step 21750: loss: 0.3524315878748894, lr: 5e-05
2023-12-20 22:18:46 INFO     	 * (global step 21800: loss: 0.30932992696762085, lr: 5e-05
2023-12-20 22:18:54 INFO     	 * (global step 21850: loss: 0.3709392696619034, lr: 5e-05
2023-12-20 22:19:02 INFO     	 * (global step 21900: loss: 0.2765331268310547, lr: 5e-05
2023-12-20 22:19:10 INFO     	 * (global step 21950: loss: 0.29546280950307846, lr: 5e-05
2023-12-20 22:19:18 INFO     	 * (global step 22000: loss: 0.2938445881009102, lr: 5e-05
2023-12-20 22:19:26 INFO     	 * (global step 22050: loss: 0.3477049097418785, lr: 5e-05
2023-12-20 22:19:34 INFO     	 * (global step 22100: loss: 0.4022633954882622, lr: 5e-05
2023-12-20 22:19:42 INFO     	 * (global step 22150: loss: 0.4417257457971573, lr: 5e-05
2023-12-20 22:19:50 INFO     	 * (global step 22200: loss: 0.20386876910924911, lr: 5e-05
2023-12-20 22:19:58 INFO     	 * (global step 22250: loss: 0.26180265098810196, lr: 5e-05
2023-12-20 22:20:06 INFO     	 * (global step 22300: loss: 0.29709264636039734, lr: 5e-05
2023-12-20 22:20:14 INFO     	 * (global step 22350: loss: 0.36838874220848083, lr: 5e-05
2023-12-20 22:20:22 INFO     	 * (global step 22400: loss: 0.19971014559268951, lr: 5e-05
2023-12-20 22:20:30 INFO     	 * (global step 22450: loss: 0.3967032879590988, lr: 5e-05
2023-12-20 22:20:38 INFO     	 * (global step 22500: loss: 0.2998463362455368, lr: 5e-05
2023-12-20 22:20:46 INFO     	 * (global step 22550: loss: 0.2824309766292572, lr: 5e-05
2023-12-20 22:20:55 INFO     	 * (global step 22600: loss: 0.30261482298374176, lr: 5e-05
2023-12-20 22:21:03 INFO     	 * (global step 22650: loss: 0.16001922637224197, lr: 5e-05
2023-12-20 22:21:11 INFO     	 * (global step 22700: loss: 0.39800307154655457, lr: 5e-05
2023-12-20 22:21:19 INFO     	 * (global step 22750: loss: 0.32354576885700226, lr: 5e-05
2023-12-20 22:21:27 INFO     	 * (global step 22800: loss: 0.38733914494514465, lr: 5e-05
2023-12-20 22:21:35 INFO     	 * (global step 22850: loss: 0.3009386882185936, lr: 5e-05
2023-12-20 22:21:43 INFO     	 * (global step 22900: loss: 0.31567060947418213, lr: 5e-05
2023-12-20 22:21:51 INFO     	 * (global step 22950: loss: 0.2253957763314247, lr: 5e-05
2023-12-20 22:21:59 INFO     	 * (global step 23000: loss: 0.36454665660858154, lr: 5e-05
2023-12-20 22:22:07 INFO     	 * (global step 23050: loss: 0.45033836364746094, lr: 5e-05
2023-12-20 22:22:15 INFO     	 * (global step 23100: loss: 0.3335120677947998, lr: 5e-05
2023-12-20 22:22:23 INFO     	 * (global step 23150: loss: 0.26860348880290985, lr: 5e-05
2023-12-20 22:22:31 INFO     	 * (global step 23200: loss: 0.28252094984054565, lr: 5e-05
2023-12-20 22:22:39 INFO     	 * (global step 23250: loss: 0.15542538836598396, lr: 5e-05
2023-12-20 22:22:47 INFO     	 * (global step 23300: loss: 0.2676771730184555, lr: 5e-05
2023-12-20 22:22:55 INFO     	 * (global step 23350: loss: 0.271566241979599, lr: 5e-05
2023-12-20 22:23:03 INFO     	 * (global step 23400: loss: 0.35040003061294556, lr: 5e-05
2023-12-20 22:23:11 INFO     	 * (global step 23450: loss: 0.3841891586780548, lr: 5e-05
2023-12-20 22:23:19 INFO     	 * (global step 23500: loss: 0.2407028004527092, lr: 5e-05
2023-12-20 22:23:28 INFO     	 * (global step 23550: loss: 0.29038530588150024, lr: 5e-05
2023-12-20 22:23:36 INFO     	 * (global step 23600: loss: 0.3560890704393387, lr: 5e-05
2023-12-20 22:23:38 INFO     [epoch 4/15] average loss: 0.296, lr: 5e-05
2023-12-20 22:23:38 INFO     saving model related files
2023-12-20 22:23:38 INFO     saving model
2023-12-20 22:23:38 INFO     saving tokenizer
2023-12-20 22:23:39 INFO     saving optimizer
2023-12-20 22:23:39 INFO     remove old optimizer files
2023-12-20 22:23:45 INFO     	 * (global step 23650: loss: 0.4300341010093689, lr: 5e-05
2023-12-20 22:23:53 INFO     	 * (global step 23700: loss: 0.24171878397464752, lr: 5e-05
2023-12-20 22:24:01 INFO     	 * (global step 23750: loss: 0.30730699747800827, lr: 5e-05
2023-12-20 22:24:09 INFO     	 * (global step 23800: loss: 0.23391343653202057, lr: 5e-05
2023-12-20 22:24:17 INFO     	 * (global step 23850: loss: 0.3384518623352051, lr: 5e-05
2023-12-20 22:24:26 INFO     	 * (global step 23900: loss: 0.2545296922326088, lr: 5e-05
2023-12-20 22:24:34 INFO     	 * (global step 23950: loss: 0.2343166097998619, lr: 5e-05
2023-12-20 22:24:42 INFO     	 * (global step 24000: loss: 0.34175682067871094, lr: 5e-05
2023-12-20 22:24:50 INFO     	 * (global step 24050: loss: 0.44259143620729446, lr: 5e-05
2023-12-20 22:24:58 INFO     	 * (global step 24100: loss: 0.26538948714733124, lr: 5e-05
2023-12-20 22:25:06 INFO     	 * (global step 24150: loss: 0.3592262417078018, lr: 5e-05
2023-12-20 22:25:14 INFO     	 * (global step 24200: loss: 0.34083472192287445, lr: 5e-05
2023-12-20 22:25:22 INFO     	 * (global step 24250: loss: 0.42266930639743805, lr: 5e-05
2023-12-20 22:25:30 INFO     	 * (global step 24300: loss: 0.2770487368106842, lr: 5e-05
2023-12-20 22:25:38 INFO     	 * (global step 24350: loss: 0.3742845803499222, lr: 5e-05
2023-12-20 22:25:46 INFO     	 * (global step 24400: loss: 0.2788141369819641, lr: 5e-05
2023-12-20 22:25:54 INFO     	 * (global step 24450: loss: 0.3733198344707489, lr: 5e-05
2023-12-20 22:26:02 INFO     	 * (global step 24500: loss: 0.24306008964776993, lr: 5e-05
2023-12-20 22:26:10 INFO     	 * (global step 24550: loss: 0.3383787274360657, lr: 5e-05
2023-12-20 22:26:18 INFO     	 * (global step 24600: loss: 0.32193005084991455, lr: 5e-05
2023-12-20 22:26:26 INFO     	 * (global step 24650: loss: 0.21802736818790436, lr: 5e-05
2023-12-20 22:26:35 INFO     	 * (global step 24700: loss: 0.3255062401294708, lr: 5e-05
2023-12-20 22:26:43 INFO     	 * (global step 24750: loss: 0.2829909399151802, lr: 5e-05
2023-12-20 22:26:51 INFO     	 * (global step 24800: loss: 0.28635069727897644, lr: 5e-05
2023-12-20 22:26:59 INFO     	 * (global step 24850: loss: 0.34612832218408585, lr: 5e-05
2023-12-20 22:27:07 INFO     	 * (global step 24900: loss: 0.3397195041179657, lr: 5e-05
2023-12-20 22:27:15 INFO     	 * (global step 24950: loss: 0.2824920490384102, lr: 5e-05
2023-12-20 22:27:23 INFO     	 * (global step 25000: loss: 0.3238387554883957, lr: 5e-05
2023-12-20 22:27:31 INFO     	 * (global step 25050: loss: 0.17028120905160904, lr: 5e-05
2023-12-20 22:27:39 INFO     	 * (global step 25100: loss: 0.344134196639061, lr: 5e-05
2023-12-20 22:27:47 INFO     	 * (global step 25150: loss: 0.7776815593242645, lr: 5e-05
2023-12-20 22:27:55 INFO     	 * (global step 25200: loss: 0.3095027804374695, lr: 5e-05
2023-12-20 22:28:03 INFO     	 * (global step 25250: loss: 0.26264000684022903, lr: 5e-05
2023-12-20 22:28:12 INFO     	 * (global step 25300: loss: 0.32922983169555664, lr: 5e-05
2023-12-20 22:28:20 INFO     	 * (global step 25350: loss: 0.27586545050144196, lr: 5e-05
2023-12-20 22:28:28 INFO     	 * (global step 25400: loss: 0.26271094381809235, lr: 5e-05
2023-12-20 22:28:36 INFO     	 * (global step 25450: loss: 0.26489462703466415, lr: 5e-05
2023-12-20 22:28:44 INFO     	 * (global step 25500: loss: 0.3121424466371536, lr: 5e-05
2023-12-20 22:28:52 INFO     	 * (global step 25550: loss: 0.3583340495824814, lr: 5e-05
2023-12-20 22:29:00 INFO     	 * (global step 25600: loss: 0.26241789758205414, lr: 5e-05
2023-12-20 22:29:08 INFO     	 * (global step 25650: loss: 0.2174338400363922, lr: 5e-05
2023-12-20 22:29:16 INFO     	 * (global step 25700: loss: 0.3398920148611069, lr: 5e-05
2023-12-20 22:29:24 INFO     	 * (global step 25750: loss: 0.24683375656604767, lr: 5e-05
2023-12-20 22:29:32 INFO     	 * (global step 25800: loss: 0.28513993322849274, lr: 5e-05
2023-12-20 22:29:40 INFO     	 * (global step 25850: loss: 0.26479802280664444, lr: 5e-05
2023-12-20 22:29:48 INFO     	 * (global step 25900: loss: 0.5402373671531677, lr: 5e-05
2023-12-20 22:29:57 INFO     	 * (global step 25950: loss: 0.19310824573040009, lr: 5e-05
2023-12-20 22:30:05 INFO     	 * (global step 26000: loss: 0.33991195261478424, lr: 5e-05
2023-12-20 22:30:13 INFO     	 * (global step 26050: loss: 0.4065515398979187, lr: 5e-05
2023-12-20 22:30:21 INFO     	 * (global step 26100: loss: 0.31995613873004913, lr: 5e-05
2023-12-20 22:30:29 INFO     	 * (global step 26150: loss: 0.2776896134018898, lr: 5e-05
2023-12-20 22:30:37 INFO     	 * (global step 26200: loss: 0.44902969896793365, lr: 5e-05
2023-12-20 22:30:45 INFO     	 * (global step 26250: loss: 0.2588483467698097, lr: 5e-05
2023-12-20 22:30:53 INFO     	 * (global step 26300: loss: 0.3953893333673477, lr: 5e-05
2023-12-20 22:31:01 INFO     	 * (global step 26350: loss: 0.23597852885723114, lr: 5e-05
2023-12-20 22:31:09 INFO     	 * (global step 26400: loss: 0.3733954578638077, lr: 5e-05
2023-12-20 22:31:17 INFO     	 * (global step 26450: loss: 0.24908583611249924, lr: 5e-05
2023-12-20 22:31:26 INFO     	 * (global step 26500: loss: 0.24333259463310242, lr: 5e-05
2023-12-20 22:31:34 INFO     	 * (global step 26550: loss: 0.2683679684996605, lr: 5e-05
2023-12-20 22:31:42 INFO     	 * (global step 26600: loss: 0.21720949560403824, lr: 5e-05
2023-12-20 22:31:50 INFO     	 * (global step 26650: loss: 0.34759020805358887, lr: 5e-05
2023-12-20 22:31:58 INFO     	 * (global step 26700: loss: 0.14831805229187012, lr: 5e-05
2023-12-20 22:32:06 INFO     	 * (global step 26750: loss: 0.23315611481666565, lr: 5e-05
2023-12-20 22:32:14 INFO     	 * (global step 26800: loss: 0.3128894120454788, lr: 5e-05
2023-12-20 22:32:22 INFO     	 * (global step 26850: loss: 0.20770253241062164, lr: 5e-05
2023-12-20 22:32:30 INFO     	 * (global step 26900: loss: 0.22800388932228088, lr: 5e-05
2023-12-20 22:32:38 INFO     	 * (global step 26950: loss: 0.3108856678009033, lr: 5e-05
2023-12-20 22:32:46 INFO     	 * (global step 27000: loss: 0.30627377331256866, lr: 5e-05
2023-12-20 22:32:54 INFO     	 * (global step 27050: loss: 0.19394470751285553, lr: 5e-05
2023-12-20 22:33:02 INFO     	 * (global step 27100: loss: 0.29520606994628906, lr: 5e-05
2023-12-20 22:33:10 INFO     	 * (global step 27150: loss: 0.32473331689834595, lr: 5e-05
2023-12-20 22:33:19 INFO     	 * (global step 27200: loss: 0.47534988820552826, lr: 5e-05
2023-12-20 22:33:27 INFO     	 * (global step 27250: loss: 0.2478727474808693, lr: 5e-05
2023-12-20 22:33:35 INFO     	 * (global step 27300: loss: 0.2971480190753937, lr: 5e-05
2023-12-20 22:33:43 INFO     	 * (global step 27350: loss: 0.2012699767947197, lr: 5e-05
2023-12-20 22:33:51 INFO     	 * (global step 27400: loss: 0.2641151398420334, lr: 5e-05
2023-12-20 22:33:59 INFO     	 * (global step 27450: loss: 0.3316836953163147, lr: 5e-05
2023-12-20 22:34:07 INFO     	 * (global step 27500: loss: 0.3096926212310791, lr: 5e-05
2023-12-20 22:34:15 INFO     	 * (global step 27550: loss: 0.4079368785023689, lr: 5e-05
2023-12-20 22:34:23 INFO     	 * (global step 27600: loss: 0.27892065048217773, lr: 5e-05
2023-12-20 22:34:31 INFO     	 * (global step 27650: loss: 0.2853919714689255, lr: 5e-05
2023-12-20 22:34:39 INFO     	 * (global step 27700: loss: 0.3100712150335312, lr: 5e-05
2023-12-20 22:34:47 INFO     	 * (global step 27750: loss: 0.23786155879497528, lr: 5e-05
2023-12-20 22:34:55 INFO     	 * (global step 27800: loss: 0.28048789501190186, lr: 5e-05
2023-12-20 22:35:03 INFO     	 * (global step 27850: loss: 0.3644624501466751, lr: 5e-05
2023-12-20 22:35:11 INFO     	 * (global step 27900: loss: 0.28361377865076065, lr: 5e-05
2023-12-20 22:35:19 INFO     	 * (global step 27950: loss: 0.42466312646865845, lr: 5e-05
2023-12-20 22:35:27 INFO     	 * (global step 28000: loss: 0.32043588161468506, lr: 5e-05
2023-12-20 22:35:35 INFO     	 * (global step 28050: loss: 0.27932044118642807, lr: 5e-05
2023-12-20 22:35:44 INFO     	 * (global step 28100: loss: 0.39892612397670746, lr: 5e-05
2023-12-20 22:35:52 INFO     	 * (global step 28150: loss: 0.34994275867938995, lr: 5e-05
2023-12-20 22:36:00 INFO     	 * (global step 28200: loss: 0.5178214460611343, lr: 5e-05
2023-12-20 22:36:08 INFO     	 * (global step 28250: loss: 0.20280978828668594, lr: 5e-05
2023-12-20 22:36:16 INFO     	 * (global step 28300: loss: 0.13987202942371368, lr: 5e-05
2023-12-20 22:36:22 INFO     [epoch 5/15] average loss: 0.29, lr: 5e-05
2023-12-20 22:36:22 INFO     saving model related files
2023-12-20 22:36:22 INFO     saving model
2023-12-20 22:36:23 INFO     saving tokenizer
2023-12-20 22:36:23 INFO     saving optimizer
2023-12-20 22:36:24 INFO     remove old optimizer files
2023-12-20 22:36:25 INFO     	 * (global step 28350: loss: 0.3357364982366562, lr: 5e-05
2023-12-20 22:36:34 INFO     	 * (global step 28400: loss: 0.3074800670146942, lr: 5e-05
2023-12-20 22:36:42 INFO     	 * (global step 28450: loss: 0.15352610498666763, lr: 5e-05
2023-12-20 22:36:50 INFO     	 * (global step 28500: loss: 0.3241972327232361, lr: 5e-05
2023-12-20 22:36:58 INFO     	 * (global step 28550: loss: 0.1619800664484501, lr: 5e-05
2023-12-20 22:37:06 INFO     	 * (global step 28600: loss: 0.24140580743551254, lr: 5e-05
2023-12-20 22:37:14 INFO     	 * (global step 28650: loss: 0.33991312980651855, lr: 5e-05
2023-12-20 22:37:22 INFO     	 * (global step 28700: loss: 0.2389753982424736, lr: 5e-05
2023-12-20 22:37:30 INFO     	 * (global step 28750: loss: 0.19570717215538025, lr: 5e-05
2023-12-20 22:37:38 INFO     	 * (global step 28800: loss: 0.20045918226242065, lr: 5e-05
2023-12-20 22:37:46 INFO     	 * (global step 28850: loss: 0.28114695847034454, lr: 5e-05
2023-12-20 22:37:54 INFO     	 * (global step 28900: loss: 0.3681456819176674, lr: 5e-05
2023-12-20 22:38:02 INFO     	 * (global step 28950: loss: 0.18403396755456924, lr: 5e-05
2023-12-20 22:38:10 INFO     	 * (global step 29000: loss: 0.23958592861890793, lr: 5e-05
2023-12-20 22:38:18 INFO     	 * (global step 29050: loss: 0.20433665066957474, lr: 5e-05
2023-12-20 22:38:26 INFO     	 * (global step 29100: loss: 0.2830384597182274, lr: 5e-05
2023-12-20 22:38:35 INFO     	 * (global step 29150: loss: 0.3218575716018677, lr: 5e-05
2023-12-20 22:38:43 INFO     	 * (global step 29200: loss: 0.38648417592048645, lr: 5e-05
2023-12-20 22:38:51 INFO     	 * (global step 29250: loss: 0.24067136645317078, lr: 5e-05
2023-12-20 22:38:59 INFO     	 * (global step 29300: loss: 0.2200831025838852, lr: 5e-05
2023-12-20 22:39:07 INFO     	 * (global step 29350: loss: 0.29121095687150955, lr: 5e-05
2023-12-20 22:39:15 INFO     	 * (global step 29400: loss: 0.19592762365937233, lr: 5e-05
2023-12-20 22:39:23 INFO     	 * (global step 29450: loss: 0.3037615418434143, lr: 5e-05
2023-12-20 22:39:31 INFO     	 * (global step 29500: loss: 0.21899964660406113, lr: 5e-05
2023-12-20 22:39:39 INFO     	 * (global step 29550: loss: 0.2468509003520012, lr: 5e-05
2023-12-20 22:39:48 INFO     	 * (global step 29600: loss: 0.30645062029361725, lr: 5e-05
2023-12-20 22:39:56 INFO     	 * (global step 29650: loss: 0.22326256334781647, lr: 5e-05
2023-12-20 22:40:04 INFO     	 * (global step 29700: loss: 0.24585331231355667, lr: 5e-05
2023-12-20 22:40:12 INFO     	 * (global step 29750: loss: 0.24059686809778214, lr: 5e-05
2023-12-20 22:40:20 INFO     	 * (global step 29800: loss: 0.594583585858345, lr: 5e-05
2023-12-20 22:40:28 INFO     	 * (global step 29850: loss: 0.21884401887655258, lr: 5e-05
2023-12-20 22:40:36 INFO     	 * (global step 29900: loss: 0.3505762070417404, lr: 5e-05
2023-12-20 22:40:44 INFO     	 * (global step 29950: loss: 0.25682416558265686, lr: 5e-05
2023-12-20 22:40:53 INFO     	 * (global step 30000: loss: 0.15447862446308136, lr: 5e-05
2023-12-20 22:41:01 INFO     	 * (global step 30050: loss: 0.2045598328113556, lr: 5e-05
2023-12-20 22:41:09 INFO     	 * (global step 30100: loss: 0.2724083960056305, lr: 5e-05
2023-12-20 22:41:17 INFO     	 * (global step 30150: loss: 0.2933104708790779, lr: 5e-05
2023-12-20 22:41:25 INFO     	 * (global step 30200: loss: 0.18515323102474213, lr: 5e-05
2023-12-20 22:41:33 INFO     	 * (global step 30250: loss: 0.2906264066696167, lr: 5e-05
2023-12-20 22:41:41 INFO     	 * (global step 30300: loss: 0.2020515576004982, lr: 5e-05
2023-12-20 22:41:49 INFO     	 * (global step 30350: loss: 0.2531363219022751, lr: 5e-05
2023-12-20 22:41:57 INFO     	 * (global step 30400: loss: 0.2502291649580002, lr: 5e-05
2023-12-20 22:42:05 INFO     	 * (global step 30450: loss: 0.3060927391052246, lr: 5e-05
2023-12-20 22:42:13 INFO     	 * (global step 30500: loss: 0.19914435595273972, lr: 5e-05
2023-12-20 22:42:21 INFO     	 * (global step 30550: loss: 0.24275552481412888, lr: 5e-05
2023-12-20 22:42:30 INFO     	 * (global step 30600: loss: 0.49489256739616394, lr: 5e-05
2023-12-20 22:42:38 INFO     	 * (global step 30650: loss: 0.22702927887439728, lr: 5e-05
2023-12-20 22:42:46 INFO     	 * (global step 30700: loss: 0.27675020694732666, lr: 5e-05
2023-12-20 22:42:54 INFO     	 * (global step 30750: loss: 0.25888966768980026, lr: 5e-05
2023-12-20 22:43:02 INFO     	 * (global step 30800: loss: 0.17287705093622208, lr: 5e-05
2023-12-20 22:43:10 INFO     	 * (global step 30850: loss: 0.21317686885595322, lr: 5e-05
2023-12-20 22:43:18 INFO     	 * (global step 30900: loss: 0.25792211294174194, lr: 5e-05
2023-12-20 22:43:26 INFO     	 * (global step 30950: loss: 0.35911373794078827, lr: 5e-05
2023-12-20 22:43:34 INFO     	 * (global step 31000: loss: 0.2811655253171921, lr: 5e-05
2023-12-20 22:43:42 INFO     	 * (global step 31050: loss: 0.26644808799028397, lr: 5e-05
2023-12-20 22:43:50 INFO     	 * (global step 31100: loss: 0.25400645285844803, lr: 5e-05
2023-12-20 22:43:58 INFO     	 * (global step 31150: loss: 0.22961967438459396, lr: 5e-05
2023-12-20 22:44:06 INFO     	 * (global step 31200: loss: 0.3216850608587265, lr: 5e-05
2023-12-20 22:44:15 INFO     	 * (global step 31250: loss: 0.2580881640315056, lr: 5e-05
2023-12-20 22:44:23 INFO     	 * (global step 31300: loss: 0.23552357405424118, lr: 5e-05
2023-12-20 22:44:31 INFO     	 * (global step 31350: loss: 0.3509786128997803, lr: 5e-05
2023-12-20 22:44:39 INFO     	 * (global step 31400: loss: 0.2606803700327873, lr: 5e-05
2023-12-20 22:44:47 INFO     	 * (global step 31450: loss: 0.2829468697309494, lr: 5e-05
2023-12-20 22:44:55 INFO     	 * (global step 31500: loss: 0.47592297196388245, lr: 5e-05
2023-12-20 22:45:03 INFO     	 * (global step 31550: loss: 0.22702185064554214, lr: 5e-05
2023-12-20 22:45:11 INFO     	 * (global step 31600: loss: 0.24880264699459076, lr: 5e-05
2023-12-20 22:45:19 INFO     	 * (global step 31650: loss: 0.18852171301841736, lr: 5e-05
2023-12-20 22:45:27 INFO     	 * (global step 31700: loss: 0.29660169780254364, lr: 5e-05
2023-12-20 22:45:36 INFO     	 * (global step 31750: loss: 0.41227492690086365, lr: 5e-05
2023-12-20 22:45:44 INFO     	 * (global step 31800: loss: 0.2047853320837021, lr: 5e-05
2023-12-20 22:45:52 INFO     	 * (global step 31850: loss: 0.2707644924521446, lr: 5e-05
2023-12-20 22:46:00 INFO     	 * (global step 31900: loss: 0.276231087744236, lr: 5e-05
2023-12-20 22:46:08 INFO     	 * (global step 31950: loss: 0.23050692677497864, lr: 5e-05
2023-12-20 22:46:16 INFO     	 * (global step 32000: loss: 0.2234150916337967, lr: 5e-05
2023-12-20 22:46:24 INFO     	 * (global step 32050: loss: 0.32379597425460815, lr: 5e-05
2023-12-20 22:46:32 INFO     	 * (global step 32100: loss: 0.3062075972557068, lr: 5e-05
2023-12-20 22:46:40 INFO     	 * (global step 32150: loss: 0.3199795335531235, lr: 5e-05
2023-12-20 22:46:48 INFO     	 * (global step 32200: loss: 0.29577386379241943, lr: 5e-05
2023-12-20 22:46:56 INFO     	 * (global step 32250: loss: 0.19539984315633774, lr: 5e-05
2023-12-20 22:47:04 INFO     	 * (global step 32300: loss: 0.2748925983905792, lr: 5e-05
2023-12-20 22:47:12 INFO     	 * (global step 32350: loss: 0.255406029522419, lr: 5e-05
2023-12-20 22:47:21 INFO     	 * (global step 32400: loss: 0.1895710676908493, lr: 5e-05
2023-12-20 22:47:29 INFO     	 * (global step 32450: loss: 0.23508809506893158, lr: 5e-05
2023-12-20 22:47:37 INFO     	 * (global step 32500: loss: 0.300193727016449, lr: 5e-05
2023-12-20 22:47:45 INFO     	 * (global step 32550: loss: 0.2922786772251129, lr: 5e-05
2023-12-20 22:47:53 INFO     	 * (global step 32600: loss: 0.15579108893871307, lr: 5e-05
2023-12-20 22:48:01 INFO     	 * (global step 32650: loss: 0.4420248121023178, lr: 5e-05
2023-12-20 22:48:09 INFO     	 * (global step 32700: loss: 0.2416571080684662, lr: 5e-05
2023-12-20 22:48:17 INFO     	 * (global step 32750: loss: 0.3293914645910263, lr: 5e-05
2023-12-20 22:48:25 INFO     	 * (global step 32800: loss: 0.19496124237775803, lr: 5e-05
2023-12-20 22:48:33 INFO     	 * (global step 32850: loss: 0.21229024231433868, lr: 5e-05
2023-12-20 22:48:42 INFO     	 * (global step 32900: loss: 0.3907923698425293, lr: 5e-05
2023-12-20 22:48:50 INFO     	 * (global step 32950: loss: 0.37960806488990784, lr: 5e-05
2023-12-20 22:48:58 INFO     	 * (global step 33000: loss: 0.23017684370279312, lr: 5e-05
2023-12-20 22:49:06 INFO     	 * (global step 33050: loss: 0.25216028839349747, lr: 5e-05
2023-12-20 22:49:08 INFO     [epoch 6/15] average loss: 0.283, lr: 5e-05
2023-12-20 22:49:08 INFO     saving model related files
2023-12-20 22:49:08 INFO     saving model
2023-12-20 22:49:08 INFO     saving tokenizer
2023-12-20 22:49:08 INFO     saving optimizer
2023-12-20 22:49:09 INFO     remove old optimizer files
2023-12-20 22:49:15 INFO     	 * (global step 33100: loss: 0.27224985510110855, lr: 5e-05
2023-12-20 22:49:24 INFO     	 * (global step 33150: loss: 0.3157385438680649, lr: 5e-05
2023-12-20 22:49:32 INFO     	 * (global step 33200: loss: 0.33200204372406006, lr: 5e-05
2023-12-20 22:49:40 INFO     	 * (global step 33250: loss: 0.27261535078287125, lr: 5e-05
2023-12-20 22:49:48 INFO     	 * (global step 33300: loss: 0.24052827060222626, lr: 5e-05
2023-12-20 22:49:56 INFO     	 * (global step 33350: loss: 0.27798043191432953, lr: 5e-05
2023-12-20 22:50:04 INFO     	 * (global step 33400: loss: 0.22040680795907974, lr: 5e-05
2023-12-20 22:50:12 INFO     	 * (global step 33450: loss: 0.2691153287887573, lr: 5e-05
2023-12-20 22:50:20 INFO     	 * (global step 33500: loss: 0.21790777146816254, lr: 5e-05
2023-12-20 22:50:28 INFO     	 * (global step 33550: loss: 0.2994735985994339, lr: 5e-05
2023-12-20 22:50:36 INFO     	 * (global step 33600: loss: 0.26279987394809723, lr: 5e-05
2023-12-20 22:50:44 INFO     	 * (global step 33650: loss: 0.34722092747688293, lr: 5e-05
2023-12-20 22:50:52 INFO     	 * (global step 33700: loss: 0.20396195352077484, lr: 5e-05
2023-12-20 22:51:00 INFO     	 * (global step 33750: loss: 0.3883122205734253, lr: 5e-05
2023-12-20 22:51:08 INFO     	 * (global step 33800: loss: 0.33996888250112534, lr: 5e-05
2023-12-20 22:51:17 INFO     	 * (global step 33850: loss: 0.29337430000305176, lr: 5e-05
2023-12-20 22:51:25 INFO     	 * (global step 33900: loss: 0.2836848795413971, lr: 5e-05
2023-12-20 22:51:33 INFO     	 * (global step 33950: loss: 0.27192696928977966, lr: 5e-05
2023-12-20 22:51:41 INFO     	 * (global step 34000: loss: 0.21241895109415054, lr: 5e-05
2023-12-20 22:51:49 INFO     	 * (global step 34050: loss: 0.31603485345840454, lr: 5e-05
2023-12-20 22:51:57 INFO     	 * (global step 34100: loss: 0.2625390812754631, lr: 5e-05
2023-12-20 22:52:05 INFO     	 * (global step 34150: loss: 0.457136869430542, lr: 5e-05
2023-12-20 22:52:13 INFO     	 * (global step 34200: loss: 0.34866924583911896, lr: 5e-05
2023-12-20 22:52:21 INFO     	 * (global step 34250: loss: 0.2717835381627083, lr: 5e-05
2023-12-20 22:52:29 INFO     	 * (global step 34300: loss: 0.23668208718299866, lr: 5e-05
2023-12-20 22:52:37 INFO     	 * (global step 34350: loss: 0.1667679250240326, lr: 5e-05
2023-12-20 22:52:45 INFO     	 * (global step 34400: loss: 0.23874682188034058, lr: 5e-05
2023-12-20 22:52:53 INFO     	 * (global step 34450: loss: 0.3244824558496475, lr: 5e-05
2023-12-20 22:53:01 INFO     	 * (global step 34500: loss: 0.266684427857399, lr: 5e-05
2023-12-20 22:53:09 INFO     	 * (global step 34550: loss: 0.2765527069568634, lr: 5e-05
2023-12-20 22:53:18 INFO     	 * (global step 34600: loss: 0.2139074131846428, lr: 5e-05
2023-12-20 22:53:26 INFO     	 * (global step 34650: loss: 0.22268462926149368, lr: 5e-05
2023-12-20 22:53:34 INFO     	 * (global step 34700: loss: 0.33643317222595215, lr: 5e-05
2023-12-20 22:53:42 INFO     	 * (global step 34750: loss: 0.23899934440851212, lr: 5e-05
2023-12-20 22:53:50 INFO     	 * (global step 34800: loss: 0.19518163055181503, lr: 5e-05
2023-12-20 22:53:58 INFO     	 * (global step 34850: loss: 0.2455773800611496, lr: 5e-05
2023-12-20 22:54:06 INFO     	 * (global step 34900: loss: 0.20151016488671303, lr: 5e-05
2023-12-20 22:54:14 INFO     	 * (global step 34950: loss: 0.2795923799276352, lr: 5e-05
2023-12-20 22:54:22 INFO     	 * (global step 35000: loss: 0.2801974415779114, lr: 5e-05
2023-12-20 22:54:30 INFO     	 * (global step 35050: loss: 0.4108463227748871, lr: 5e-05
2023-12-20 22:54:38 INFO     	 * (global step 35100: loss: 0.34773850440979004, lr: 5e-05
2023-12-20 22:54:46 INFO     	 * (global step 35150: loss: 0.3125127777457237, lr: 5e-05
2023-12-20 22:54:54 INFO     	 * (global step 35200: loss: 0.28264375776052475, lr: 5e-05
2023-12-20 22:55:02 INFO     	 * (global step 35250: loss: 0.2950795590877533, lr: 5e-05
2023-12-20 22:55:10 INFO     	 * (global step 35300: loss: 0.3387882560491562, lr: 5e-05
2023-12-20 22:55:18 INFO     	 * (global step 35350: loss: 0.21815300732851028, lr: 5e-05
2023-12-20 22:55:26 INFO     	 * (global step 35400: loss: 0.19980748742818832, lr: 5e-05
2023-12-20 22:55:35 INFO     	 * (global step 35450: loss: 0.35942013561725616, lr: 5e-05
2023-12-20 22:55:43 INFO     	 * (global step 35500: loss: 0.2989668771624565, lr: 5e-05
2023-12-20 22:55:51 INFO     	 * (global step 35550: loss: 0.17189061641693115, lr: 5e-05
2023-12-20 22:55:59 INFO     	 * (global step 35600: loss: 0.27973607182502747, lr: 5e-05
2023-12-20 22:56:07 INFO     	 * (global step 35650: loss: 0.306859515607357, lr: 5e-05
2023-12-20 22:56:15 INFO     	 * (global step 35700: loss: 0.34104397892951965, lr: 5e-05
2023-12-20 22:56:23 INFO     	 * (global step 35750: loss: 0.22688128799200058, lr: 5e-05
2023-12-20 22:56:31 INFO     	 * (global step 35800: loss: 0.2202375829219818, lr: 5e-05
2023-12-20 22:56:39 INFO     	 * (global step 35850: loss: 0.23709652572870255, lr: 5e-05
2023-12-20 22:56:47 INFO     	 * (global step 35900: loss: 0.2669328898191452, lr: 5e-05
2023-12-20 22:56:55 INFO     	 * (global step 35950: loss: 0.40066827833652496, lr: 5e-05
2023-12-20 22:57:03 INFO     	 * (global step 36000: loss: 0.220094233751297, lr: 5e-05
2023-12-20 22:57:11 INFO     	 * (global step 36050: loss: 0.09550698474049568, lr: 5e-05
2023-12-20 22:57:19 INFO     	 * (global step 36100: loss: 0.3246908560395241, lr: 5e-05
2023-12-20 22:57:27 INFO     	 * (global step 36150: loss: 0.4681621640920639, lr: 5e-05
2023-12-20 22:57:36 INFO     	 * (global step 36200: loss: 0.33516621589660645, lr: 5e-05
2023-12-20 22:57:44 INFO     	 * (global step 36250: loss: 0.230165496468544, lr: 5e-05
2023-12-20 22:57:52 INFO     	 * (global step 36300: loss: 0.2631591707468033, lr: 5e-05
2023-12-20 22:58:00 INFO     	 * (global step 36350: loss: 0.35574381053447723, lr: 5e-05
2023-12-20 22:58:08 INFO     	 * (global step 36400: loss: 0.23837514966726303, lr: 5e-05
2023-12-20 22:58:16 INFO     	 * (global step 36450: loss: 0.2229071781039238, lr: 5e-05
2023-12-20 22:58:24 INFO     	 * (global step 36500: loss: 0.40763549506664276, lr: 5e-05
2023-12-20 22:58:32 INFO     	 * (global step 36550: loss: 0.1982235237956047, lr: 5e-05
2023-12-20 22:58:40 INFO     	 * (global step 36600: loss: 0.47095195949077606, lr: 5e-05
2023-12-20 22:58:48 INFO     	 * (global step 36650: loss: 0.27315662801265717, lr: 5e-05
2023-12-20 22:58:56 INFO     	 * (global step 36700: loss: 0.24301284551620483, lr: 5e-05
2023-12-20 22:59:04 INFO     	 * (global step 36750: loss: 0.3720846623182297, lr: 5e-05
2023-12-20 22:59:12 INFO     	 * (global step 36800: loss: 0.2165898010134697, lr: 5e-05
2023-12-20 22:59:20 INFO     	 * (global step 36850: loss: 0.20600979775190353, lr: 5e-05
2023-12-20 22:59:28 INFO     	 * (global step 36900: loss: 0.31603895127773285, lr: 5e-05
2023-12-20 22:59:36 INFO     	 * (global step 36950: loss: 0.29211433976888657, lr: 5e-05
2023-12-20 22:59:44 INFO     	 * (global step 37000: loss: 0.2971889525651932, lr: 5e-05
2023-12-20 22:59:52 INFO     	 * (global step 37050: loss: 0.2084396779537201, lr: 5e-05
2023-12-20 23:00:00 INFO     	 * (global step 37100: loss: 0.29242053627967834, lr: 5e-05
2023-12-20 23:00:08 INFO     	 * (global step 37150: loss: 0.21413534134626389, lr: 5e-05
2023-12-20 23:00:17 INFO     	 * (global step 37200: loss: 0.4758417159318924, lr: 5e-05
2023-12-20 23:00:25 INFO     	 * (global step 37250: loss: 0.2148178443312645, lr: 5e-05
2023-12-20 23:00:33 INFO     	 * (global step 37300: loss: 0.2820326238870621, lr: 5e-05
2023-12-20 23:00:41 INFO     	 * (global step 37350: loss: 0.13095609098672867, lr: 5e-05
2023-12-20 23:00:49 INFO     	 * (global step 37400: loss: 0.24114256352186203, lr: 5e-05
2023-12-20 23:00:57 INFO     	 * (global step 37450: loss: 0.33252838253974915, lr: 5e-05
2023-12-20 23:01:05 INFO     	 * (global step 37500: loss: 0.2729802057147026, lr: 5e-05
2023-12-20 23:01:13 INFO     	 * (global step 37550: loss: 0.21782894432544708, lr: 5e-05
2023-12-20 23:01:21 INFO     	 * (global step 37600: loss: 0.2248334139585495, lr: 5e-05
2023-12-20 23:01:29 INFO     	 * (global step 37650: loss: 0.14999015256762505, lr: 5e-05
2023-12-20 23:01:37 INFO     	 * (global step 37700: loss: 0.25623486936092377, lr: 5e-05
2023-12-20 23:01:45 INFO     	 * (global step 37750: loss: 0.1770002245903015, lr: 5e-05
2023-12-20 23:01:51 INFO     [epoch 7/15] average loss: 0.278, lr: 5e-05
2023-12-20 23:01:51 INFO     saving model related files
2023-12-20 23:01:51 INFO     saving model
2023-12-20 23:01:51 INFO     saving tokenizer
2023-12-20 23:01:51 INFO     saving optimizer
2023-12-20 23:01:52 INFO     remove old optimizer files
2023-12-20 23:01:55 INFO     	 * (global step 37800: loss: 0.37265871465206146, lr: 5e-05
2023-12-20 23:02:03 INFO     	 * (global step 37850: loss: 0.24785198271274567, lr: 5e-05
2023-12-20 23:02:11 INFO     	 * (global step 37900: loss: 0.26822812855243683, lr: 5e-05
2023-12-20 23:02:19 INFO     	 * (global step 37950: loss: 0.2763657793402672, lr: 5e-05
2023-12-20 23:02:27 INFO     	 * (global step 38000: loss: 0.3150288462638855, lr: 5e-05
2023-12-20 23:02:35 INFO     	 * (global step 38050: loss: 0.2545376941561699, lr: 5e-05
2023-12-20 23:02:43 INFO     	 * (global step 38100: loss: 0.28010840713977814, lr: 5e-05
2023-12-20 23:02:51 INFO     	 * (global step 38150: loss: 0.2664985731244087, lr: 5e-05
2023-12-20 23:02:59 INFO     	 * (global step 38200: loss: 0.3619121015071869, lr: 5e-05
2023-12-20 23:03:07 INFO     	 * (global step 38250: loss: 0.34677914530038834, lr: 5e-05
2023-12-20 23:03:16 INFO     	 * (global step 38300: loss: 0.18834898620843887, lr: 5e-05
2023-12-20 23:03:24 INFO     	 * (global step 38350: loss: 0.3055870831012726, lr: 5e-05
2023-12-20 23:03:32 INFO     	 * (global step 38400: loss: 0.3189954161643982, lr: 5e-05
2023-12-20 23:03:40 INFO     	 * (global step 38450: loss: 0.32445788383483887, lr: 5e-05
2023-12-20 23:03:48 INFO     	 * (global step 38500: loss: 0.23127788305282593, lr: 5e-05
2023-12-20 23:03:56 INFO     	 * (global step 38550: loss: 0.29702936112880707, lr: 5e-05
2023-12-20 23:04:04 INFO     	 * (global step 38600: loss: 0.3293659836053848, lr: 5e-05
2023-12-20 23:04:12 INFO     	 * (global step 38650: loss: 0.13852204382419586, lr: 5e-05
2023-12-20 23:04:20 INFO     	 * (global step 38700: loss: 0.23883755505084991, lr: 5e-05
2023-12-20 23:04:29 INFO     	 * (global step 38750: loss: 0.27492713183164597, lr: 5e-05
2023-12-20 23:04:37 INFO     	 * (global step 38800: loss: 0.19961056113243103, lr: 5e-05
2023-12-20 23:04:45 INFO     	 * (global step 38850: loss: 0.4086778163909912, lr: 5e-05
2023-12-20 23:04:53 INFO     	 * (global step 38900: loss: 0.21978801488876343, lr: 5e-05
2023-12-20 23:05:01 INFO     	 * (global step 38950: loss: 0.23288771510124207, lr: 5e-05
2023-12-20 23:05:09 INFO     	 * (global step 39000: loss: 0.25839127600193024, lr: 5e-05
2023-12-20 23:05:18 INFO     	 * (global step 39050: loss: 0.28762392699718475, lr: 5e-05
2023-12-20 23:05:26 INFO     	 * (global step 39100: loss: 0.21956279873847961, lr: 5e-05
2023-12-20 23:05:34 INFO     	 * (global step 39150: loss: 0.1879625841975212, lr: 5e-05
2023-12-20 23:05:42 INFO     	 * (global step 39200: loss: 0.28791824728250504, lr: 5e-05
2023-12-20 23:05:51 INFO     	 * (global step 39250: loss: 0.23592150956392288, lr: 5e-05
2023-12-20 23:05:59 INFO     	 * (global step 39300: loss: 0.18654778599739075, lr: 5e-05
2023-12-20 23:06:07 INFO     	 * (global step 39350: loss: 0.22345535457134247, lr: 5e-05
2023-12-20 23:06:15 INFO     	 * (global step 39400: loss: 0.1934785321354866, lr: 5e-05
2023-12-20 23:06:23 INFO     	 * (global step 39450: loss: 0.2183009535074234, lr: 5e-05
2023-12-20 23:06:31 INFO     	 * (global step 39500: loss: 0.21809807419776917, lr: 5e-05
2023-12-20 23:06:39 INFO     	 * (global step 39550: loss: 0.20884600281715393, lr: 5e-05
2023-12-20 23:06:47 INFO     	 * (global step 39600: loss: 0.2734993100166321, lr: 5e-05
2023-12-20 23:06:55 INFO     	 * (global step 39650: loss: 0.20837783813476562, lr: 5e-05
2023-12-20 23:07:04 INFO     	 * (global step 39700: loss: 0.21142909675836563, lr: 5e-05
2023-12-20 23:07:12 INFO     	 * (global step 39750: loss: 0.31831102073192596, lr: 5e-05
2023-12-20 23:07:20 INFO     	 * (global step 39800: loss: 0.36451028287410736, lr: 5e-05
2023-12-20 23:07:28 INFO     	 * (global step 39850: loss: 0.3521149605512619, lr: 5e-05
2023-12-20 23:07:36 INFO     	 * (global step 39900: loss: 0.347558930516243, lr: 5e-05
2023-12-20 23:07:44 INFO     	 * (global step 39950: loss: 0.3467864841222763, lr: 5e-05
2023-12-20 23:07:52 INFO     	 * (global step 40000: loss: 0.20708756521344185, lr: 5e-05
2023-12-20 23:08:00 INFO     	 * (global step 40050: loss: 0.24571681022644043, lr: 5e-05
2023-12-20 23:08:09 INFO     	 * (global step 40100: loss: 0.2700878158211708, lr: 5e-05
2023-12-20 23:08:17 INFO     	 * (global step 40150: loss: 0.22764235734939575, lr: 5e-05
2023-12-20 23:08:25 INFO     	 * (global step 40200: loss: 0.2875373139977455, lr: 5e-05
2023-12-20 23:08:33 INFO     	 * (global step 40250: loss: 0.32880882918834686, lr: 5e-05
2023-12-20 23:08:41 INFO     	 * (global step 40300: loss: 0.2620897963643074, lr: 5e-05
2023-12-20 23:08:49 INFO     	 * (global step 40350: loss: 0.25564437359571457, lr: 5e-05
2023-12-20 23:08:57 INFO     	 * (global step 40400: loss: 0.2236352562904358, lr: 5e-05
2023-12-20 23:09:05 INFO     	 * (global step 40450: loss: 0.20439547300338745, lr: 5e-05
2023-12-20 23:09:13 INFO     	 * (global step 40500: loss: 0.180356964468956, lr: 5e-05
2023-12-20 23:09:22 INFO     	 * (global step 40550: loss: 0.23041356354951859, lr: 5e-05
2023-12-20 23:09:30 INFO     	 * (global step 40600: loss: 0.19173576682806015, lr: 5e-05
2023-12-20 23:09:38 INFO     	 * (global step 40650: loss: 0.297889769077301, lr: 5e-05
2023-12-20 23:09:46 INFO     	 * (global step 40700: loss: 0.3700694441795349, lr: 5e-05
2023-12-20 23:09:54 INFO     	 * (global step 40750: loss: 0.24317672103643417, lr: 5e-05
2023-12-20 23:10:02 INFO     	 * (global step 40800: loss: 0.33334773778915405, lr: 5e-05
2023-12-20 23:10:10 INFO     	 * (global step 40850: loss: 0.2836628034710884, lr: 5e-05
2023-12-20 23:10:18 INFO     	 * (global step 40900: loss: 0.28047211468219757, lr: 5e-05
2023-12-20 23:10:27 INFO     	 * (global step 40950: loss: 0.2076839581131935, lr: 5e-05
2023-12-20 23:10:35 INFO     	 * (global step 41000: loss: 0.36034512519836426, lr: 5e-05
2023-12-20 23:10:43 INFO     	 * (global step 41050: loss: 0.24021068960428238, lr: 5e-05
2023-12-20 23:10:51 INFO     	 * (global step 41100: loss: 0.24099906533956528, lr: 5e-05
2023-12-20 23:10:59 INFO     	 * (global step 41150: loss: 0.25925251096487045, lr: 5e-05
2023-12-20 23:11:07 INFO     	 * (global step 41200: loss: 0.24519473314285278, lr: 5e-05
2023-12-20 23:11:15 INFO     	 * (global step 41250: loss: 0.1787981539964676, lr: 5e-05
2023-12-20 23:11:23 INFO     	 * (global step 41300: loss: 0.33483826369047165, lr: 5e-05
2023-12-20 23:11:31 INFO     	 * (global step 41350: loss: 0.21710151433944702, lr: 5e-05
2023-12-20 23:11:40 INFO     	 * (global step 41400: loss: 0.17905006557703018, lr: 5e-05
2023-12-20 23:11:48 INFO     	 * (global step 41450: loss: 0.3520675003528595, lr: 5e-05
2023-12-20 23:11:56 INFO     	 * (global step 41500: loss: 0.24972382932901382, lr: 5e-05
2023-12-20 23:12:04 INFO     	 * (global step 41550: loss: 0.2866051644086838, lr: 5e-05
2023-12-20 23:12:12 INFO     	 * (global step 41600: loss: 0.3546021431684494, lr: 5e-05
2023-12-20 23:12:20 INFO     	 * (global step 41650: loss: 0.2525147721171379, lr: 5e-05
2023-12-20 23:12:28 INFO     	 * (global step 41700: loss: 0.33203358948230743, lr: 5e-05
2023-12-20 23:12:36 INFO     	 * (global step 41750: loss: 0.17023301124572754, lr: 5e-05
2023-12-20 23:12:44 INFO     	 * (global step 41800: loss: 0.2846129685640335, lr: 5e-05
2023-12-20 23:12:53 INFO     	 * (global step 41850: loss: 0.3062099665403366, lr: 5e-05
2023-12-20 23:13:01 INFO     	 * (global step 41900: loss: 0.3117362931370735, lr: 5e-05
2023-12-20 23:13:09 INFO     	 * (global step 41950: loss: 0.2874869704246521, lr: 5e-05
2023-12-20 23:13:17 INFO     	 * (global step 42000: loss: 0.18367163091897964, lr: 5e-05
2023-12-20 23:13:25 INFO     	 * (global step 42050: loss: 0.1933150514960289, lr: 5e-05
2023-12-20 23:13:33 INFO     	 * (global step 42100: loss: 0.3006742149591446, lr: 5e-05
2023-12-20 23:13:41 INFO     	 * (global step 42150: loss: 0.19352106750011444, lr: 5e-05
2023-12-20 23:13:49 INFO     	 * (global step 42200: loss: 0.2591584771871567, lr: 5e-05
2023-12-20 23:13:57 INFO     	 * (global step 42250: loss: 0.29011809825897217, lr: 5e-05
2023-12-20 23:14:06 INFO     	 * (global step 42300: loss: 0.2579430788755417, lr: 5e-05
2023-12-20 23:14:14 INFO     	 * (global step 42350: loss: 0.17522260546684265, lr: 5e-05
2023-12-20 23:14:22 INFO     	 * (global step 42400: loss: 0.29783104360103607, lr: 5e-05
2023-12-20 23:14:30 INFO     	 * (global step 42450: loss: 0.38161860406398773, lr: 5e-05
2023-12-20 23:14:38 INFO     	 * (global step 42500: loss: 0.2204945608973503, lr: 5e-05
2023-12-20 23:14:39 INFO     [epoch 8/15] average loss: 0.273, lr: 5e-05
2023-12-20 23:14:39 INFO     saving model related files
2023-12-20 23:14:39 INFO     saving model
2023-12-20 23:14:40 INFO     saving tokenizer
2023-12-20 23:14:40 INFO     saving optimizer
2023-12-20 23:14:41 INFO     remove old optimizer files
2023-12-20 23:14:48 INFO     	 * (global step 42550: loss: 0.28574275970458984, lr: 5e-05
2023-12-20 23:14:56 INFO     	 * (global step 42600: loss: 0.290225014090538, lr: 5e-05
2023-12-20 23:15:04 INFO     	 * (global step 42650: loss: 0.32706907391548157, lr: 5e-05
2023-12-20 23:15:12 INFO     	 * (global step 42700: loss: 0.18667814135551453, lr: 5e-05
2023-12-20 23:15:20 INFO     	 * (global step 42750: loss: 0.27541884779930115, lr: 5e-05
2023-12-20 23:15:28 INFO     	 * (global step 42800: loss: 0.49486295878887177, lr: 5e-05
2023-12-20 23:15:36 INFO     	 * (global step 42850: loss: 0.21878036856651306, lr: 5e-05
2023-12-20 23:15:45 INFO     	 * (global step 42900: loss: 0.28671345114707947, lr: 5e-05
2023-12-20 23:15:53 INFO     	 * (global step 42950: loss: 0.28452441841363907, lr: 5e-05
2023-12-20 23:16:01 INFO     	 * (global step 43000: loss: 0.3412710204720497, lr: 5e-05
2023-12-20 23:16:09 INFO     	 * (global step 43050: loss: 0.2554154470562935, lr: 5e-05
2023-12-20 23:16:17 INFO     	 * (global step 43100: loss: 0.30854663252830505, lr: 5e-05
2023-12-20 23:16:25 INFO     	 * (global step 43150: loss: 0.25797489285469055, lr: 5e-05
2023-12-20 23:16:33 INFO     	 * (global step 43200: loss: 0.2516723573207855, lr: 5e-05
2023-12-20 23:16:42 INFO     	 * (global step 43250: loss: 0.21518856287002563, lr: 5e-05
2023-12-20 23:16:50 INFO     	 * (global step 43300: loss: 0.46737344563007355, lr: 5e-05
2023-12-20 23:16:58 INFO     	 * (global step 43350: loss: 0.3042309284210205, lr: 5e-05
2023-12-20 23:17:06 INFO     	 * (global step 43400: loss: 0.2103971689939499, lr: 5e-05
2023-12-20 23:17:14 INFO     	 * (global step 43450: loss: 0.2438517063856125, lr: 5e-05
2023-12-20 23:17:22 INFO     	 * (global step 43500: loss: 0.3898336887359619, lr: 5e-05
2023-12-20 23:17:30 INFO     	 * (global step 43550: loss: 0.2898930460214615, lr: 5e-05
2023-12-20 23:17:38 INFO     	 * (global step 43600: loss: 0.31587448716163635, lr: 5e-05
2023-12-20 23:17:47 INFO     	 * (global step 43650: loss: 0.47153930366039276, lr: 5e-05
2023-12-20 23:17:55 INFO     	 * (global step 43700: loss: 0.25062044709920883, lr: 5e-05
2023-12-20 23:18:03 INFO     	 * (global step 43750: loss: 0.30277982354164124, lr: 5e-05
2023-12-20 23:18:11 INFO     	 * (global step 43800: loss: 0.2322784885764122, lr: 5e-05
2023-12-20 23:18:19 INFO     	 * (global step 43850: loss: 0.27054905891418457, lr: 5e-05
2023-12-20 23:18:27 INFO     	 * (global step 43900: loss: 0.33041812479496, lr: 5e-05
2023-12-20 23:18:35 INFO     	 * (global step 43950: loss: 0.20672712475061417, lr: 5e-05
2023-12-20 23:18:43 INFO     	 * (global step 44000: loss: 0.3247572034597397, lr: 5e-05
2023-12-20 23:18:51 INFO     	 * (global step 44050: loss: 0.24929168075323105, lr: 5e-05
2023-12-20 23:19:00 INFO     	 * (global step 44100: loss: 0.20718374103307724, lr: 5e-05
2023-12-20 23:19:08 INFO     	 * (global step 44150: loss: 0.37392091751098633, lr: 5e-05
2023-12-20 23:19:16 INFO     	 * (global step 44200: loss: 0.18028251454234123, lr: 5e-05
2023-12-20 23:19:24 INFO     	 * (global step 44250: loss: 0.2922065481543541, lr: 5e-05
2023-12-20 23:19:32 INFO     	 * (global step 44300: loss: 0.6285474002361298, lr: 5e-05
2023-12-20 23:19:40 INFO     	 * (global step 44350: loss: 0.23463770747184753, lr: 5e-05
2023-12-20 23:19:48 INFO     	 * (global step 44400: loss: 0.17975885048508644, lr: 5e-05
2023-12-20 23:19:57 INFO     	 * (global step 44450: loss: 0.32422420382499695, lr: 5e-05
2023-12-20 23:20:05 INFO     	 * (global step 44500: loss: 0.3645487278699875, lr: 5e-05
2023-12-20 23:20:13 INFO     	 * (global step 44550: loss: 0.16009679436683655, lr: 5e-05
2023-12-20 23:20:21 INFO     	 * (global step 44600: loss: 0.19801387935876846, lr: 5e-05
2023-12-20 23:20:29 INFO     	 * (global step 44650: loss: 0.33206868171691895, lr: 5e-05
2023-12-20 23:20:37 INFO     	 * (global step 44700: loss: 0.23715263605117798, lr: 5e-05
2023-12-20 23:20:45 INFO     	 * (global step 44750: loss: 0.18741130083799362, lr: 5e-05
2023-12-20 23:20:53 INFO     	 * (global step 44800: loss: 0.24570706486701965, lr: 5e-05
2023-12-20 23:21:02 INFO     	 * (global step 44850: loss: 0.254177488386631, lr: 5e-05
2023-12-20 23:21:10 INFO     	 * (global step 44900: loss: 0.21086662262678146, lr: 5e-05
2023-12-20 23:21:18 INFO     	 * (global step 44950: loss: 0.2641874924302101, lr: 5e-05
2023-12-20 23:21:26 INFO     	 * (global step 45000: loss: 0.22557630389928818, lr: 5e-05
2023-12-20 23:21:34 INFO     	 * (global step 45050: loss: 0.2583339065313339, lr: 5e-05
2023-12-20 23:21:42 INFO     	 * (global step 45100: loss: 0.30209746956825256, lr: 5e-05
2023-12-20 23:21:50 INFO     	 * (global step 45150: loss: 0.3262424021959305, lr: 5e-05
2023-12-20 23:21:58 INFO     	 * (global step 45200: loss: 0.18971288949251175, lr: 5e-05
2023-12-20 23:22:07 INFO     	 * (global step 45250: loss: 0.2793213799595833, lr: 5e-05
2023-12-20 23:22:15 INFO     	 * (global step 45300: loss: 0.19596508890390396, lr: 5e-05
2023-12-20 23:22:23 INFO     	 * (global step 45350: loss: 0.20832187682390213, lr: 5e-05
2023-12-20 23:22:31 INFO     	 * (global step 45400: loss: 0.23613454401493073, lr: 5e-05
2023-12-20 23:22:39 INFO     	 * (global step 45450: loss: 0.187063317745924, lr: 5e-05
2023-12-20 23:22:47 INFO     	 * (global step 45500: loss: 0.19314555823802948, lr: 5e-05
2023-12-20 23:22:55 INFO     	 * (global step 45550: loss: 0.2604500651359558, lr: 5e-05
2023-12-20 23:23:04 INFO     	 * (global step 45600: loss: 0.26306886225938797, lr: 5e-05
2023-12-20 23:23:12 INFO     	 * (global step 45650: loss: 0.30214376747608185, lr: 5e-05
2023-12-20 23:23:20 INFO     	 * (global step 45700: loss: 0.21919739991426468, lr: 5e-05
2023-12-20 23:23:28 INFO     	 * (global step 45750: loss: 0.4232400208711624, lr: 5e-05
2023-12-20 23:23:36 INFO     	 * (global step 45800: loss: 0.2698180004954338, lr: 5e-05
2023-12-20 23:23:44 INFO     	 * (global step 45850: loss: 0.2704629674553871, lr: 5e-05
2023-12-20 23:23:52 INFO     	 * (global step 45900: loss: 0.41719290614128113, lr: 5e-05
2023-12-20 23:24:00 INFO     	 * (global step 45950: loss: 0.3323400393128395, lr: 5e-05
2023-12-20 23:24:09 INFO     	 * (global step 46000: loss: 0.20929435640573502, lr: 5e-05
2023-12-20 23:24:17 INFO     	 * (global step 46050: loss: 0.23155047744512558, lr: 5e-05
2023-12-20 23:24:25 INFO     	 * (global step 46100: loss: 0.37449952214956284, lr: 5e-05
2023-12-20 23:24:33 INFO     	 * (global step 46150: loss: 0.17950034886598587, lr: 5e-05
2023-12-20 23:24:41 INFO     	 * (global step 46200: loss: 0.18185630440711975, lr: 5e-05
2023-12-20 23:24:49 INFO     	 * (global step 46250: loss: 0.2353205382823944, lr: 5e-05
2023-12-20 23:24:57 INFO     	 * (global step 46300: loss: 0.21201561391353607, lr: 5e-05
2023-12-20 23:25:06 INFO     	 * (global step 46350: loss: 0.2694929167628288, lr: 5e-05
2023-12-20 23:25:14 INFO     	 * (global step 46400: loss: 0.2803936302661896, lr: 5e-05
2023-12-20 23:25:22 INFO     	 * (global step 46450: loss: 0.33450569212436676, lr: 5e-05
2023-12-20 23:25:30 INFO     	 * (global step 46500: loss: 0.16183384507894516, lr: 5e-05
2023-12-20 23:25:38 INFO     	 * (global step 46550: loss: 0.557280033826828, lr: 5e-05
2023-12-20 23:25:46 INFO     	 * (global step 46600: loss: 0.29408954083919525, lr: 5e-05
2023-12-20 23:25:54 INFO     	 * (global step 46650: loss: 0.36066506803035736, lr: 5e-05
2023-12-20 23:26:02 INFO     	 * (global step 46700: loss: 0.3093748390674591, lr: 5e-05
2023-12-20 23:26:10 INFO     	 * (global step 46750: loss: 0.27738749980926514, lr: 5e-05
2023-12-20 23:26:19 INFO     	 * (global step 46800: loss: 0.1785939335823059, lr: 5e-05
2023-12-20 23:26:27 INFO     	 * (global step 46850: loss: 0.42496834695339203, lr: 5e-05
2023-12-20 23:26:35 INFO     	 * (global step 46900: loss: 0.18567214906215668, lr: 5e-05
2023-12-20 23:26:43 INFO     	 * (global step 46950: loss: 0.1954667530953884, lr: 5e-05
2023-12-20 23:26:51 INFO     	 * (global step 47000: loss: 0.36267971247434616, lr: 5e-05
2023-12-20 23:26:59 INFO     	 * (global step 47050: loss: 0.2626430541276932, lr: 5e-05
2023-12-20 23:27:07 INFO     	 * (global step 47100: loss: 0.21374117955565453, lr: 5e-05
2023-12-20 23:27:15 INFO     	 * (global step 47150: loss: 0.22795405983924866, lr: 5e-05
2023-12-20 23:27:24 INFO     	 * (global step 47200: loss: 0.22417954355478287, lr: 5e-05
2023-12-20 23:27:29 INFO     [epoch 9/15] average loss: 0.269, lr: 5e-05
2023-12-20 23:27:29 INFO     saving model related files
2023-12-20 23:27:29 INFO     saving model
2023-12-20 23:27:29 INFO     saving tokenizer
2023-12-20 23:27:29 INFO     saving optimizer
2023-12-20 23:27:30 INFO     remove old optimizer files
2023-12-20 23:27:30 INFO     complete training: model ckpt was saved at small_combined_trained_ckpt/model_woixzh
2023-12-20 23:27:30 INFO     ## 1st RUN: Configuration 6/12 ##
2023-12-20 23:27:30 INFO     initialize model trainer
2023-12-20 23:27:30 INFO     initialize checkpoint at small_combined_trained_ckpt/model_sdkaaa
2023-12-20 23:27:30 INFO     hyperparameters
2023-12-20 23:27:30 INFO     	 * dataset_path: StellarMilk/squad_newsqa
2023-12-20 23:27:30 INFO     	 * dataset_name: default
2023-12-20 23:27:30 INFO     	 * input_types: ['paragraph']
2023-12-20 23:27:30 INFO     	 * output_types: ['questions_answers']
2023-12-20 23:27:30 INFO     	 * prefix_types: ['qag']
2023-12-20 23:27:30 INFO     	 * model: t5-small
2023-12-20 23:27:30 INFO     	 * max_length: 512
2023-12-20 23:27:30 INFO     	 * max_length_output: 512
2023-12-20 23:27:30 INFO     	 * epoch: 15
2023-12-20 23:27:30 INFO     	 * batch: 2
2023-12-20 23:27:30 INFO     	 * lr: 5e-05
2023-12-20 23:27:30 INFO     	 * fp16: False
2023-12-20 23:27:30 INFO     	 * random_seed: 1
2023-12-20 23:27:30 INFO     	 * gradient_accumulation_steps: 4
2023-12-20 23:27:30 INFO     	 * label_smoothing: 0.0
2023-12-20 23:27:30 INFO     initialize checkpoint with t5-small
2023-12-20 23:27:32 INFO     use spaCy answer extraction model: positionrank
2023-12-20 23:27:32 INFO     Model `t5-small`
2023-12-20 23:27:32 INFO     	 * Num of GPU in use: 1
2023-12-20 23:27:32 INFO     	 * Prefix: True
2023-12-20 23:27:32 INFO     	 * Language: en (ignore at the training phase)
2023-12-20 23:27:32 INFO     dataset preprocessing
2023-12-20 23:27:33 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/squad_newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-20 23:27:35 INFO     start model training
2023-12-20 23:27:51 INFO     	 * (global step 50: loss: 1.2514507919549942, lr: 5e-05
2023-12-20 23:28:07 INFO     	 * (global step 100: loss: 1.031420722603798, lr: 5e-05
2023-12-20 23:28:22 INFO     	 * (global step 150: loss: 0.6309006661176682, lr: 5e-05
2023-12-20 23:28:38 INFO     	 * (global step 200: loss: 0.6078341603279114, lr: 5e-05
2023-12-20 23:28:54 INFO     	 * (global step 250: loss: 0.6571569293737411, lr: 5e-05
2023-12-20 23:29:10 INFO     	 * (global step 300: loss: 0.5342007353901863, lr: 5e-05
2023-12-20 23:29:25 INFO     	 * (global step 350: loss: 0.5863303542137146, lr: 5e-05
2023-12-20 23:29:41 INFO     	 * (global step 400: loss: 0.3965480178594589, lr: 5e-05
2023-12-20 23:29:57 INFO     	 * (global step 450: loss: 0.46170470118522644, lr: 5e-05
2023-12-20 23:30:12 INFO     	 * (global step 500: loss: 0.41489125043153763, lr: 5e-05
2023-12-20 23:30:28 INFO     	 * (global step 550: loss: 0.4932679757475853, lr: 5e-05
2023-12-20 23:30:44 INFO     	 * (global step 600: loss: 0.5063530504703522, lr: 5e-05
2023-12-20 23:31:00 INFO     	 * (global step 650: loss: 0.3773292228579521, lr: 5e-05
2023-12-20 23:31:15 INFO     	 * (global step 700: loss: 0.389016754925251, lr: 5e-05
2023-12-20 23:31:31 INFO     	 * (global step 750: loss: 0.502255342900753, lr: 5e-05
2023-12-20 23:31:47 INFO     	 * (global step 800: loss: 0.40103085339069366, lr: 5e-05
2023-12-20 23:32:03 INFO     	 * (global step 850: loss: 0.3633226156234741, lr: 5e-05
2023-12-20 23:32:18 INFO     	 * (global step 900: loss: 0.37505991756916046, lr: 5e-05
2023-12-20 23:32:34 INFO     	 * (global step 950: loss: 0.36701276898384094, lr: 5e-05
2023-12-20 23:32:50 INFO     	 * (global step 1000: loss: 0.3881572261452675, lr: 5e-05
2023-12-20 23:33:06 INFO     	 * (global step 1050: loss: 0.3922189697623253, lr: 5e-05
2023-12-20 23:33:21 INFO     	 * (global step 1100: loss: 0.4134680777788162, lr: 5e-05
2023-12-20 23:33:37 INFO     	 * (global step 1150: loss: 0.6286459788680077, lr: 5e-05
2023-12-20 23:33:53 INFO     	 * (global step 1200: loss: 0.36941061168909073, lr: 5e-05
2023-12-20 23:34:09 INFO     	 * (global step 1250: loss: 0.324233278632164, lr: 5e-05
2023-12-20 23:34:25 INFO     	 * (global step 1300: loss: 0.37653743475675583, lr: 5e-05
2023-12-20 23:34:40 INFO     	 * (global step 1350: loss: 0.3270960673689842, lr: 5e-05
2023-12-20 23:34:56 INFO     	 * (global step 1400: loss: 0.42872346192598343, lr: 5e-05
2023-12-20 23:35:12 INFO     	 * (global step 1450: loss: 0.37908757477998734, lr: 5e-05
2023-12-20 23:35:28 INFO     	 * (global step 1500: loss: 0.4681720696389675, lr: 5e-05
2023-12-20 23:35:43 INFO     	 * (global step 1550: loss: 0.31873980164527893, lr: 5e-05
2023-12-20 23:35:59 INFO     	 * (global step 1600: loss: 0.31488513201475143, lr: 5e-05
2023-12-20 23:36:15 INFO     	 * (global step 1650: loss: 0.42861074954271317, lr: 5e-05
2023-12-20 23:36:31 INFO     	 * (global step 1700: loss: 0.33426015079021454, lr: 5e-05
2023-12-20 23:36:46 INFO     	 * (global step 1750: loss: 0.3223353326320648, lr: 5e-05
2023-12-20 23:37:02 INFO     	 * (global step 1800: loss: 0.33300214633345604, lr: 5e-05
2023-12-20 23:37:18 INFO     	 * (global step 1850: loss: 0.26847608387470245, lr: 5e-05
2023-12-20 23:37:34 INFO     	 * (global step 1900: loss: 0.3567707762122154, lr: 5e-05
2023-12-20 23:37:49 INFO     	 * (global step 1950: loss: 0.3229430839419365, lr: 5e-05
2023-12-20 23:38:05 INFO     	 * (global step 2000: loss: 0.27716489881277084, lr: 5e-05
2023-12-20 23:38:21 INFO     	 * (global step 2050: loss: 0.29819265380501747, lr: 5e-05
2023-12-20 23:38:37 INFO     	 * (global step 2100: loss: 0.2992737926542759, lr: 5e-05
2023-12-20 23:38:52 INFO     	 * (global step 2150: loss: 0.43449316918849945, lr: 5e-05
2023-12-20 23:39:08 INFO     	 * (global step 2200: loss: 0.46852172911167145, lr: 5e-05
2023-12-20 23:39:24 INFO     	 * (global step 2250: loss: 0.30048681423068047, lr: 5e-05
2023-12-20 23:39:40 INFO     	 * (global step 2300: loss: 0.3949219211935997, lr: 5e-05
2023-12-20 23:39:55 INFO     	 * (global step 2350: loss: 0.36387690529227257, lr: 5e-05
2023-12-20 23:39:59 INFO     [epoch 0/15] average loss: 0.506, lr: 5e-05
2023-12-20 23:39:59 INFO     saving model related files
2023-12-20 23:39:59 INFO     saving model
2023-12-20 23:39:59 INFO     saving tokenizer
2023-12-20 23:39:59 INFO     saving optimizer
2023-12-20 23:40:00 INFO     remove old optimizer files
2023-12-20 23:40:13 INFO     	 * (global step 2400: loss: 0.33917830511927605, lr: 5e-05
2023-12-20 23:40:28 INFO     	 * (global step 2450: loss: 0.3770027831196785, lr: 5e-05
2023-12-20 23:40:44 INFO     	 * (global step 2500: loss: 0.35855506360530853, lr: 5e-05
2023-12-20 23:41:00 INFO     	 * (global step 2550: loss: 0.33545688539743423, lr: 5e-05
2023-12-20 23:41:16 INFO     	 * (global step 2600: loss: 0.3343246653676033, lr: 5e-05
2023-12-20 23:41:31 INFO     	 * (global step 2650: loss: 0.5834588930010796, lr: 5e-05
2023-12-20 23:41:47 INFO     	 * (global step 2700: loss: 0.4141761139035225, lr: 5e-05
2023-12-20 23:42:03 INFO     	 * (global step 2750: loss: 0.2968132346868515, lr: 5e-05
2023-12-20 23:42:18 INFO     	 * (global step 2800: loss: 0.4524480327963829, lr: 5e-05
2023-12-20 23:42:34 INFO     	 * (global step 2850: loss: 0.44873588532209396, lr: 5e-05
2023-12-20 23:42:49 INFO     	 * (global step 2900: loss: 0.43517279624938965, lr: 5e-05
2023-12-20 23:43:05 INFO     	 * (global step 2950: loss: 0.350632481276989, lr: 5e-05
2023-12-20 23:43:20 INFO     	 * (global step 3000: loss: 0.4087050333619118, lr: 5e-05
2023-12-20 23:43:36 INFO     	 * (global step 3050: loss: 0.36553366854786873, lr: 5e-05
2023-12-20 23:43:52 INFO     	 * (global step 3100: loss: 0.3539527878165245, lr: 5e-05
2023-12-20 23:44:07 INFO     	 * (global step 3150: loss: 0.5079644173383713, lr: 5e-05
2023-12-20 23:44:23 INFO     	 * (global step 3200: loss: 0.3175157532095909, lr: 5e-05
2023-12-20 23:44:38 INFO     	 * (global step 3250: loss: 0.4015091806650162, lr: 5e-05
2023-12-20 23:44:54 INFO     	 * (global step 3300: loss: 0.3427926301956177, lr: 5e-05
2023-12-20 23:45:09 INFO     	 * (global step 3350: loss: 0.314695555716753, lr: 5e-05
2023-12-20 23:45:25 INFO     	 * (global step 3400: loss: 0.2890702225267887, lr: 5e-05
2023-12-20 23:45:40 INFO     	 * (global step 3450: loss: 0.4007195308804512, lr: 5e-05
2023-12-20 23:45:56 INFO     	 * (global step 3500: loss: 0.5199279636144638, lr: 5e-05
2023-12-20 23:46:12 INFO     	 * (global step 3550: loss: 0.33684296160936356, lr: 5e-05
2023-12-20 23:46:28 INFO     	 * (global step 3600: loss: 0.3741217702627182, lr: 5e-05
2023-12-20 23:46:43 INFO     	 * (global step 3650: loss: 0.332588504999876, lr: 5e-05
2023-12-20 23:46:59 INFO     	 * (global step 3700: loss: 0.3688104674220085, lr: 5e-05
2023-12-20 23:47:14 INFO     	 * (global step 3750: loss: 0.3515837639570236, lr: 5e-05
2023-12-20 23:47:30 INFO     	 * (global step 3800: loss: 0.2723842151463032, lr: 5e-05
2023-12-20 23:47:46 INFO     	 * (global step 3850: loss: 0.3553410917520523, lr: 5e-05
2023-12-20 23:48:01 INFO     	 * (global step 3900: loss: 0.28981858864426613, lr: 5e-05
2023-12-20 23:48:17 INFO     	 * (global step 3950: loss: 0.3680156320333481, lr: 5e-05
2023-12-20 23:48:32 INFO     	 * (global step 4000: loss: 0.30535856634378433, lr: 5e-05
2023-12-20 23:48:48 INFO     	 * (global step 4050: loss: 0.4208297058939934, lr: 5e-05
2023-12-20 23:49:04 INFO     	 * (global step 4100: loss: 0.2793261967599392, lr: 5e-05
2023-12-20 23:49:19 INFO     	 * (global step 4150: loss: 0.2765987180173397, lr: 5e-05
2023-12-20 23:49:35 INFO     	 * (global step 4200: loss: 0.3768826872110367, lr: 5e-05
2023-12-20 23:49:50 INFO     	 * (global step 4250: loss: 0.4123363345861435, lr: 5e-05
2023-12-20 23:50:06 INFO     	 * (global step 4300: loss: 0.30036476626992226, lr: 5e-05
2023-12-20 23:50:22 INFO     	 * (global step 4350: loss: 0.2777355872094631, lr: 5e-05
2023-12-20 23:50:37 INFO     	 * (global step 4400: loss: 0.31297317147254944, lr: 5e-05
2023-12-20 23:50:53 INFO     	 * (global step 4450: loss: 0.280557282269001, lr: 5e-05
2023-12-20 23:51:09 INFO     	 * (global step 4500: loss: 0.2898745872080326, lr: 5e-05
2023-12-20 23:51:24 INFO     	 * (global step 4550: loss: 0.34747878462076187, lr: 5e-05
2023-12-20 23:51:40 INFO     	 * (global step 4600: loss: 0.2293122224509716, lr: 5e-05
2023-12-20 23:51:55 INFO     	 * (global step 4650: loss: 0.3560496121644974, lr: 5e-05
2023-12-20 23:52:11 INFO     	 * (global step 4700: loss: 0.3216923549771309, lr: 5e-05
2023-12-20 23:52:18 INFO     [epoch 1/15] average loss: 0.354, lr: 5e-05
2023-12-20 23:52:18 INFO     saving model related files
2023-12-20 23:52:18 INFO     saving model
2023-12-20 23:52:19 INFO     saving tokenizer
2023-12-20 23:52:19 INFO     saving optimizer
2023-12-20 23:52:20 INFO     remove old optimizer files
2023-12-20 23:52:28 INFO     	 * (global step 4750: loss: 0.49299515783786774, lr: 5e-05
2023-12-20 23:52:44 INFO     	 * (global step 4800: loss: 0.3018070198595524, lr: 5e-05
2023-12-20 23:53:00 INFO     	 * (global step 4850: loss: 0.26204423420131207, lr: 5e-05
2023-12-20 23:53:16 INFO     	 * (global step 4900: loss: 0.3013293296098709, lr: 5e-05
2023-12-20 23:53:31 INFO     	 * (global step 4950: loss: 0.3055648133158684, lr: 5e-05
2023-12-20 23:53:47 INFO     	 * (global step 5000: loss: 0.49503380060195923, lr: 5e-05
2023-12-20 23:54:03 INFO     	 * (global step 5050: loss: 0.3810172565281391, lr: 5e-05
2023-12-20 23:54:18 INFO     	 * (global step 5100: loss: 0.2751794904470444, lr: 5e-05
2023-12-20 23:54:34 INFO     	 * (global step 5150: loss: 0.314722016453743, lr: 5e-05
2023-12-20 23:54:50 INFO     	 * (global step 5200: loss: 0.38899099826812744, lr: 5e-05
2023-12-20 23:55:05 INFO     	 * (global step 5250: loss: 0.3177236244082451, lr: 5e-05
2023-12-20 23:55:21 INFO     	 * (global step 5300: loss: 0.3389419727027416, lr: 5e-05
2023-12-20 23:55:36 INFO     	 * (global step 5350: loss: 0.28357503935694695, lr: 5e-05
2023-12-20 23:55:52 INFO     	 * (global step 5400: loss: 0.23152142763137817, lr: 5e-05
2023-12-20 23:56:08 INFO     	 * (global step 5450: loss: 0.45426154881715775, lr: 5e-05
2023-12-20 23:56:23 INFO     	 * (global step 5500: loss: 0.44332563877105713, lr: 5e-05
2023-12-20 23:56:39 INFO     	 * (global step 5550: loss: 0.2970256358385086, lr: 5e-05
2023-12-20 23:56:55 INFO     	 * (global step 5600: loss: 0.26291001960635185, lr: 5e-05
2023-12-20 23:57:10 INFO     	 * (global step 5650: loss: 0.27674170210957527, lr: 5e-05
2023-12-20 23:57:26 INFO     	 * (global step 5700: loss: 0.44791267812252045, lr: 5e-05
2023-12-20 23:57:42 INFO     	 * (global step 5750: loss: 0.37054232135415077, lr: 5e-05
2023-12-20 23:57:57 INFO     	 * (global step 5800: loss: 0.34380362927913666, lr: 5e-05
2023-12-20 23:58:13 INFO     	 * (global step 5850: loss: 0.3928806036710739, lr: 5e-05
2023-12-20 23:58:29 INFO     	 * (global step 5900: loss: 0.333132304251194, lr: 5e-05
2023-12-20 23:58:44 INFO     	 * (global step 5950: loss: 0.39979881048202515, lr: 5e-05
2023-12-20 23:59:00 INFO     	 * (global step 6000: loss: 0.37520166486501694, lr: 5e-05
2023-12-20 23:59:15 INFO     	 * (global step 6050: loss: 0.2963172197341919, lr: 5e-05
2023-12-20 23:59:31 INFO     	 * (global step 6100: loss: 0.3794361650943756, lr: 5e-05
2023-12-20 23:59:47 INFO     	 * (global step 6150: loss: 0.3818548172712326, lr: 5e-05
2023-12-21 00:00:02 INFO     	 * (global step 6200: loss: 0.30733639374375343, lr: 5e-05
2023-12-21 00:00:18 INFO     	 * (global step 6250: loss: 0.278874009847641, lr: 5e-05
2023-12-21 00:00:34 INFO     	 * (global step 6300: loss: 0.3773486688733101, lr: 5e-05
2023-12-21 00:00:49 INFO     	 * (global step 6350: loss: 0.40024247765541077, lr: 5e-05
2023-12-21 00:01:05 INFO     	 * (global step 6400: loss: 0.2851819023489952, lr: 5e-05
2023-12-21 00:01:21 INFO     	 * (global step 6450: loss: 0.3264225758612156, lr: 5e-05
2023-12-21 00:01:37 INFO     	 * (global step 6500: loss: 0.3168735131621361, lr: 5e-05
2023-12-21 00:01:52 INFO     	 * (global step 6550: loss: 0.4381834641098976, lr: 5e-05
2023-12-21 00:02:08 INFO     	 * (global step 6600: loss: 0.4087904542684555, lr: 5e-05
2023-12-21 00:02:24 INFO     	 * (global step 6650: loss: 0.31696968525648117, lr: 5e-05
2023-12-21 00:02:39 INFO     	 * (global step 6700: loss: 0.3130033388733864, lr: 5e-05
2023-12-21 00:02:55 INFO     	 * (global step 6750: loss: 0.37888240441679955, lr: 5e-05
2023-12-21 00:03:11 INFO     	 * (global step 6800: loss: 0.32255005836486816, lr: 5e-05
2023-12-21 00:03:26 INFO     	 * (global step 6850: loss: 0.25864553824067116, lr: 5e-05
2023-12-21 00:03:42 INFO     	 * (global step 6900: loss: 0.3979403153061867, lr: 5e-05
2023-12-21 00:03:58 INFO     	 * (global step 6950: loss: 0.30062083527445793, lr: 5e-05
2023-12-21 00:04:13 INFO     	 * (global step 7000: loss: 0.2978886589407921, lr: 5e-05
2023-12-21 00:04:29 INFO     	 * (global step 7050: loss: 0.30651721358299255, lr: 5e-05
2023-12-21 00:04:40 INFO     [epoch 2/15] average loss: 0.332, lr: 5e-05
2023-12-21 00:04:40 INFO     saving model related files
2023-12-21 00:04:40 INFO     saving model
2023-12-21 00:04:40 INFO     saving tokenizer
2023-12-21 00:04:40 INFO     saving optimizer
2023-12-21 00:04:41 INFO     remove old optimizer files
2023-12-21 00:04:46 INFO     	 * (global step 7100: loss: 0.28404540941119194, lr: 5e-05
2023-12-21 00:05:02 INFO     	 * (global step 7150: loss: 0.4477771446108818, lr: 5e-05
2023-12-21 00:05:18 INFO     	 * (global step 7200: loss: 0.3205093964934349, lr: 5e-05
2023-12-21 00:05:33 INFO     	 * (global step 7250: loss: 0.24396327883005142, lr: 5e-05
2023-12-21 00:05:49 INFO     	 * (global step 7300: loss: 0.44154638051986694, lr: 5e-05
2023-12-21 00:06:05 INFO     	 * (global step 7350: loss: 0.2821950130164623, lr: 5e-05
2023-12-21 00:06:20 INFO     	 * (global step 7400: loss: 0.3560139574110508, lr: 5e-05
2023-12-21 00:06:36 INFO     	 * (global step 7450: loss: 0.2751721106469631, lr: 5e-05
2023-12-21 00:06:52 INFO     	 * (global step 7500: loss: 0.2827438600361347, lr: 5e-05
2023-12-21 00:07:07 INFO     	 * (global step 7550: loss: 0.3392236940562725, lr: 5e-05
2023-12-21 00:07:23 INFO     	 * (global step 7600: loss: 0.33128805458545685, lr: 5e-05
2023-12-21 00:07:39 INFO     	 * (global step 7650: loss: 0.28407691791653633, lr: 5e-05
2023-12-21 00:07:55 INFO     	 * (global step 7700: loss: 0.3521042764186859, lr: 5e-05
2023-12-21 00:08:10 INFO     	 * (global step 7750: loss: 0.3326069712638855, lr: 5e-05
2023-12-21 00:08:26 INFO     	 * (global step 7800: loss: 0.27179888635873795, lr: 5e-05
2023-12-21 00:08:41 INFO     	 * (global step 7850: loss: 0.24890724569559097, lr: 5e-05
2023-12-21 00:08:57 INFO     	 * (global step 7900: loss: 0.25745052099227905, lr: 5e-05
2023-12-21 00:09:13 INFO     	 * (global step 7950: loss: 0.2448427714407444, lr: 5e-05
2023-12-21 00:09:29 INFO     	 * (global step 8000: loss: 0.3414418026804924, lr: 5e-05
2023-12-21 00:09:44 INFO     	 * (global step 8050: loss: 0.27846378460526466, lr: 5e-05
2023-12-21 00:10:00 INFO     	 * (global step 8100: loss: 0.44990237429738045, lr: 5e-05
2023-12-21 00:10:16 INFO     	 * (global step 8150: loss: 0.26651937142014503, lr: 5e-05
2023-12-21 00:10:31 INFO     	 * (global step 8200: loss: 0.293557845056057, lr: 5e-05
2023-12-21 00:10:47 INFO     	 * (global step 8250: loss: 0.3068651482462883, lr: 5e-05
2023-12-21 00:11:03 INFO     	 * (global step 8300: loss: 0.281809214502573, lr: 5e-05
2023-12-21 00:11:18 INFO     	 * (global step 8350: loss: 0.3593912199139595, lr: 5e-05
2023-12-21 00:11:34 INFO     	 * (global step 8400: loss: 0.3031705990433693, lr: 5e-05
2023-12-21 00:11:49 INFO     	 * (global step 8450: loss: 0.307629831135273, lr: 5e-05
2023-12-21 00:12:05 INFO     	 * (global step 8500: loss: 0.31913189589977264, lr: 5e-05
2023-12-21 00:12:21 INFO     	 * (global step 8550: loss: 0.24630553275346756, lr: 5e-05
2023-12-21 00:12:36 INFO     	 * (global step 8600: loss: 0.3608175739645958, lr: 5e-05
2023-12-21 00:12:52 INFO     	 * (global step 8650: loss: 0.44472159817814827, lr: 5e-05
2023-12-21 00:13:08 INFO     	 * (global step 8700: loss: 0.3660810627043247, lr: 5e-05
2023-12-21 00:13:23 INFO     	 * (global step 8750: loss: 0.24399851262569427, lr: 5e-05
2023-12-21 00:13:39 INFO     	 * (global step 8800: loss: 0.32913271337747574, lr: 5e-05
2023-12-21 00:13:55 INFO     	 * (global step 8850: loss: 0.30927328392863274, lr: 5e-05
2023-12-21 00:14:10 INFO     	 * (global step 8900: loss: 0.24646228179335594, lr: 5e-05
2023-12-21 00:14:26 INFO     	 * (global step 8950: loss: 0.3003811873495579, lr: 5e-05
2023-12-21 00:14:42 INFO     	 * (global step 9000: loss: 0.3401215448975563, lr: 5e-05
2023-12-21 00:14:57 INFO     	 * (global step 9050: loss: 0.29867013543844223, lr: 5e-05
2023-12-21 00:15:13 INFO     	 * (global step 9100: loss: 0.3192230500280857, lr: 5e-05
2023-12-21 00:15:29 INFO     	 * (global step 9150: loss: 0.2690129652619362, lr: 5e-05
2023-12-21 00:15:44 INFO     	 * (global step 9200: loss: 0.2514899671077728, lr: 5e-05
2023-12-21 00:16:00 INFO     	 * (global step 9250: loss: 0.21302274614572525, lr: 5e-05
2023-12-21 00:16:16 INFO     	 * (global step 9300: loss: 0.2519058361649513, lr: 5e-05
2023-12-21 00:16:31 INFO     	 * (global step 9350: loss: 0.31657395884394646, lr: 5e-05
2023-12-21 00:16:47 INFO     	 * (global step 9400: loss: 0.3523371294140816, lr: 5e-05
2023-12-21 00:17:01 INFO     [epoch 3/15] average loss: 0.319, lr: 5e-05
2023-12-21 00:17:01 INFO     saving model related files
2023-12-21 00:17:01 INFO     saving model
2023-12-21 00:17:01 INFO     saving tokenizer
2023-12-21 00:17:02 INFO     saving optimizer
2023-12-21 00:17:03 INFO     remove old optimizer files
2023-12-21 00:17:04 INFO     	 * (global step 9450: loss: 0.24532660469412804, lr: 5e-05
2023-12-21 00:17:20 INFO     	 * (global step 9500: loss: 0.3909818306565285, lr: 5e-05
2023-12-21 00:17:36 INFO     	 * (global step 9550: loss: 0.31722912564873695, lr: 5e-05
2023-12-21 00:17:51 INFO     	 * (global step 9600: loss: 0.39062323421239853, lr: 5e-05
2023-12-21 00:18:07 INFO     	 * (global step 9650: loss: 0.3075711037963629, lr: 5e-05
2023-12-21 00:18:22 INFO     	 * (global step 9700: loss: 0.28312962874770164, lr: 5e-05
2023-12-21 00:18:38 INFO     	 * (global step 9750: loss: 0.2833046540617943, lr: 5e-05
2023-12-21 00:18:54 INFO     	 * (global step 9800: loss: 0.2738660238683224, lr: 5e-05
2023-12-21 00:19:09 INFO     	 * (global step 9850: loss: 0.2613283693790436, lr: 5e-05
2023-12-21 00:19:25 INFO     	 * (global step 9900: loss: 0.3598838672041893, lr: 5e-05
2023-12-21 00:19:41 INFO     	 * (global step 9950: loss: 0.3969932720065117, lr: 5e-05
2023-12-21 00:19:56 INFO     	 * (global step 10000: loss: 0.2654412854462862, lr: 5e-05
2023-12-21 00:20:12 INFO     	 * (global step 10050: loss: 0.4495128467679024, lr: 5e-05
2023-12-21 00:20:28 INFO     	 * (global step 10100: loss: 0.2265174761414528, lr: 5e-05
2023-12-21 00:20:43 INFO     	 * (global step 10150: loss: 0.42743659019470215, lr: 5e-05
2023-12-21 00:20:59 INFO     	 * (global step 10200: loss: 0.3732101172208786, lr: 5e-05
2023-12-21 00:21:14 INFO     	 * (global step 10250: loss: 0.2090887911617756, lr: 5e-05
2023-12-21 00:21:30 INFO     	 * (global step 10300: loss: 0.25828272104263306, lr: 5e-05
2023-12-21 00:21:46 INFO     	 * (global step 10350: loss: 0.37129757553339005, lr: 5e-05
2023-12-21 00:22:01 INFO     	 * (global step 10400: loss: 0.23353398963809013, lr: 5e-05
2023-12-21 00:22:17 INFO     	 * (global step 10450: loss: 0.396123506128788, lr: 5e-05
2023-12-21 00:22:32 INFO     	 * (global step 10500: loss: 0.3467966541647911, lr: 5e-05
2023-12-21 00:22:48 INFO     	 * (global step 10550: loss: 0.2866619825363159, lr: 5e-05
2023-12-21 00:23:04 INFO     	 * (global step 10600: loss: 0.3283865302801132, lr: 5e-05
2023-12-21 00:23:19 INFO     	 * (global step 10650: loss: 0.3271193876862526, lr: 5e-05
2023-12-21 00:23:35 INFO     	 * (global step 10700: loss: 0.45398610085248947, lr: 5e-05
2023-12-21 00:23:50 INFO     	 * (global step 10750: loss: 0.4815448373556137, lr: 5e-05
2023-12-21 00:24:06 INFO     	 * (global step 10800: loss: 0.2785295732319355, lr: 5e-05
2023-12-21 00:24:22 INFO     	 * (global step 10850: loss: 0.28767021000385284, lr: 5e-05
2023-12-21 00:24:37 INFO     	 * (global step 10900: loss: 0.3095037639141083, lr: 5e-05
2023-12-21 00:24:53 INFO     	 * (global step 10950: loss: 0.19252237305045128, lr: 5e-05
2023-12-21 00:25:09 INFO     	 * (global step 11000: loss: 0.294195506721735, lr: 5e-05
2023-12-21 00:25:24 INFO     	 * (global step 11050: loss: 0.33553575351834297, lr: 5e-05
2023-12-21 00:25:40 INFO     	 * (global step 11100: loss: 0.24841664358973503, lr: 5e-05
2023-12-21 00:25:55 INFO     	 * (global step 11150: loss: 0.27065359801054, lr: 5e-05
2023-12-21 00:26:11 INFO     	 * (global step 11200: loss: 0.35781172662973404, lr: 5e-05
2023-12-21 00:26:27 INFO     	 * (global step 11250: loss: 0.3760751336812973, lr: 5e-05
2023-12-21 00:26:42 INFO     	 * (global step 11300: loss: 0.28993021696805954, lr: 5e-05
2023-12-21 00:26:58 INFO     	 * (global step 11350: loss: 0.30830442160367966, lr: 5e-05
2023-12-21 00:27:13 INFO     	 * (global step 11400: loss: 0.2578957751393318, lr: 5e-05
2023-12-21 00:27:29 INFO     	 * (global step 11450: loss: 0.2885126881301403, lr: 5e-05
2023-12-21 00:27:45 INFO     	 * (global step 11500: loss: 0.23274170979857445, lr: 5e-05
2023-12-21 00:28:00 INFO     	 * (global step 11550: loss: 0.3103047087788582, lr: 5e-05
2023-12-21 00:28:16 INFO     	 * (global step 11600: loss: 0.34550629928708076, lr: 5e-05
2023-12-21 00:28:32 INFO     	 * (global step 11650: loss: 0.3270331919193268, lr: 5e-05
2023-12-21 00:28:47 INFO     	 * (global step 11700: loss: 0.33892974629998207, lr: 5e-05
2023-12-21 00:29:03 INFO     	 * (global step 11750: loss: 0.29167998768389225, lr: 5e-05
2023-12-21 00:29:18 INFO     	 * (global step 11800: loss: 0.37399471551179886, lr: 5e-05
2023-12-21 00:29:20 INFO     [epoch 4/15] average loss: 0.309, lr: 5e-05
2023-12-21 00:29:20 INFO     saving model related files
2023-12-21 00:29:20 INFO     saving model
2023-12-21 00:29:21 INFO     saving tokenizer
2023-12-21 00:29:21 INFO     saving optimizer
2023-12-21 00:29:22 INFO     remove old optimizer files
2023-12-21 00:29:36 INFO     	 * (global step 11850: loss: 0.4198937714099884, lr: 5e-05
2023-12-21 00:29:51 INFO     	 * (global step 11900: loss: 0.32284823805093765, lr: 5e-05
2023-12-21 00:30:07 INFO     	 * (global step 11950: loss: 0.28115546330809593, lr: 5e-05
2023-12-21 00:30:23 INFO     	 * (global step 12000: loss: 0.31850820407271385, lr: 5e-05
2023-12-21 00:30:38 INFO     	 * (global step 12050: loss: 0.3059108406305313, lr: 5e-05
2023-12-21 00:30:54 INFO     	 * (global step 12100: loss: 0.3633512705564499, lr: 5e-05
2023-12-21 00:31:10 INFO     	 * (global step 12150: loss: 0.3675271198153496, lr: 5e-05
2023-12-21 00:31:25 INFO     	 * (global step 12200: loss: 0.3650164529681206, lr: 5e-05
2023-12-21 00:31:41 INFO     	 * (global step 12250: loss: 0.24695316329598427, lr: 5e-05
2023-12-21 00:31:56 INFO     	 * (global step 12300: loss: 0.3211098313331604, lr: 5e-05
2023-12-21 00:32:12 INFO     	 * (global step 12350: loss: 0.2533741854131222, lr: 5e-05
2023-12-21 00:32:28 INFO     	 * (global step 12400: loss: 0.32240036502480507, lr: 5e-05
2023-12-21 00:32:43 INFO     	 * (global step 12450: loss: 0.30683714896440506, lr: 5e-05
2023-12-21 00:32:59 INFO     	 * (global step 12500: loss: 0.23790408298373222, lr: 5e-05
2023-12-21 00:33:15 INFO     	 * (global step 12550: loss: 0.33307458460330963, lr: 5e-05
2023-12-21 00:33:30 INFO     	 * (global step 12600: loss: 0.3507738411426544, lr: 5e-05
2023-12-21 00:33:46 INFO     	 * (global step 12650: loss: 0.30041829869151115, lr: 5e-05
2023-12-21 00:34:02 INFO     	 * (global step 12700: loss: 0.3112109862267971, lr: 5e-05
2023-12-21 00:34:17 INFO     	 * (global step 12750: loss: 0.29021355882287025, lr: 5e-05
2023-12-21 00:34:33 INFO     	 * (global step 12800: loss: 0.33143506199121475, lr: 5e-05
2023-12-21 00:34:49 INFO     	 * (global step 12850: loss: 0.21908563748002052, lr: 5e-05
2023-12-21 00:35:04 INFO     	 * (global step 12900: loss: 0.30495844781398773, lr: 5e-05
2023-12-21 00:35:20 INFO     	 * (global step 12950: loss: 0.23991383984684944, lr: 5e-05
2023-12-21 00:35:36 INFO     	 * (global step 13000: loss: 0.36461129039525986, lr: 5e-05
2023-12-21 00:35:51 INFO     	 * (global step 13050: loss: 0.22625906392931938, lr: 5e-05
2023-12-21 00:36:07 INFO     	 * (global step 13100: loss: 0.29309912398457527, lr: 5e-05
2023-12-21 00:36:22 INFO     	 * (global step 13150: loss: 0.2731994092464447, lr: 5e-05
2023-12-21 00:36:38 INFO     	 * (global step 13200: loss: 0.23907078430056572, lr: 5e-05
2023-12-21 00:36:54 INFO     	 * (global step 13250: loss: 0.2948162965476513, lr: 5e-05
2023-12-21 00:37:09 INFO     	 * (global step 13300: loss: 0.37154215946793556, lr: 5e-05
2023-12-21 00:37:25 INFO     	 * (global step 13350: loss: 0.3064553774893284, lr: 5e-05
2023-12-21 00:37:41 INFO     	 * (global step 13400: loss: 0.3168192505836487, lr: 5e-05
2023-12-21 00:37:56 INFO     	 * (global step 13450: loss: 0.27452488243579865, lr: 5e-05
2023-12-21 00:38:12 INFO     	 * (global step 13500: loss: 0.23467260971665382, lr: 5e-05
2023-12-21 00:38:28 INFO     	 * (global step 13550: loss: 0.33797791600227356, lr: 5e-05
2023-12-21 00:38:43 INFO     	 * (global step 13600: loss: 0.3429400138556957, lr: 5e-05
2023-12-21 00:38:59 INFO     	 * (global step 13650: loss: 0.30835140869021416, lr: 5e-05
2023-12-21 00:39:15 INFO     	 * (global step 13700: loss: 0.3555232100188732, lr: 5e-05
2023-12-21 00:39:30 INFO     	 * (global step 13750: loss: 0.2004925273358822, lr: 5e-05
2023-12-21 00:39:46 INFO     	 * (global step 13800: loss: 0.3080192059278488, lr: 5e-05
2023-12-21 00:40:02 INFO     	 * (global step 13850: loss: 0.3306274637579918, lr: 5e-05
2023-12-21 00:40:17 INFO     	 * (global step 13900: loss: 0.33012203127145767, lr: 5e-05
2023-12-21 00:40:33 INFO     	 * (global step 13950: loss: 0.3836202882230282, lr: 5e-05
2023-12-21 00:40:49 INFO     	 * (global step 14000: loss: 0.3058270588517189, lr: 5e-05
2023-12-21 00:41:04 INFO     	 * (global step 14050: loss: 0.27988239005208015, lr: 5e-05
2023-12-21 00:41:20 INFO     	 * (global step 14100: loss: 0.3792308419942856, lr: 5e-05
2023-12-21 00:41:36 INFO     	 * (global step 14150: loss: 0.32991211116313934, lr: 5e-05
2023-12-21 00:41:41 INFO     [epoch 5/15] average loss: 0.302, lr: 5e-05
2023-12-21 00:41:41 INFO     saving model related files
2023-12-21 00:41:41 INFO     saving model
2023-12-21 00:41:41 INFO     saving tokenizer
2023-12-21 00:41:42 INFO     saving optimizer
2023-12-21 00:41:42 INFO     remove old optimizer files
2023-12-21 00:41:53 INFO     	 * (global step 14200: loss: 0.34708620607852936, lr: 5e-05
2023-12-21 00:42:09 INFO     	 * (global step 14250: loss: 0.355902723968029, lr: 5e-05
2023-12-21 00:42:25 INFO     	 * (global step 14300: loss: 0.3094586208462715, lr: 5e-05
2023-12-21 00:42:40 INFO     	 * (global step 14350: loss: 0.2845844626426697, lr: 5e-05
2023-12-21 00:42:56 INFO     	 * (global step 14400: loss: 0.361501794308424, lr: 5e-05
2023-12-21 00:43:11 INFO     	 * (global step 14450: loss: 0.2699129991233349, lr: 5e-05
2023-12-21 00:43:27 INFO     	 * (global step 14500: loss: 0.37498415634036064, lr: 5e-05
2023-12-21 00:43:43 INFO     	 * (global step 14550: loss: 0.3555586151778698, lr: 5e-05
2023-12-21 00:43:58 INFO     	 * (global step 14600: loss: 0.2837633416056633, lr: 5e-05
2023-12-21 00:44:14 INFO     	 * (global step 14650: loss: 0.3062523864209652, lr: 5e-05
2023-12-21 00:44:30 INFO     	 * (global step 14700: loss: 0.2506966069340706, lr: 5e-05
2023-12-21 00:44:46 INFO     	 * (global step 14750: loss: 0.3282434269785881, lr: 5e-05
2023-12-21 00:45:01 INFO     	 * (global step 14800: loss: 0.3292386308312416, lr: 5e-05
2023-12-21 00:45:17 INFO     	 * (global step 14850: loss: 0.26985981687903404, lr: 5e-05
2023-12-21 00:45:32 INFO     	 * (global step 14900: loss: 0.2484848089516163, lr: 5e-05
2023-12-21 00:45:48 INFO     	 * (global step 14950: loss: 0.378544881939888, lr: 5e-05
2023-12-21 00:46:04 INFO     	 * (global step 15000: loss: 0.3354713171720505, lr: 5e-05
2023-12-21 00:46:19 INFO     	 * (global step 15050: loss: 0.3079729564487934, lr: 5e-05
2023-12-21 00:46:35 INFO     	 * (global step 15100: loss: 0.30447205156087875, lr: 5e-05
2023-12-21 00:46:51 INFO     	 * (global step 15150: loss: 0.23880759999155998, lr: 5e-05
2023-12-21 00:47:06 INFO     	 * (global step 15200: loss: 0.36606520786881447, lr: 5e-05
2023-12-21 00:47:22 INFO     	 * (global step 15250: loss: 0.19925419986248016, lr: 5e-05
2023-12-21 00:47:38 INFO     	 * (global step 15300: loss: 0.22594623267650604, lr: 5e-05
2023-12-21 00:47:53 INFO     	 * (global step 15350: loss: 0.21407213807106018, lr: 5e-05
2023-12-21 00:48:09 INFO     	 * (global step 15400: loss: 0.30033913999795914, lr: 5e-05
2023-12-21 00:48:25 INFO     	 * (global step 15450: loss: 0.2889576554298401, lr: 5e-05
2023-12-21 00:48:40 INFO     	 * (global step 15500: loss: 0.3175494894385338, lr: 5e-05
2023-12-21 00:48:56 INFO     	 * (global step 15550: loss: 0.2433236576616764, lr: 5e-05
2023-12-21 00:49:12 INFO     	 * (global step 15600: loss: 0.27153845876455307, lr: 5e-05
2023-12-21 00:49:27 INFO     	 * (global step 15650: loss: 0.30535734444856644, lr: 5e-05
2023-12-21 00:49:43 INFO     	 * (global step 15700: loss: 0.2716434858739376, lr: 5e-05
2023-12-21 00:49:58 INFO     	 * (global step 15750: loss: 0.297977551817894, lr: 5e-05
2023-12-21 00:50:14 INFO     	 * (global step 15800: loss: 0.2768876701593399, lr: 5e-05
2023-12-21 00:50:30 INFO     	 * (global step 15850: loss: 0.352714866399765, lr: 5e-05
2023-12-21 00:50:45 INFO     	 * (global step 15900: loss: 0.3090793564915657, lr: 5e-05
2023-12-21 00:51:01 INFO     	 * (global step 15950: loss: 0.2544538900256157, lr: 5e-05
2023-12-21 00:51:17 INFO     	 * (global step 16000: loss: 0.2205713428556919, lr: 5e-05
2023-12-21 00:51:32 INFO     	 * (global step 16050: loss: 0.2487938404083252, lr: 5e-05
2023-12-21 00:51:48 INFO     	 * (global step 16100: loss: 0.3069967105984688, lr: 5e-05
2023-12-21 00:52:04 INFO     	 * (global step 16150: loss: 0.2852795720100403, lr: 5e-05
2023-12-21 00:52:19 INFO     	 * (global step 16200: loss: 0.24248698353767395, lr: 5e-05
2023-12-21 00:52:35 INFO     	 * (global step 16250: loss: 0.26915414445102215, lr: 5e-05
2023-12-21 00:52:51 INFO     	 * (global step 16300: loss: 0.29229558259248734, lr: 5e-05
2023-12-21 00:53:06 INFO     	 * (global step 16350: loss: 0.3236349746584892, lr: 5e-05
2023-12-21 00:53:22 INFO     	 * (global step 16400: loss: 0.22505223751068115, lr: 5e-05
2023-12-21 00:53:38 INFO     	 * (global step 16450: loss: 0.34188756346702576, lr: 5e-05
2023-12-21 00:53:53 INFO     	 * (global step 16500: loss: 0.22881494835019112, lr: 5e-05
2023-12-21 00:54:02 INFO     [epoch 6/15] average loss: 0.296, lr: 5e-05
2023-12-21 00:54:02 INFO     saving model related files
2023-12-21 00:54:02 INFO     saving model
2023-12-21 00:54:02 INFO     saving tokenizer
2023-12-21 00:54:02 INFO     saving optimizer
2023-12-21 00:54:03 INFO     remove old optimizer files
2023-12-21 00:54:11 INFO     	 * (global step 16550: loss: 0.3332788273692131, lr: 5e-05
2023-12-21 00:54:26 INFO     	 * (global step 16600: loss: 0.21914240717887878, lr: 5e-05
2023-12-21 00:54:42 INFO     	 * (global step 16650: loss: 0.26401279121637344, lr: 5e-05
2023-12-21 00:54:57 INFO     	 * (global step 16700: loss: 0.305047944188118, lr: 5e-05
2023-12-21 00:55:13 INFO     	 * (global step 16750: loss: 0.32921259105205536, lr: 5e-05
2023-12-21 00:55:29 INFO     	 * (global step 16800: loss: 0.3341184929013252, lr: 5e-05
2023-12-21 00:55:44 INFO     	 * (global step 16850: loss: 0.27244726568460464, lr: 5e-05
2023-12-21 00:56:00 INFO     	 * (global step 16900: loss: 0.2586113251745701, lr: 5e-05
2023-12-21 00:56:16 INFO     	 * (global step 16950: loss: 0.3693969026207924, lr: 5e-05
2023-12-21 00:56:31 INFO     	 * (global step 17000: loss: 0.25422706082463264, lr: 5e-05
2023-12-21 00:56:47 INFO     	 * (global step 17050: loss: 0.3769301176071167, lr: 5e-05
2023-12-21 00:57:02 INFO     	 * (global step 17100: loss: 0.43110817670822144, lr: 5e-05
2023-12-21 00:57:18 INFO     	 * (global step 17150: loss: 0.2782127521932125, lr: 5e-05
2023-12-21 00:57:34 INFO     	 * (global step 17200: loss: 0.22771378606557846, lr: 5e-05
2023-12-21 00:57:49 INFO     	 * (global step 17250: loss: 0.349971242249012, lr: 5e-05
2023-12-21 00:58:05 INFO     	 * (global step 17300: loss: 0.3075163662433624, lr: 5e-05
2023-12-21 00:58:21 INFO     	 * (global step 17350: loss: 0.4348018653690815, lr: 5e-05
2023-12-21 00:58:36 INFO     	 * (global step 17400: loss: 0.27077094092965126, lr: 5e-05
2023-12-21 00:58:52 INFO     	 * (global step 17450: loss: 0.2371828928589821, lr: 5e-05
2023-12-21 00:59:08 INFO     	 * (global step 17500: loss: 0.27151164785027504, lr: 5e-05
2023-12-21 00:59:23 INFO     	 * (global step 17550: loss: 0.27553149685263634, lr: 5e-05
2023-12-21 00:59:39 INFO     	 * (global step 17600: loss: 0.298464123159647, lr: 5e-05
2023-12-21 00:59:55 INFO     	 * (global step 17650: loss: 0.2938249818980694, lr: 5e-05
2023-12-21 01:00:10 INFO     	 * (global step 17700: loss: 0.33981630951166153, lr: 5e-05
2023-12-21 01:00:26 INFO     	 * (global step 17750: loss: 0.22556887939572334, lr: 5e-05
2023-12-21 01:00:41 INFO     	 * (global step 17800: loss: 0.24098018929362297, lr: 5e-05
2023-12-21 01:00:57 INFO     	 * (global step 17850: loss: 0.2657959833741188, lr: 5e-05
2023-12-21 01:01:13 INFO     	 * (global step 17900: loss: 0.2483506202697754, lr: 5e-05
2023-12-21 01:01:28 INFO     	 * (global step 17950: loss: 0.2932905741035938, lr: 5e-05
2023-12-21 01:01:44 INFO     	 * (global step 18000: loss: 0.294330470263958, lr: 5e-05
2023-12-21 01:02:00 INFO     	 * (global step 18050: loss: 0.29036662727594376, lr: 5e-05
2023-12-21 01:02:15 INFO     	 * (global step 18100: loss: 0.25094788521528244, lr: 5e-05
2023-12-21 01:02:31 INFO     	 * (global step 18150: loss: 0.30205874145030975, lr: 5e-05
2023-12-21 01:02:46 INFO     	 * (global step 18200: loss: 0.28848841041326523, lr: 5e-05
2023-12-21 01:03:02 INFO     	 * (global step 18250: loss: 0.18127499893307686, lr: 5e-05
2023-12-21 01:03:18 INFO     	 * (global step 18300: loss: 0.36036186665296555, lr: 5e-05
2023-12-21 01:03:33 INFO     	 * (global step 18350: loss: 0.42917755991220474, lr: 5e-05
2023-12-21 01:03:49 INFO     	 * (global step 18400: loss: 0.26024962589144707, lr: 5e-05
2023-12-21 01:04:05 INFO     	 * (global step 18450: loss: 0.19666902348399162, lr: 5e-05
2023-12-21 01:04:20 INFO     	 * (global step 18500: loss: 0.3647560141980648, lr: 5e-05
2023-12-21 01:04:36 INFO     	 * (global step 18550: loss: 0.19484606571495533, lr: 5e-05
2023-12-21 01:04:51 INFO     	 * (global step 18600: loss: 0.259154312312603, lr: 5e-05
2023-12-21 01:05:07 INFO     	 * (global step 18650: loss: 0.2436097338795662, lr: 5e-05
2023-12-21 01:05:23 INFO     	 * (global step 18700: loss: 0.3912576250731945, lr: 5e-05
2023-12-21 01:05:38 INFO     	 * (global step 18750: loss: 0.4306974485516548, lr: 5e-05
2023-12-21 01:05:54 INFO     	 * (global step 18800: loss: 0.27712975814938545, lr: 5e-05
2023-12-21 01:06:09 INFO     	 * (global step 18850: loss: 0.2012652289122343, lr: 5e-05
2023-12-21 01:06:21 INFO     [epoch 7/15] average loss: 0.29, lr: 5e-05
2023-12-21 01:06:21 INFO     saving model related files
2023-12-21 01:06:21 INFO     saving model
2023-12-21 01:06:22 INFO     saving tokenizer
2023-12-21 01:06:22 INFO     saving optimizer
2023-12-21 01:06:23 INFO     remove old optimizer files
2023-12-21 01:06:27 INFO     	 * (global step 18900: loss: 0.31522858887910843, lr: 5e-05
2023-12-21 01:06:42 INFO     	 * (global step 18950: loss: 0.31784674897789955, lr: 5e-05
2023-12-21 01:06:58 INFO     	 * (global step 19000: loss: 0.25442985445261, lr: 5e-05
2023-12-21 01:07:14 INFO     	 * (global step 19050: loss: 0.2863067500293255, lr: 5e-05
2023-12-21 01:07:29 INFO     	 * (global step 19100: loss: 0.26225122809410095, lr: 5e-05
2023-12-21 01:07:45 INFO     	 * (global step 19150: loss: 0.2771754749119282, lr: 5e-05
2023-12-21 01:08:01 INFO     	 * (global step 19200: loss: 0.31742892041802406, lr: 5e-05
2023-12-21 01:08:16 INFO     	 * (global step 19250: loss: 0.26099810004234314, lr: 5e-05
2023-12-21 01:08:32 INFO     	 * (global step 19300: loss: 0.2445296123623848, lr: 5e-05
2023-12-21 01:08:47 INFO     	 * (global step 19350: loss: 0.25448114797472954, lr: 5e-05
2023-12-21 01:09:03 INFO     	 * (global step 19400: loss: 0.2822481133043766, lr: 5e-05
2023-12-21 01:09:19 INFO     	 * (global step 19450: loss: 0.2867617830634117, lr: 5e-05
2023-12-21 01:09:34 INFO     	 * (global step 19500: loss: 0.4169023707509041, lr: 5e-05
2023-12-21 01:09:50 INFO     	 * (global step 19550: loss: 0.22852012142539024, lr: 5e-05
2023-12-21 01:10:05 INFO     	 * (global step 19600: loss: 0.2527734488248825, lr: 5e-05
2023-12-21 01:10:21 INFO     	 * (global step 19650: loss: 0.2542137913405895, lr: 5e-05
2023-12-21 01:10:37 INFO     	 * (global step 19700: loss: 0.3552641160786152, lr: 5e-05
2023-12-21 01:10:52 INFO     	 * (global step 19750: loss: 0.21637441217899323, lr: 5e-05
2023-12-21 01:11:08 INFO     	 * (global step 19800: loss: 0.261707354336977, lr: 5e-05
2023-12-21 01:11:24 INFO     	 * (global step 19850: loss: 0.28256040066480637, lr: 5e-05
2023-12-21 01:11:39 INFO     	 * (global step 19900: loss: 0.25934872031211853, lr: 5e-05
2023-12-21 01:11:55 INFO     	 * (global step 19950: loss: 0.2923159673810005, lr: 5e-05
2023-12-21 01:12:11 INFO     	 * (global step 20000: loss: 0.2590944580733776, lr: 5e-05
2023-12-21 01:12:26 INFO     	 * (global step 20050: loss: 0.2439232524484396, lr: 5e-05
2023-12-21 01:12:42 INFO     	 * (global step 20100: loss: 0.30088450759649277, lr: 5e-05
2023-12-21 01:12:58 INFO     	 * (global step 20150: loss: 0.2813235744833946, lr: 5e-05
2023-12-21 01:13:13 INFO     	 * (global step 20200: loss: 0.4888905081897974, lr: 5e-05
2023-12-21 01:13:29 INFO     	 * (global step 20250: loss: 0.3121199868619442, lr: 5e-05
2023-12-21 01:13:45 INFO     	 * (global step 20300: loss: 0.29503031447529793, lr: 5e-05
2023-12-21 01:14:00 INFO     	 * (global step 20350: loss: 0.29116661474108696, lr: 5e-05
2023-12-21 01:14:16 INFO     	 * (global step 20400: loss: 0.23796845972537994, lr: 5e-05
2023-12-21 01:14:31 INFO     	 * (global step 20450: loss: 0.3552364185452461, lr: 5e-05
2023-12-21 01:14:47 INFO     	 * (global step 20500: loss: 0.2093338705599308, lr: 5e-05
2023-12-21 01:15:03 INFO     	 * (global step 20550: loss: 0.2990562133491039, lr: 5e-05
2023-12-21 01:15:18 INFO     	 * (global step 20600: loss: 0.3138859122991562, lr: 5e-05
2023-12-21 01:15:34 INFO     	 * (global step 20650: loss: 0.25705957412719727, lr: 5e-05
2023-12-21 01:15:50 INFO     	 * (global step 20700: loss: 0.2916560433804989, lr: 5e-05
2023-12-21 01:16:05 INFO     	 * (global step 20750: loss: 0.2877148613333702, lr: 5e-05
2023-12-21 01:16:21 INFO     	 * (global step 20800: loss: 0.3189797066152096, lr: 5e-05
2023-12-21 01:16:37 INFO     	 * (global step 20850: loss: 0.30670609697699547, lr: 5e-05
2023-12-21 01:16:52 INFO     	 * (global step 20900: loss: 0.3294382765889168, lr: 5e-05
2023-12-21 01:17:08 INFO     	 * (global step 20950: loss: 0.29823708161711693, lr: 5e-05
2023-12-21 01:17:23 INFO     	 * (global step 21000: loss: 0.2910183221101761, lr: 5e-05
2023-12-21 01:17:39 INFO     	 * (global step 21050: loss: 0.21929924376308918, lr: 5e-05
2023-12-21 01:17:55 INFO     	 * (global step 21100: loss: 0.2310165911912918, lr: 5e-05
2023-12-21 01:18:10 INFO     	 * (global step 21150: loss: 0.2571440078318119, lr: 5e-05
2023-12-21 01:18:26 INFO     	 * (global step 21200: loss: 0.31921274960041046, lr: 5e-05
2023-12-21 01:18:41 INFO     [epoch 8/15] average loss: 0.286, lr: 5e-05
2023-12-21 01:18:41 INFO     saving model related files
2023-12-21 01:18:41 INFO     saving model
2023-12-21 01:18:42 INFO     saving tokenizer
2023-12-21 01:18:42 INFO     saving optimizer
2023-12-21 01:18:43 INFO     remove old optimizer files
2023-12-21 01:18:43 INFO     	 * (global step 21250: loss: 0.33395687490701675, lr: 5e-05
2023-12-21 01:18:59 INFO     	 * (global step 21300: loss: 0.23711403831839561, lr: 5e-05
2023-12-21 01:19:14 INFO     	 * (global step 21350: loss: 0.27659550681710243, lr: 5e-05
2023-12-21 01:19:30 INFO     	 * (global step 21400: loss: 0.33254310861229897, lr: 5e-05
2023-12-21 01:19:46 INFO     	 * (global step 21450: loss: 0.3417467996478081, lr: 5e-05
2023-12-21 01:20:01 INFO     	 * (global step 21500: loss: 0.26344985514879227, lr: 5e-05
2023-12-21 01:20:17 INFO     	 * (global step 21550: loss: 0.30875182524323463, lr: 5e-05
2023-12-21 01:20:33 INFO     	 * (global step 21600: loss: 0.24385585263371468, lr: 5e-05
2023-12-21 01:20:49 INFO     	 * (global step 21650: loss: 0.2945130094885826, lr: 5e-05
2023-12-21 01:21:04 INFO     	 * (global step 21700: loss: 0.2770574279129505, lr: 5e-05
2023-12-21 01:21:20 INFO     	 * (global step 21750: loss: 0.23033160343766212, lr: 5e-05
2023-12-21 01:21:36 INFO     	 * (global step 21800: loss: 0.40226369723677635, lr: 5e-05
2023-12-21 01:21:52 INFO     	 * (global step 21850: loss: 0.28591494634747505, lr: 5e-05
2023-12-21 01:22:07 INFO     	 * (global step 21900: loss: 0.38155315816402435, lr: 5e-05
2023-12-21 01:22:23 INFO     	 * (global step 21950: loss: 0.39782511070370674, lr: 5e-05
2023-12-21 01:22:39 INFO     	 * (global step 22000: loss: 0.3343198783695698, lr: 5e-05
2023-12-21 01:22:55 INFO     	 * (global step 22050: loss: 0.3563004806637764, lr: 5e-05
2023-12-21 01:23:11 INFO     	 * (global step 22100: loss: 0.25434835255146027, lr: 5e-05
2023-12-21 01:23:26 INFO     	 * (global step 22150: loss: 0.2651328183710575, lr: 5e-05
2023-12-21 01:23:42 INFO     	 * (global step 22200: loss: 0.19288185983896255, lr: 5e-05
2023-12-21 01:23:58 INFO     	 * (global step 22250: loss: 0.2965237647294998, lr: 5e-05
2023-12-21 01:24:14 INFO     	 * (global step 22300: loss: 0.27994126081466675, lr: 5e-05
2023-12-21 01:24:29 INFO     	 * (global step 22350: loss: 0.33377545326948166, lr: 5e-05
2023-12-21 01:24:45 INFO     	 * (global step 22400: loss: 0.31209656596183777, lr: 5e-05
2023-12-21 01:25:01 INFO     	 * (global step 22450: loss: 0.25131356343626976, lr: 5e-05
2023-12-21 01:25:17 INFO     	 * (global step 22500: loss: 0.21964603289961815, lr: 5e-05
2023-12-21 01:25:33 INFO     	 * (global step 22550: loss: 0.31837113574147224, lr: 5e-05
2023-12-21 01:25:48 INFO     	 * (global step 22600: loss: 0.34215956553816795, lr: 5e-05
2023-12-21 01:26:04 INFO     	 * (global step 22650: loss: 0.28082234784960747, lr: 5e-05
2023-12-21 01:26:20 INFO     	 * (global step 22700: loss: 0.2511562407016754, lr: 5e-05
2023-12-21 01:26:35 INFO     	 * (global step 22750: loss: 0.31337499618530273, lr: 5e-05
2023-12-21 01:26:51 INFO     	 * (global step 22800: loss: 0.24498648941516876, lr: 5e-05
2023-12-21 01:27:07 INFO     	 * (global step 22850: loss: 0.3019307777285576, lr: 5e-05
2023-12-21 01:27:23 INFO     	 * (global step 22900: loss: 0.250364288687706, lr: 5e-05
2023-12-21 01:27:38 INFO     	 * (global step 22950: loss: 0.32012490183115005, lr: 5e-05
2023-12-21 01:27:54 INFO     	 * (global step 23000: loss: 0.2756831720471382, lr: 5e-05
2023-12-21 01:28:10 INFO     	 * (global step 23050: loss: 0.24425215646624565, lr: 5e-05
2023-12-21 01:28:26 INFO     	 * (global step 23100: loss: 0.3311324939131737, lr: 5e-05
2023-12-21 01:28:42 INFO     	 * (global step 23150: loss: 0.3270963951945305, lr: 5e-05
2023-12-21 01:28:57 INFO     	 * (global step 23200: loss: 0.2952475845813751, lr: 5e-05
2023-12-21 01:29:13 INFO     	 * (global step 23250: loss: 0.26182615384459496, lr: 5e-05
2023-12-21 01:29:29 INFO     	 * (global step 23300: loss: 0.23730820044875145, lr: 5e-05
2023-12-21 01:29:45 INFO     	 * (global step 23350: loss: 0.302082397043705, lr: 5e-05
2023-12-21 01:30:01 INFO     	 * (global step 23400: loss: 0.3047582246363163, lr: 5e-05
2023-12-21 01:30:16 INFO     	 * (global step 23450: loss: 0.2633514478802681, lr: 5e-05
2023-12-21 01:30:32 INFO     	 * (global step 23500: loss: 0.2646372877061367, lr: 5e-05
2023-12-21 01:30:48 INFO     	 * (global step 23550: loss: 0.34476790949702263, lr: 5e-05
2023-12-21 01:31:04 INFO     	 * (global step 23600: loss: 0.23359578102827072, lr: 5e-05
2023-12-21 01:31:07 INFO     [epoch 9/15] average loss: 0.282, lr: 5e-05
2023-12-21 01:31:07 INFO     saving model related files
2023-12-21 01:31:07 INFO     saving model
2023-12-21 01:31:08 INFO     saving tokenizer
2023-12-21 01:31:08 INFO     saving optimizer
2023-12-21 01:31:08 INFO     remove old optimizer files
2023-12-21 01:31:09 INFO     complete training: model ckpt was saved at small_combined_trained_ckpt/model_sdkaaa
2023-12-21 01:31:09 INFO     ## 1st RUN: Configuration 7/12 ##
2023-12-21 01:31:09 INFO     initialize model trainer
2023-12-21 01:31:09 INFO     initialize checkpoint at small_combined_trained_ckpt/model_uramvg
2023-12-21 01:31:09 INFO     hyperparameters
2023-12-21 01:31:09 INFO     	 * dataset_path: StellarMilk/squad_newsqa
2023-12-21 01:31:09 INFO     	 * dataset_name: default
2023-12-21 01:31:09 INFO     	 * input_types: ['paragraph']
2023-12-21 01:31:09 INFO     	 * output_types: ['questions_answers']
2023-12-21 01:31:09 INFO     	 * prefix_types: ['qag']
2023-12-21 01:31:09 INFO     	 * model: t5-small
2023-12-21 01:31:09 INFO     	 * max_length: 512
2023-12-21 01:31:09 INFO     	 * max_length_output: 512
2023-12-21 01:31:09 INFO     	 * epoch: 15
2023-12-21 01:31:09 INFO     	 * batch: 2
2023-12-21 01:31:09 INFO     	 * lr: 5e-05
2023-12-21 01:31:09 INFO     	 * fp16: False
2023-12-21 01:31:09 INFO     	 * random_seed: 1
2023-12-21 01:31:09 INFO     	 * gradient_accumulation_steps: 2
2023-12-21 01:31:09 INFO     	 * label_smoothing: 0.0
2023-12-21 01:31:09 INFO     initialize checkpoint with t5-small
2023-12-21 01:31:11 INFO     use spaCy answer extraction model: positionrank
2023-12-21 01:31:11 INFO     Model `t5-small`
2023-12-21 01:31:11 INFO     	 * Num of GPU in use: 1
2023-12-21 01:31:11 INFO     	 * Prefix: True
2023-12-21 01:31:11 INFO     	 * Language: en (ignore at the training phase)
2023-12-21 01:31:11 INFO     dataset preprocessing
2023-12-21 01:31:13 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/squad_newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-21 01:31:15 INFO     start model training
2023-12-21 01:31:23 INFO     	 * (global step 50: loss: 1.4694690108299255, lr: 5e-05
2023-12-21 01:31:31 INFO     	 * (global step 100: loss: 0.8707203269004822, lr: 5e-05
2023-12-21 01:31:39 INFO     	 * (global step 150: loss: 0.8047786951065063, lr: 5e-05
2023-12-21 01:31:47 INFO     	 * (global step 200: loss: 0.664919376373291, lr: 5e-05
2023-12-21 01:31:55 INFO     	 * (global step 250: loss: 0.500159814953804, lr: 5e-05
2023-12-21 01:32:03 INFO     	 * (global step 300: loss: 0.5390867739915848, lr: 5e-05
2023-12-21 01:32:11 INFO     	 * (global step 350: loss: 0.5248388051986694, lr: 5e-05
2023-12-21 01:32:19 INFO     	 * (global step 400: loss: 0.5038136690855026, lr: 5e-05
2023-12-21 01:32:28 INFO     	 * (global step 450: loss: 0.4992511421442032, lr: 5e-05
2023-12-21 01:32:36 INFO     	 * (global step 500: loss: 0.5699698626995087, lr: 5e-05
2023-12-21 01:32:44 INFO     	 * (global step 550: loss: 0.5779638290405273, lr: 5e-05
2023-12-21 01:32:52 INFO     	 * (global step 600: loss: 0.3393259048461914, lr: 5e-05
2023-12-21 01:33:00 INFO     	 * (global step 650: loss: 0.4055522084236145, lr: 5e-05
2023-12-21 01:33:08 INFO     	 * (global step 700: loss: 0.5606154501438141, lr: 5e-05
2023-12-21 01:33:16 INFO     	 * (global step 750: loss: 0.3992961049079895, lr: 5e-05
2023-12-21 01:33:24 INFO     	 * (global step 800: loss: 0.378390833735466, lr: 5e-05
2023-12-21 01:33:33 INFO     	 * (global step 850: loss: 0.2946271598339081, lr: 5e-05
2023-12-21 01:33:41 INFO     	 * (global step 900: loss: 0.415529265999794, lr: 5e-05
2023-12-21 01:33:49 INFO     	 * (global step 950: loss: 0.3692760318517685, lr: 5e-05
2023-12-21 01:33:57 INFO     	 * (global step 1000: loss: 0.340553879737854, lr: 5e-05
2023-12-21 01:34:05 INFO     	 * (global step 1050: loss: 0.3775583356618881, lr: 5e-05
2023-12-21 01:34:13 INFO     	 * (global step 1100: loss: 0.48937974870204926, lr: 5e-05
2023-12-21 01:34:21 INFO     	 * (global step 1150: loss: 0.36847984790802, lr: 5e-05
2023-12-21 01:34:29 INFO     	 * (global step 1200: loss: 0.4846610873937607, lr: 5e-05
2023-12-21 01:34:38 INFO     	 * (global step 1250: loss: 0.4859403520822525, lr: 5e-05
2023-12-21 01:34:46 INFO     	 * (global step 1300: loss: 0.35467664897441864, lr: 5e-05
2023-12-21 01:34:54 INFO     	 * (global step 1350: loss: 0.46001939475536346, lr: 5e-05
2023-12-21 01:35:02 INFO     	 * (global step 1400: loss: 0.4172058403491974, lr: 5e-05
2023-12-21 01:35:10 INFO     	 * (global step 1450: loss: 0.4030950963497162, lr: 5e-05
2023-12-21 01:35:18 INFO     	 * (global step 1500: loss: 0.382183775305748, lr: 5e-05
2023-12-21 01:35:26 INFO     	 * (global step 1550: loss: 0.34076617658138275, lr: 5e-05
2023-12-21 01:35:34 INFO     	 * (global step 1600: loss: 0.39296895265579224, lr: 5e-05
2023-12-21 01:35:43 INFO     	 * (global step 1650: loss: 0.3524627387523651, lr: 5e-05
2023-12-21 01:35:51 INFO     	 * (global step 1700: loss: 0.4165772348642349, lr: 5e-05
2023-12-21 01:35:59 INFO     	 * (global step 1750: loss: 0.30743008852005005, lr: 5e-05
2023-12-21 01:36:07 INFO     	 * (global step 1800: loss: 0.34543491899967194, lr: 5e-05
2023-12-21 01:36:15 INFO     	 * (global step 1850: loss: 0.5098693817853928, lr: 5e-05
2023-12-21 01:36:23 INFO     	 * (global step 1900: loss: 0.27542661130428314, lr: 5e-05
2023-12-21 01:36:31 INFO     	 * (global step 1950: loss: 0.36366724967956543, lr: 5e-05
2023-12-21 01:36:39 INFO     	 * (global step 2000: loss: 0.3644634038209915, lr: 5e-05
2023-12-21 01:36:48 INFO     	 * (global step 2050: loss: 0.36361198127269745, lr: 5e-05
2023-12-21 01:36:56 INFO     	 * (global step 2100: loss: 0.32571449875831604, lr: 5e-05
2023-12-21 01:37:04 INFO     	 * (global step 2150: loss: 0.44484764337539673, lr: 5e-05
2023-12-21 01:37:12 INFO     	 * (global step 2200: loss: 0.37773069739341736, lr: 5e-05
2023-12-21 01:37:20 INFO     	 * (global step 2250: loss: 0.418443962931633, lr: 5e-05
2023-12-21 01:37:28 INFO     	 * (global step 2300: loss: 0.3778160363435745, lr: 5e-05
2023-12-21 01:37:36 INFO     	 * (global step 2350: loss: 0.31284213066101074, lr: 5e-05
2023-12-21 01:37:45 INFO     	 * (global step 2400: loss: 0.4032820612192154, lr: 5e-05
2023-12-21 01:37:53 INFO     	 * (global step 2450: loss: 0.29708340764045715, lr: 5e-05
2023-12-21 01:38:01 INFO     	 * (global step 2500: loss: 0.3312201276421547, lr: 5e-05
2023-12-21 01:38:09 INFO     	 * (global step 2550: loss: 0.6520441621541977, lr: 5e-05
2023-12-21 01:38:17 INFO     	 * (global step 2600: loss: 0.4568774998188019, lr: 5e-05
2023-12-21 01:38:25 INFO     	 * (global step 2650: loss: 0.4517098069190979, lr: 5e-05
2023-12-21 01:38:34 INFO     	 * (global step 2700: loss: 0.28790129721164703, lr: 5e-05
2023-12-21 01:38:42 INFO     	 * (global step 2750: loss: 0.43668732047080994, lr: 5e-05
2023-12-21 01:38:50 INFO     	 * (global step 2800: loss: 0.3940373957157135, lr: 5e-05
2023-12-21 01:38:58 INFO     	 * (global step 2850: loss: 0.24999267607927322, lr: 5e-05
2023-12-21 01:39:06 INFO     	 * (global step 2900: loss: 0.3732480853796005, lr: 5e-05
2023-12-21 01:39:14 INFO     	 * (global step 2950: loss: 0.2084834799170494, lr: 5e-05
2023-12-21 01:39:22 INFO     	 * (global step 3000: loss: 0.5849083214998245, lr: 5e-05
2023-12-21 01:39:30 INFO     	 * (global step 3050: loss: 0.34100954234600067, lr: 5e-05
2023-12-21 01:39:39 INFO     	 * (global step 3100: loss: 0.3463921993970871, lr: 5e-05
2023-12-21 01:39:47 INFO     	 * (global step 3150: loss: 0.3381357043981552, lr: 5e-05
2023-12-21 01:39:55 INFO     	 * (global step 3200: loss: 0.30213961005210876, lr: 5e-05
2023-12-21 01:40:03 INFO     	 * (global step 3250: loss: 0.3623880445957184, lr: 5e-05
2023-12-21 01:40:11 INFO     	 * (global step 3300: loss: 0.49386948347091675, lr: 5e-05
2023-12-21 01:40:19 INFO     	 * (global step 3350: loss: 0.3345247209072113, lr: 5e-05
2023-12-21 01:40:27 INFO     	 * (global step 3400: loss: 0.3475605547428131, lr: 5e-05
2023-12-21 01:40:35 INFO     	 * (global step 3450: loss: 0.41429637372493744, lr: 5e-05
2023-12-21 01:40:44 INFO     	 * (global step 3500: loss: 0.32043714821338654, lr: 5e-05
2023-12-21 01:40:52 INFO     	 * (global step 3550: loss: 0.4900576323270798, lr: 5e-05
2023-12-21 01:41:00 INFO     	 * (global step 3600: loss: 0.4014546126127243, lr: 5e-05
2023-12-21 01:41:08 INFO     	 * (global step 3650: loss: 0.4249327927827835, lr: 5e-05
2023-12-21 01:41:16 INFO     	 * (global step 3700: loss: 0.32871130108833313, lr: 5e-05
2023-12-21 01:41:24 INFO     	 * (global step 3750: loss: 0.33325787633657455, lr: 5e-05
2023-12-21 01:41:32 INFO     	 * (global step 3800: loss: 0.32825006544589996, lr: 5e-05
2023-12-21 01:41:40 INFO     	 * (global step 3850: loss: 0.2005608007311821, lr: 5e-05
2023-12-21 01:41:49 INFO     	 * (global step 3900: loss: 0.41273950040340424, lr: 5e-05
2023-12-21 01:41:57 INFO     	 * (global step 3950: loss: 0.2732807621359825, lr: 5e-05
2023-12-21 01:42:05 INFO     	 * (global step 4000: loss: 0.24826553463935852, lr: 5e-05
2023-12-21 01:42:13 INFO     	 * (global step 4050: loss: 0.24983343482017517, lr: 5e-05
2023-12-21 01:42:21 INFO     	 * (global step 4100: loss: 0.33397457003593445, lr: 5e-05
2023-12-21 01:42:29 INFO     	 * (global step 4150: loss: 0.2798596918582916, lr: 5e-05
2023-12-21 01:42:37 INFO     	 * (global step 4200: loss: 0.2940416857600212, lr: 5e-05
2023-12-21 01:42:45 INFO     	 * (global step 4250: loss: 0.37184572219848633, lr: 5e-05
2023-12-21 01:42:54 INFO     	 * (global step 4300: loss: 0.4492526799440384, lr: 5e-05
2023-12-21 01:43:02 INFO     	 * (global step 4350: loss: 0.2766614258289337, lr: 5e-05
2023-12-21 01:43:10 INFO     	 * (global step 4400: loss: 0.4024282395839691, lr: 5e-05
2023-12-21 01:43:18 INFO     	 * (global step 4450: loss: 0.4205382764339447, lr: 5e-05
2023-12-21 01:43:26 INFO     	 * (global step 4500: loss: 0.2828887104988098, lr: 5e-05
2023-12-21 01:43:34 INFO     	 * (global step 4550: loss: 0.30217444896698, lr: 5e-05
2023-12-21 01:43:42 INFO     	 * (global step 4600: loss: 0.3395213335752487, lr: 5e-05
2023-12-21 01:43:51 INFO     	 * (global step 4650: loss: 0.38997337222099304, lr: 5e-05
2023-12-21 01:43:59 INFO     	 * (global step 4700: loss: 0.3370988443493843, lr: 5e-05
2023-12-21 01:44:02 INFO     [epoch 0/15] average loss: 0.449, lr: 5e-05
2023-12-21 01:44:02 INFO     saving model related files
2023-12-21 01:44:02 INFO     saving model
2023-12-21 01:44:03 INFO     saving tokenizer
2023-12-21 01:44:03 INFO     saving optimizer
2023-12-21 01:44:04 INFO     remove old optimizer files
2023-12-21 01:44:09 INFO     	 * (global step 4750: loss: 0.4262048304080963, lr: 5e-05
2023-12-21 01:44:17 INFO     	 * (global step 4800: loss: 0.32191338390111923, lr: 5e-05
2023-12-21 01:44:25 INFO     	 * (global step 4850: loss: 0.2786779850721359, lr: 5e-05
2023-12-21 01:44:33 INFO     	 * (global step 4900: loss: 0.40097402036190033, lr: 5e-05
2023-12-21 01:44:41 INFO     	 * (global step 4950: loss: 0.3491165488958359, lr: 5e-05
2023-12-21 01:44:49 INFO     	 * (global step 5000: loss: 0.23553843051195145, lr: 5e-05
2023-12-21 01:44:57 INFO     	 * (global step 5050: loss: 0.3872740566730499, lr: 5e-05
2023-12-21 01:45:06 INFO     	 * (global step 5100: loss: 0.33547544479370117, lr: 5e-05
2023-12-21 01:45:14 INFO     	 * (global step 5150: loss: 0.25715454667806625, lr: 5e-05
2023-12-21 01:45:22 INFO     	 * (global step 5200: loss: 0.3213687837123871, lr: 5e-05
2023-12-21 01:45:30 INFO     	 * (global step 5250: loss: 0.25105030834674835, lr: 5e-05
2023-12-21 01:45:38 INFO     	 * (global step 5300: loss: 0.3489900827407837, lr: 5e-05
2023-12-21 01:45:46 INFO     	 * (global step 5350: loss: 0.41978152096271515, lr: 5e-05
2023-12-21 01:45:54 INFO     	 * (global step 5400: loss: 0.3931209444999695, lr: 5e-05
2023-12-21 01:46:03 INFO     	 * (global step 5450: loss: 0.6372427344322205, lr: 5e-05
2023-12-21 01:46:11 INFO     	 * (global step 5500: loss: 0.29053647816181183, lr: 5e-05
2023-12-21 01:46:19 INFO     	 * (global step 5550: loss: 0.2802109643816948, lr: 5e-05
2023-12-21 01:46:27 INFO     	 * (global step 5600: loss: 0.4526786357164383, lr: 5e-05
2023-12-21 01:46:35 INFO     	 * (global step 5650: loss: 0.22843992710113525, lr: 5e-05
2023-12-21 01:46:43 INFO     	 * (global step 5700: loss: 0.3037380576133728, lr: 5e-05
2023-12-21 01:46:51 INFO     	 * (global step 5750: loss: 0.38469672203063965, lr: 5e-05
2023-12-21 01:46:59 INFO     	 * (global step 5800: loss: 0.2977406531572342, lr: 5e-05
2023-12-21 01:47:08 INFO     	 * (global step 5850: loss: 0.32861998677253723, lr: 5e-05
2023-12-21 01:47:16 INFO     	 * (global step 5900: loss: 0.2544500082731247, lr: 5e-05
2023-12-21 01:47:24 INFO     	 * (global step 5950: loss: 0.42092469334602356, lr: 5e-05
2023-12-21 01:47:32 INFO     	 * (global step 6000: loss: 0.33996330201625824, lr: 5e-05
2023-12-21 01:47:40 INFO     	 * (global step 6050: loss: 0.30146048963069916, lr: 5e-05
2023-12-21 01:47:48 INFO     	 * (global step 6100: loss: 0.2636791840195656, lr: 5e-05
2023-12-21 01:47:56 INFO     	 * (global step 6150: loss: 0.31652703881263733, lr: 5e-05
2023-12-21 01:48:05 INFO     	 * (global step 6200: loss: 0.39071042835712433, lr: 5e-05
2023-12-21 01:48:13 INFO     	 * (global step 6250: loss: 0.3355560898780823, lr: 5e-05
2023-12-21 01:48:21 INFO     	 * (global step 6300: loss: 0.6583372205495834, lr: 5e-05
2023-12-21 01:48:29 INFO     	 * (global step 6350: loss: 0.19966911524534225, lr: 5e-05
2023-12-21 01:48:37 INFO     	 * (global step 6400: loss: 0.3047928512096405, lr: 5e-05
2023-12-21 01:48:45 INFO     	 * (global step 6450: loss: 0.27576569467782974, lr: 5e-05
2023-12-21 01:48:53 INFO     	 * (global step 6500: loss: 0.26222530007362366, lr: 5e-05
2023-12-21 01:49:01 INFO     	 * (global step 6550: loss: 0.17569176852703094, lr: 5e-05
2023-12-21 01:49:09 INFO     	 * (global step 6600: loss: 0.3404414653778076, lr: 5e-05
2023-12-21 01:49:17 INFO     	 * (global step 6650: loss: 0.31162577867507935, lr: 5e-05
2023-12-21 01:49:26 INFO     	 * (global step 6700: loss: 0.3197499215602875, lr: 5e-05
2023-12-21 01:49:34 INFO     	 * (global step 6750: loss: 0.4353184401988983, lr: 5e-05
2023-12-21 01:49:42 INFO     	 * (global step 6800: loss: 0.29986169934272766, lr: 5e-05
2023-12-21 01:49:50 INFO     	 * (global step 6850: loss: 0.6750082969665527, lr: 5e-05
2023-12-21 01:49:58 INFO     	 * (global step 6900: loss: 0.307513490319252, lr: 5e-05
2023-12-21 01:50:06 INFO     	 * (global step 6950: loss: 0.26398253440856934, lr: 5e-05
2023-12-21 01:50:14 INFO     	 * (global step 7000: loss: 0.4555339515209198, lr: 5e-05
2023-12-21 01:50:23 INFO     	 * (global step 7050: loss: 0.2987794727087021, lr: 5e-05
2023-12-21 01:50:31 INFO     	 * (global step 7100: loss: 0.34532664716243744, lr: 5e-05
2023-12-21 01:50:39 INFO     	 * (global step 7150: loss: 0.2763501852750778, lr: 5e-05
2023-12-21 01:50:47 INFO     	 * (global step 7200: loss: 0.301585890352726, lr: 5e-05
2023-12-21 01:50:55 INFO     	 * (global step 7250: loss: 0.3209274709224701, lr: 5e-05
2023-12-21 01:51:03 INFO     	 * (global step 7300: loss: 0.313582144677639, lr: 5e-05
2023-12-21 01:51:11 INFO     	 * (global step 7350: loss: 0.21756517887115479, lr: 5e-05
2023-12-21 01:51:20 INFO     	 * (global step 7400: loss: 0.4566320776939392, lr: 5e-05
2023-12-21 01:51:28 INFO     	 * (global step 7450: loss: 0.4665980935096741, lr: 5e-05
2023-12-21 01:51:36 INFO     	 * (global step 7500: loss: 0.33430901914834976, lr: 5e-05
2023-12-21 01:51:44 INFO     	 * (global step 7550: loss: 0.41428378224372864, lr: 5e-05
2023-12-21 01:51:52 INFO     	 * (global step 7600: loss: 0.2417927160859108, lr: 5e-05
2023-12-21 01:52:00 INFO     	 * (global step 7650: loss: 0.27838200330734253, lr: 5e-05
2023-12-21 01:52:08 INFO     	 * (global step 7700: loss: 0.381589412689209, lr: 5e-05
2023-12-21 01:52:17 INFO     	 * (global step 7750: loss: 0.2952837273478508, lr: 5e-05
2023-12-21 01:52:25 INFO     	 * (global step 7800: loss: 0.29606587439775467, lr: 5e-05
2023-12-21 01:52:33 INFO     	 * (global step 7850: loss: 0.3505202531814575, lr: 5e-05
2023-12-21 01:52:41 INFO     	 * (global step 7900: loss: 0.38241586089134216, lr: 5e-05
2023-12-21 01:52:49 INFO     	 * (global step 7950: loss: 0.37148579955101013, lr: 5e-05
2023-12-21 01:52:57 INFO     	 * (global step 8000: loss: 0.24643106013536453, lr: 5e-05
2023-12-21 01:53:05 INFO     	 * (global step 8050: loss: 0.38826408982276917, lr: 5e-05
2023-12-21 01:53:14 INFO     	 * (global step 8100: loss: 0.42444874346256256, lr: 5e-05
2023-12-21 01:53:22 INFO     	 * (global step 8150: loss: 0.4999205321073532, lr: 5e-05
2023-12-21 01:53:30 INFO     	 * (global step 8200: loss: 0.23947126418352127, lr: 5e-05
2023-12-21 01:53:38 INFO     	 * (global step 8250: loss: 0.4567016363143921, lr: 5e-05
2023-12-21 01:53:46 INFO     	 * (global step 8300: loss: 0.2919926568865776, lr: 5e-05
2023-12-21 01:53:54 INFO     	 * (global step 8350: loss: 0.4315149337053299, lr: 5e-05
2023-12-21 01:54:02 INFO     	 * (global step 8400: loss: 0.29243339598178864, lr: 5e-05
2023-12-21 01:54:10 INFO     	 * (global step 8450: loss: 0.3633621484041214, lr: 5e-05
2023-12-21 01:54:19 INFO     	 * (global step 8500: loss: 0.47144410014152527, lr: 5e-05
2023-12-21 01:54:27 INFO     	 * (global step 8550: loss: 0.4892416298389435, lr: 5e-05
2023-12-21 01:54:35 INFO     	 * (global step 8600: loss: 0.20206304639577866, lr: 5e-05
2023-12-21 01:54:43 INFO     	 * (global step 8650: loss: 0.4194517433643341, lr: 5e-05
2023-12-21 01:54:51 INFO     	 * (global step 8700: loss: 0.18099746108055115, lr: 5e-05
2023-12-21 01:54:59 INFO     	 * (global step 8750: loss: 0.2268526628613472, lr: 5e-05
2023-12-21 01:55:08 INFO     	 * (global step 8800: loss: 0.28665389120578766, lr: 5e-05
2023-12-21 01:55:16 INFO     	 * (global step 8850: loss: 0.3965510427951813, lr: 5e-05
2023-12-21 01:55:24 INFO     	 * (global step 8900: loss: 0.24944128096103668, lr: 5e-05
2023-12-21 01:55:32 INFO     	 * (global step 8950: loss: 0.24586425721645355, lr: 5e-05
2023-12-21 01:55:40 INFO     	 * (global step 9000: loss: 0.25393659621477127, lr: 5e-05
2023-12-21 01:55:48 INFO     	 * (global step 9050: loss: 0.4834088608622551, lr: 5e-05
2023-12-21 01:55:56 INFO     	 * (global step 9100: loss: 0.3793555200099945, lr: 5e-05
2023-12-21 01:56:04 INFO     	 * (global step 9150: loss: 0.27211447060108185, lr: 5e-05
2023-12-21 01:56:13 INFO     	 * (global step 9200: loss: 0.23592191189527512, lr: 5e-05
2023-12-21 01:56:21 INFO     	 * (global step 9250: loss: 0.5724727287888527, lr: 5e-05
2023-12-21 01:56:29 INFO     	 * (global step 9300: loss: 0.3046146631240845, lr: 5e-05
2023-12-21 01:56:37 INFO     	 * (global step 9350: loss: 0.41254301369190216, lr: 5e-05
2023-12-21 01:56:45 INFO     	 * (global step 9400: loss: 0.3244435489177704, lr: 5e-05
2023-12-21 01:56:53 INFO     [epoch 1/15] average loss: 0.336, lr: 5e-05
2023-12-21 01:56:53 INFO     saving model related files
2023-12-21 01:56:53 INFO     saving model
2023-12-21 01:56:53 INFO     saving tokenizer
2023-12-21 01:56:53 INFO     saving optimizer
2023-12-21 01:56:54 INFO     remove old optimizer files
2023-12-21 01:56:55 INFO     	 * (global step 9450: loss: 0.4080018699169159, lr: 5e-05
2023-12-21 01:57:03 INFO     	 * (global step 9500: loss: 0.29647205024957657, lr: 5e-05
2023-12-21 01:57:11 INFO     	 * (global step 9550: loss: 0.36984654515981674, lr: 5e-05
2023-12-21 01:57:19 INFO     	 * (global step 9600: loss: 0.2885785326361656, lr: 5e-05
2023-12-21 01:57:27 INFO     	 * (global step 9650: loss: 0.25240686535835266, lr: 5e-05
2023-12-21 01:57:36 INFO     	 * (global step 9700: loss: 0.2309986874461174, lr: 5e-05
2023-12-21 01:57:44 INFO     	 * (global step 9750: loss: 0.41165363788604736, lr: 5e-05
2023-12-21 01:57:52 INFO     	 * (global step 9800: loss: 0.2575499787926674, lr: 5e-05
2023-12-21 01:58:00 INFO     	 * (global step 9850: loss: 0.262955866754055, lr: 5e-05
2023-12-21 01:58:08 INFO     	 * (global step 9900: loss: 0.19479398429393768, lr: 5e-05
2023-12-21 01:58:16 INFO     	 * (global step 9950: loss: 0.3065773546695709, lr: 5e-05
2023-12-21 01:58:24 INFO     	 * (global step 10000: loss: 0.23854731023311615, lr: 5e-05
2023-12-21 01:58:32 INFO     	 * (global step 10050: loss: 0.2174072265625, lr: 5e-05
2023-12-21 01:58:41 INFO     	 * (global step 10100: loss: 0.29229340702295303, lr: 5e-05
2023-12-21 01:58:49 INFO     	 * (global step 10150: loss: 0.2965487390756607, lr: 5e-05
2023-12-21 01:58:57 INFO     	 * (global step 10200: loss: 0.29479871690273285, lr: 5e-05
2023-12-21 01:59:05 INFO     	 * (global step 10250: loss: 0.47604554146528244, lr: 5e-05
2023-12-21 01:59:13 INFO     	 * (global step 10300: loss: 0.4041699916124344, lr: 5e-05
2023-12-21 01:59:21 INFO     	 * (global step 10350: loss: 0.4029921740293503, lr: 5e-05
2023-12-21 01:59:29 INFO     	 * (global step 10400: loss: 0.3052178621292114, lr: 5e-05
2023-12-21 01:59:37 INFO     	 * (global step 10450: loss: 0.29419784247875214, lr: 5e-05
2023-12-21 01:59:45 INFO     	 * (global step 10500: loss: 0.2659979537129402, lr: 5e-05
2023-12-21 01:59:54 INFO     	 * (global step 10550: loss: 0.32338573783636093, lr: 5e-05
2023-12-21 02:00:02 INFO     	 * (global step 10600: loss: 0.31411947309970856, lr: 5e-05
2023-12-21 02:00:10 INFO     	 * (global step 10650: loss: 0.31851164996623993, lr: 5e-05
2023-12-21 02:00:18 INFO     	 * (global step 10700: loss: 0.270098939538002, lr: 5e-05
2023-12-21 02:00:26 INFO     	 * (global step 10750: loss: 0.3799494206905365, lr: 5e-05
2023-12-21 02:00:34 INFO     	 * (global step 10800: loss: 0.37898752093315125, lr: 5e-05
2023-12-21 02:00:42 INFO     	 * (global step 10850: loss: 0.28627926111221313, lr: 5e-05
2023-12-21 02:00:50 INFO     	 * (global step 10900: loss: 0.3218190521001816, lr: 5e-05
2023-12-21 02:00:58 INFO     	 * (global step 10950: loss: 0.299844428896904, lr: 5e-05
2023-12-21 02:01:07 INFO     	 * (global step 11000: loss: 0.55124132335186, lr: 5e-05
2023-12-21 02:01:15 INFO     	 * (global step 11050: loss: 0.3022880479693413, lr: 5e-05
2023-12-21 02:01:23 INFO     	 * (global step 11100: loss: 0.5380293875932693, lr: 5e-05
2023-12-21 02:01:31 INFO     	 * (global step 11150: loss: 0.3312738835811615, lr: 5e-05
2023-12-21 02:01:39 INFO     	 * (global step 11200: loss: 0.24391841888427734, lr: 5e-05
2023-12-21 02:01:47 INFO     	 * (global step 11250: loss: 0.20523670315742493, lr: 5e-05
2023-12-21 02:01:55 INFO     	 * (global step 11300: loss: 0.31614647805690765, lr: 5e-05
2023-12-21 02:02:03 INFO     	 * (global step 11350: loss: 0.27652707695961, lr: 5e-05
2023-12-21 02:02:11 INFO     	 * (global step 11400: loss: 0.21694627404212952, lr: 5e-05
2023-12-21 02:02:19 INFO     	 * (global step 11450: loss: 0.26729314774274826, lr: 5e-05
2023-12-21 02:02:28 INFO     	 * (global step 11500: loss: 0.23195292800664902, lr: 5e-05
2023-12-21 02:02:36 INFO     	 * (global step 11550: loss: 0.4551226645708084, lr: 5e-05
2023-12-21 02:02:44 INFO     	 * (global step 11600: loss: 0.37623950839042664, lr: 5e-05
2023-12-21 02:02:52 INFO     	 * (global step 11650: loss: 0.26637449860572815, lr: 5e-05
2023-12-21 02:03:00 INFO     	 * (global step 11700: loss: 0.4652310982346535, lr: 5e-05
2023-12-21 02:03:08 INFO     	 * (global step 11750: loss: 0.25609883666038513, lr: 5e-05
2023-12-21 02:03:16 INFO     	 * (global step 11800: loss: 0.19740831851959229, lr: 5e-05
2023-12-21 02:03:24 INFO     	 * (global step 11850: loss: 0.30517300963401794, lr: 5e-05
2023-12-21 02:03:32 INFO     	 * (global step 11900: loss: 0.3580072447657585, lr: 5e-05
2023-12-21 02:03:41 INFO     	 * (global step 11950: loss: 0.3138323426246643, lr: 5e-05
2023-12-21 02:03:49 INFO     	 * (global step 12000: loss: 0.2644442543387413, lr: 5e-05
2023-12-21 02:03:57 INFO     	 * (global step 12050: loss: 0.2741920202970505, lr: 5e-05
2023-12-21 02:04:05 INFO     	 * (global step 12100: loss: 0.35213638842105865, lr: 5e-05
2023-12-21 02:04:13 INFO     	 * (global step 12150: loss: 0.3688410073518753, lr: 5e-05
2023-12-21 02:04:21 INFO     	 * (global step 12200: loss: 0.3174816966056824, lr: 5e-05
2023-12-21 02:04:29 INFO     	 * (global step 12250: loss: 0.30121685564517975, lr: 5e-05
2023-12-21 02:04:37 INFO     	 * (global step 12300: loss: 0.24626287817955017, lr: 5e-05
2023-12-21 02:04:45 INFO     	 * (global step 12350: loss: 0.3734205514192581, lr: 5e-05
2023-12-21 02:04:54 INFO     	 * (global step 12400: loss: 0.6441972553730011, lr: 5e-05
2023-12-21 02:05:02 INFO     	 * (global step 12450: loss: 0.20445092767477036, lr: 5e-05
2023-12-21 02:05:10 INFO     	 * (global step 12500: loss: 0.37768834829330444, lr: 5e-05
2023-12-21 02:05:18 INFO     	 * (global step 12550: loss: 0.3639635145664215, lr: 5e-05
2023-12-21 02:05:26 INFO     	 * (global step 12600: loss: 0.4568772315979004, lr: 5e-05
2023-12-21 02:05:34 INFO     	 * (global step 12650: loss: 0.31565435230731964, lr: 5e-05
2023-12-21 02:05:42 INFO     	 * (global step 12700: loss: 0.2581126540899277, lr: 5e-05
2023-12-21 02:05:50 INFO     	 * (global step 12750: loss: 0.3496267795562744, lr: 5e-05
2023-12-21 02:05:58 INFO     	 * (global step 12800: loss: 0.2797067388892174, lr: 5e-05
2023-12-21 02:06:06 INFO     	 * (global step 12850: loss: 0.25339779257774353, lr: 5e-05
2023-12-21 02:06:14 INFO     	 * (global step 12900: loss: 0.27517056465148926, lr: 5e-05
2023-12-21 02:06:22 INFO     	 * (global step 12950: loss: 0.356011763215065, lr: 5e-05
2023-12-21 02:06:30 INFO     	 * (global step 13000: loss: 0.40580011904239655, lr: 5e-05
2023-12-21 02:06:38 INFO     	 * (global step 13050: loss: 0.24896392226219177, lr: 5e-05
2023-12-21 02:06:46 INFO     	 * (global step 13100: loss: 0.32809580862522125, lr: 5e-05
2023-12-21 02:06:54 INFO     	 * (global step 13150: loss: 0.35702258348464966, lr: 5e-05
2023-12-21 02:07:02 INFO     	 * (global step 13200: loss: 0.29151828587055206, lr: 5e-05
2023-12-21 02:07:10 INFO     	 * (global step 13250: loss: 0.2432921603322029, lr: 5e-05
2023-12-21 02:07:18 INFO     	 * (global step 13300: loss: 0.3891395330429077, lr: 5e-05
2023-12-21 02:07:27 INFO     	 * (global step 13350: loss: 0.39597587287425995, lr: 5e-05
2023-12-21 02:07:35 INFO     	 * (global step 13400: loss: 0.23459292203187943, lr: 5e-05
2023-12-21 02:07:43 INFO     	 * (global step 13450: loss: 0.3485209345817566, lr: 5e-05
2023-12-21 02:07:51 INFO     	 * (global step 13500: loss: 0.27373313903808594, lr: 5e-05
2023-12-21 02:07:59 INFO     	 * (global step 13550: loss: 0.21504666656255722, lr: 5e-05
2023-12-21 02:08:07 INFO     	 * (global step 13600: loss: 0.5486842095851898, lr: 5e-05
2023-12-21 02:08:15 INFO     	 * (global step 13650: loss: 0.21039129793643951, lr: 5e-05
2023-12-21 02:08:23 INFO     	 * (global step 13700: loss: 0.31167780607938766, lr: 5e-05
2023-12-21 02:08:31 INFO     	 * (global step 13750: loss: 0.2846648842096329, lr: 5e-05
2023-12-21 02:08:39 INFO     	 * (global step 13800: loss: 0.3512328639626503, lr: 5e-05
2023-12-21 02:08:47 INFO     	 * (global step 13850: loss: 0.49861177057027817, lr: 5e-05
2023-12-21 02:08:55 INFO     	 * (global step 13900: loss: 0.23544328659772873, lr: 5e-05
2023-12-21 02:09:03 INFO     	 * (global step 13950: loss: 0.29145608842372894, lr: 5e-05
2023-12-21 02:09:11 INFO     	 * (global step 14000: loss: 0.3470425680279732, lr: 5e-05
2023-12-21 02:09:20 INFO     	 * (global step 14050: loss: 0.3541555404663086, lr: 5e-05
2023-12-21 02:09:28 INFO     	 * (global step 14100: loss: 0.23869816958904266, lr: 5e-05
2023-12-21 02:09:36 INFO     	 * (global step 14150: loss: 0.3370714634656906, lr: 5e-05
2023-12-21 02:09:39 INFO     [epoch 2/15] average loss: 0.317, lr: 5e-05
2023-12-21 02:09:39 INFO     saving model related files
2023-12-21 02:09:39 INFO     saving model
2023-12-21 02:09:39 INFO     saving tokenizer
2023-12-21 02:09:39 INFO     saving optimizer
2023-12-21 02:09:40 INFO     remove old optimizer files
2023-12-21 02:09:45 INFO     	 * (global step 14200: loss: 0.3338601142168045, lr: 5e-05
2023-12-21 02:09:54 INFO     	 * (global step 14250: loss: 0.2629806026816368, lr: 5e-05
2023-12-21 02:10:02 INFO     	 * (global step 14300: loss: 0.16162701696157455, lr: 5e-05
2023-12-21 02:10:10 INFO     	 * (global step 14350: loss: 0.34351149946451187, lr: 5e-05
2023-12-21 02:10:18 INFO     	 * (global step 14400: loss: 0.32036685943603516, lr: 5e-05
2023-12-21 02:10:26 INFO     	 * (global step 14450: loss: 0.24154184013605118, lr: 5e-05
2023-12-21 02:10:34 INFO     	 * (global step 14500: loss: 0.4849095493555069, lr: 5e-05
2023-12-21 02:10:42 INFO     	 * (global step 14550: loss: 0.22155485302209854, lr: 5e-05
2023-12-21 02:10:50 INFO     	 * (global step 14600: loss: 0.22222958505153656, lr: 5e-05
2023-12-21 02:10:58 INFO     	 * (global step 14650: loss: 0.2819180190563202, lr: 5e-05
2023-12-21 02:11:06 INFO     	 * (global step 14700: loss: 0.39949898421764374, lr: 5e-05
2023-12-21 02:11:15 INFO     	 * (global step 14750: loss: 0.2618713155388832, lr: 5e-05
2023-12-21 02:11:23 INFO     	 * (global step 14800: loss: 0.34352031350135803, lr: 5e-05
2023-12-21 02:11:31 INFO     	 * (global step 14850: loss: 0.28560028970241547, lr: 5e-05
2023-12-21 02:11:39 INFO     	 * (global step 14900: loss: 0.4012064039707184, lr: 5e-05
2023-12-21 02:11:47 INFO     	 * (global step 14950: loss: 0.31416335701942444, lr: 5e-05
2023-12-21 02:11:55 INFO     	 * (global step 15000: loss: 0.23281466215848923, lr: 5e-05
2023-12-21 02:12:03 INFO     	 * (global step 15050: loss: 0.25393984466791153, lr: 5e-05
2023-12-21 02:12:11 INFO     	 * (global step 15100: loss: 0.24988969415426254, lr: 5e-05
2023-12-21 02:12:19 INFO     	 * (global step 15150: loss: 0.40884697437286377, lr: 5e-05
2023-12-21 02:12:27 INFO     	 * (global step 15200: loss: 0.4465435743331909, lr: 5e-05
2023-12-21 02:12:36 INFO     	 * (global step 15250: loss: 0.4733869880437851, lr: 5e-05
2023-12-21 02:12:44 INFO     	 * (global step 15300: loss: 0.322667732834816, lr: 5e-05
2023-12-21 02:12:52 INFO     	 * (global step 15350: loss: 0.2897244170308113, lr: 5e-05
2023-12-21 02:13:00 INFO     	 * (global step 15400: loss: 0.23234423249959946, lr: 5e-05
2023-12-21 02:13:08 INFO     	 * (global step 15450: loss: 0.29032668471336365, lr: 5e-05
2023-12-21 02:13:16 INFO     	 * (global step 15500: loss: 0.30735181272029877, lr: 5e-05
2023-12-21 02:13:24 INFO     	 * (global step 15550: loss: 0.23857435584068298, lr: 5e-05
2023-12-21 02:13:32 INFO     	 * (global step 15600: loss: 0.38732776045799255, lr: 5e-05
2023-12-21 02:13:40 INFO     	 * (global step 15650: loss: 0.34734077751636505, lr: 5e-05
2023-12-21 02:13:48 INFO     	 * (global step 15700: loss: 0.33937065303325653, lr: 5e-05
2023-12-21 02:13:56 INFO     	 * (global step 15750: loss: 0.3893700838088989, lr: 5e-05
2023-12-21 02:14:04 INFO     	 * (global step 15800: loss: 0.211372509598732, lr: 5e-05
2023-12-21 02:14:12 INFO     	 * (global step 15850: loss: 0.3586185723543167, lr: 5e-05
2023-12-21 02:14:20 INFO     	 * (global step 15900: loss: 0.22649703174829483, lr: 5e-05
2023-12-21 02:14:28 INFO     	 * (global step 15950: loss: 0.31355322897434235, lr: 5e-05
2023-12-21 02:14:37 INFO     	 * (global step 16000: loss: 0.1636691763997078, lr: 5e-05
2023-12-21 02:14:45 INFO     	 * (global step 16050: loss: 0.4321164935827255, lr: 5e-05
2023-12-21 02:14:53 INFO     	 * (global step 16100: loss: 0.3544512018561363, lr: 5e-05
2023-12-21 02:15:01 INFO     	 * (global step 16150: loss: 0.260684996843338, lr: 5e-05
2023-12-21 02:15:09 INFO     	 * (global step 16200: loss: 0.2925665080547333, lr: 5e-05
2023-12-21 02:15:17 INFO     	 * (global step 16250: loss: 0.33981387317180634, lr: 5e-05
2023-12-21 02:15:25 INFO     	 * (global step 16300: loss: 0.26148951053619385, lr: 5e-05
2023-12-21 02:15:33 INFO     	 * (global step 16350: loss: 0.31902359426021576, lr: 5e-05
2023-12-21 02:15:41 INFO     	 * (global step 16400: loss: 0.33337603509426117, lr: 5e-05
2023-12-21 02:15:49 INFO     	 * (global step 16450: loss: 0.3056844547390938, lr: 5e-05
2023-12-21 02:15:57 INFO     	 * (global step 16500: loss: 0.2990906834602356, lr: 5e-05
2023-12-21 02:16:05 INFO     	 * (global step 16550: loss: 0.31851229816675186, lr: 5e-05
2023-12-21 02:16:13 INFO     	 * (global step 16600: loss: 0.2615521401166916, lr: 5e-05
2023-12-21 02:16:21 INFO     	 * (global step 16650: loss: 0.3120334595441818, lr: 5e-05
2023-12-21 02:16:29 INFO     	 * (global step 16700: loss: 0.20994745939970016, lr: 5e-05
2023-12-21 02:16:37 INFO     	 * (global step 16750: loss: 0.14108256995677948, lr: 5e-05
2023-12-21 02:16:45 INFO     	 * (global step 16800: loss: 0.31999026238918304, lr: 5e-05
2023-12-21 02:16:53 INFO     	 * (global step 16850: loss: 0.36784105002880096, lr: 5e-05
2023-12-21 02:17:02 INFO     	 * (global step 16900: loss: 0.2812661826610565, lr: 5e-05
2023-12-21 02:17:10 INFO     	 * (global step 16950: loss: 0.3438723683357239, lr: 5e-05
2023-12-21 02:17:18 INFO     	 * (global step 17000: loss: 0.2418799102306366, lr: 5e-05
2023-12-21 02:17:26 INFO     	 * (global step 17050: loss: 0.21599678695201874, lr: 5e-05
2023-12-21 02:17:34 INFO     	 * (global step 17100: loss: 0.3293085843324661, lr: 5e-05
2023-12-21 02:17:42 INFO     	 * (global step 17150: loss: 0.2569727450609207, lr: 5e-05
2023-12-21 02:17:50 INFO     	 * (global step 17200: loss: 0.3115086108446121, lr: 5e-05
2023-12-21 02:17:58 INFO     	 * (global step 17250: loss: 0.2855529934167862, lr: 5e-05
2023-12-21 02:18:06 INFO     	 * (global step 17300: loss: 0.21527942642569542, lr: 5e-05
2023-12-21 02:18:14 INFO     	 * (global step 17350: loss: 0.36689551174640656, lr: 5e-05
2023-12-21 02:18:22 INFO     	 * (global step 17400: loss: 0.19750915467739105, lr: 5e-05
2023-12-21 02:18:30 INFO     	 * (global step 17450: loss: 0.45541878044605255, lr: 5e-05
2023-12-21 02:18:38 INFO     	 * (global step 17500: loss: 0.3146605044603348, lr: 5e-05
2023-12-21 02:18:47 INFO     	 * (global step 17550: loss: 0.28206491470336914, lr: 5e-05
2023-12-21 02:18:55 INFO     	 * (global step 17600: loss: 0.2989683151245117, lr: 5e-05
2023-12-21 02:19:03 INFO     	 * (global step 17650: loss: 0.411750853061676, lr: 5e-05
2023-12-21 02:19:11 INFO     	 * (global step 17700: loss: 0.2704358845949173, lr: 5e-05
2023-12-21 02:19:19 INFO     	 * (global step 17750: loss: 0.35037242621183395, lr: 5e-05
2023-12-21 02:19:27 INFO     	 * (global step 17800: loss: 0.4707862585783005, lr: 5e-05
2023-12-21 02:19:35 INFO     	 * (global step 17850: loss: 0.23873066902160645, lr: 5e-05
2023-12-21 02:19:43 INFO     	 * (global step 17900: loss: 0.3206275403499603, lr: 5e-05
2023-12-21 02:19:51 INFO     	 * (global step 17950: loss: 0.2839094400405884, lr: 5e-05
2023-12-21 02:19:59 INFO     	 * (global step 18000: loss: 0.3008187487721443, lr: 5e-05
2023-12-21 02:20:07 INFO     	 * (global step 18050: loss: 0.20254261046648026, lr: 5e-05
2023-12-21 02:20:16 INFO     	 * (global step 18100: loss: 0.32287997007369995, lr: 5e-05
2023-12-21 02:20:24 INFO     	 * (global step 18150: loss: 0.30395588278770447, lr: 5e-05
2023-12-21 02:20:32 INFO     	 * (global step 18200: loss: 0.3827839493751526, lr: 5e-05
2023-12-21 02:20:40 INFO     	 * (global step 18250: loss: 0.25261835008859634, lr: 5e-05
2023-12-21 02:20:48 INFO     	 * (global step 18300: loss: 0.28028687834739685, lr: 5e-05
2023-12-21 02:20:56 INFO     	 * (global step 18350: loss: 0.3164200186729431, lr: 5e-05
2023-12-21 02:21:04 INFO     	 * (global step 18400: loss: 0.2977460101246834, lr: 5e-05
2023-12-21 02:21:12 INFO     	 * (global step 18450: loss: 0.3369392305612564, lr: 5e-05
2023-12-21 02:21:20 INFO     	 * (global step 18500: loss: 0.29375699162483215, lr: 5e-05
2023-12-21 02:21:28 INFO     	 * (global step 18550: loss: 0.3488923907279968, lr: 5e-05
2023-12-21 02:21:36 INFO     	 * (global step 18600: loss: 0.4247339814901352, lr: 5e-05
2023-12-21 02:21:44 INFO     	 * (global step 18650: loss: 0.5368661284446716, lr: 5e-05
2023-12-21 02:21:53 INFO     	 * (global step 18700: loss: 0.3739720284938812, lr: 5e-05
2023-12-21 02:22:01 INFO     	 * (global step 18750: loss: 0.20778482407331467, lr: 5e-05
2023-12-21 02:22:09 INFO     	 * (global step 18800: loss: 0.5127785578370094, lr: 5e-05
2023-12-21 02:22:17 INFO     	 * (global step 18850: loss: 0.2591233104467392, lr: 5e-05
2023-12-21 02:22:24 INFO     [epoch 3/15] average loss: 0.305, lr: 5e-05
2023-12-21 02:22:24 INFO     saving model related files
2023-12-21 02:22:24 INFO     saving model
2023-12-21 02:22:24 INFO     saving tokenizer
2023-12-21 02:22:24 INFO     saving optimizer
2023-12-21 02:22:25 INFO     remove old optimizer files
2023-12-21 02:22:26 INFO     	 * (global step 18900: loss: 0.2597327008843422, lr: 5e-05
2023-12-21 02:22:35 INFO     	 * (global step 18950: loss: 0.28103721141815186, lr: 5e-05
2023-12-21 02:22:43 INFO     	 * (global step 19000: loss: 0.5162516087293625, lr: 5e-05
2023-12-21 02:22:51 INFO     	 * (global step 19050: loss: 0.37475715577602386, lr: 5e-05
2023-12-21 02:22:59 INFO     	 * (global step 19100: loss: 0.23339006304740906, lr: 5e-05
2023-12-21 02:23:07 INFO     	 * (global step 19150: loss: 0.23280133306980133, lr: 5e-05
2023-12-21 02:23:15 INFO     	 * (global step 19200: loss: 0.320724681019783, lr: 5e-05
2023-12-21 02:23:23 INFO     	 * (global step 19250: loss: 0.21257180720567703, lr: 5e-05
2023-12-21 02:23:31 INFO     	 * (global step 19300: loss: 0.21816660463809967, lr: 5e-05
2023-12-21 02:23:39 INFO     	 * (global step 19350: loss: 0.18198733031749725, lr: 5e-05
2023-12-21 02:23:47 INFO     	 * (global step 19400: loss: 0.29898063838481903, lr: 5e-05
2023-12-21 02:23:55 INFO     	 * (global step 19450: loss: 0.18292517215013504, lr: 5e-05
2023-12-21 02:24:03 INFO     	 * (global step 19500: loss: 0.3155505210161209, lr: 5e-05
2023-12-21 02:24:11 INFO     	 * (global step 19550: loss: 0.3062901049852371, lr: 5e-05
2023-12-21 02:24:19 INFO     	 * (global step 19600: loss: 0.22335653752088547, lr: 5e-05
2023-12-21 02:24:27 INFO     	 * (global step 19650: loss: 0.5437907129526138, lr: 5e-05
2023-12-21 02:24:35 INFO     	 * (global step 19700: loss: 0.29417311400175095, lr: 5e-05
2023-12-21 02:24:44 INFO     	 * (global step 19750: loss: 0.29177796840667725, lr: 5e-05
2023-12-21 02:24:52 INFO     	 * (global step 19800: loss: 0.2443215399980545, lr: 5e-05
2023-12-21 02:25:00 INFO     	 * (global step 19850: loss: 0.19885020703077316, lr: 5e-05
2023-12-21 02:25:08 INFO     	 * (global step 19900: loss: 0.3178868144750595, lr: 5e-05
2023-12-21 02:25:16 INFO     	 * (global step 19950: loss: 0.2706698402762413, lr: 5e-05
2023-12-21 02:25:24 INFO     	 * (global step 20000: loss: 0.19959721714258194, lr: 5e-05
2023-12-21 02:25:32 INFO     	 * (global step 20050: loss: 0.22965922951698303, lr: 5e-05
2023-12-21 02:25:40 INFO     	 * (global step 20100: loss: 0.3568739742040634, lr: 5e-05
2023-12-21 02:25:48 INFO     	 * (global step 20150: loss: 0.26205068081617355, lr: 5e-05
2023-12-21 02:25:56 INFO     	 * (global step 20200: loss: 0.2787307798862457, lr: 5e-05
2023-12-21 02:26:04 INFO     	 * (global step 20250: loss: 0.28999002277851105, lr: 5e-05
2023-12-21 02:26:12 INFO     	 * (global step 20300: loss: 0.3050339072942734, lr: 5e-05
2023-12-21 02:26:20 INFO     	 * (global step 20350: loss: 0.23893287777900696, lr: 5e-05
2023-12-21 02:26:28 INFO     	 * (global step 20400: loss: 0.24846801906824112, lr: 5e-05
2023-12-21 02:26:36 INFO     	 * (global step 20450: loss: 0.21583671122789383, lr: 5e-05
2023-12-21 02:26:45 INFO     	 * (global step 20500: loss: 0.23244532942771912, lr: 5e-05
2023-12-21 02:26:53 INFO     	 * (global step 20550: loss: 0.24542687088251114, lr: 5e-05
2023-12-21 02:27:01 INFO     	 * (global step 20600: loss: 0.29340432584285736, lr: 5e-05
2023-12-21 02:27:09 INFO     	 * (global step 20650: loss: 0.41637229919433594, lr: 5e-05
2023-12-21 02:27:17 INFO     	 * (global step 20700: loss: 0.24097245186567307, lr: 5e-05
2023-12-21 02:27:25 INFO     	 * (global step 20750: loss: 0.24637188762426376, lr: 5e-05
2023-12-21 02:27:33 INFO     	 * (global step 20800: loss: 0.3380254879593849, lr: 5e-05
2023-12-21 02:27:41 INFO     	 * (global step 20850: loss: 0.30501043796539307, lr: 5e-05
2023-12-21 02:27:49 INFO     	 * (global step 20900: loss: 0.3170924633741379, lr: 5e-05
2023-12-21 02:27:57 INFO     	 * (global step 20950: loss: 0.29593925178050995, lr: 5e-05
2023-12-21 02:28:05 INFO     	 * (global step 21000: loss: 0.4481329768896103, lr: 5e-05
2023-12-21 02:28:14 INFO     	 * (global step 21050: loss: 0.2641400247812271, lr: 5e-05
2023-12-21 02:28:22 INFO     	 * (global step 21100: loss: 0.23591751605272293, lr: 5e-05
2023-12-21 02:28:30 INFO     	 * (global step 21150: loss: 0.27405497431755066, lr: 5e-05
2023-12-21 02:28:38 INFO     	 * (global step 21200: loss: 0.34128715097904205, lr: 5e-05
2023-12-21 02:28:46 INFO     	 * (global step 21250: loss: 0.2491779550909996, lr: 5e-05
2023-12-21 02:28:54 INFO     	 * (global step 21300: loss: 0.3072229102253914, lr: 5e-05
2023-12-21 02:29:02 INFO     	 * (global step 21350: loss: 0.2404157668352127, lr: 5e-05
2023-12-21 02:29:10 INFO     	 * (global step 21400: loss: 0.20204904675483704, lr: 5e-05
2023-12-21 02:29:18 INFO     	 * (global step 21450: loss: 0.49400730431079865, lr: 5e-05
2023-12-21 02:29:26 INFO     	 * (global step 21500: loss: 0.26865970343351364, lr: 5e-05
2023-12-21 02:29:34 INFO     	 * (global step 21550: loss: 0.29500526189804077, lr: 5e-05
2023-12-21 02:29:42 INFO     	 * (global step 21600: loss: 0.35718830674886703, lr: 5e-05
2023-12-21 02:29:50 INFO     	 * (global step 21650: loss: 0.2535419762134552, lr: 5e-05
2023-12-21 02:29:58 INFO     	 * (global step 21700: loss: 0.18229079246520996, lr: 5e-05
2023-12-21 02:30:07 INFO     	 * (global step 21750: loss: 0.3524315878748894, lr: 5e-05
2023-12-21 02:30:15 INFO     	 * (global step 21800: loss: 0.30932992696762085, lr: 5e-05
2023-12-21 02:30:23 INFO     	 * (global step 21850: loss: 0.3709392696619034, lr: 5e-05
2023-12-21 02:30:31 INFO     	 * (global step 21900: loss: 0.2765331268310547, lr: 5e-05
2023-12-21 02:30:39 INFO     	 * (global step 21950: loss: 0.29546280950307846, lr: 5e-05
2023-12-21 02:30:47 INFO     	 * (global step 22000: loss: 0.2938445881009102, lr: 5e-05
2023-12-21 02:30:55 INFO     	 * (global step 22050: loss: 0.3477049097418785, lr: 5e-05
2023-12-21 02:31:03 INFO     	 * (global step 22100: loss: 0.4022633954882622, lr: 5e-05
2023-12-21 02:31:11 INFO     	 * (global step 22150: loss: 0.4417257457971573, lr: 5e-05
2023-12-21 02:31:19 INFO     	 * (global step 22200: loss: 0.20386876910924911, lr: 5e-05
2023-12-21 02:31:27 INFO     	 * (global step 22250: loss: 0.26180265098810196, lr: 5e-05
2023-12-21 02:31:36 INFO     	 * (global step 22300: loss: 0.29709264636039734, lr: 5e-05
2023-12-21 02:31:44 INFO     	 * (global step 22350: loss: 0.36838874220848083, lr: 5e-05
2023-12-21 02:31:52 INFO     	 * (global step 22400: loss: 0.19971014559268951, lr: 5e-05
2023-12-21 02:32:00 INFO     	 * (global step 22450: loss: 0.3967032879590988, lr: 5e-05
2023-12-21 02:32:08 INFO     	 * (global step 22500: loss: 0.2998463362455368, lr: 5e-05
2023-12-21 02:32:16 INFO     	 * (global step 22550: loss: 0.2824309766292572, lr: 5e-05
2023-12-21 02:32:24 INFO     	 * (global step 22600: loss: 0.30261482298374176, lr: 5e-05
2023-12-21 02:32:32 INFO     	 * (global step 22650: loss: 0.16001922637224197, lr: 5e-05
2023-12-21 02:32:40 INFO     	 * (global step 22700: loss: 0.39800307154655457, lr: 5e-05
2023-12-21 02:32:48 INFO     	 * (global step 22750: loss: 0.32354576885700226, lr: 5e-05
2023-12-21 02:32:56 INFO     	 * (global step 22800: loss: 0.38733914494514465, lr: 5e-05
2023-12-21 02:33:04 INFO     	 * (global step 22850: loss: 0.3009386882185936, lr: 5e-05
2023-12-21 02:33:12 INFO     	 * (global step 22900: loss: 0.31567060947418213, lr: 5e-05
2023-12-21 02:33:21 INFO     	 * (global step 22950: loss: 0.2253957763314247, lr: 5e-05
2023-12-21 02:33:29 INFO     	 * (global step 23000: loss: 0.36454665660858154, lr: 5e-05
2023-12-21 02:33:37 INFO     	 * (global step 23050: loss: 0.45033836364746094, lr: 5e-05
2023-12-21 02:33:45 INFO     	 * (global step 23100: loss: 0.3335120677947998, lr: 5e-05
2023-12-21 02:33:53 INFO     	 * (global step 23150: loss: 0.26860348880290985, lr: 5e-05
2023-12-21 02:34:01 INFO     	 * (global step 23200: loss: 0.28252094984054565, lr: 5e-05
2023-12-21 02:34:09 INFO     	 * (global step 23250: loss: 0.15542538836598396, lr: 5e-05
2023-12-21 02:34:17 INFO     	 * (global step 23300: loss: 0.2676771730184555, lr: 5e-05
2023-12-21 02:34:25 INFO     	 * (global step 23350: loss: 0.271566241979599, lr: 5e-05
2023-12-21 02:34:34 INFO     	 * (global step 23400: loss: 0.35040003061294556, lr: 5e-05
2023-12-21 02:34:42 INFO     	 * (global step 23450: loss: 0.3841891586780548, lr: 5e-05
2023-12-21 02:34:50 INFO     	 * (global step 23500: loss: 0.2407028004527092, lr: 5e-05
2023-12-21 02:34:58 INFO     	 * (global step 23550: loss: 0.29038530588150024, lr: 5e-05
2023-12-21 02:35:06 INFO     	 * (global step 23600: loss: 0.3560890704393387, lr: 5e-05
2023-12-21 02:35:08 INFO     [epoch 4/15] average loss: 0.296, lr: 5e-05
2023-12-21 02:35:08 INFO     saving model related files
2023-12-21 02:35:08 INFO     saving model
2023-12-21 02:35:09 INFO     saving tokenizer
2023-12-21 02:35:09 INFO     saving optimizer
2023-12-21 02:35:10 INFO     remove old optimizer files
2023-12-21 02:35:16 INFO     	 * (global step 23650: loss: 0.4300341010093689, lr: 5e-05
2023-12-21 02:35:24 INFO     	 * (global step 23700: loss: 0.24171878397464752, lr: 5e-05
2023-12-21 02:35:32 INFO     	 * (global step 23750: loss: 0.30730699747800827, lr: 5e-05
2023-12-21 02:35:40 INFO     	 * (global step 23800: loss: 0.23391343653202057, lr: 5e-05
2023-12-21 02:35:48 INFO     	 * (global step 23850: loss: 0.3384518623352051, lr: 5e-05
2023-12-21 02:35:56 INFO     	 * (global step 23900: loss: 0.2545296922326088, lr: 5e-05
2023-12-21 02:36:04 INFO     	 * (global step 23950: loss: 0.2343166097998619, lr: 5e-05
2023-12-21 02:36:12 INFO     	 * (global step 24000: loss: 0.34175682067871094, lr: 5e-05
2023-12-21 02:36:20 INFO     	 * (global step 24050: loss: 0.44259143620729446, lr: 5e-05
2023-12-21 02:36:28 INFO     	 * (global step 24100: loss: 0.26538948714733124, lr: 5e-05
2023-12-21 02:36:36 INFO     	 * (global step 24150: loss: 0.3592262417078018, lr: 5e-05
2023-12-21 02:36:45 INFO     	 * (global step 24200: loss: 0.34083472192287445, lr: 5e-05
2023-12-21 02:36:53 INFO     	 * (global step 24250: loss: 0.42266930639743805, lr: 5e-05
2023-12-21 02:37:01 INFO     	 * (global step 24300: loss: 0.2770487368106842, lr: 5e-05
2023-12-21 02:37:09 INFO     	 * (global step 24350: loss: 0.3742845803499222, lr: 5e-05
2023-12-21 02:37:17 INFO     	 * (global step 24400: loss: 0.2788141369819641, lr: 5e-05
2023-12-21 02:37:25 INFO     	 * (global step 24450: loss: 0.3733198344707489, lr: 5e-05
2023-12-21 02:37:33 INFO     	 * (global step 24500: loss: 0.24306008964776993, lr: 5e-05
2023-12-21 02:37:41 INFO     	 * (global step 24550: loss: 0.3383787274360657, lr: 5e-05
2023-12-21 02:37:49 INFO     	 * (global step 24600: loss: 0.32193005084991455, lr: 5e-05
2023-12-21 02:37:57 INFO     	 * (global step 24650: loss: 0.21802736818790436, lr: 5e-05
2023-12-21 02:38:05 INFO     	 * (global step 24700: loss: 0.3255062401294708, lr: 5e-05
2023-12-21 02:38:14 INFO     	 * (global step 24750: loss: 0.2829909399151802, lr: 5e-05
2023-12-21 02:38:22 INFO     	 * (global step 24800: loss: 0.28635069727897644, lr: 5e-05
2023-12-21 02:38:30 INFO     	 * (global step 24850: loss: 0.34612832218408585, lr: 5e-05
2023-12-21 02:38:38 INFO     	 * (global step 24900: loss: 0.3397195041179657, lr: 5e-05
2023-12-21 02:38:46 INFO     	 * (global step 24950: loss: 0.2824920490384102, lr: 5e-05
2023-12-21 02:38:54 INFO     	 * (global step 25000: loss: 0.3238387554883957, lr: 5e-05
2023-12-21 02:39:02 INFO     	 * (global step 25050: loss: 0.17028120905160904, lr: 5e-05
2023-12-21 02:39:10 INFO     	 * (global step 25100: loss: 0.344134196639061, lr: 5e-05
2023-12-21 02:39:18 INFO     	 * (global step 25150: loss: 0.7776815593242645, lr: 5e-05
2023-12-21 02:39:26 INFO     	 * (global step 25200: loss: 0.3095027804374695, lr: 5e-05
2023-12-21 02:39:34 INFO     	 * (global step 25250: loss: 0.26264000684022903, lr: 5e-05
2023-12-21 02:39:42 INFO     	 * (global step 25300: loss: 0.32922983169555664, lr: 5e-05
2023-12-21 02:39:50 INFO     	 * (global step 25350: loss: 0.27586545050144196, lr: 5e-05
2023-12-21 02:39:58 INFO     	 * (global step 25400: loss: 0.26271094381809235, lr: 5e-05
2023-12-21 02:40:06 INFO     	 * (global step 25450: loss: 0.26489462703466415, lr: 5e-05
2023-12-21 02:40:15 INFO     	 * (global step 25500: loss: 0.3121424466371536, lr: 5e-05
2023-12-21 02:40:23 INFO     	 * (global step 25550: loss: 0.3583340495824814, lr: 5e-05
2023-12-21 02:40:31 INFO     	 * (global step 25600: loss: 0.26241789758205414, lr: 5e-05
2023-12-21 02:40:39 INFO     	 * (global step 25650: loss: 0.2174338400363922, lr: 5e-05
2023-12-21 02:40:47 INFO     	 * (global step 25700: loss: 0.3398920148611069, lr: 5e-05
2023-12-21 02:40:55 INFO     	 * (global step 25750: loss: 0.24683375656604767, lr: 5e-05
2023-12-21 02:41:03 INFO     	 * (global step 25800: loss: 0.28513993322849274, lr: 5e-05
2023-12-21 02:41:11 INFO     	 * (global step 25850: loss: 0.26479802280664444, lr: 5e-05
2023-12-21 02:41:19 INFO     	 * (global step 25900: loss: 0.5402373671531677, lr: 5e-05
2023-12-21 02:41:27 INFO     	 * (global step 25950: loss: 0.19310824573040009, lr: 5e-05
2023-12-21 02:41:35 INFO     	 * (global step 26000: loss: 0.33991195261478424, lr: 5e-05
2023-12-21 02:41:43 INFO     	 * (global step 26050: loss: 0.4065515398979187, lr: 5e-05
2023-12-21 02:41:51 INFO     	 * (global step 26100: loss: 0.31995613873004913, lr: 5e-05
2023-12-21 02:41:59 INFO     	 * (global step 26150: loss: 0.2776896134018898, lr: 5e-05
2023-12-21 02:42:07 INFO     	 * (global step 26200: loss: 0.44902969896793365, lr: 5e-05
2023-12-21 02:42:15 INFO     	 * (global step 26250: loss: 0.2588483467698097, lr: 5e-05
2023-12-21 02:42:24 INFO     	 * (global step 26300: loss: 0.3953893333673477, lr: 5e-05
2023-12-21 02:42:32 INFO     	 * (global step 26350: loss: 0.23597852885723114, lr: 5e-05
2023-12-21 02:42:40 INFO     	 * (global step 26400: loss: 0.3733954578638077, lr: 5e-05
2023-12-21 02:42:48 INFO     	 * (global step 26450: loss: 0.24908583611249924, lr: 5e-05
2023-12-21 02:42:56 INFO     	 * (global step 26500: loss: 0.24333259463310242, lr: 5e-05
2023-12-21 02:43:04 INFO     	 * (global step 26550: loss: 0.2683679684996605, lr: 5e-05
2023-12-21 02:43:12 INFO     	 * (global step 26600: loss: 0.21720949560403824, lr: 5e-05
2023-12-21 02:43:20 INFO     	 * (global step 26650: loss: 0.34759020805358887, lr: 5e-05
2023-12-21 02:43:28 INFO     	 * (global step 26700: loss: 0.14831805229187012, lr: 5e-05
2023-12-21 02:43:36 INFO     	 * (global step 26750: loss: 0.23315611481666565, lr: 5e-05
2023-12-21 02:43:44 INFO     	 * (global step 26800: loss: 0.3128894120454788, lr: 5e-05
2023-12-21 02:43:52 INFO     	 * (global step 26850: loss: 0.20770253241062164, lr: 5e-05
2023-12-21 02:44:00 INFO     	 * (global step 26900: loss: 0.22800388932228088, lr: 5e-05
2023-12-21 02:44:08 INFO     	 * (global step 26950: loss: 0.3108856678009033, lr: 5e-05
2023-12-21 02:44:17 INFO     	 * (global step 27000: loss: 0.30627377331256866, lr: 5e-05
2023-12-21 02:44:25 INFO     	 * (global step 27050: loss: 0.19394470751285553, lr: 5e-05
2023-12-21 02:44:33 INFO     	 * (global step 27100: loss: 0.29520606994628906, lr: 5e-05
2023-12-21 02:44:41 INFO     	 * (global step 27150: loss: 0.32473331689834595, lr: 5e-05
2023-12-21 02:44:49 INFO     	 * (global step 27200: loss: 0.47534988820552826, lr: 5e-05
2023-12-21 02:44:57 INFO     	 * (global step 27250: loss: 0.2478727474808693, lr: 5e-05
2023-12-21 02:45:05 INFO     	 * (global step 27300: loss: 0.2971480190753937, lr: 5e-05
2023-12-21 02:45:13 INFO     	 * (global step 27350: loss: 0.2012699767947197, lr: 5e-05
2023-12-21 02:45:21 INFO     	 * (global step 27400: loss: 0.2641151398420334, lr: 5e-05
2023-12-21 02:45:29 INFO     	 * (global step 27450: loss: 0.3316836953163147, lr: 5e-05
2023-12-21 02:45:37 INFO     	 * (global step 27500: loss: 0.3096926212310791, lr: 5e-05
2023-12-21 02:45:45 INFO     	 * (global step 27550: loss: 0.4079368785023689, lr: 5e-05
2023-12-21 02:45:53 INFO     	 * (global step 27600: loss: 0.27892065048217773, lr: 5e-05
2023-12-21 02:46:01 INFO     	 * (global step 27650: loss: 0.2853919714689255, lr: 5e-05
2023-12-21 02:46:10 INFO     	 * (global step 27700: loss: 0.3100712150335312, lr: 5e-05
2023-12-21 02:46:18 INFO     	 * (global step 27750: loss: 0.23786155879497528, lr: 5e-05
2023-12-21 02:46:26 INFO     	 * (global step 27800: loss: 0.28048789501190186, lr: 5e-05
2023-12-21 02:46:34 INFO     	 * (global step 27850: loss: 0.3644624501466751, lr: 5e-05
2023-12-21 02:46:42 INFO     	 * (global step 27900: loss: 0.28361377865076065, lr: 5e-05
2023-12-21 02:46:50 INFO     	 * (global step 27950: loss: 0.42466312646865845, lr: 5e-05
2023-12-21 02:46:58 INFO     	 * (global step 28000: loss: 0.32043588161468506, lr: 5e-05
2023-12-21 02:47:06 INFO     	 * (global step 28050: loss: 0.27932044118642807, lr: 5e-05
2023-12-21 02:47:14 INFO     	 * (global step 28100: loss: 0.39892612397670746, lr: 5e-05
2023-12-21 02:47:22 INFO     	 * (global step 28150: loss: 0.34994275867938995, lr: 5e-05
2023-12-21 02:47:30 INFO     	 * (global step 28200: loss: 0.5178214460611343, lr: 5e-05
2023-12-21 02:47:38 INFO     	 * (global step 28250: loss: 0.20280978828668594, lr: 5e-05
2023-12-21 02:47:46 INFO     	 * (global step 28300: loss: 0.13987202942371368, lr: 5e-05
2023-12-21 02:47:53 INFO     [epoch 5/15] average loss: 0.29, lr: 5e-05
2023-12-21 02:47:53 INFO     saving model related files
2023-12-21 02:47:53 INFO     saving model
2023-12-21 02:47:53 INFO     saving tokenizer
2023-12-21 02:47:53 INFO     saving optimizer
2023-12-21 02:47:54 INFO     remove old optimizer files
2023-12-21 02:47:56 INFO     	 * (global step 28350: loss: 0.3357364982366562, lr: 5e-05
2023-12-21 02:48:04 INFO     	 * (global step 28400: loss: 0.3074800670146942, lr: 5e-05
2023-12-21 02:48:12 INFO     	 * (global step 28450: loss: 0.15352610498666763, lr: 5e-05
2023-12-21 02:48:21 INFO     	 * (global step 28500: loss: 0.3241972327232361, lr: 5e-05
2023-12-21 02:48:29 INFO     	 * (global step 28550: loss: 0.1619800664484501, lr: 5e-05
2023-12-21 02:48:37 INFO     	 * (global step 28600: loss: 0.24140580743551254, lr: 5e-05
2023-12-21 02:48:45 INFO     	 * (global step 28650: loss: 0.33991312980651855, lr: 5e-05
2023-12-21 02:48:53 INFO     	 * (global step 28700: loss: 0.2389753982424736, lr: 5e-05
2023-12-21 02:49:01 INFO     	 * (global step 28750: loss: 0.19570717215538025, lr: 5e-05
2023-12-21 02:49:09 INFO     	 * (global step 28800: loss: 0.20045918226242065, lr: 5e-05
2023-12-21 02:49:17 INFO     	 * (global step 28850: loss: 0.28114695847034454, lr: 5e-05
2023-12-21 02:49:25 INFO     	 * (global step 28900: loss: 0.3681456819176674, lr: 5e-05
2023-12-21 02:49:33 INFO     	 * (global step 28950: loss: 0.18403396755456924, lr: 5e-05
2023-12-21 02:49:42 INFO     	 * (global step 29000: loss: 0.23958592861890793, lr: 5e-05
2023-12-21 02:49:50 INFO     	 * (global step 29050: loss: 0.20433665066957474, lr: 5e-05
2023-12-21 02:49:58 INFO     	 * (global step 29100: loss: 0.2830384597182274, lr: 5e-05
2023-12-21 02:50:06 INFO     	 * (global step 29150: loss: 0.3218575716018677, lr: 5e-05
2023-12-21 02:50:14 INFO     	 * (global step 29200: loss: 0.38648417592048645, lr: 5e-05
2023-12-21 02:50:22 INFO     	 * (global step 29250: loss: 0.24067136645317078, lr: 5e-05
2023-12-21 02:50:30 INFO     	 * (global step 29300: loss: 0.2200831025838852, lr: 5e-05
2023-12-21 02:50:38 INFO     	 * (global step 29350: loss: 0.29121095687150955, lr: 5e-05
2023-12-21 02:50:46 INFO     	 * (global step 29400: loss: 0.19592762365937233, lr: 5e-05
2023-12-21 02:50:54 INFO     	 * (global step 29450: loss: 0.3037615418434143, lr: 5e-05
2023-12-21 02:51:02 INFO     	 * (global step 29500: loss: 0.21899964660406113, lr: 5e-05
2023-12-21 02:51:11 INFO     	 * (global step 29550: loss: 0.2468509003520012, lr: 5e-05
2023-12-21 02:51:19 INFO     	 * (global step 29600: loss: 0.30645062029361725, lr: 5e-05
2023-12-21 02:51:27 INFO     	 * (global step 29650: loss: 0.22326256334781647, lr: 5e-05
2023-12-21 02:51:35 INFO     	 * (global step 29700: loss: 0.24585331231355667, lr: 5e-05
2023-12-21 02:51:43 INFO     	 * (global step 29750: loss: 0.24059686809778214, lr: 5e-05
2023-12-21 02:51:51 INFO     	 * (global step 29800: loss: 0.594583585858345, lr: 5e-05
2023-12-21 02:51:59 INFO     	 * (global step 29850: loss: 0.21884401887655258, lr: 5e-05
2023-12-21 02:52:07 INFO     	 * (global step 29900: loss: 0.3505762070417404, lr: 5e-05
2023-12-21 02:52:15 INFO     	 * (global step 29950: loss: 0.25682416558265686, lr: 5e-05
2023-12-21 02:52:23 INFO     	 * (global step 30000: loss: 0.15447862446308136, lr: 5e-05
2023-12-21 02:52:31 INFO     	 * (global step 30050: loss: 0.2045598328113556, lr: 5e-05
2023-12-21 02:52:39 INFO     	 * (global step 30100: loss: 0.2724083960056305, lr: 5e-05
2023-12-21 02:52:47 INFO     	 * (global step 30150: loss: 0.2933104708790779, lr: 5e-05
2023-12-21 02:52:55 INFO     	 * (global step 30200: loss: 0.18515323102474213, lr: 5e-05
2023-12-21 02:53:03 INFO     	 * (global step 30250: loss: 0.2906264066696167, lr: 5e-05
2023-12-21 02:53:11 INFO     	 * (global step 30300: loss: 0.2020515576004982, lr: 5e-05
2023-12-21 02:53:19 INFO     	 * (global step 30350: loss: 0.2531363219022751, lr: 5e-05
2023-12-21 02:53:28 INFO     	 * (global step 30400: loss: 0.2502291649580002, lr: 5e-05
2023-12-21 02:53:36 INFO     	 * (global step 30450: loss: 0.3060927391052246, lr: 5e-05
2023-12-21 02:53:44 INFO     	 * (global step 30500: loss: 0.19914435595273972, lr: 5e-05
2023-12-21 02:53:52 INFO     	 * (global step 30550: loss: 0.24275552481412888, lr: 5e-05
2023-12-21 02:54:00 INFO     	 * (global step 30600: loss: 0.49489256739616394, lr: 5e-05
2023-12-21 02:54:08 INFO     	 * (global step 30650: loss: 0.22702927887439728, lr: 5e-05
2023-12-21 02:54:16 INFO     	 * (global step 30700: loss: 0.27675020694732666, lr: 5e-05
2023-12-21 02:54:24 INFO     	 * (global step 30750: loss: 0.25888966768980026, lr: 5e-05
2023-12-21 02:54:32 INFO     	 * (global step 30800: loss: 0.17287705093622208, lr: 5e-05
2023-12-21 02:54:40 INFO     	 * (global step 30850: loss: 0.21317686885595322, lr: 5e-05
2023-12-21 02:54:48 INFO     	 * (global step 30900: loss: 0.25792211294174194, lr: 5e-05
2023-12-21 02:54:57 INFO     	 * (global step 30950: loss: 0.35911373794078827, lr: 5e-05
2023-12-21 02:55:05 INFO     	 * (global step 31000: loss: 0.2811655253171921, lr: 5e-05
2023-12-21 02:55:13 INFO     	 * (global step 31050: loss: 0.26644808799028397, lr: 5e-05
2023-12-21 02:55:21 INFO     	 * (global step 31100: loss: 0.25400645285844803, lr: 5e-05
2023-12-21 02:55:29 INFO     	 * (global step 31150: loss: 0.22961967438459396, lr: 5e-05
2023-12-21 02:55:37 INFO     	 * (global step 31200: loss: 0.3216850608587265, lr: 5e-05
2023-12-21 02:55:45 INFO     	 * (global step 31250: loss: 0.2580881640315056, lr: 5e-05
2023-12-21 02:55:53 INFO     	 * (global step 31300: loss: 0.23552357405424118, lr: 5e-05
2023-12-21 02:56:01 INFO     	 * (global step 31350: loss: 0.3509786128997803, lr: 5e-05
2023-12-21 02:56:09 INFO     	 * (global step 31400: loss: 0.2606803700327873, lr: 5e-05
2023-12-21 02:56:17 INFO     	 * (global step 31450: loss: 0.2829468697309494, lr: 5e-05
2023-12-21 02:56:25 INFO     	 * (global step 31500: loss: 0.47592297196388245, lr: 5e-05
2023-12-21 02:56:33 INFO     	 * (global step 31550: loss: 0.22702185064554214, lr: 5e-05
2023-12-21 02:56:41 INFO     	 * (global step 31600: loss: 0.24880264699459076, lr: 5e-05
2023-12-21 02:56:49 INFO     	 * (global step 31650: loss: 0.18852171301841736, lr: 5e-05
2023-12-21 02:56:58 INFO     	 * (global step 31700: loss: 0.29660169780254364, lr: 5e-05
2023-12-21 02:57:06 INFO     	 * (global step 31750: loss: 0.41227492690086365, lr: 5e-05
2023-12-21 02:57:14 INFO     	 * (global step 31800: loss: 0.2047853320837021, lr: 5e-05
2023-12-21 02:57:22 INFO     	 * (global step 31850: loss: 0.2707644924521446, lr: 5e-05
2023-12-21 02:57:30 INFO     	 * (global step 31900: loss: 0.276231087744236, lr: 5e-05
2023-12-21 02:57:38 INFO     	 * (global step 31950: loss: 0.23050692677497864, lr: 5e-05
2023-12-21 02:57:46 INFO     	 * (global step 32000: loss: 0.2234150916337967, lr: 5e-05
2023-12-21 02:57:54 INFO     	 * (global step 32050: loss: 0.32379597425460815, lr: 5e-05
2023-12-21 02:58:02 INFO     	 * (global step 32100: loss: 0.3062075972557068, lr: 5e-05
2023-12-21 02:58:10 INFO     	 * (global step 32150: loss: 0.3199795335531235, lr: 5e-05
2023-12-21 02:58:18 INFO     	 * (global step 32200: loss: 0.29577386379241943, lr: 5e-05
2023-12-21 02:58:26 INFO     	 * (global step 32250: loss: 0.19539984315633774, lr: 5e-05
2023-12-21 02:58:35 INFO     	 * (global step 32300: loss: 0.2748925983905792, lr: 5e-05
2023-12-21 02:58:43 INFO     	 * (global step 32350: loss: 0.255406029522419, lr: 5e-05
2023-12-21 02:58:51 INFO     	 * (global step 32400: loss: 0.1895710676908493, lr: 5e-05
2023-12-21 02:58:59 INFO     	 * (global step 32450: loss: 0.23508809506893158, lr: 5e-05
2023-12-21 02:59:07 INFO     	 * (global step 32500: loss: 0.300193727016449, lr: 5e-05
2023-12-21 02:59:15 INFO     	 * (global step 32550: loss: 0.2922786772251129, lr: 5e-05
2023-12-21 02:59:23 INFO     	 * (global step 32600: loss: 0.15579108893871307, lr: 5e-05
2023-12-21 02:59:31 INFO     	 * (global step 32650: loss: 0.4420248121023178, lr: 5e-05
2023-12-21 02:59:39 INFO     	 * (global step 32700: loss: 0.2416571080684662, lr: 5e-05
2023-12-21 02:59:47 INFO     	 * (global step 32750: loss: 0.3293914645910263, lr: 5e-05
2023-12-21 02:59:55 INFO     	 * (global step 32800: loss: 0.19496124237775803, lr: 5e-05
2023-12-21 03:00:03 INFO     	 * (global step 32850: loss: 0.21229024231433868, lr: 5e-05
2023-12-21 03:00:11 INFO     	 * (global step 32900: loss: 0.3907923698425293, lr: 5e-05
2023-12-21 03:00:19 INFO     	 * (global step 32950: loss: 0.37960806488990784, lr: 5e-05
2023-12-21 03:00:27 INFO     	 * (global step 33000: loss: 0.23017684370279312, lr: 5e-05
2023-12-21 03:00:35 INFO     	 * (global step 33050: loss: 0.25216028839349747, lr: 5e-05
2023-12-21 03:00:37 INFO     [epoch 6/15] average loss: 0.283, lr: 5e-05
2023-12-21 03:00:37 INFO     saving model related files
2023-12-21 03:00:37 INFO     saving model
2023-12-21 03:00:38 INFO     saving tokenizer
2023-12-21 03:00:38 INFO     saving optimizer
2023-12-21 03:00:39 INFO     remove old optimizer files
2023-12-21 03:00:45 INFO     	 * (global step 33100: loss: 0.27224985510110855, lr: 5e-05
2023-12-21 03:00:53 INFO     	 * (global step 33150: loss: 0.3157385438680649, lr: 5e-05
2023-12-21 03:01:01 INFO     	 * (global step 33200: loss: 0.33200204372406006, lr: 5e-05
2023-12-21 03:01:10 INFO     	 * (global step 33250: loss: 0.27261535078287125, lr: 5e-05
2023-12-21 03:01:18 INFO     	 * (global step 33300: loss: 0.24052827060222626, lr: 5e-05
2023-12-21 03:01:26 INFO     	 * (global step 33350: loss: 0.27798043191432953, lr: 5e-05
2023-12-21 03:01:34 INFO     	 * (global step 33400: loss: 0.22040680795907974, lr: 5e-05
2023-12-21 03:01:42 INFO     	 * (global step 33450: loss: 0.2691153287887573, lr: 5e-05
2023-12-21 03:01:50 INFO     	 * (global step 33500: loss: 0.21790777146816254, lr: 5e-05
2023-12-21 03:01:58 INFO     	 * (global step 33550: loss: 0.2994735985994339, lr: 5e-05
2023-12-21 03:02:06 INFO     	 * (global step 33600: loss: 0.26279987394809723, lr: 5e-05
2023-12-21 03:02:14 INFO     	 * (global step 33650: loss: 0.34722092747688293, lr: 5e-05
2023-12-21 03:02:22 INFO     	 * (global step 33700: loss: 0.20396195352077484, lr: 5e-05
2023-12-21 03:02:30 INFO     	 * (global step 33750: loss: 0.3883122205734253, lr: 5e-05
2023-12-21 03:02:38 INFO     	 * (global step 33800: loss: 0.33996888250112534, lr: 5e-05
2023-12-21 03:02:46 INFO     	 * (global step 33850: loss: 0.29337430000305176, lr: 5e-05
2023-12-21 03:02:54 INFO     	 * (global step 33900: loss: 0.2836848795413971, lr: 5e-05
2023-12-21 03:03:02 INFO     	 * (global step 33950: loss: 0.27192696928977966, lr: 5e-05
2023-12-21 03:03:10 INFO     	 * (global step 34000: loss: 0.21241895109415054, lr: 5e-05
2023-12-21 03:03:19 INFO     	 * (global step 34050: loss: 0.31603485345840454, lr: 5e-05
2023-12-21 03:03:27 INFO     	 * (global step 34100: loss: 0.2625390812754631, lr: 5e-05
2023-12-21 03:03:35 INFO     	 * (global step 34150: loss: 0.457136869430542, lr: 5e-05
2023-12-21 03:03:43 INFO     	 * (global step 34200: loss: 0.34866924583911896, lr: 5e-05
2023-12-21 03:03:51 INFO     	 * (global step 34250: loss: 0.2717835381627083, lr: 5e-05
2023-12-21 03:03:59 INFO     	 * (global step 34300: loss: 0.23668208718299866, lr: 5e-05
2023-12-21 03:04:07 INFO     	 * (global step 34350: loss: 0.1667679250240326, lr: 5e-05
2023-12-21 03:04:15 INFO     	 * (global step 34400: loss: 0.23874682188034058, lr: 5e-05
2023-12-21 03:04:23 INFO     	 * (global step 34450: loss: 0.3244824558496475, lr: 5e-05
2023-12-21 03:04:31 INFO     	 * (global step 34500: loss: 0.266684427857399, lr: 5e-05
2023-12-21 03:04:39 INFO     	 * (global step 34550: loss: 0.2765527069568634, lr: 5e-05
2023-12-21 03:04:47 INFO     	 * (global step 34600: loss: 0.2139074131846428, lr: 5e-05
2023-12-21 03:04:55 INFO     	 * (global step 34650: loss: 0.22268462926149368, lr: 5e-05
2023-12-21 03:05:03 INFO     	 * (global step 34700: loss: 0.33643317222595215, lr: 5e-05
2023-12-21 03:05:11 INFO     	 * (global step 34750: loss: 0.23899934440851212, lr: 5e-05
2023-12-21 03:05:19 INFO     	 * (global step 34800: loss: 0.19518163055181503, lr: 5e-05
2023-12-21 03:05:27 INFO     	 * (global step 34850: loss: 0.2455773800611496, lr: 5e-05
2023-12-21 03:05:35 INFO     	 * (global step 34900: loss: 0.20151016488671303, lr: 5e-05
2023-12-21 03:05:43 INFO     	 * (global step 34950: loss: 0.2795923799276352, lr: 5e-05
2023-12-21 03:05:51 INFO     	 * (global step 35000: loss: 0.2801974415779114, lr: 5e-05
2023-12-21 03:06:00 INFO     	 * (global step 35050: loss: 0.4108463227748871, lr: 5e-05
2023-12-21 03:06:08 INFO     	 * (global step 35100: loss: 0.34773850440979004, lr: 5e-05
2023-12-21 03:06:16 INFO     	 * (global step 35150: loss: 0.3125127777457237, lr: 5e-05
2023-12-21 03:06:24 INFO     	 * (global step 35200: loss: 0.28264375776052475, lr: 5e-05
2023-12-21 03:06:32 INFO     	 * (global step 35250: loss: 0.2950795590877533, lr: 5e-05
2023-12-21 03:06:40 INFO     	 * (global step 35300: loss: 0.3387882560491562, lr: 5e-05
2023-12-21 03:06:48 INFO     	 * (global step 35350: loss: 0.21815300732851028, lr: 5e-05
2023-12-21 03:06:56 INFO     	 * (global step 35400: loss: 0.19980748742818832, lr: 5e-05
2023-12-21 03:07:04 INFO     	 * (global step 35450: loss: 0.35942013561725616, lr: 5e-05
2023-12-21 03:07:12 INFO     	 * (global step 35500: loss: 0.2989668771624565, lr: 5e-05
2023-12-21 03:07:20 INFO     	 * (global step 35550: loss: 0.17189061641693115, lr: 5e-05
2023-12-21 03:07:28 INFO     	 * (global step 35600: loss: 0.27973607182502747, lr: 5e-05
2023-12-21 03:07:36 INFO     	 * (global step 35650: loss: 0.306859515607357, lr: 5e-05
2023-12-21 03:07:44 INFO     	 * (global step 35700: loss: 0.34104397892951965, lr: 5e-05
2023-12-21 03:07:52 INFO     	 * (global step 35750: loss: 0.22688128799200058, lr: 5e-05
2023-12-21 03:08:00 INFO     	 * (global step 35800: loss: 0.2202375829219818, lr: 5e-05
2023-12-21 03:08:08 INFO     	 * (global step 35850: loss: 0.23709652572870255, lr: 5e-05
2023-12-21 03:08:16 INFO     	 * (global step 35900: loss: 0.2669328898191452, lr: 5e-05
2023-12-21 03:08:24 INFO     	 * (global step 35950: loss: 0.40066827833652496, lr: 5e-05
2023-12-21 03:08:33 INFO     	 * (global step 36000: loss: 0.220094233751297, lr: 5e-05
2023-12-21 03:08:41 INFO     	 * (global step 36050: loss: 0.09550698474049568, lr: 5e-05
2023-12-21 03:08:49 INFO     	 * (global step 36100: loss: 0.3246908560395241, lr: 5e-05
2023-12-21 03:08:57 INFO     	 * (global step 36150: loss: 0.4681621640920639, lr: 5e-05
2023-12-21 03:09:05 INFO     	 * (global step 36200: loss: 0.33516621589660645, lr: 5e-05
2023-12-21 03:09:13 INFO     	 * (global step 36250: loss: 0.230165496468544, lr: 5e-05
2023-12-21 03:09:21 INFO     	 * (global step 36300: loss: 0.2631591707468033, lr: 5e-05
2023-12-21 03:09:29 INFO     	 * (global step 36350: loss: 0.35574381053447723, lr: 5e-05
2023-12-21 03:09:37 INFO     	 * (global step 36400: loss: 0.23837514966726303, lr: 5e-05
2023-12-21 03:09:45 INFO     	 * (global step 36450: loss: 0.2229071781039238, lr: 5e-05
2023-12-21 03:09:53 INFO     	 * (global step 36500: loss: 0.40763549506664276, lr: 5e-05
2023-12-21 03:10:01 INFO     	 * (global step 36550: loss: 0.1982235237956047, lr: 5e-05
2023-12-21 03:10:09 INFO     	 * (global step 36600: loss: 0.47095195949077606, lr: 5e-05
2023-12-21 03:10:17 INFO     	 * (global step 36650: loss: 0.27315662801265717, lr: 5e-05
2023-12-21 03:10:25 INFO     	 * (global step 36700: loss: 0.24301284551620483, lr: 5e-05
2023-12-21 03:10:33 INFO     	 * (global step 36750: loss: 0.3720846623182297, lr: 5e-05
2023-12-21 03:10:41 INFO     	 * (global step 36800: loss: 0.2165898010134697, lr: 5e-05
2023-12-21 03:10:49 INFO     	 * (global step 36850: loss: 0.20600979775190353, lr: 5e-05
2023-12-21 03:10:57 INFO     	 * (global step 36900: loss: 0.31603895127773285, lr: 5e-05
2023-12-21 03:11:06 INFO     	 * (global step 36950: loss: 0.29211433976888657, lr: 5e-05
2023-12-21 03:11:14 INFO     	 * (global step 37000: loss: 0.2971889525651932, lr: 5e-05
2023-12-21 03:11:22 INFO     	 * (global step 37050: loss: 0.2084396779537201, lr: 5e-05
2023-12-21 03:11:30 INFO     	 * (global step 37100: loss: 0.29242053627967834, lr: 5e-05
2023-12-21 03:11:38 INFO     	 * (global step 37150: loss: 0.21413534134626389, lr: 5e-05
2023-12-21 03:11:46 INFO     	 * (global step 37200: loss: 0.4758417159318924, lr: 5e-05
2023-12-21 03:11:54 INFO     	 * (global step 37250: loss: 0.2148178443312645, lr: 5e-05
2023-12-21 03:12:02 INFO     	 * (global step 37300: loss: 0.2820326238870621, lr: 5e-05
2023-12-21 03:12:10 INFO     	 * (global step 37350: loss: 0.13095609098672867, lr: 5e-05
2023-12-21 03:12:18 INFO     	 * (global step 37400: loss: 0.24114256352186203, lr: 5e-05
2023-12-21 03:12:26 INFO     	 * (global step 37450: loss: 0.33252838253974915, lr: 5e-05
2023-12-21 03:12:34 INFO     	 * (global step 37500: loss: 0.2729802057147026, lr: 5e-05
2023-12-21 03:12:42 INFO     	 * (global step 37550: loss: 0.21782894432544708, lr: 5e-05
2023-12-21 03:12:50 INFO     	 * (global step 37600: loss: 0.2248334139585495, lr: 5e-05
2023-12-21 03:12:58 INFO     	 * (global step 37650: loss: 0.14999015256762505, lr: 5e-05
2023-12-21 03:13:06 INFO     	 * (global step 37700: loss: 0.25623486936092377, lr: 5e-05
2023-12-21 03:13:14 INFO     	 * (global step 37750: loss: 0.1770002245903015, lr: 5e-05
2023-12-21 03:13:20 INFO     [epoch 7/15] average loss: 0.278, lr: 5e-05
2023-12-21 03:13:20 INFO     saving model related files
2023-12-21 03:13:20 INFO     saving model
2023-12-21 03:13:20 INFO     saving tokenizer
2023-12-21 03:13:20 INFO     saving optimizer
2023-12-21 03:13:21 INFO     remove old optimizer files
2023-12-21 03:13:24 INFO     	 * (global step 37800: loss: 0.37265871465206146, lr: 5e-05
2023-12-21 03:13:32 INFO     	 * (global step 37850: loss: 0.24785198271274567, lr: 5e-05
2023-12-21 03:13:40 INFO     	 * (global step 37900: loss: 0.26822812855243683, lr: 5e-05
2023-12-21 03:13:48 INFO     	 * (global step 37950: loss: 0.2763657793402672, lr: 5e-05
2023-12-21 03:13:56 INFO     	 * (global step 38000: loss: 0.3150288462638855, lr: 5e-05
2023-12-21 03:14:04 INFO     	 * (global step 38050: loss: 0.2545376941561699, lr: 5e-05
2023-12-21 03:14:12 INFO     	 * (global step 38100: loss: 0.28010840713977814, lr: 5e-05
2023-12-21 03:14:20 INFO     	 * (global step 38150: loss: 0.2664985731244087, lr: 5e-05
2023-12-21 03:14:29 INFO     	 * (global step 38200: loss: 0.3619121015071869, lr: 5e-05
2023-12-21 03:14:37 INFO     	 * (global step 38250: loss: 0.34677914530038834, lr: 5e-05
2023-12-21 03:14:45 INFO     	 * (global step 38300: loss: 0.18834898620843887, lr: 5e-05
2023-12-21 03:14:53 INFO     	 * (global step 38350: loss: 0.3055870831012726, lr: 5e-05
2023-12-21 03:15:01 INFO     	 * (global step 38400: loss: 0.3189954161643982, lr: 5e-05
2023-12-21 03:15:09 INFO     	 * (global step 38450: loss: 0.32445788383483887, lr: 5e-05
2023-12-21 03:15:17 INFO     	 * (global step 38500: loss: 0.23127788305282593, lr: 5e-05
2023-12-21 03:15:25 INFO     	 * (global step 38550: loss: 0.29702936112880707, lr: 5e-05
2023-12-21 03:15:33 INFO     	 * (global step 38600: loss: 0.3293659836053848, lr: 5e-05
2023-12-21 03:15:42 INFO     	 * (global step 38650: loss: 0.13852204382419586, lr: 5e-05
2023-12-21 03:15:50 INFO     	 * (global step 38700: loss: 0.23883755505084991, lr: 5e-05
2023-12-21 03:15:58 INFO     	 * (global step 38750: loss: 0.27492713183164597, lr: 5e-05
2023-12-21 03:16:06 INFO     	 * (global step 38800: loss: 0.19961056113243103, lr: 5e-05
2023-12-21 03:16:14 INFO     	 * (global step 38850: loss: 0.4086778163909912, lr: 5e-05
2023-12-21 03:16:22 INFO     	 * (global step 38900: loss: 0.21978801488876343, lr: 5e-05
2023-12-21 03:16:30 INFO     	 * (global step 38950: loss: 0.23288771510124207, lr: 5e-05
2023-12-21 03:16:38 INFO     	 * (global step 39000: loss: 0.25839127600193024, lr: 5e-05
2023-12-21 03:16:46 INFO     	 * (global step 39050: loss: 0.28762392699718475, lr: 5e-05
2023-12-21 03:16:54 INFO     	 * (global step 39100: loss: 0.21956279873847961, lr: 5e-05
2023-12-21 03:17:02 INFO     	 * (global step 39150: loss: 0.1879625841975212, lr: 5e-05
2023-12-21 03:17:10 INFO     	 * (global step 39200: loss: 0.28791824728250504, lr: 5e-05
2023-12-21 03:17:18 INFO     	 * (global step 39250: loss: 0.23592150956392288, lr: 5e-05
2023-12-21 03:17:27 INFO     	 * (global step 39300: loss: 0.18654778599739075, lr: 5e-05
2023-12-21 03:17:35 INFO     	 * (global step 39350: loss: 0.22345535457134247, lr: 5e-05
2023-12-21 03:17:43 INFO     	 * (global step 39400: loss: 0.1934785321354866, lr: 5e-05
2023-12-21 03:17:51 INFO     	 * (global step 39450: loss: 0.2183009535074234, lr: 5e-05
2023-12-21 03:17:59 INFO     	 * (global step 39500: loss: 0.21809807419776917, lr: 5e-05
2023-12-21 03:18:07 INFO     	 * (global step 39550: loss: 0.20884600281715393, lr: 5e-05
2023-12-21 03:18:15 INFO     	 * (global step 39600: loss: 0.2734993100166321, lr: 5e-05
2023-12-21 03:18:23 INFO     	 * (global step 39650: loss: 0.20837783813476562, lr: 5e-05
2023-12-21 03:18:31 INFO     	 * (global step 39700: loss: 0.21142909675836563, lr: 5e-05
2023-12-21 03:18:39 INFO     	 * (global step 39750: loss: 0.31831102073192596, lr: 5e-05
2023-12-21 03:18:47 INFO     	 * (global step 39800: loss: 0.36451028287410736, lr: 5e-05
2023-12-21 03:18:55 INFO     	 * (global step 39850: loss: 0.3521149605512619, lr: 5e-05
2023-12-21 03:19:04 INFO     	 * (global step 39900: loss: 0.347558930516243, lr: 5e-05
2023-12-21 03:19:12 INFO     	 * (global step 39950: loss: 0.3467864841222763, lr: 5e-05
2023-12-21 03:19:20 INFO     	 * (global step 40000: loss: 0.20708756521344185, lr: 5e-05
2023-12-21 03:19:28 INFO     	 * (global step 40050: loss: 0.24571681022644043, lr: 5e-05
2023-12-21 03:19:36 INFO     	 * (global step 40100: loss: 0.2700878158211708, lr: 5e-05
2023-12-21 03:19:44 INFO     	 * (global step 40150: loss: 0.22764235734939575, lr: 5e-05
2023-12-21 03:19:52 INFO     	 * (global step 40200: loss: 0.2875373139977455, lr: 5e-05
2023-12-21 03:20:00 INFO     	 * (global step 40250: loss: 0.32880882918834686, lr: 5e-05
2023-12-21 03:20:08 INFO     	 * (global step 40300: loss: 0.2620897963643074, lr: 5e-05
2023-12-21 03:20:16 INFO     	 * (global step 40350: loss: 0.25564437359571457, lr: 5e-05
2023-12-21 03:20:24 INFO     	 * (global step 40400: loss: 0.2236352562904358, lr: 5e-05
2023-12-21 03:20:32 INFO     	 * (global step 40450: loss: 0.20439547300338745, lr: 5e-05
2023-12-21 03:20:40 INFO     	 * (global step 40500: loss: 0.180356964468956, lr: 5e-05
2023-12-21 03:20:48 INFO     	 * (global step 40550: loss: 0.23041356354951859, lr: 5e-05
2023-12-21 03:20:56 INFO     	 * (global step 40600: loss: 0.19173576682806015, lr: 5e-05
2023-12-21 03:21:05 INFO     	 * (global step 40650: loss: 0.297889769077301, lr: 5e-05
2023-12-21 03:21:13 INFO     	 * (global step 40700: loss: 0.3700694441795349, lr: 5e-05
2023-12-21 03:21:21 INFO     	 * (global step 40750: loss: 0.24317672103643417, lr: 5e-05
2023-12-21 03:21:29 INFO     	 * (global step 40800: loss: 0.33334773778915405, lr: 5e-05
2023-12-21 03:21:37 INFO     	 * (global step 40850: loss: 0.2836628034710884, lr: 5e-05
2023-12-21 03:21:45 INFO     	 * (global step 40900: loss: 0.28047211468219757, lr: 5e-05
2023-12-21 03:21:53 INFO     	 * (global step 40950: loss: 0.2076839581131935, lr: 5e-05
2023-12-21 03:22:01 INFO     	 * (global step 41000: loss: 0.36034512519836426, lr: 5e-05
2023-12-21 03:22:09 INFO     	 * (global step 41050: loss: 0.24021068960428238, lr: 5e-05
2023-12-21 03:22:17 INFO     	 * (global step 41100: loss: 0.24099906533956528, lr: 5e-05
2023-12-21 03:22:25 INFO     	 * (global step 41150: loss: 0.25925251096487045, lr: 5e-05
2023-12-21 03:22:33 INFO     	 * (global step 41200: loss: 0.24519473314285278, lr: 5e-05
2023-12-21 03:22:41 INFO     	 * (global step 41250: loss: 0.1787981539964676, lr: 5e-05
2023-12-21 03:22:49 INFO     	 * (global step 41300: loss: 0.33483826369047165, lr: 5e-05
2023-12-21 03:22:58 INFO     	 * (global step 41350: loss: 0.21710151433944702, lr: 5e-05
2023-12-21 03:23:06 INFO     	 * (global step 41400: loss: 0.17905006557703018, lr: 5e-05
2023-12-21 03:23:14 INFO     	 * (global step 41450: loss: 0.3520675003528595, lr: 5e-05
2023-12-21 03:23:22 INFO     	 * (global step 41500: loss: 0.24972382932901382, lr: 5e-05
2023-12-21 03:23:30 INFO     	 * (global step 41550: loss: 0.2866051644086838, lr: 5e-05
2023-12-21 03:23:38 INFO     	 * (global step 41600: loss: 0.3546021431684494, lr: 5e-05
2023-12-21 03:23:46 INFO     	 * (global step 41650: loss: 0.2525147721171379, lr: 5e-05
2023-12-21 03:23:54 INFO     	 * (global step 41700: loss: 0.33203358948230743, lr: 5e-05
2023-12-21 03:24:02 INFO     	 * (global step 41750: loss: 0.17023301124572754, lr: 5e-05
2023-12-21 03:24:10 INFO     	 * (global step 41800: loss: 0.2846129685640335, lr: 5e-05
2023-12-21 03:24:18 INFO     	 * (global step 41850: loss: 0.3062099665403366, lr: 5e-05
2023-12-21 03:24:26 INFO     	 * (global step 41900: loss: 0.3117362931370735, lr: 5e-05
2023-12-21 03:24:34 INFO     	 * (global step 41950: loss: 0.2874869704246521, lr: 5e-05
2023-12-21 03:24:42 INFO     	 * (global step 42000: loss: 0.18367163091897964, lr: 5e-05
2023-12-21 03:24:51 INFO     	 * (global step 42050: loss: 0.1933150514960289, lr: 5e-05
2023-12-21 03:24:59 INFO     	 * (global step 42100: loss: 0.3006742149591446, lr: 5e-05
2023-12-21 03:25:07 INFO     	 * (global step 42150: loss: 0.19352106750011444, lr: 5e-05
2023-12-21 03:25:15 INFO     	 * (global step 42200: loss: 0.2591584771871567, lr: 5e-05
2023-12-21 03:25:23 INFO     	 * (global step 42250: loss: 0.29011809825897217, lr: 5e-05
2023-12-21 03:25:31 INFO     	 * (global step 42300: loss: 0.2579430788755417, lr: 5e-05
2023-12-21 03:25:39 INFO     	 * (global step 42350: loss: 0.17522260546684265, lr: 5e-05
2023-12-21 03:25:47 INFO     	 * (global step 42400: loss: 0.29783104360103607, lr: 5e-05
2023-12-21 03:25:55 INFO     	 * (global step 42450: loss: 0.38161860406398773, lr: 5e-05
2023-12-21 03:26:03 INFO     	 * (global step 42500: loss: 0.2204945608973503, lr: 5e-05
2023-12-21 03:26:04 INFO     [epoch 8/15] average loss: 0.273, lr: 5e-05
2023-12-21 03:26:04 INFO     saving model related files
2023-12-21 03:26:04 INFO     saving model
2023-12-21 03:26:05 INFO     saving tokenizer
2023-12-21 03:26:05 INFO     saving optimizer
2023-12-21 03:26:06 INFO     remove old optimizer files
2023-12-21 03:26:13 INFO     	 * (global step 42550: loss: 0.28574275970458984, lr: 5e-05
2023-12-21 03:26:21 INFO     	 * (global step 42600: loss: 0.290225014090538, lr: 5e-05
2023-12-21 03:26:29 INFO     	 * (global step 42650: loss: 0.32706907391548157, lr: 5e-05
2023-12-21 03:26:37 INFO     	 * (global step 42700: loss: 0.18667814135551453, lr: 5e-05
2023-12-21 03:26:45 INFO     	 * (global step 42750: loss: 0.27541884779930115, lr: 5e-05
2023-12-21 03:26:53 INFO     	 * (global step 42800: loss: 0.49486295878887177, lr: 5e-05
2023-12-21 03:27:01 INFO     	 * (global step 42850: loss: 0.21878036856651306, lr: 5e-05
2023-12-21 03:27:09 INFO     	 * (global step 42900: loss: 0.28671345114707947, lr: 5e-05
2023-12-21 03:27:17 INFO     	 * (global step 42950: loss: 0.28452441841363907, lr: 5e-05
2023-12-21 03:27:25 INFO     	 * (global step 43000: loss: 0.3412710204720497, lr: 5e-05
2023-12-21 03:27:33 INFO     	 * (global step 43050: loss: 0.2554154470562935, lr: 5e-05
2023-12-21 03:27:41 INFO     	 * (global step 43100: loss: 0.30854663252830505, lr: 5e-05
2023-12-21 03:27:49 INFO     	 * (global step 43150: loss: 0.25797489285469055, lr: 5e-05
2023-12-21 03:27:57 INFO     	 * (global step 43200: loss: 0.2516723573207855, lr: 5e-05
2023-12-21 03:28:05 INFO     	 * (global step 43250: loss: 0.21518856287002563, lr: 5e-05
2023-12-21 03:28:14 INFO     	 * (global step 43300: loss: 0.46737344563007355, lr: 5e-05
2023-12-21 03:28:22 INFO     	 * (global step 43350: loss: 0.3042309284210205, lr: 5e-05
2023-12-21 03:28:30 INFO     	 * (global step 43400: loss: 0.2103971689939499, lr: 5e-05
2023-12-21 03:28:38 INFO     	 * (global step 43450: loss: 0.2438517063856125, lr: 5e-05
2023-12-21 03:28:46 INFO     	 * (global step 43500: loss: 0.3898336887359619, lr: 5e-05
2023-12-21 03:28:54 INFO     	 * (global step 43550: loss: 0.2898930460214615, lr: 5e-05
2023-12-21 03:29:02 INFO     	 * (global step 43600: loss: 0.31587448716163635, lr: 5e-05
2023-12-21 03:29:10 INFO     	 * (global step 43650: loss: 0.47153930366039276, lr: 5e-05
2023-12-21 03:29:18 INFO     	 * (global step 43700: loss: 0.25062044709920883, lr: 5e-05
2023-12-21 03:29:26 INFO     	 * (global step 43750: loss: 0.30277982354164124, lr: 5e-05
2023-12-21 03:29:34 INFO     	 * (global step 43800: loss: 0.2322784885764122, lr: 5e-05
2023-12-21 03:29:42 INFO     	 * (global step 43850: loss: 0.27054905891418457, lr: 5e-05
2023-12-21 03:29:50 INFO     	 * (global step 43900: loss: 0.33041812479496, lr: 5e-05
2023-12-21 03:29:59 INFO     	 * (global step 43950: loss: 0.20672712475061417, lr: 5e-05
2023-12-21 03:30:07 INFO     	 * (global step 44000: loss: 0.3247572034597397, lr: 5e-05
2023-12-21 03:30:15 INFO     	 * (global step 44050: loss: 0.24929168075323105, lr: 5e-05
2023-12-21 03:30:23 INFO     	 * (global step 44100: loss: 0.20718374103307724, lr: 5e-05
2023-12-21 03:30:31 INFO     	 * (global step 44150: loss: 0.37392091751098633, lr: 5e-05
2023-12-21 03:30:39 INFO     	 * (global step 44200: loss: 0.18028251454234123, lr: 5e-05
2023-12-21 03:30:47 INFO     	 * (global step 44250: loss: 0.2922065481543541, lr: 5e-05
2023-12-21 03:30:55 INFO     	 * (global step 44300: loss: 0.6285474002361298, lr: 5e-05
2023-12-21 03:31:03 INFO     	 * (global step 44350: loss: 0.23463770747184753, lr: 5e-05
2023-12-21 03:31:11 INFO     	 * (global step 44400: loss: 0.17975885048508644, lr: 5e-05
2023-12-21 03:31:19 INFO     	 * (global step 44450: loss: 0.32422420382499695, lr: 5e-05
2023-12-21 03:31:27 INFO     	 * (global step 44500: loss: 0.3645487278699875, lr: 5e-05
2023-12-21 03:31:35 INFO     	 * (global step 44550: loss: 0.16009679436683655, lr: 5e-05
2023-12-21 03:31:43 INFO     	 * (global step 44600: loss: 0.19801387935876846, lr: 5e-05
2023-12-21 03:31:51 INFO     	 * (global step 44650: loss: 0.33206868171691895, lr: 5e-05
2023-12-21 03:31:59 INFO     	 * (global step 44700: loss: 0.23715263605117798, lr: 5e-05
2023-12-21 03:32:07 INFO     	 * (global step 44750: loss: 0.18741130083799362, lr: 5e-05
2023-12-21 03:32:15 INFO     	 * (global step 44800: loss: 0.24570706486701965, lr: 5e-05
2023-12-21 03:32:23 INFO     	 * (global step 44850: loss: 0.254177488386631, lr: 5e-05
2023-12-21 03:32:31 INFO     	 * (global step 44900: loss: 0.21086662262678146, lr: 5e-05
2023-12-21 03:32:40 INFO     	 * (global step 44950: loss: 0.2641874924302101, lr: 5e-05
2023-12-21 03:32:48 INFO     	 * (global step 45000: loss: 0.22557630389928818, lr: 5e-05
2023-12-21 03:32:56 INFO     	 * (global step 45050: loss: 0.2583339065313339, lr: 5e-05
2023-12-21 03:33:04 INFO     	 * (global step 45100: loss: 0.30209746956825256, lr: 5e-05
2023-12-21 03:33:12 INFO     	 * (global step 45150: loss: 0.3262424021959305, lr: 5e-05
2023-12-21 03:33:20 INFO     	 * (global step 45200: loss: 0.18971288949251175, lr: 5e-05
2023-12-21 03:33:28 INFO     	 * (global step 45250: loss: 0.2793213799595833, lr: 5e-05
2023-12-21 03:33:36 INFO     	 * (global step 45300: loss: 0.19596508890390396, lr: 5e-05
2023-12-21 03:33:44 INFO     	 * (global step 45350: loss: 0.20832187682390213, lr: 5e-05
2023-12-21 03:33:52 INFO     	 * (global step 45400: loss: 0.23613454401493073, lr: 5e-05
2023-12-21 03:34:00 INFO     	 * (global step 45450: loss: 0.187063317745924, lr: 5e-05
2023-12-21 03:34:08 INFO     	 * (global step 45500: loss: 0.19314555823802948, lr: 5e-05
2023-12-21 03:34:16 INFO     	 * (global step 45550: loss: 0.2604500651359558, lr: 5e-05
2023-12-21 03:34:25 INFO     	 * (global step 45600: loss: 0.26306886225938797, lr: 5e-05
2023-12-21 03:34:33 INFO     	 * (global step 45650: loss: 0.30214376747608185, lr: 5e-05
2023-12-21 03:34:41 INFO     	 * (global step 45700: loss: 0.21919739991426468, lr: 5e-05
2023-12-21 03:34:49 INFO     	 * (global step 45750: loss: 0.4232400208711624, lr: 5e-05
2023-12-21 03:34:57 INFO     	 * (global step 45800: loss: 0.2698180004954338, lr: 5e-05
2023-12-21 03:35:05 INFO     	 * (global step 45850: loss: 0.2704629674553871, lr: 5e-05
2023-12-21 03:35:13 INFO     	 * (global step 45900: loss: 0.41719290614128113, lr: 5e-05
2023-12-21 03:35:21 INFO     	 * (global step 45950: loss: 0.3323400393128395, lr: 5e-05
2023-12-21 03:35:29 INFO     	 * (global step 46000: loss: 0.20929435640573502, lr: 5e-05
2023-12-21 03:35:37 INFO     	 * (global step 46050: loss: 0.23155047744512558, lr: 5e-05
2023-12-21 03:35:45 INFO     	 * (global step 46100: loss: 0.37449952214956284, lr: 5e-05
2023-12-21 03:35:53 INFO     	 * (global step 46150: loss: 0.17950034886598587, lr: 5e-05
2023-12-21 03:36:01 INFO     	 * (global step 46200: loss: 0.18185630440711975, lr: 5e-05
2023-12-21 03:36:09 INFO     	 * (global step 46250: loss: 0.2353205382823944, lr: 5e-05
2023-12-21 03:36:17 INFO     	 * (global step 46300: loss: 0.21201561391353607, lr: 5e-05
2023-12-21 03:36:25 INFO     	 * (global step 46350: loss: 0.2694929167628288, lr: 5e-05
2023-12-21 03:36:33 INFO     	 * (global step 46400: loss: 0.2803936302661896, lr: 5e-05
2023-12-21 03:36:41 INFO     	 * (global step 46450: loss: 0.33450569212436676, lr: 5e-05
2023-12-21 03:36:50 INFO     	 * (global step 46500: loss: 0.16183384507894516, lr: 5e-05
2023-12-21 03:36:58 INFO     	 * (global step 46550: loss: 0.557280033826828, lr: 5e-05
2023-12-21 03:37:06 INFO     	 * (global step 46600: loss: 0.29408954083919525, lr: 5e-05
2023-12-21 03:37:14 INFO     	 * (global step 46650: loss: 0.36066506803035736, lr: 5e-05
2023-12-21 03:37:22 INFO     	 * (global step 46700: loss: 0.3093748390674591, lr: 5e-05
2023-12-21 03:37:30 INFO     	 * (global step 46750: loss: 0.27738749980926514, lr: 5e-05
2023-12-21 03:37:38 INFO     	 * (global step 46800: loss: 0.1785939335823059, lr: 5e-05
2023-12-21 03:37:46 INFO     	 * (global step 46850: loss: 0.42496834695339203, lr: 5e-05
2023-12-21 03:37:54 INFO     	 * (global step 46900: loss: 0.18567214906215668, lr: 5e-05
2023-12-21 03:38:02 INFO     	 * (global step 46950: loss: 0.1954667530953884, lr: 5e-05
2023-12-21 03:38:10 INFO     	 * (global step 47000: loss: 0.36267971247434616, lr: 5e-05
2023-12-21 03:38:18 INFO     	 * (global step 47050: loss: 0.2626430541276932, lr: 5e-05
2023-12-21 03:38:26 INFO     	 * (global step 47100: loss: 0.21374117955565453, lr: 5e-05
2023-12-21 03:38:34 INFO     	 * (global step 47150: loss: 0.22795405983924866, lr: 5e-05
2023-12-21 03:38:42 INFO     	 * (global step 47200: loss: 0.22417954355478287, lr: 5e-05
2023-12-21 03:38:47 INFO     [epoch 9/15] average loss: 0.269, lr: 5e-05
2023-12-21 03:38:47 INFO     saving model related files
2023-12-21 03:38:47 INFO     saving model
2023-12-21 03:38:48 INFO     saving tokenizer
2023-12-21 03:38:48 INFO     saving optimizer
2023-12-21 03:38:49 INFO     remove old optimizer files
2023-12-21 03:38:49 INFO     complete training: model ckpt was saved at small_combined_trained_ckpt/model_uramvg
2023-12-21 03:38:49 INFO     ## 1st RUN: Configuration 8/12 ##
2023-12-21 03:38:49 INFO     initialize model trainer
2023-12-21 03:38:49 INFO     initialize checkpoint at small_combined_trained_ckpt/model_nxaqhy
2023-12-21 03:38:49 INFO     hyperparameters
2023-12-21 03:38:49 INFO     	 * dataset_path: StellarMilk/squad_newsqa
2023-12-21 03:38:49 INFO     	 * dataset_name: default
2023-12-21 03:38:49 INFO     	 * input_types: ['paragraph']
2023-12-21 03:38:49 INFO     	 * output_types: ['questions_answers']
2023-12-21 03:38:49 INFO     	 * prefix_types: ['qag']
2023-12-21 03:38:49 INFO     	 * model: t5-small
2023-12-21 03:38:49 INFO     	 * max_length: 512
2023-12-21 03:38:49 INFO     	 * max_length_output: 512
2023-12-21 03:38:49 INFO     	 * epoch: 15
2023-12-21 03:38:49 INFO     	 * batch: 2
2023-12-21 03:38:49 INFO     	 * lr: 1e-05
2023-12-21 03:38:49 INFO     	 * fp16: False
2023-12-21 03:38:49 INFO     	 * random_seed: 1
2023-12-21 03:38:49 INFO     	 * gradient_accumulation_steps: 4
2023-12-21 03:38:49 INFO     	 * label_smoothing: 0.15
2023-12-21 03:38:49 INFO     initialize checkpoint with t5-small
2023-12-21 03:38:50 INFO     use spaCy answer extraction model: positionrank
2023-12-21 03:38:51 INFO     Model `t5-small`
2023-12-21 03:38:51 INFO     	 * Num of GPU in use: 1
2023-12-21 03:38:51 INFO     	 * Prefix: True
2023-12-21 03:38:51 INFO     	 * Language: en (ignore at the training phase)
2023-12-21 03:38:51 INFO     dataset preprocessing
2023-12-21 03:38:52 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/squad_newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-21 03:38:54 INFO     start model training
2023-12-21 03:39:10 INFO     	 * (global step 50: loss: 4.9993051290512085, lr: 1e-05
2023-12-21 03:39:25 INFO     	 * (global step 100: loss: 2.6304880380630493, lr: 1e-05
2023-12-21 03:39:41 INFO     	 * (global step 150: loss: 2.11302986741066, lr: 1e-05
2023-12-21 03:39:57 INFO     	 * (global step 200: loss: 1.7670029699802399, lr: 1e-05
2023-12-21 03:40:12 INFO     	 * (global step 250: loss: 1.4231001138687134, lr: 1e-05
2023-12-21 03:40:28 INFO     	 * (global step 300: loss: 1.2368205040693283, lr: 1e-05
2023-12-21 03:40:43 INFO     	 * (global step 350: loss: 1.1538020074367523, lr: 1e-05
2023-12-21 03:40:59 INFO     	 * (global step 400: loss: 0.8391617238521576, lr: 1e-05
2023-12-21 03:41:15 INFO     	 * (global step 450: loss: 0.7780889421701431, lr: 1e-05
2023-12-21 03:41:30 INFO     	 * (global step 500: loss: 0.7121539413928986, lr: 1e-05
2023-12-21 03:41:46 INFO     	 * (global step 550: loss: 0.7944482564926147, lr: 1e-05
2023-12-21 03:42:02 INFO     	 * (global step 600: loss: 0.7913478761911392, lr: 1e-05
2023-12-21 03:42:17 INFO     	 * (global step 650: loss: 0.6261240690946579, lr: 1e-05
2023-12-21 03:42:33 INFO     	 * (global step 700: loss: 0.6075136810541153, lr: 1e-05
2023-12-21 03:42:49 INFO     	 * (global step 750: loss: 0.770898699760437, lr: 1e-05
2023-12-21 03:43:04 INFO     	 * (global step 800: loss: 0.6155637204647064, lr: 1e-05
2023-12-21 03:43:20 INFO     	 * (global step 850: loss: 0.600632332265377, lr: 1e-05
2023-12-21 03:43:35 INFO     	 * (global step 900: loss: 0.5872211158275604, lr: 1e-05
2023-12-21 03:43:51 INFO     	 * (global step 950: loss: 0.5892715379595757, lr: 1e-05
2023-12-21 03:44:07 INFO     	 * (global step 1000: loss: 0.576823778450489, lr: 1e-05
2023-12-21 03:44:22 INFO     	 * (global step 1050: loss: 0.5985307544469833, lr: 1e-05
2023-12-21 03:44:38 INFO     	 * (global step 1100: loss: 0.6072792708873749, lr: 1e-05
2023-12-21 03:44:54 INFO     	 * (global step 1150: loss: 0.8434543237090111, lr: 1e-05
2023-12-21 03:45:09 INFO     	 * (global step 1200: loss: 0.5258514061570168, lr: 1e-05
2023-12-21 03:45:25 INFO     	 * (global step 1250: loss: 0.4998392388224602, lr: 1e-05
2023-12-21 03:45:41 INFO     	 * (global step 1300: loss: 0.5548710003495216, lr: 1e-05
2023-12-21 03:45:56 INFO     	 * (global step 1350: loss: 0.4994741156697273, lr: 1e-05
2023-12-21 03:46:12 INFO     	 * (global step 1400: loss: 0.5944292545318604, lr: 1e-05
2023-12-21 03:46:28 INFO     	 * (global step 1450: loss: 0.5333956554532051, lr: 1e-05
2023-12-21 03:46:44 INFO     	 * (global step 1500: loss: 0.6335693895816803, lr: 1e-05
2023-12-21 03:46:59 INFO     	 * (global step 1550: loss: 0.45359544456005096, lr: 1e-05
2023-12-21 03:47:15 INFO     	 * (global step 1600: loss: 0.4516808018088341, lr: 1e-05
2023-12-21 03:47:31 INFO     	 * (global step 1650: loss: 0.5743836089968681, lr: 1e-05
2023-12-21 03:47:47 INFO     	 * (global step 1700: loss: 0.4498150870203972, lr: 1e-05
2023-12-21 03:48:03 INFO     	 * (global step 1750: loss: 0.42925315350294113, lr: 1e-05
2023-12-21 03:48:18 INFO     	 * (global step 1800: loss: 0.4449857324361801, lr: 1e-05
2023-12-21 03:48:34 INFO     	 * (global step 1850: loss: 0.3853289633989334, lr: 1e-05
2023-12-21 03:48:50 INFO     	 * (global step 1900: loss: 0.4881161004304886, lr: 1e-05
2023-12-21 03:49:06 INFO     	 * (global step 1950: loss: 0.44532080739736557, lr: 1e-05
2023-12-21 03:49:21 INFO     	 * (global step 2000: loss: 0.3879989758133888, lr: 1e-05
2023-12-21 03:49:37 INFO     	 * (global step 2050: loss: 0.41059695929288864, lr: 1e-05
2023-12-21 03:49:53 INFO     	 * (global step 2100: loss: 0.4086983725428581, lr: 1e-05
2023-12-21 03:50:09 INFO     	 * (global step 2150: loss: 0.5615677312016487, lr: 1e-05
2023-12-21 03:50:24 INFO     	 * (global step 2200: loss: 0.6007189527153969, lr: 1e-05
2023-12-21 03:50:40 INFO     	 * (global step 2250: loss: 0.40429677069187164, lr: 1e-05
2023-12-21 03:50:56 INFO     	 * (global step 2300: loss: 0.49800345301628113, lr: 1e-05
2023-12-21 03:51:12 INFO     	 * (global step 2350: loss: 0.479861356317997, lr: 1e-05
2023-12-21 03:51:15 INFO     [epoch 0/15] average loss: 0.917, lr: 1e-05
2023-12-21 03:51:15 INFO     saving model related files
2023-12-21 03:51:15 INFO     saving model
2023-12-21 03:51:16 INFO     saving tokenizer
2023-12-21 03:51:16 INFO     saving optimizer
2023-12-21 03:51:17 INFO     remove old optimizer files
2023-12-21 03:51:29 INFO     	 * (global step 2400: loss: 0.42973946779966354, lr: 1e-05
2023-12-21 03:51:45 INFO     	 * (global step 2450: loss: 0.4783950522542, lr: 1e-05
2023-12-21 03:52:01 INFO     	 * (global step 2500: loss: 0.500094436109066, lr: 1e-05
2023-12-21 03:52:16 INFO     	 * (global step 2550: loss: 0.435795146971941, lr: 1e-05
2023-12-21 03:52:32 INFO     	 * (global step 2600: loss: 0.42568351328372955, lr: 1e-05
2023-12-21 03:52:48 INFO     	 * (global step 2650: loss: 0.7038384974002838, lr: 1e-05
2023-12-21 03:53:04 INFO     	 * (global step 2700: loss: 0.5031572207808495, lr: 1e-05
2023-12-21 03:53:19 INFO     	 * (global step 2750: loss: 0.378230981528759, lr: 1e-05
2023-12-21 03:53:35 INFO     	 * (global step 2800: loss: 0.5800271704792976, lr: 1e-05
2023-12-21 03:53:51 INFO     	 * (global step 2850: loss: 0.573373943567276, lr: 1e-05
2023-12-21 03:54:06 INFO     	 * (global step 2900: loss: 0.5311386957764626, lr: 1e-05
2023-12-21 03:54:22 INFO     	 * (global step 2950: loss: 0.4331899434328079, lr: 1e-05
2023-12-21 03:54:38 INFO     	 * (global step 3000: loss: 0.4906816780567169, lr: 1e-05
2023-12-21 03:54:54 INFO     	 * (global step 3050: loss: 0.44736193120479584, lr: 1e-05
2023-12-21 03:55:10 INFO     	 * (global step 3100: loss: 0.43193238973617554, lr: 1e-05
2023-12-21 03:55:25 INFO     	 * (global step 3150: loss: 0.5998374745249748, lr: 1e-05
2023-12-21 03:55:41 INFO     	 * (global step 3200: loss: 0.39568790793418884, lr: 1e-05
2023-12-21 03:55:57 INFO     	 * (global step 3250: loss: 0.4740102291107178, lr: 1e-05
2023-12-21 03:56:13 INFO     	 * (global step 3300: loss: 0.401681587100029, lr: 1e-05
2023-12-21 03:56:28 INFO     	 * (global step 3350: loss: 0.39075034111738205, lr: 1e-05
2023-12-21 03:56:44 INFO     	 * (global step 3400: loss: 0.37084972113370895, lr: 1e-05
2023-12-21 03:57:00 INFO     	 * (global step 3450: loss: 0.4946783632040024, lr: 1e-05
2023-12-21 03:57:16 INFO     	 * (global step 3500: loss: 0.6415796652436256, lr: 1e-05
2023-12-21 03:57:31 INFO     	 * (global step 3550: loss: 0.3979099839925766, lr: 1e-05
2023-12-21 03:57:47 INFO     	 * (global step 3600: loss: 0.44487766921520233, lr: 1e-05
2023-12-21 03:58:03 INFO     	 * (global step 3650: loss: 0.391689658164978, lr: 1e-05
2023-12-21 03:58:19 INFO     	 * (global step 3700: loss: 0.44420280307531357, lr: 1e-05
2023-12-21 03:58:35 INFO     	 * (global step 3750: loss: 0.4206285700201988, lr: 1e-05
2023-12-21 03:58:50 INFO     	 * (global step 3800: loss: 0.3345048055052757, lr: 1e-05
2023-12-21 03:59:06 INFO     	 * (global step 3850: loss: 0.4148864448070526, lr: 1e-05
2023-12-21 03:59:22 INFO     	 * (global step 3900: loss: 0.34619829431176186, lr: 1e-05
2023-12-21 03:59:38 INFO     	 * (global step 3950: loss: 0.43357764929533005, lr: 1e-05
2023-12-21 03:59:53 INFO     	 * (global step 4000: loss: 0.36891451850533485, lr: 1e-05
2023-12-21 04:00:09 INFO     	 * (global step 4050: loss: 0.49513808637857437, lr: 1e-05
2023-12-21 04:00:25 INFO     	 * (global step 4100: loss: 0.39146876335144043, lr: 1e-05
2023-12-21 04:00:41 INFO     	 * (global step 4150: loss: 0.33108483999967575, lr: 1e-05
2023-12-21 04:00:56 INFO     	 * (global step 4200: loss: 0.4443097189068794, lr: 1e-05
2023-12-21 04:01:12 INFO     	 * (global step 4250: loss: 0.489250972867012, lr: 1e-05
2023-12-21 04:01:28 INFO     	 * (global step 4300: loss: 0.35474177077412605, lr: 1e-05
2023-12-21 04:01:44 INFO     	 * (global step 4350: loss: 0.3473157584667206, lr: 1e-05
2023-12-21 04:01:59 INFO     	 * (global step 4400: loss: 0.3612605631351471, lr: 1e-05
2023-12-21 04:02:15 INFO     	 * (global step 4450: loss: 0.33040232956409454, lr: 1e-05
2023-12-21 04:02:31 INFO     	 * (global step 4500: loss: 0.35560715198516846, lr: 1e-05
2023-12-21 04:02:47 INFO     	 * (global step 4550: loss: 0.4210636168718338, lr: 1e-05
2023-12-21 04:03:03 INFO     	 * (global step 4600: loss: 0.26572616398334503, lr: 1e-05
2023-12-21 04:03:18 INFO     	 * (global step 4650: loss: 0.4262065291404724, lr: 1e-05
2023-12-21 04:03:34 INFO     	 * (global step 4700: loss: 0.37484634667634964, lr: 1e-05
2023-12-21 04:03:41 INFO     [epoch 1/15] average loss: 0.431, lr: 1e-05
2023-12-21 04:03:41 INFO     saving model related files
2023-12-21 04:03:41 INFO     saving model
2023-12-21 04:03:42 INFO     saving tokenizer
2023-12-21 04:03:42 INFO     saving optimizer
2023-12-21 04:03:43 INFO     remove old optimizer files
2023-12-21 04:03:52 INFO     	 * (global step 4750: loss: 0.5721472054719925, lr: 1e-05
2023-12-21 04:04:08 INFO     	 * (global step 4800: loss: 0.3528956398367882, lr: 1e-05
2023-12-21 04:04:23 INFO     	 * (global step 4850: loss: 0.3146781884133816, lr: 1e-05
2023-12-21 04:04:39 INFO     	 * (global step 4900: loss: 0.3696332797408104, lr: 1e-05
2023-12-21 04:04:55 INFO     	 * (global step 4950: loss: 0.37488502264022827, lr: 1e-05
2023-12-21 04:05:11 INFO     	 * (global step 5000: loss: 0.6068108975887299, lr: 1e-05
2023-12-21 04:05:27 INFO     	 * (global step 5050: loss: 0.4551422595977783, lr: 1e-05
2023-12-21 04:05:42 INFO     	 * (global step 5100: loss: 0.3355177417397499, lr: 1e-05
2023-12-21 04:05:58 INFO     	 * (global step 5150: loss: 0.362757571041584, lr: 1e-05
2023-12-21 04:06:14 INFO     	 * (global step 5200: loss: 0.4507182836532593, lr: 1e-05
2023-12-21 04:06:29 INFO     	 * (global step 5250: loss: 0.38090649992227554, lr: 1e-05
2023-12-21 04:06:45 INFO     	 * (global step 5300: loss: 0.42069244384765625, lr: 1e-05
2023-12-21 04:07:01 INFO     	 * (global step 5350: loss: 0.3357563428580761, lr: 1e-05
2023-12-21 04:07:17 INFO     	 * (global step 5400: loss: 0.27975087985396385, lr: 1e-05
2023-12-21 04:07:32 INFO     	 * (global step 5450: loss: 0.5379257276654243, lr: 1e-05
2023-12-21 04:07:48 INFO     	 * (global step 5500: loss: 0.5324843898415565, lr: 1e-05
2023-12-21 04:08:04 INFO     	 * (global step 5550: loss: 0.3536023572087288, lr: 1e-05
2023-12-21 04:08:19 INFO     	 * (global step 5600: loss: 0.3131752349436283, lr: 1e-05
2023-12-21 04:08:35 INFO     	 * (global step 5650: loss: 0.31677621603012085, lr: 1e-05
2023-12-21 04:08:51 INFO     	 * (global step 5700: loss: 0.5301818549633026, lr: 1e-05
2023-12-21 04:09:07 INFO     	 * (global step 5750: loss: 0.41718681156635284, lr: 1e-05
2023-12-21 04:09:22 INFO     	 * (global step 5800: loss: 0.398206427693367, lr: 1e-05
2023-12-21 04:09:38 INFO     	 * (global step 5850: loss: 0.44670969992876053, lr: 1e-05
2023-12-21 04:09:53 INFO     	 * (global step 5900: loss: 0.39223887026309967, lr: 1e-05
2023-12-21 04:10:09 INFO     	 * (global step 5950: loss: 0.47255390137434006, lr: 1e-05
2023-12-21 04:10:25 INFO     	 * (global step 6000: loss: 0.4510406032204628, lr: 1e-05
2023-12-21 04:10:40 INFO     	 * (global step 6050: loss: 0.34412046521902084, lr: 1e-05
2023-12-21 04:10:56 INFO     	 * (global step 6100: loss: 0.4448418617248535, lr: 1e-05
2023-12-21 04:11:11 INFO     	 * (global step 6150: loss: 0.44977929443120956, lr: 1e-05
2023-12-21 04:11:27 INFO     	 * (global step 6200: loss: 0.36055710911750793, lr: 1e-05
2023-12-21 04:11:43 INFO     	 * (global step 6250: loss: 0.32144559919834137, lr: 1e-05
2023-12-21 04:11:58 INFO     	 * (global step 6300: loss: 0.44186070933938026, lr: 1e-05
2023-12-21 04:12:14 INFO     	 * (global step 6350: loss: 0.4592844396829605, lr: 1e-05
2023-12-21 04:12:29 INFO     	 * (global step 6400: loss: 0.33507247641682625, lr: 1e-05
2023-12-21 04:12:45 INFO     	 * (global step 6450: loss: 0.36730609089136124, lr: 1e-05
2023-12-21 04:13:01 INFO     	 * (global step 6500: loss: 0.36119256168603897, lr: 1e-05
2023-12-21 04:13:16 INFO     	 * (global step 6550: loss: 0.4986652471125126, lr: 1e-05
2023-12-21 04:13:32 INFO     	 * (global step 6600: loss: 0.4728501960635185, lr: 1e-05
2023-12-21 04:13:47 INFO     	 * (global step 6650: loss: 0.3719885125756264, lr: 1e-05
2023-12-21 04:14:03 INFO     	 * (global step 6700: loss: 0.3579346500337124, lr: 1e-05
2023-12-21 04:14:19 INFO     	 * (global step 6750: loss: 0.438174307346344, lr: 1e-05
2023-12-21 04:14:34 INFO     	 * (global step 6800: loss: 0.37145185470581055, lr: 1e-05
2023-12-21 04:14:50 INFO     	 * (global step 6850: loss: 0.3014301098883152, lr: 1e-05
2023-12-21 04:15:05 INFO     	 * (global step 6900: loss: 0.4584697559475899, lr: 1e-05
2023-12-21 04:15:21 INFO     	 * (global step 6950: loss: 0.3388475440442562, lr: 1e-05
2023-12-21 04:15:37 INFO     	 * (global step 7000: loss: 0.3495436757802963, lr: 1e-05
2023-12-21 04:15:52 INFO     	 * (global step 7050: loss: 0.33837833255529404, lr: 1e-05
2023-12-21 04:16:03 INFO     [epoch 2/15] average loss: 0.39, lr: 1e-05
2023-12-21 04:16:03 INFO     saving model related files
2023-12-21 04:16:03 INFO     saving model
2023-12-21 04:16:03 INFO     saving tokenizer
2023-12-21 04:16:03 INFO     saving optimizer
2023-12-21 04:16:04 INFO     remove old optimizer files
2023-12-21 04:16:09 INFO     	 * (global step 7100: loss: 0.322360597550869, lr: 1e-05
2023-12-21 04:16:25 INFO     	 * (global step 7150: loss: 0.5299533940851688, lr: 1e-05
2023-12-21 04:16:41 INFO     	 * (global step 7200: loss: 0.38477717339992523, lr: 1e-05
2023-12-21 04:16:56 INFO     	 * (global step 7250: loss: 0.2874959148466587, lr: 1e-05
2023-12-21 04:17:12 INFO     	 * (global step 7300: loss: 0.5089610740542412, lr: 1e-05
2023-12-21 04:17:27 INFO     	 * (global step 7350: loss: 0.32922685146331787, lr: 1e-05
2023-12-21 04:17:43 INFO     	 * (global step 7400: loss: 0.41146326810121536, lr: 1e-05
2023-12-21 04:17:59 INFO     	 * (global step 7450: loss: 0.32971489429473877, lr: 1e-05
2023-12-21 04:18:14 INFO     	 * (global step 7500: loss: 0.33974702283740044, lr: 1e-05
2023-12-21 04:18:30 INFO     	 * (global step 7550: loss: 0.39820289611816406, lr: 1e-05
2023-12-21 04:18:45 INFO     	 * (global step 7600: loss: 0.3778560981154442, lr: 1e-05
2023-12-21 04:19:01 INFO     	 * (global step 7650: loss: 0.33110034465789795, lr: 1e-05
2023-12-21 04:19:17 INFO     	 * (global step 7700: loss: 0.40512268245220184, lr: 1e-05
2023-12-21 04:19:32 INFO     	 * (global step 7750: loss: 0.3922514170408249, lr: 1e-05
2023-12-21 04:19:48 INFO     	 * (global step 7800: loss: 0.31798743084073067, lr: 1e-05
2023-12-21 04:20:03 INFO     	 * (global step 7850: loss: 0.28677405416965485, lr: 1e-05
2023-12-21 04:20:19 INFO     	 * (global step 7900: loss: 0.3076356127858162, lr: 1e-05
2023-12-21 04:20:34 INFO     	 * (global step 7950: loss: 0.2886856682598591, lr: 1e-05
2023-12-21 04:20:50 INFO     	 * (global step 8000: loss: 0.39200539886951447, lr: 1e-05
2023-12-21 04:21:06 INFO     	 * (global step 8050: loss: 0.31617090851068497, lr: 1e-05
2023-12-21 04:21:21 INFO     	 * (global step 8100: loss: 0.5203120820224285, lr: 1e-05
2023-12-21 04:21:37 INFO     	 * (global step 8150: loss: 0.3125578314065933, lr: 1e-05
2023-12-21 04:21:52 INFO     	 * (global step 8200: loss: 0.33990734070539474, lr: 1e-05
2023-12-21 04:22:08 INFO     	 * (global step 8250: loss: 0.36246348172426224, lr: 1e-05
2023-12-21 04:22:24 INFO     	 * (global step 8300: loss: 0.3245164752006531, lr: 1e-05
2023-12-21 04:22:39 INFO     	 * (global step 8350: loss: 0.4240028038620949, lr: 1e-05
2023-12-21 04:22:55 INFO     	 * (global step 8400: loss: 0.3574777990579605, lr: 1e-05
2023-12-21 04:23:10 INFO     	 * (global step 8450: loss: 0.3563239723443985, lr: 1e-05
2023-12-21 04:23:26 INFO     	 * (global step 8500: loss: 0.3719296008348465, lr: 1e-05
2023-12-21 04:23:42 INFO     	 * (global step 8550: loss: 0.2845746614038944, lr: 1e-05
2023-12-21 04:23:57 INFO     	 * (global step 8600: loss: 0.4217645898461342, lr: 1e-05
2023-12-21 04:24:13 INFO     	 * (global step 8650: loss: 0.503559909760952, lr: 1e-05
2023-12-21 04:24:28 INFO     	 * (global step 8700: loss: 0.4192178398370743, lr: 1e-05
2023-12-21 04:24:44 INFO     	 * (global step 8750: loss: 0.2808411829173565, lr: 1e-05
2023-12-21 04:24:59 INFO     	 * (global step 8800: loss: 0.3739582449197769, lr: 1e-05
2023-12-21 04:25:15 INFO     	 * (global step 8850: loss: 0.35417359694838524, lr: 1e-05
2023-12-21 04:25:31 INFO     	 * (global step 8900: loss: 0.28442056104540825, lr: 1e-05
2023-12-21 04:25:46 INFO     	 * (global step 8950: loss: 0.3467163071036339, lr: 1e-05
2023-12-21 04:26:02 INFO     	 * (global step 9000: loss: 0.3822757825255394, lr: 1e-05
2023-12-21 04:26:17 INFO     	 * (global step 9050: loss: 0.3431682512164116, lr: 1e-05
2023-12-21 04:26:33 INFO     	 * (global step 9100: loss: 0.36610250174999237, lr: 1e-05
2023-12-21 04:26:48 INFO     	 * (global step 9150: loss: 0.29526376724243164, lr: 1e-05
2023-12-21 04:27:04 INFO     	 * (global step 9200: loss: 0.2948102056980133, lr: 1e-05
2023-12-21 04:27:19 INFO     	 * (global step 9250: loss: 0.25004045478999615, lr: 1e-05
2023-12-21 04:27:35 INFO     	 * (global step 9300: loss: 0.28811322152614594, lr: 1e-05
2023-12-21 04:27:51 INFO     	 * (global step 9350: loss: 0.35872746631503105, lr: 1e-05
2023-12-21 04:28:06 INFO     	 * (global step 9400: loss: 0.4038036912679672, lr: 1e-05
2023-12-21 04:28:20 INFO     [epoch 3/15] average loss: 0.37, lr: 1e-05
2023-12-21 04:28:20 INFO     saving model related files
2023-12-21 04:28:20 INFO     saving model
2023-12-21 04:28:20 INFO     saving tokenizer
2023-12-21 04:28:20 INFO     saving optimizer
2023-12-21 04:28:21 INFO     remove old optimizer files
2023-12-21 04:28:23 INFO     	 * (global step 9450: loss: 0.282600961625576, lr: 1e-05
2023-12-21 04:28:39 INFO     	 * (global step 9500: loss: 0.4622882977128029, lr: 1e-05
2023-12-21 04:28:54 INFO     	 * (global step 9550: loss: 0.3519905135035515, lr: 1e-05
2023-12-21 04:29:10 INFO     	 * (global step 9600: loss: 0.44305192679166794, lr: 1e-05
2023-12-21 04:29:25 INFO     	 * (global step 9650: loss: 0.3521352484822273, lr: 1e-05
2023-12-21 04:29:41 INFO     	 * (global step 9700: loss: 0.34390465170145035, lr: 1e-05
2023-12-21 04:29:56 INFO     	 * (global step 9750: loss: 0.3268740773200989, lr: 1e-05
2023-12-21 04:30:12 INFO     	 * (global step 9800: loss: 0.3113832622766495, lr: 1e-05
2023-12-21 04:30:27 INFO     	 * (global step 9850: loss: 0.30964838340878487, lr: 1e-05
2023-12-21 04:30:42 INFO     	 * (global step 9900: loss: 0.41025160998106003, lr: 1e-05
2023-12-21 04:30:58 INFO     	 * (global step 9950: loss: 0.4603424668312073, lr: 1e-05
2023-12-21 04:31:13 INFO     	 * (global step 10000: loss: 0.32102803885936737, lr: 1e-05
2023-12-21 04:31:29 INFO     	 * (global step 10050: loss: 0.5183799043297768, lr: 1e-05
2023-12-21 04:31:44 INFO     	 * (global step 10100: loss: 0.2604031264781952, lr: 1e-05
2023-12-21 04:32:00 INFO     	 * (global step 10150: loss: 0.5082173123955727, lr: 1e-05
2023-12-21 04:32:15 INFO     	 * (global step 10200: loss: 0.4195276126265526, lr: 1e-05
2023-12-21 04:32:31 INFO     	 * (global step 10250: loss: 0.2483743131160736, lr: 1e-05
2023-12-21 04:32:46 INFO     	 * (global step 10300: loss: 0.3007972687482834, lr: 1e-05
2023-12-21 04:33:02 INFO     	 * (global step 10350: loss: 0.43231144547462463, lr: 1e-05
2023-12-21 04:33:17 INFO     	 * (global step 10400: loss: 0.26848386600613594, lr: 1e-05
2023-12-21 04:33:33 INFO     	 * (global step 10450: loss: 0.4518662467598915, lr: 1e-05
2023-12-21 04:33:48 INFO     	 * (global step 10500: loss: 0.4072785973548889, lr: 1e-05
2023-12-21 04:34:04 INFO     	 * (global step 10550: loss: 0.322633258998394, lr: 1e-05
2023-12-21 04:34:19 INFO     	 * (global step 10600: loss: 0.37647607177495956, lr: 1e-05
2023-12-21 04:34:35 INFO     	 * (global step 10650: loss: 0.3854203261435032, lr: 1e-05
2023-12-21 04:34:50 INFO     	 * (global step 10700: loss: 0.5266114845871925, lr: 1e-05
2023-12-21 04:35:06 INFO     	 * (global step 10750: loss: 0.580638200044632, lr: 1e-05
2023-12-21 04:35:22 INFO     	 * (global step 10800: loss: 0.3216850832104683, lr: 1e-05
2023-12-21 04:35:37 INFO     	 * (global step 10850: loss: 0.334880106151104, lr: 1e-05
2023-12-21 04:35:53 INFO     	 * (global step 10900: loss: 0.3670203760266304, lr: 1e-05
2023-12-21 04:36:08 INFO     	 * (global step 10950: loss: 0.2224162332713604, lr: 1e-05
2023-12-21 04:36:24 INFO     	 * (global step 11000: loss: 0.3407832384109497, lr: 1e-05
2023-12-21 04:36:39 INFO     	 * (global step 11050: loss: 0.38885604590177536, lr: 1e-05
2023-12-21 04:36:55 INFO     	 * (global step 11100: loss: 0.2779714986681938, lr: 1e-05
2023-12-21 04:37:10 INFO     	 * (global step 11150: loss: 0.30668892338871956, lr: 1e-05
2023-12-21 04:37:26 INFO     	 * (global step 11200: loss: 0.4034930467605591, lr: 1e-05
2023-12-21 04:37:41 INFO     	 * (global step 11250: loss: 0.451328806579113, lr: 1e-05
2023-12-21 04:37:57 INFO     	 * (global step 11300: loss: 0.3286783769726753, lr: 1e-05
2023-12-21 04:38:12 INFO     	 * (global step 11350: loss: 0.3629375472664833, lr: 1e-05
2023-12-21 04:38:28 INFO     	 * (global step 11400: loss: 0.29277805611491203, lr: 1e-05
2023-12-21 04:38:43 INFO     	 * (global step 11450: loss: 0.32079026103019714, lr: 1e-05
2023-12-21 04:38:59 INFO     	 * (global step 11500: loss: 0.2678952254354954, lr: 1e-05
2023-12-21 04:39:14 INFO     	 * (global step 11550: loss: 0.3582957349717617, lr: 1e-05
2023-12-21 04:39:30 INFO     	 * (global step 11600: loss: 0.4109279327094555, lr: 1e-05
2023-12-21 04:39:45 INFO     	 * (global step 11650: loss: 0.3742440938949585, lr: 1e-05
2023-12-21 04:40:01 INFO     	 * (global step 11700: loss: 0.3758402317762375, lr: 1e-05
2023-12-21 04:40:16 INFO     	 * (global step 11750: loss: 0.33127792924642563, lr: 1e-05
2023-12-21 04:40:32 INFO     	 * (global step 11800: loss: 0.41837237775325775, lr: 1e-05
2023-12-21 04:40:34 INFO     [epoch 4/15] average loss: 0.358, lr: 1e-05
2023-12-21 04:40:34 INFO     saving model related files
2023-12-21 04:40:34 INFO     saving model
2023-12-21 04:40:34 INFO     saving tokenizer
2023-12-21 04:40:34 INFO     saving optimizer
2023-12-21 04:40:35 INFO     remove old optimizer files
2023-12-21 04:40:49 INFO     	 * (global step 11850: loss: 0.4765969514846802, lr: 1e-05
2023-12-21 04:41:05 INFO     	 * (global step 11900: loss: 0.3762078210711479, lr: 1e-05
2023-12-21 04:41:20 INFO     	 * (global step 11950: loss: 0.32488778978586197, lr: 1e-05
2023-12-21 04:41:36 INFO     	 * (global step 12000: loss: 0.35501382127404213, lr: 1e-05
2023-12-21 04:41:51 INFO     	 * (global step 12050: loss: 0.3498517833650112, lr: 1e-05
2023-12-21 04:42:07 INFO     	 * (global step 12100: loss: 0.4250820651650429, lr: 1e-05
2023-12-21 04:42:22 INFO     	 * (global step 12150: loss: 0.43027475103735924, lr: 1e-05
2023-12-21 04:42:38 INFO     	 * (global step 12200: loss: 0.4307517185807228, lr: 1e-05
2023-12-21 04:42:53 INFO     	 * (global step 12250: loss: 0.2879149615764618, lr: 1e-05
2023-12-21 04:43:09 INFO     	 * (global step 12300: loss: 0.3774288445711136, lr: 1e-05
2023-12-21 04:43:24 INFO     	 * (global step 12350: loss: 0.289398867636919, lr: 1e-05
2023-12-21 04:43:40 INFO     	 * (global step 12400: loss: 0.3746553659439087, lr: 1e-05
2023-12-21 04:43:55 INFO     	 * (global step 12450: loss: 0.36955930665135384, lr: 1e-05
2023-12-21 04:44:11 INFO     	 * (global step 12500: loss: 0.27524491772055626, lr: 1e-05
2023-12-21 04:44:26 INFO     	 * (global step 12550: loss: 0.3772807717323303, lr: 1e-05
2023-12-21 04:44:42 INFO     	 * (global step 12600: loss: 0.40135519951581955, lr: 1e-05
2023-12-21 04:44:57 INFO     	 * (global step 12650: loss: 0.3420075848698616, lr: 1e-05
2023-12-21 04:45:13 INFO     	 * (global step 12700: loss: 0.3609168380498886, lr: 1e-05
2023-12-21 04:45:28 INFO     	 * (global step 12750: loss: 0.3333849497139454, lr: 1e-05
2023-12-21 04:45:44 INFO     	 * (global step 12800: loss: 0.3819800056517124, lr: 1e-05
2023-12-21 04:45:59 INFO     	 * (global step 12850: loss: 0.2484569549560547, lr: 1e-05
2023-12-21 04:46:14 INFO     	 * (global step 12900: loss: 0.3630107641220093, lr: 1e-05
2023-12-21 04:46:30 INFO     	 * (global step 12950: loss: 0.2854120284318924, lr: 1e-05
2023-12-21 04:46:45 INFO     	 * (global step 13000: loss: 0.42612699419260025, lr: 1e-05
2023-12-21 04:47:01 INFO     	 * (global step 13050: loss: 0.26653333753347397, lr: 1e-05
2023-12-21 04:47:16 INFO     	 * (global step 13100: loss: 0.34353170543909073, lr: 1e-05
2023-12-21 04:47:32 INFO     	 * (global step 13150: loss: 0.3118767887353897, lr: 1e-05
2023-12-21 04:47:47 INFO     	 * (global step 13200: loss: 0.27913954854011536, lr: 1e-05
2023-12-21 04:48:02 INFO     	 * (global step 13250: loss: 0.33859487995505333, lr: 1e-05
2023-12-21 04:48:18 INFO     	 * (global step 13300: loss: 0.4241417944431305, lr: 1e-05
2023-12-21 04:48:33 INFO     	 * (global step 13350: loss: 0.34183597937226295, lr: 1e-05
2023-12-21 04:48:49 INFO     	 * (global step 13400: loss: 0.3685329370200634, lr: 1e-05
2023-12-21 04:49:04 INFO     	 * (global step 13450: loss: 0.32387276738882065, lr: 1e-05
2023-12-21 04:49:20 INFO     	 * (global step 13500: loss: 0.27274438738822937, lr: 1e-05
2023-12-21 04:49:35 INFO     	 * (global step 13550: loss: 0.38735729455947876, lr: 1e-05
2023-12-21 04:49:50 INFO     	 * (global step 13600: loss: 0.3914475850760937, lr: 1e-05
2023-12-21 04:50:06 INFO     	 * (global step 13650: loss: 0.35766108334064484, lr: 1e-05
2023-12-21 04:50:21 INFO     	 * (global step 13700: loss: 0.41358959674835205, lr: 1e-05
2023-12-21 04:50:37 INFO     	 * (global step 13750: loss: 0.22365644946694374, lr: 1e-05
2023-12-21 04:50:52 INFO     	 * (global step 13800: loss: 0.3577532395720482, lr: 1e-05
2023-12-21 04:51:08 INFO     	 * (global step 13850: loss: 0.37686237692832947, lr: 1e-05
2023-12-21 04:51:23 INFO     	 * (global step 13900: loss: 0.3768468573689461, lr: 1e-05
2023-12-21 04:51:39 INFO     	 * (global step 13950: loss: 0.4365147054195404, lr: 1e-05
2023-12-21 04:51:54 INFO     	 * (global step 14000: loss: 0.3564700111746788, lr: 1e-05
2023-12-21 04:52:09 INFO     	 * (global step 14050: loss: 0.32340453192591667, lr: 1e-05
2023-12-21 04:52:25 INFO     	 * (global step 14100: loss: 0.4288737550377846, lr: 1e-05
2023-12-21 04:52:40 INFO     	 * (global step 14150: loss: 0.380720354616642, lr: 1e-05
2023-12-21 04:52:45 INFO     [epoch 5/15] average loss: 0.349, lr: 1e-05
2023-12-21 04:52:45 INFO     saving model related files
2023-12-21 04:52:45 INFO     saving model
2023-12-21 04:52:46 INFO     saving tokenizer
2023-12-21 04:52:46 INFO     saving optimizer
2023-12-21 04:52:47 INFO     remove old optimizer files
2023-12-21 04:52:57 INFO     	 * (global step 14200: loss: 0.39545178785920143, lr: 1e-05
2023-12-21 04:53:13 INFO     	 * (global step 14250: loss: 0.40650730580091476, lr: 1e-05
2023-12-21 04:53:28 INFO     	 * (global step 14300: loss: 0.3648250624537468, lr: 1e-05
2023-12-21 04:53:44 INFO     	 * (global step 14350: loss: 0.33321869373321533, lr: 1e-05
2023-12-21 04:53:59 INFO     	 * (global step 14400: loss: 0.4214532747864723, lr: 1e-05
2023-12-21 04:54:15 INFO     	 * (global step 14450: loss: 0.3012200631201267, lr: 1e-05
2023-12-21 04:54:30 INFO     	 * (global step 14500: loss: 0.44013237208127975, lr: 1e-05
2023-12-21 04:54:46 INFO     	 * (global step 14550: loss: 0.4076389744877815, lr: 1e-05
2023-12-21 04:55:02 INFO     	 * (global step 14600: loss: 0.32754790410399437, lr: 1e-05
2023-12-21 04:55:17 INFO     	 * (global step 14650: loss: 0.35394537448883057, lr: 1e-05
2023-12-21 04:55:33 INFO     	 * (global step 14700: loss: 0.2994518205523491, lr: 1e-05
2023-12-21 04:55:48 INFO     	 * (global step 14750: loss: 0.3825371339917183, lr: 1e-05
2023-12-21 04:56:04 INFO     	 * (global step 14800: loss: 0.3864148482680321, lr: 1e-05
2023-12-21 04:56:19 INFO     	 * (global step 14850: loss: 0.30775628983974457, lr: 1e-05
2023-12-21 04:56:35 INFO     	 * (global step 14900: loss: 0.287685614079237, lr: 1e-05
2023-12-21 04:56:50 INFO     	 * (global step 14950: loss: 0.44029367715120316, lr: 1e-05
2023-12-21 04:57:06 INFO     	 * (global step 15000: loss: 0.3962501585483551, lr: 1e-05
2023-12-21 04:57:21 INFO     	 * (global step 15050: loss: 0.35326574742794037, lr: 1e-05
2023-12-21 04:57:37 INFO     	 * (global step 15100: loss: 0.34413161873817444, lr: 1e-05
2023-12-21 04:57:52 INFO     	 * (global step 15150: loss: 0.26712873205542564, lr: 1e-05
2023-12-21 04:58:08 INFO     	 * (global step 15200: loss: 0.4275907836854458, lr: 1e-05
2023-12-21 04:58:23 INFO     	 * (global step 15250: loss: 0.23208773508667946, lr: 1e-05
2023-12-21 04:58:39 INFO     	 * (global step 15300: loss: 0.267328891903162, lr: 1e-05
2023-12-21 04:58:54 INFO     	 * (global step 15350: loss: 0.24962838739156723, lr: 1e-05
2023-12-21 04:59:10 INFO     	 * (global step 15400: loss: 0.3369436226785183, lr: 1e-05
2023-12-21 04:59:25 INFO     	 * (global step 15450: loss: 0.3337826728820801, lr: 1e-05
2023-12-21 04:59:41 INFO     	 * (global step 15500: loss: 0.3765576481819153, lr: 1e-05
2023-12-21 04:59:56 INFO     	 * (global step 15550: loss: 0.2792202942073345, lr: 1e-05
2023-12-21 05:00:12 INFO     	 * (global step 15600: loss: 0.307927493005991, lr: 1e-05
2023-12-21 05:00:27 INFO     	 * (global step 15650: loss: 0.3464125916361809, lr: 1e-05
2023-12-21 05:00:43 INFO     	 * (global step 15700: loss: 0.3022596798837185, lr: 1e-05
2023-12-21 05:00:58 INFO     	 * (global step 15750: loss: 0.34535063430666924, lr: 1e-05
2023-12-21 05:01:14 INFO     	 * (global step 15800: loss: 0.31950508058071136, lr: 1e-05
2023-12-21 05:01:29 INFO     	 * (global step 15850: loss: 0.39688678085803986, lr: 1e-05
2023-12-21 05:01:45 INFO     	 * (global step 15900: loss: 0.3619823679327965, lr: 1e-05
2023-12-21 05:02:00 INFO     	 * (global step 15950: loss: 0.2974989265203476, lr: 1e-05
2023-12-21 05:02:16 INFO     	 * (global step 16000: loss: 0.250683780759573, lr: 1e-05
2023-12-21 05:02:32 INFO     	 * (global step 16050: loss: 0.29460591822862625, lr: 1e-05
2023-12-21 05:02:47 INFO     	 * (global step 16100: loss: 0.35250476747751236, lr: 1e-05
2023-12-21 05:03:03 INFO     	 * (global step 16150: loss: 0.3381119780242443, lr: 1e-05
2023-12-21 05:03:18 INFO     	 * (global step 16200: loss: 0.2735123559832573, lr: 1e-05
2023-12-21 05:03:34 INFO     	 * (global step 16250: loss: 0.3031870648264885, lr: 1e-05
2023-12-21 05:03:49 INFO     	 * (global step 16300: loss: 0.32651861011981964, lr: 1e-05
2023-12-21 05:04:05 INFO     	 * (global step 16350: loss: 0.3640820160508156, lr: 1e-05
2023-12-21 05:04:20 INFO     	 * (global step 16400: loss: 0.2631087154150009, lr: 1e-05
2023-12-21 05:04:36 INFO     	 * (global step 16450: loss: 0.379646971821785, lr: 1e-05
2023-12-21 05:04:51 INFO     	 * (global step 16500: loss: 0.26138048991560936, lr: 1e-05
2023-12-21 05:05:00 INFO     [epoch 6/15] average loss: 0.342, lr: 1e-05
2023-12-21 05:05:00 INFO     saving model related files
2023-12-21 05:05:00 INFO     saving model
2023-12-21 05:05:00 INFO     saving tokenizer
2023-12-21 05:05:00 INFO     saving optimizer
2023-12-21 05:05:01 INFO     remove old optimizer files
2023-12-21 05:05:09 INFO     	 * (global step 16550: loss: 0.3870059922337532, lr: 1e-05
2023-12-21 05:05:24 INFO     	 * (global step 16600: loss: 0.2538530305027962, lr: 1e-05
2023-12-21 05:05:40 INFO     	 * (global step 16650: loss: 0.30177130177617073, lr: 1e-05
2023-12-21 05:05:55 INFO     	 * (global step 16700: loss: 0.3530140668153763, lr: 1e-05
2023-12-21 05:06:11 INFO     	 * (global step 16750: loss: 0.37439773231744766, lr: 1e-05
2023-12-21 05:06:26 INFO     	 * (global step 16800: loss: 0.3895663395524025, lr: 1e-05
2023-12-21 05:06:42 INFO     	 * (global step 16850: loss: 0.3198334276676178, lr: 1e-05
2023-12-21 05:06:57 INFO     	 * (global step 16900: loss: 0.29472240060567856, lr: 1e-05
2023-12-21 05:07:13 INFO     	 * (global step 16950: loss: 0.4273654744029045, lr: 1e-05
2023-12-21 05:07:28 INFO     	 * (global step 17000: loss: 0.2892494387924671, lr: 1e-05
2023-12-21 05:07:44 INFO     	 * (global step 17050: loss: 0.4437895342707634, lr: 1e-05
2023-12-21 05:07:59 INFO     	 * (global step 17100: loss: 0.5123838111758232, lr: 1e-05
2023-12-21 05:08:15 INFO     	 * (global step 17150: loss: 0.32195188477635384, lr: 1e-05
2023-12-21 05:08:30 INFO     	 * (global step 17200: loss: 0.2672388516366482, lr: 1e-05
2023-12-21 05:08:46 INFO     	 * (global step 17250: loss: 0.4169175699353218, lr: 1e-05
2023-12-21 05:09:01 INFO     	 * (global step 17300: loss: 0.3507951721549034, lr: 1e-05
2023-12-21 05:09:17 INFO     	 * (global step 17350: loss: 0.5295219495892525, lr: 1e-05
2023-12-21 05:09:32 INFO     	 * (global step 17400: loss: 0.3097448796033859, lr: 1e-05
2023-12-21 05:09:48 INFO     	 * (global step 17450: loss: 0.27925514802336693, lr: 1e-05
2023-12-21 05:10:04 INFO     	 * (global step 17500: loss: 0.3087363764643669, lr: 1e-05
2023-12-21 05:10:19 INFO     	 * (global step 17550: loss: 0.3074798509478569, lr: 1e-05
2023-12-21 05:10:35 INFO     	 * (global step 17600: loss: 0.3371589258313179, lr: 1e-05
2023-12-21 05:10:50 INFO     	 * (global step 17650: loss: 0.34371813386678696, lr: 1e-05
2023-12-21 05:11:06 INFO     	 * (global step 17700: loss: 0.39021166414022446, lr: 1e-05
2023-12-21 05:11:21 INFO     	 * (global step 17750: loss: 0.25688377767801285, lr: 1e-05
2023-12-21 05:11:37 INFO     	 * (global step 17800: loss: 0.29261817783117294, lr: 1e-05
2023-12-21 05:11:52 INFO     	 * (global step 17850: loss: 0.30900637060403824, lr: 1e-05
2023-12-21 05:12:08 INFO     	 * (global step 17900: loss: 0.30728384107351303, lr: 1e-05
2023-12-21 05:12:23 INFO     	 * (global step 17950: loss: 0.32928790897130966, lr: 1e-05
2023-12-21 05:12:39 INFO     	 * (global step 18000: loss: 0.3618398495018482, lr: 1e-05
2023-12-21 05:12:54 INFO     	 * (global step 18050: loss: 0.3371916711330414, lr: 1e-05
2023-12-21 05:13:10 INFO     	 * (global step 18100: loss: 0.28145553544163704, lr: 1e-05
2023-12-21 05:13:25 INFO     	 * (global step 18150: loss: 0.34119177609682083, lr: 1e-05
2023-12-21 05:13:41 INFO     	 * (global step 18200: loss: 0.337535809725523, lr: 1e-05
2023-12-21 05:13:57 INFO     	 * (global step 18250: loss: 0.2189142443239689, lr: 1e-05
2023-12-21 05:14:12 INFO     	 * (global step 18300: loss: 0.4064112976193428, lr: 1e-05
2023-12-21 05:14:28 INFO     	 * (global step 18350: loss: 0.47391320765018463, lr: 1e-05
2023-12-21 05:14:43 INFO     	 * (global step 18400: loss: 0.3030265346169472, lr: 1e-05
2023-12-21 05:14:59 INFO     	 * (global step 18450: loss: 0.24496162682771683, lr: 1e-05
2023-12-21 05:15:14 INFO     	 * (global step 18500: loss: 0.43225768208503723, lr: 1e-05
2023-12-21 05:15:30 INFO     	 * (global step 18550: loss: 0.21901191025972366, lr: 1e-05
2023-12-21 05:15:45 INFO     	 * (global step 18600: loss: 0.30531424283981323, lr: 1e-05
2023-12-21 05:16:01 INFO     	 * (global step 18650: loss: 0.2775244750082493, lr: 1e-05
2023-12-21 05:16:16 INFO     	 * (global step 18700: loss: 0.45296962559223175, lr: 1e-05
2023-12-21 05:16:32 INFO     	 * (global step 18750: loss: 0.5031965002417564, lr: 1e-05
2023-12-21 05:16:47 INFO     	 * (global step 18800: loss: 0.31467561051249504, lr: 1e-05
2023-12-21 05:17:03 INFO     	 * (global step 18850: loss: 0.22647752426564693, lr: 1e-05
2023-12-21 05:17:15 INFO     [epoch 7/15] average loss: 0.336, lr: 1e-05
2023-12-21 05:17:15 INFO     saving model related files
2023-12-21 05:17:15 INFO     saving model
2023-12-21 05:17:15 INFO     saving tokenizer
2023-12-21 05:17:15 INFO     saving optimizer
2023-12-21 05:17:16 INFO     remove old optimizer files
2023-12-21 05:17:20 INFO     	 * (global step 18900: loss: 0.3686882443726063, lr: 1e-05
2023-12-21 05:17:36 INFO     	 * (global step 18950: loss: 0.3763706646859646, lr: 1e-05
2023-12-21 05:17:51 INFO     	 * (global step 19000: loss: 0.30443913117051125, lr: 1e-05
2023-12-21 05:18:07 INFO     	 * (global step 19050: loss: 0.3404683656990528, lr: 1e-05
2023-12-21 05:18:22 INFO     	 * (global step 19100: loss: 0.30017613992094994, lr: 1e-05
2023-12-21 05:18:38 INFO     	 * (global step 19150: loss: 0.3317159488797188, lr: 1e-05
2023-12-21 05:18:53 INFO     	 * (global step 19200: loss: 0.36850372329354286, lr: 1e-05
2023-12-21 05:19:09 INFO     	 * (global step 19250: loss: 0.30924927443265915, lr: 1e-05
2023-12-21 05:19:24 INFO     	 * (global step 19300: loss: 0.2964922785758972, lr: 1e-05
2023-12-21 05:19:40 INFO     	 * (global step 19350: loss: 0.2958991080522537, lr: 1e-05
2023-12-21 05:19:55 INFO     	 * (global step 19400: loss: 0.3387377969920635, lr: 1e-05
2023-12-21 05:20:11 INFO     	 * (global step 19450: loss: 0.34091804176568985, lr: 1e-05
2023-12-21 05:20:26 INFO     	 * (global step 19500: loss: 0.48103588074445724, lr: 1e-05
2023-12-21 05:20:42 INFO     	 * (global step 19550: loss: 0.26470213755965233, lr: 1e-05
2023-12-21 05:20:57 INFO     	 * (global step 19600: loss: 0.2851356863975525, lr: 1e-05
2023-12-21 05:21:12 INFO     	 * (global step 19650: loss: 0.28996358811855316, lr: 1e-05
2023-12-21 05:21:28 INFO     	 * (global step 19700: loss: 0.41418566927313805, lr: 1e-05
2023-12-21 05:21:43 INFO     	 * (global step 19750: loss: 0.2671463266015053, lr: 1e-05
2023-12-21 05:21:59 INFO     	 * (global step 19800: loss: 0.3027101084589958, lr: 1e-05
2023-12-21 05:22:14 INFO     	 * (global step 19850: loss: 0.33017006143927574, lr: 1e-05
2023-12-21 05:22:30 INFO     	 * (global step 19900: loss: 0.29978660121560097, lr: 1e-05
2023-12-21 05:22:45 INFO     	 * (global step 19950: loss: 0.3408556133508682, lr: 1e-05
2023-12-21 05:23:01 INFO     	 * (global step 20000: loss: 0.2952405549585819, lr: 1e-05
2023-12-21 05:23:16 INFO     	 * (global step 20050: loss: 0.2792309783399105, lr: 1e-05
2023-12-21 05:23:32 INFO     	 * (global step 20100: loss: 0.3532078489661217, lr: 1e-05
2023-12-21 05:23:47 INFO     	 * (global step 20150: loss: 0.3314530998468399, lr: 1e-05
2023-12-21 05:24:03 INFO     	 * (global step 20200: loss: 0.5480127744376659, lr: 1e-05
2023-12-21 05:24:18 INFO     	 * (global step 20250: loss: 0.36478104814887047, lr: 1e-05
2023-12-21 05:24:34 INFO     	 * (global step 20300: loss: 0.349499624222517, lr: 1e-05
2023-12-21 05:24:49 INFO     	 * (global step 20350: loss: 0.33665086328983307, lr: 1e-05
2023-12-21 05:25:05 INFO     	 * (global step 20400: loss: 0.273059967905283, lr: 1e-05
2023-12-21 05:25:20 INFO     	 * (global step 20450: loss: 0.4169511944055557, lr: 1e-05
2023-12-21 05:25:36 INFO     	 * (global step 20500: loss: 0.25121450796723366, lr: 1e-05
2023-12-21 05:25:51 INFO     	 * (global step 20550: loss: 0.3449825905263424, lr: 1e-05
2023-12-21 05:26:07 INFO     	 * (global step 20600: loss: 0.36136142909526825, lr: 1e-05
2023-12-21 05:26:22 INFO     	 * (global step 20650: loss: 0.2972612492740154, lr: 1e-05
2023-12-21 05:26:38 INFO     	 * (global step 20700: loss: 0.33888356760144234, lr: 1e-05
2023-12-21 05:26:53 INFO     	 * (global step 20750: loss: 0.32968607544898987, lr: 1e-05
2023-12-21 05:27:09 INFO     	 * (global step 20800: loss: 0.36933116614818573, lr: 1e-05
2023-12-21 05:27:24 INFO     	 * (global step 20850: loss: 0.33948975801467896, lr: 1e-05
2023-12-21 05:27:40 INFO     	 * (global step 20900: loss: 0.385256826877594, lr: 1e-05
2023-12-21 05:27:55 INFO     	 * (global step 20950: loss: 0.33394739776849747, lr: 1e-05
2023-12-21 05:28:11 INFO     	 * (global step 21000: loss: 0.33762677758932114, lr: 1e-05
2023-12-21 05:28:26 INFO     	 * (global step 21050: loss: 0.2562604695558548, lr: 1e-05
2023-12-21 05:28:42 INFO     	 * (global step 21100: loss: 0.27502989023923874, lr: 1e-05
2023-12-21 05:28:57 INFO     	 * (global step 21150: loss: 0.2903888635337353, lr: 1e-05
2023-12-21 05:29:13 INFO     	 * (global step 21200: loss: 0.36324431747198105, lr: 1e-05
2023-12-21 05:29:28 INFO     [epoch 8/15] average loss: 0.332, lr: 1e-05
2023-12-21 05:29:28 INFO     saving model related files
2023-12-21 05:29:28 INFO     saving model
2023-12-21 05:29:28 INFO     saving tokenizer
2023-12-21 05:29:28 INFO     saving optimizer
2023-12-21 05:29:29 INFO     remove old optimizer files
2023-12-21 05:29:30 INFO     	 * (global step 21250: loss: 0.40066593885421753, lr: 1e-05
2023-12-21 05:29:45 INFO     	 * (global step 21300: loss: 0.2691076397895813, lr: 1e-05
2023-12-21 05:30:01 INFO     	 * (global step 21350: loss: 0.3182792440056801, lr: 1e-05
2023-12-21 05:30:16 INFO     	 * (global step 21400: loss: 0.38893166929483414, lr: 1e-05
2023-12-21 05:30:32 INFO     	 * (global step 21450: loss: 0.42712049186229706, lr: 1e-05
2023-12-21 05:30:47 INFO     	 * (global step 21500: loss: 0.30217182263731956, lr: 1e-05
2023-12-21 05:31:03 INFO     	 * (global step 21550: loss: 0.36010686680674553, lr: 1e-05
2023-12-21 05:31:18 INFO     	 * (global step 21600: loss: 0.2851356454193592, lr: 1e-05
2023-12-21 05:31:34 INFO     	 * (global step 21650: loss: 0.3355678543448448, lr: 1e-05
2023-12-21 05:31:49 INFO     	 * (global step 21700: loss: 0.31602322310209274, lr: 1e-05
2023-12-21 05:32:05 INFO     	 * (global step 21750: loss: 0.27100199460983276, lr: 1e-05
2023-12-21 05:32:20 INFO     	 * (global step 21800: loss: 0.47231266647577286, lr: 1e-05
2023-12-21 05:32:36 INFO     	 * (global step 21850: loss: 0.32111791893839836, lr: 1e-05
2023-12-21 05:32:51 INFO     	 * (global step 21900: loss: 0.4417842999100685, lr: 1e-05
2023-12-21 05:33:07 INFO     	 * (global step 21950: loss: 0.46421560645103455, lr: 1e-05
2023-12-21 05:33:22 INFO     	 * (global step 22000: loss: 0.3832695260643959, lr: 1e-05
2023-12-21 05:33:38 INFO     	 * (global step 22050: loss: 0.41889194399118423, lr: 1e-05
2023-12-21 05:33:53 INFO     	 * (global step 22100: loss: 0.2919619530439377, lr: 1e-05
2023-12-21 05:34:09 INFO     	 * (global step 22150: loss: 0.3135584220290184, lr: 1e-05
2023-12-21 05:34:24 INFO     	 * (global step 22200: loss: 0.22166017815470695, lr: 1e-05
2023-12-21 05:34:40 INFO     	 * (global step 22250: loss: 0.3524651378393173, lr: 1e-05
2023-12-21 05:34:56 INFO     	 * (global step 22300: loss: 0.32984118163585663, lr: 1e-05
2023-12-21 05:35:11 INFO     	 * (global step 22350: loss: 0.3749644607305527, lr: 1e-05
2023-12-21 05:35:27 INFO     	 * (global step 22400: loss: 0.3565455973148346, lr: 1e-05
2023-12-21 05:35:42 INFO     	 * (global step 22450: loss: 0.295319389551878, lr: 1e-05
2023-12-21 05:35:58 INFO     	 * (global step 22500: loss: 0.2725228685885668, lr: 1e-05
2023-12-21 05:36:13 INFO     	 * (global step 22550: loss: 0.35827746987342834, lr: 1e-05
2023-12-21 05:36:29 INFO     	 * (global step 22600: loss: 0.38871999830007553, lr: 1e-05
2023-12-21 05:36:44 INFO     	 * (global step 22650: loss: 0.32310228049755096, lr: 1e-05
2023-12-21 05:37:00 INFO     	 * (global step 22700: loss: 0.28550436347723007, lr: 1e-05
2023-12-21 05:37:15 INFO     	 * (global step 22750: loss: 0.36162514984607697, lr: 1e-05
2023-12-21 05:37:31 INFO     	 * (global step 22800: loss: 0.2886006124317646, lr: 1e-05
2023-12-21 05:37:46 INFO     	 * (global step 22850: loss: 0.34960925579071045, lr: 1e-05
2023-12-21 05:38:02 INFO     	 * (global step 22900: loss: 0.2875555083155632, lr: 1e-05
2023-12-21 05:38:17 INFO     	 * (global step 22950: loss: 0.35502907633781433, lr: 1e-05
2023-12-21 05:38:33 INFO     	 * (global step 23000: loss: 0.3244573399424553, lr: 1e-05
2023-12-21 05:38:48 INFO     	 * (global step 23050: loss: 0.2806906960904598, lr: 1e-05
2023-12-21 05:39:04 INFO     	 * (global step 23100: loss: 0.3893502354621887, lr: 1e-05
2023-12-21 05:39:20 INFO     	 * (global step 23150: loss: 0.3669709041714668, lr: 1e-05
2023-12-21 05:39:35 INFO     	 * (global step 23200: loss: 0.341853603720665, lr: 1e-05
2023-12-21 05:39:51 INFO     	 * (global step 23250: loss: 0.31248895823955536, lr: 1e-05
2023-12-21 05:40:06 INFO     	 * (global step 23300: loss: 0.27805686369538307, lr: 1e-05
2023-12-21 05:40:22 INFO     	 * (global step 23350: loss: 0.362852368503809, lr: 1e-05
2023-12-21 05:40:37 INFO     	 * (global step 23400: loss: 0.3649032637476921, lr: 1e-05
2023-12-21 05:40:53 INFO     	 * (global step 23450: loss: 0.3069284036755562, lr: 1e-05
2023-12-21 05:41:08 INFO     	 * (global step 23500: loss: 0.3050190396606922, lr: 1e-05
2023-12-21 05:41:24 INFO     	 * (global step 23550: loss: 0.3878449499607086, lr: 1e-05
2023-12-21 05:41:39 INFO     	 * (global step 23600: loss: 0.2744550108909607, lr: 1e-05
2023-12-21 05:41:42 INFO     [epoch 9/15] average loss: 0.328, lr: 1e-05
2023-12-21 05:41:42 INFO     saving model related files
2023-12-21 05:41:42 INFO     saving model
2023-12-21 05:41:43 INFO     saving tokenizer
2023-12-21 05:41:43 INFO     saving optimizer
2023-12-21 05:41:44 INFO     remove old optimizer files
2023-12-21 05:41:44 INFO     complete training: model ckpt was saved at small_combined_trained_ckpt/model_nxaqhy
2023-12-21 05:41:44 INFO     ## 1st RUN: Configuration 9/12 ##
2023-12-21 05:41:44 INFO     initialize model trainer
2023-12-21 05:41:44 INFO     initialize checkpoint at small_combined_trained_ckpt/model_oprhlh
2023-12-21 05:41:44 INFO     hyperparameters
2023-12-21 05:41:44 INFO     	 * dataset_path: StellarMilk/squad_newsqa
2023-12-21 05:41:44 INFO     	 * dataset_name: default
2023-12-21 05:41:44 INFO     	 * input_types: ['paragraph']
2023-12-21 05:41:44 INFO     	 * output_types: ['questions_answers']
2023-12-21 05:41:44 INFO     	 * prefix_types: ['qag']
2023-12-21 05:41:44 INFO     	 * model: t5-small
2023-12-21 05:41:44 INFO     	 * max_length: 512
2023-12-21 05:41:44 INFO     	 * max_length_output: 512
2023-12-21 05:41:44 INFO     	 * epoch: 15
2023-12-21 05:41:44 INFO     	 * batch: 2
2023-12-21 05:41:44 INFO     	 * lr: 1e-05
2023-12-21 05:41:44 INFO     	 * fp16: False
2023-12-21 05:41:44 INFO     	 * random_seed: 1
2023-12-21 05:41:44 INFO     	 * gradient_accumulation_steps: 2
2023-12-21 05:41:44 INFO     	 * label_smoothing: 0.15
2023-12-21 05:41:44 INFO     initialize checkpoint with t5-small
2023-12-21 05:41:45 INFO     use spaCy answer extraction model: positionrank
2023-12-21 05:41:46 INFO     Model `t5-small`
2023-12-21 05:41:46 INFO     	 * Num of GPU in use: 1
2023-12-21 05:41:46 INFO     	 * Prefix: True
2023-12-21 05:41:46 INFO     	 * Language: en (ignore at the training phase)
2023-12-21 05:41:46 INFO     dataset preprocessing
2023-12-21 05:41:47 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/squad_newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-21 05:41:49 INFO     start model training
2023-12-21 05:41:57 INFO     	 * (global step 50: loss: 6.4232892990112305, lr: 1e-05
2023-12-21 05:42:05 INFO     	 * (global step 100: loss: 2.088875472545624, lr: 1e-05
2023-12-21 05:42:13 INFO     	 * (global step 150: loss: 1.9978147745132446, lr: 1e-05
2023-12-21 05:42:21 INFO     	 * (global step 200: loss: 1.637089192867279, lr: 1e-05
2023-12-21 05:42:29 INFO     	 * (global step 250: loss: 1.3043537735939026, lr: 1e-05
2023-12-21 05:42:37 INFO     	 * (global step 300: loss: 1.2349849939346313, lr: 1e-05
2023-12-21 05:42:45 INFO     	 * (global step 350: loss: 1.2145938873291016, lr: 1e-05
2023-12-21 05:42:53 INFO     	 * (global step 400: loss: 1.3204255104064941, lr: 1e-05
2023-12-21 05:43:01 INFO     	 * (global step 450: loss: 0.95344677567482, lr: 1e-05
2023-12-21 05:43:09 INFO     	 * (global step 500: loss: 0.9689079225063324, lr: 1e-05
2023-12-21 05:43:17 INFO     	 * (global step 550: loss: 1.0554715394973755, lr: 1e-05
2023-12-21 05:43:25 INFO     	 * (global step 600: loss: 0.6254541575908661, lr: 1e-05
2023-12-21 05:43:33 INFO     	 * (global step 650: loss: 0.6661103665828705, lr: 1e-05
2023-12-21 05:43:41 INFO     	 * (global step 700: loss: 0.8425436019897461, lr: 1e-05
2023-12-21 05:43:49 INFO     	 * (global step 750: loss: 0.6061383187770844, lr: 1e-05
2023-12-21 05:43:57 INFO     	 * (global step 800: loss: 0.6092453896999359, lr: 1e-05
2023-12-21 05:44:05 INFO     	 * (global step 850: loss: 0.4918735772371292, lr: 1e-05
2023-12-21 05:44:13 INFO     	 * (global step 900: loss: 0.6430191397666931, lr: 1e-05
2023-12-21 05:44:21 INFO     	 * (global step 950: loss: 0.5847509205341339, lr: 1e-05
2023-12-21 05:44:29 INFO     	 * (global step 1000: loss: 0.530880481004715, lr: 1e-05
2023-12-21 05:44:37 INFO     	 * (global step 1050: loss: 0.5511801838874817, lr: 1e-05
2023-12-21 05:44:45 INFO     	 * (global step 1100: loss: 0.7088213264942169, lr: 1e-05
2023-12-21 05:44:53 INFO     	 * (global step 1150: loss: 0.587779626250267, lr: 1e-05
2023-12-21 05:45:01 INFO     	 * (global step 1200: loss: 0.7090177237987518, lr: 1e-05
2023-12-21 05:45:09 INFO     	 * (global step 1250: loss: 0.6959192454814911, lr: 1e-05
2023-12-21 05:45:17 INFO     	 * (global step 1300: loss: 0.5356675982475281, lr: 1e-05
2023-12-21 05:45:25 INFO     	 * (global step 1350: loss: 0.6929364800453186, lr: 1e-05
2023-12-21 05:45:33 INFO     	 * (global step 1400: loss: 0.5955376327037811, lr: 1e-05
2023-12-21 05:45:41 INFO     	 * (global step 1450: loss: 0.7399693727493286, lr: 1e-05
2023-12-21 05:45:49 INFO     	 * (global step 1500: loss: 0.5671635270118713, lr: 1e-05
2023-12-21 05:45:57 INFO     	 * (global step 1550: loss: 0.479174941778183, lr: 1e-05
2023-12-21 05:46:05 INFO     	 * (global step 1600: loss: 0.5614080727100372, lr: 1e-05
2023-12-21 05:46:13 INFO     	 * (global step 1650: loss: 0.4835606962442398, lr: 1e-05
2023-12-21 05:46:21 INFO     	 * (global step 1700: loss: 0.5728875696659088, lr: 1e-05
2023-12-21 05:46:29 INFO     	 * (global step 1750: loss: 0.43463337421417236, lr: 1e-05
2023-12-21 05:46:37 INFO     	 * (global step 1800: loss: 0.4890117794275284, lr: 1e-05
2023-12-21 05:46:44 INFO     	 * (global step 1850: loss: 0.6798700839281082, lr: 1e-05
2023-12-21 05:46:52 INFO     	 * (global step 1900: loss: 0.3856533020734787, lr: 1e-05
2023-12-21 05:47:00 INFO     	 * (global step 1950: loss: 0.5006202757358551, lr: 1e-05
2023-12-21 05:47:08 INFO     	 * (global step 2000: loss: 0.4927281588315964, lr: 1e-05
2023-12-21 05:47:16 INFO     	 * (global step 2050: loss: 0.47885483503341675, lr: 1e-05
2023-12-21 05:47:24 INFO     	 * (global step 2100: loss: 0.482559397816658, lr: 1e-05
2023-12-21 05:47:32 INFO     	 * (global step 2150: loss: 0.5871424823999405, lr: 1e-05
2023-12-21 05:47:40 INFO     	 * (global step 2200: loss: 0.4970458447933197, lr: 1e-05
2023-12-21 05:47:48 INFO     	 * (global step 2250: loss: 0.541336327791214, lr: 1e-05
2023-12-21 05:47:56 INFO     	 * (global step 2300: loss: 0.5081181675195694, lr: 1e-05
2023-12-21 05:48:04 INFO     	 * (global step 2350: loss: 0.3977254182100296, lr: 1e-05
2023-12-21 05:48:12 INFO     	 * (global step 2400: loss: 0.49083784222602844, lr: 1e-05
2023-12-21 05:48:20 INFO     	 * (global step 2450: loss: 0.3835391253232956, lr: 1e-05
2023-12-21 05:48:28 INFO     	 * (global step 2500: loss: 0.4440813511610031, lr: 1e-05
2023-12-21 05:48:36 INFO     	 * (global step 2550: loss: 0.7948910892009735, lr: 1e-05
2023-12-21 05:48:44 INFO     	 * (global step 2600: loss: 0.5770972222089767, lr: 1e-05
2023-12-21 05:48:52 INFO     	 * (global step 2650: loss: 0.5697242021560669, lr: 1e-05
2023-12-21 05:49:00 INFO     	 * (global step 2700: loss: 0.3966704159975052, lr: 1e-05
2023-12-21 05:49:08 INFO     	 * (global step 2750: loss: 0.5407078564167023, lr: 1e-05
2023-12-21 05:49:16 INFO     	 * (global step 2800: loss: 0.513765960931778, lr: 1e-05
2023-12-21 05:49:24 INFO     	 * (global step 2850: loss: 0.3261433392763138, lr: 1e-05
2023-12-21 05:49:32 INFO     	 * (global step 2900: loss: 0.4823833703994751, lr: 1e-05
2023-12-21 05:49:40 INFO     	 * (global step 2950: loss: 0.28255966305732727, lr: 1e-05
2023-12-21 05:49:48 INFO     	 * (global step 3000: loss: 0.7039917707443237, lr: 1e-05
2023-12-21 05:49:56 INFO     	 * (global step 3050: loss: 0.4528759717941284, lr: 1e-05
2023-12-21 05:50:04 INFO     	 * (global step 3100: loss: 0.44925442337989807, lr: 1e-05
2023-12-21 05:50:12 INFO     	 * (global step 3150: loss: 0.43330471217632294, lr: 1e-05
2023-12-21 05:50:20 INFO     	 * (global step 3200: loss: 0.39464761316776276, lr: 1e-05
2023-12-21 05:50:28 INFO     	 * (global step 3250: loss: 0.4428756833076477, lr: 1e-05
2023-12-21 05:50:36 INFO     	 * (global step 3300: loss: 0.5975640416145325, lr: 1e-05
2023-12-21 05:50:44 INFO     	 * (global step 3350: loss: 0.40235063433647156, lr: 1e-05
2023-12-21 05:50:52 INFO     	 * (global step 3400: loss: 0.41888926923274994, lr: 1e-05
2023-12-21 05:51:00 INFO     	 * (global step 3450: loss: 0.5095836967229843, lr: 1e-05
2023-12-21 05:51:08 INFO     	 * (global step 3500: loss: 0.3873753845691681, lr: 1e-05
2023-12-21 05:51:16 INFO     	 * (global step 3550: loss: 0.6348569989204407, lr: 1e-05
2023-12-21 05:51:24 INFO     	 * (global step 3600: loss: 0.476227268576622, lr: 1e-05
2023-12-21 05:51:32 INFO     	 * (global step 3650: loss: 0.5212833732366562, lr: 1e-05
2023-12-21 05:51:40 INFO     	 * (global step 3700: loss: 0.40286439657211304, lr: 1e-05
2023-12-21 05:51:48 INFO     	 * (global step 3750: loss: 0.4148622304201126, lr: 1e-05
2023-12-21 05:51:56 INFO     	 * (global step 3800: loss: 0.3940749317407608, lr: 1e-05
2023-12-21 05:52:04 INFO     	 * (global step 3850: loss: 0.24359028041362762, lr: 1e-05
2023-12-21 05:52:12 INFO     	 * (global step 3900: loss: 0.4864963889122009, lr: 1e-05
2023-12-21 05:52:20 INFO     	 * (global step 3950: loss: 0.3276425451040268, lr: 1e-05
2023-12-21 05:52:28 INFO     	 * (global step 4000: loss: 0.30731500685214996, lr: 1e-05
2023-12-21 05:52:36 INFO     	 * (global step 4050: loss: 0.3139745593070984, lr: 1e-05
2023-12-21 05:52:44 INFO     	 * (global step 4100: loss: 0.41745994985103607, lr: 1e-05
2023-12-21 05:52:52 INFO     	 * (global step 4150: loss: 0.322064608335495, lr: 1e-05
2023-12-21 05:53:00 INFO     	 * (global step 4200: loss: 0.3584689646959305, lr: 1e-05
2023-12-21 05:53:08 INFO     	 * (global step 4250: loss: 0.4410828799009323, lr: 1e-05
2023-12-21 05:53:16 INFO     	 * (global step 4300: loss: 0.5171850323677063, lr: 1e-05
2023-12-21 05:53:24 INFO     	 * (global step 4350: loss: 0.34500741213560104, lr: 1e-05
2023-12-21 05:53:32 INFO     	 * (global step 4400: loss: 0.4605732262134552, lr: 1e-05
2023-12-21 05:53:40 INFO     	 * (global step 4450: loss: 0.47170720994472504, lr: 1e-05
2023-12-21 05:53:48 INFO     	 * (global step 4500: loss: 0.34172680974006653, lr: 1e-05
2023-12-21 05:53:56 INFO     	 * (global step 4550: loss: 0.3584465757012367, lr: 1e-05
2023-12-21 05:54:04 INFO     	 * (global step 4600: loss: 0.4129214882850647, lr: 1e-05
2023-12-21 05:54:12 INFO     	 * (global step 4650: loss: 0.45520950853824615, lr: 1e-05
2023-12-21 05:54:20 INFO     	 * (global step 4700: loss: 0.4120926856994629, lr: 1e-05
2023-12-21 05:54:23 INFO     [epoch 0/15] average loss: 0.719, lr: 1e-05
2023-12-21 05:54:23 INFO     saving model related files
2023-12-21 05:54:23 INFO     saving model
2023-12-21 05:54:24 INFO     saving tokenizer
2023-12-21 05:54:24 INFO     saving optimizer
2023-12-21 05:54:25 INFO     remove old optimizer files
2023-12-21 05:54:29 INFO     	 * (global step 4750: loss: 0.5252730846405029, lr: 1e-05
2023-12-21 05:54:37 INFO     	 * (global step 4800: loss: 0.3826598823070526, lr: 1e-05
2023-12-21 05:54:45 INFO     	 * (global step 4850: loss: 0.3365985304117203, lr: 1e-05
2023-12-21 05:54:53 INFO     	 * (global step 4900: loss: 0.4753500819206238, lr: 1e-05
2023-12-21 05:55:01 INFO     	 * (global step 4950: loss: 0.42259135842323303, lr: 1e-05
2023-12-21 05:55:09 INFO     	 * (global step 5000: loss: 0.2933473289012909, lr: 1e-05
2023-12-21 05:55:17 INFO     	 * (global step 5050: loss: 0.45100632309913635, lr: 1e-05
2023-12-21 05:55:25 INFO     	 * (global step 5100: loss: 0.4001987874507904, lr: 1e-05
2023-12-21 05:55:33 INFO     	 * (global step 5150: loss: 0.29196852445602417, lr: 1e-05
2023-12-21 05:55:41 INFO     	 * (global step 5200: loss: 0.3791603296995163, lr: 1e-05
2023-12-21 05:55:49 INFO     	 * (global step 5250: loss: 0.3164922147989273, lr: 1e-05
2023-12-21 05:55:57 INFO     	 * (global step 5300: loss: 0.411502480506897, lr: 1e-05
2023-12-21 05:56:05 INFO     	 * (global step 5350: loss: 0.5013992935419083, lr: 1e-05
2023-12-21 05:56:13 INFO     	 * (global step 5400: loss: 0.4608525484800339, lr: 1e-05
2023-12-21 05:56:21 INFO     	 * (global step 5450: loss: 0.774744987487793, lr: 1e-05
2023-12-21 05:56:29 INFO     	 * (global step 5500: loss: 0.3351118713617325, lr: 1e-05
2023-12-21 05:56:37 INFO     	 * (global step 5550: loss: 0.32587721943855286, lr: 1e-05
2023-12-21 05:56:45 INFO     	 * (global step 5600: loss: 0.5460112690925598, lr: 1e-05
2023-12-21 05:56:53 INFO     	 * (global step 5650: loss: 0.2639656066894531, lr: 1e-05
2023-12-21 05:57:01 INFO     	 * (global step 5700: loss: 0.371078759431839, lr: 1e-05
2023-12-21 05:57:09 INFO     	 * (global step 5750: loss: 0.4357324093580246, lr: 1e-05
2023-12-21 05:57:17 INFO     	 * (global step 5800: loss: 0.34023046493530273, lr: 1e-05
2023-12-21 05:57:25 INFO     	 * (global step 5850: loss: 0.3831135481595993, lr: 1e-05
2023-12-21 05:57:33 INFO     	 * (global step 5900: loss: 0.30881233513355255, lr: 1e-05
2023-12-21 05:57:41 INFO     	 * (global step 5950: loss: 0.4923167824745178, lr: 1e-05
2023-12-21 05:57:49 INFO     	 * (global step 6000: loss: 0.40480343997478485, lr: 1e-05
2023-12-21 05:57:57 INFO     	 * (global step 6050: loss: 0.3592247813940048, lr: 1e-05
2023-12-21 05:58:05 INFO     	 * (global step 6100: loss: 0.2991103529930115, lr: 1e-05
2023-12-21 05:58:13 INFO     	 * (global step 6150: loss: 0.3811469227075577, lr: 1e-05
2023-12-21 05:58:21 INFO     	 * (global step 6200: loss: 0.46398523449897766, lr: 1e-05
2023-12-21 05:58:29 INFO     	 * (global step 6250: loss: 0.4127163141965866, lr: 1e-05
2023-12-21 05:58:37 INFO     	 * (global step 6300: loss: 0.7506860494613647, lr: 1e-05
2023-12-21 05:58:45 INFO     	 * (global step 6350: loss: 0.23432444781064987, lr: 1e-05
2023-12-21 05:58:53 INFO     	 * (global step 6400: loss: 0.3578914999961853, lr: 1e-05
2023-12-21 05:59:01 INFO     	 * (global step 6450: loss: 0.314742736518383, lr: 1e-05
2023-12-21 05:59:09 INFO     	 * (global step 6500: loss: 0.3099888414144516, lr: 1e-05
2023-12-21 05:59:17 INFO     	 * (global step 6550: loss: 0.21539147943258286, lr: 1e-05
2023-12-21 05:59:25 INFO     	 * (global step 6600: loss: 0.37740960717201233, lr: 1e-05
2023-12-21 05:59:33 INFO     	 * (global step 6650: loss: 0.37159135937690735, lr: 1e-05
2023-12-21 05:59:41 INFO     	 * (global step 6700: loss: 0.3947765678167343, lr: 1e-05
2023-12-21 05:59:49 INFO     	 * (global step 6750: loss: 0.50481878221035, lr: 1e-05
2023-12-21 05:59:57 INFO     	 * (global step 6800: loss: 0.3558315634727478, lr: 1e-05
2023-12-21 06:00:05 INFO     	 * (global step 6850: loss: 0.7657380402088165, lr: 1e-05
2023-12-21 06:00:13 INFO     	 * (global step 6900: loss: 0.37421247363090515, lr: 1e-05
2023-12-21 06:00:21 INFO     	 * (global step 6950: loss: 0.32268884778022766, lr: 1e-05
2023-12-21 06:00:29 INFO     	 * (global step 7000: loss: 0.5277785211801529, lr: 1e-05
2023-12-21 06:00:37 INFO     	 * (global step 7050: loss: 0.37940774857997894, lr: 1e-05
2023-12-21 06:00:44 INFO     	 * (global step 7100: loss: 0.39832915365695953, lr: 1e-05
2023-12-21 06:00:52 INFO     	 * (global step 7150: loss: 0.3273954540491104, lr: 1e-05
2023-12-21 06:01:00 INFO     	 * (global step 7200: loss: 0.3378543108701706, lr: 1e-05
2023-12-21 06:01:08 INFO     	 * (global step 7250: loss: 0.373934268951416, lr: 1e-05
2023-12-21 06:01:16 INFO     	 * (global step 7300: loss: 0.36641743779182434, lr: 1e-05
2023-12-21 06:01:24 INFO     	 * (global step 7350: loss: 0.2461131066083908, lr: 1e-05
2023-12-21 06:01:32 INFO     	 * (global step 7400: loss: 0.5249754339456558, lr: 1e-05
2023-12-21 06:01:40 INFO     	 * (global step 7450: loss: 0.5196754783391953, lr: 1e-05
2023-12-21 06:01:48 INFO     	 * (global step 7500: loss: 0.3777723014354706, lr: 1e-05
2023-12-21 06:01:56 INFO     	 * (global step 7550: loss: 0.46013985574245453, lr: 1e-05
2023-12-21 06:02:04 INFO     	 * (global step 7600: loss: 0.28495535999536514, lr: 1e-05
2023-12-21 06:02:12 INFO     	 * (global step 7650: loss: 0.3200099617242813, lr: 1e-05
2023-12-21 06:02:20 INFO     	 * (global step 7700: loss: 0.426260307431221, lr: 1e-05
2023-12-21 06:02:28 INFO     	 * (global step 7750: loss: 0.3457949683070183, lr: 1e-05
2023-12-21 06:02:36 INFO     	 * (global step 7800: loss: 0.34596124291419983, lr: 1e-05
2023-12-21 06:02:44 INFO     	 * (global step 7850: loss: 0.3949660509824753, lr: 1e-05
2023-12-21 06:02:52 INFO     	 * (global step 7900: loss: 0.44138482213020325, lr: 1e-05
2023-12-21 06:03:00 INFO     	 * (global step 7950: loss: 0.4112684428691864, lr: 1e-05
2023-12-21 06:03:08 INFO     	 * (global step 8000: loss: 0.2848645895719528, lr: 1e-05
2023-12-21 06:03:16 INFO     	 * (global step 8050: loss: 0.4575423374772072, lr: 1e-05
2023-12-21 06:03:24 INFO     	 * (global step 8100: loss: 0.4904688894748688, lr: 1e-05
2023-12-21 06:03:32 INFO     	 * (global step 8150: loss: 0.5790349841117859, lr: 1e-05
2023-12-21 06:03:40 INFO     	 * (global step 8200: loss: 0.27849969267845154, lr: 1e-05
2023-12-21 06:03:48 INFO     	 * (global step 8250: loss: 0.5253859609365463, lr: 1e-05
2023-12-21 06:03:56 INFO     	 * (global step 8300: loss: 0.3256200700998306, lr: 1e-05
2023-12-21 06:04:04 INFO     	 * (global step 8350: loss: 0.517067477107048, lr: 1e-05
2023-12-21 06:04:12 INFO     	 * (global step 8400: loss: 0.33872365951538086, lr: 1e-05
2023-12-21 06:04:20 INFO     	 * (global step 8450: loss: 0.4266412854194641, lr: 1e-05
2023-12-21 06:04:28 INFO     	 * (global step 8500: loss: 0.5248731821775436, lr: 1e-05
2023-12-21 06:04:36 INFO     	 * (global step 8550: loss: 0.5584128499031067, lr: 1e-05
2023-12-21 06:04:44 INFO     	 * (global step 8600: loss: 0.23737812787294388, lr: 1e-05
2023-12-21 06:04:52 INFO     	 * (global step 8650: loss: 0.5043866485357285, lr: 1e-05
2023-12-21 06:05:00 INFO     	 * (global step 8700: loss: 0.2176009714603424, lr: 1e-05
2023-12-21 06:05:08 INFO     	 * (global step 8750: loss: 0.26461297273635864, lr: 1e-05
2023-12-21 06:05:16 INFO     	 * (global step 8800: loss: 0.33295978605747223, lr: 1e-05
2023-12-21 06:05:24 INFO     	 * (global step 8850: loss: 0.445552334189415, lr: 1e-05
2023-12-21 06:05:32 INFO     	 * (global step 8900: loss: 0.2871156930923462, lr: 1e-05
2023-12-21 06:05:40 INFO     	 * (global step 8950: loss: 0.2858666703104973, lr: 1e-05
2023-12-21 06:05:48 INFO     	 * (global step 9000: loss: 0.31017347425222397, lr: 1e-05
2023-12-21 06:05:56 INFO     	 * (global step 9050: loss: 0.5627783536911011, lr: 1e-05
2023-12-21 06:06:04 INFO     	 * (global step 9100: loss: 0.4700666218996048, lr: 1e-05
2023-12-21 06:06:12 INFO     	 * (global step 9150: loss: 0.3182470500469208, lr: 1e-05
2023-12-21 06:06:20 INFO     	 * (global step 9200: loss: 0.26757345348596573, lr: 1e-05
2023-12-21 06:06:28 INFO     	 * (global step 9250: loss: 0.6677846014499664, lr: 1e-05
2023-12-21 06:06:36 INFO     	 * (global step 9300: loss: 0.36962205171585083, lr: 1e-05
2023-12-21 06:06:44 INFO     	 * (global step 9350: loss: 0.45601484179496765, lr: 1e-05
2023-12-21 06:06:52 INFO     	 * (global step 9400: loss: 0.37825043499469757, lr: 1e-05
2023-12-21 06:06:59 INFO     [epoch 1/15] average loss: 0.391, lr: 1e-05
2023-12-21 06:06:59 INFO     saving model related files
2023-12-21 06:06:59 INFO     saving model
2023-12-21 06:07:00 INFO     saving tokenizer
2023-12-21 06:07:00 INFO     saving optimizer
2023-12-21 06:07:01 INFO     remove old optimizer files
2023-12-21 06:07:01 INFO     	 * (global step 9450: loss: 0.4531451612710953, lr: 1e-05
2023-12-21 06:07:09 INFO     	 * (global step 9500: loss: 0.3187858760356903, lr: 1e-05
2023-12-21 06:07:17 INFO     	 * (global step 9550: loss: 0.4380260407924652, lr: 1e-05
2023-12-21 06:07:25 INFO     	 * (global step 9600: loss: 0.3151218146085739, lr: 1e-05
2023-12-21 06:07:33 INFO     	 * (global step 9650: loss: 0.28942449390888214, lr: 1e-05
2023-12-21 06:07:41 INFO     	 * (global step 9700: loss: 0.2725445032119751, lr: 1e-05
2023-12-21 06:07:49 INFO     	 * (global step 9750: loss: 0.46202296018600464, lr: 1e-05
2023-12-21 06:07:57 INFO     	 * (global step 9800: loss: 0.2924950420856476, lr: 1e-05
2023-12-21 06:08:05 INFO     	 * (global step 9850: loss: 0.30788160115480423, lr: 1e-05
2023-12-21 06:08:13 INFO     	 * (global step 9900: loss: 0.23688708245754242, lr: 1e-05
2023-12-21 06:08:21 INFO     	 * (global step 9950: loss: 0.3545813113451004, lr: 1e-05
2023-12-21 06:08:29 INFO     	 * (global step 10000: loss: 0.2842072695493698, lr: 1e-05
2023-12-21 06:08:37 INFO     	 * (global step 10050: loss: 0.26206573843955994, lr: 1e-05
2023-12-21 06:08:45 INFO     	 * (global step 10100: loss: 0.3443772718310356, lr: 1e-05
2023-12-21 06:08:53 INFO     	 * (global step 10150: loss: 0.3436525762081146, lr: 1e-05
2023-12-21 06:09:01 INFO     	 * (global step 10200: loss: 0.34282585978507996, lr: 1e-05
2023-12-21 06:09:09 INFO     	 * (global step 10250: loss: 0.5632565766572952, lr: 1e-05
2023-12-21 06:09:17 INFO     	 * (global step 10300: loss: 0.4761319160461426, lr: 1e-05
2023-12-21 06:09:25 INFO     	 * (global step 10350: loss: 0.4567737728357315, lr: 1e-05
2023-12-21 06:09:33 INFO     	 * (global step 10400: loss: 0.36199021339416504, lr: 1e-05
2023-12-21 06:09:41 INFO     	 * (global step 10450: loss: 0.34009358286857605, lr: 1e-05
2023-12-21 06:09:49 INFO     	 * (global step 10500: loss: 0.3190390169620514, lr: 1e-05
2023-12-21 06:09:57 INFO     	 * (global step 10550: loss: 0.3804551213979721, lr: 1e-05
2023-12-21 06:10:05 INFO     	 * (global step 10600: loss: 0.35036584734916687, lr: 1e-05
2023-12-21 06:10:13 INFO     	 * (global step 10650: loss: 0.3857780247926712, lr: 1e-05
2023-12-21 06:10:21 INFO     	 * (global step 10700: loss: 0.309002161026001, lr: 1e-05
2023-12-21 06:10:29 INFO     	 * (global step 10750: loss: 0.4316401183605194, lr: 1e-05
2023-12-21 06:10:37 INFO     	 * (global step 10800: loss: 0.42713993787765503, lr: 1e-05
2023-12-21 06:10:45 INFO     	 * (global step 10850: loss: 0.3148060739040375, lr: 1e-05
2023-12-21 06:10:53 INFO     	 * (global step 10900: loss: 0.3625479191541672, lr: 1e-05
2023-12-21 06:11:01 INFO     	 * (global step 10950: loss: 0.34087763726711273, lr: 1e-05
2023-12-21 06:11:09 INFO     	 * (global step 11000: loss: 0.6141380071640015, lr: 1e-05
2023-12-21 06:11:17 INFO     	 * (global step 11050: loss: 0.3629700243473053, lr: 1e-05
2023-12-21 06:11:25 INFO     	 * (global step 11100: loss: 0.6240962892770767, lr: 1e-05
2023-12-21 06:11:33 INFO     	 * (global step 11150: loss: 0.37079788744449615, lr: 1e-05
2023-12-21 06:11:41 INFO     	 * (global step 11200: loss: 0.28299538791179657, lr: 1e-05
2023-12-21 06:11:49 INFO     	 * (global step 11250: loss: 0.2253548949956894, lr: 1e-05
2023-12-21 06:11:57 INFO     	 * (global step 11300: loss: 0.375461146235466, lr: 1e-05
2023-12-21 06:12:05 INFO     	 * (global step 11350: loss: 0.3206861764192581, lr: 1e-05
2023-12-21 06:12:13 INFO     	 * (global step 11400: loss: 0.26135334372520447, lr: 1e-05
2023-12-21 06:12:21 INFO     	 * (global step 11450: loss: 0.31975386291742325, lr: 1e-05
2023-12-21 06:12:29 INFO     	 * (global step 11500: loss: 0.2748119682073593, lr: 1e-05
2023-12-21 06:12:37 INFO     	 * (global step 11550: loss: 0.5134679824113846, lr: 1e-05
2023-12-21 06:12:45 INFO     	 * (global step 11600: loss: 0.4384906738996506, lr: 1e-05
2023-12-21 06:12:53 INFO     	 * (global step 11650: loss: 0.3082234859466553, lr: 1e-05
2023-12-21 06:13:01 INFO     	 * (global step 11700: loss: 0.5400221794843674, lr: 1e-05
2023-12-21 06:13:09 INFO     	 * (global step 11750: loss: 0.3063272684812546, lr: 1e-05
2023-12-21 06:13:17 INFO     	 * (global step 11800: loss: 0.23331131041049957, lr: 1e-05
2023-12-21 06:13:25 INFO     	 * (global step 11850: loss: 0.35633572190999985, lr: 1e-05
2023-12-21 06:13:33 INFO     	 * (global step 11900: loss: 0.3992500975728035, lr: 1e-05
2023-12-21 06:13:41 INFO     	 * (global step 11950: loss: 0.37297476828098297, lr: 1e-05
2023-12-21 06:13:49 INFO     	 * (global step 12000: loss: 0.3068213611841202, lr: 1e-05
2023-12-21 06:13:57 INFO     	 * (global step 12050: loss: 0.2923338860273361, lr: 1e-05
2023-12-21 06:14:05 INFO     	 * (global step 12100: loss: 0.3920630067586899, lr: 1e-05
2023-12-21 06:14:13 INFO     	 * (global step 12150: loss: 0.42482928931713104, lr: 1e-05
2023-12-21 06:14:21 INFO     	 * (global step 12200: loss: 0.3558291792869568, lr: 1e-05
2023-12-21 06:14:29 INFO     	 * (global step 12250: loss: 0.34330980479717255, lr: 1e-05
2023-12-21 06:14:37 INFO     	 * (global step 12300: loss: 0.29130419343709946, lr: 1e-05
2023-12-21 06:14:46 INFO     	 * (global step 12350: loss: 0.4097893089056015, lr: 1e-05
2023-12-21 06:14:54 INFO     	 * (global step 12400: loss: 0.7629024535417557, lr: 1e-05
2023-12-21 06:15:02 INFO     	 * (global step 12450: loss: 0.23997561633586884, lr: 1e-05
2023-12-21 06:15:10 INFO     	 * (global step 12500: loss: 0.4246405363082886, lr: 1e-05
2023-12-21 06:15:18 INFO     	 * (global step 12550: loss: 0.4225628077983856, lr: 1e-05
2023-12-21 06:15:26 INFO     	 * (global step 12600: loss: 0.5093813836574554, lr: 1e-05
2023-12-21 06:15:34 INFO     	 * (global step 12650: loss: 0.3590068519115448, lr: 1e-05
2023-12-21 06:15:42 INFO     	 * (global step 12700: loss: 0.29026374220848083, lr: 1e-05
2023-12-21 06:15:50 INFO     	 * (global step 12750: loss: 0.38864728808403015, lr: 1e-05
2023-12-21 06:15:58 INFO     	 * (global step 12800: loss: 0.30776625871658325, lr: 1e-05
2023-12-21 06:16:06 INFO     	 * (global step 12850: loss: 0.30008190870285034, lr: 1e-05
2023-12-21 06:16:14 INFO     	 * (global step 12900: loss: 0.3104304149746895, lr: 1e-05
2023-12-21 06:16:22 INFO     	 * (global step 12950: loss: 0.42208926379680634, lr: 1e-05
2023-12-21 06:16:30 INFO     	 * (global step 13000: loss: 0.46363532543182373, lr: 1e-05
2023-12-21 06:16:38 INFO     	 * (global step 13050: loss: 0.2941528260707855, lr: 1e-05
2023-12-21 06:16:46 INFO     	 * (global step 13100: loss: 0.3790799155831337, lr: 1e-05
2023-12-21 06:16:54 INFO     	 * (global step 13150: loss: 0.40459302067756653, lr: 1e-05
2023-12-21 06:17:02 INFO     	 * (global step 13200: loss: 0.33899249136447906, lr: 1e-05
2023-12-21 06:17:10 INFO     	 * (global step 13250: loss: 0.28635671734809875, lr: 1e-05
2023-12-21 06:17:18 INFO     	 * (global step 13300: loss: 0.4530213475227356, lr: 1e-05
2023-12-21 06:17:26 INFO     	 * (global step 13350: loss: 0.4654712975025177, lr: 1e-05
2023-12-21 06:17:34 INFO     	 * (global step 13400: loss: 0.2598615884780884, lr: 1e-05
2023-12-21 06:17:42 INFO     	 * (global step 13450: loss: 0.4025249183177948, lr: 1e-05
2023-12-21 06:17:50 INFO     	 * (global step 13500: loss: 0.30168531835079193, lr: 1e-05
2023-12-21 06:17:58 INFO     	 * (global step 13550: loss: 0.2460315153002739, lr: 1e-05
2023-12-21 06:18:06 INFO     	 * (global step 13600: loss: 0.6330205053091049, lr: 1e-05
2023-12-21 06:18:14 INFO     	 * (global step 13650: loss: 0.24796921759843826, lr: 1e-05
2023-12-21 06:18:22 INFO     	 * (global step 13700: loss: 0.3401351347565651, lr: 1e-05
2023-12-21 06:18:30 INFO     	 * (global step 13750: loss: 0.31551697850227356, lr: 1e-05
2023-12-21 06:18:38 INFO     	 * (global step 13800: loss: 0.4115038141608238, lr: 1e-05
2023-12-21 06:18:46 INFO     	 * (global step 13850: loss: 0.6039001792669296, lr: 1e-05
2023-12-21 06:18:54 INFO     	 * (global step 13900: loss: 0.26113925874233246, lr: 1e-05
2023-12-21 06:19:02 INFO     	 * (global step 13950: loss: 0.33231478929519653, lr: 1e-05
2023-12-21 06:19:10 INFO     	 * (global step 14000: loss: 0.41615724563598633, lr: 1e-05
2023-12-21 06:19:18 INFO     	 * (global step 14050: loss: 0.4007178544998169, lr: 1e-05
2023-12-21 06:19:26 INFO     	 * (global step 14100: loss: 0.27244819700717926, lr: 1e-05
2023-12-21 06:19:34 INFO     	 * (global step 14150: loss: 0.37675319612026215, lr: 1e-05
2023-12-21 06:19:37 INFO     [epoch 2/15] average loss: 0.363, lr: 1e-05
2023-12-21 06:19:37 INFO     saving model related files
2023-12-21 06:19:37 INFO     saving model
2023-12-21 06:19:38 INFO     saving tokenizer
2023-12-21 06:19:38 INFO     saving optimizer
2023-12-21 06:19:39 INFO     remove old optimizer files
2023-12-21 06:19:44 INFO     	 * (global step 14200: loss: 0.3711121082305908, lr: 1e-05
2023-12-21 06:19:52 INFO     	 * (global step 14250: loss: 0.3040050193667412, lr: 1e-05
2023-12-21 06:20:00 INFO     	 * (global step 14300: loss: 0.18648536503314972, lr: 1e-05
2023-12-21 06:20:08 INFO     	 * (global step 14350: loss: 0.3980124667286873, lr: 1e-05
2023-12-21 06:20:16 INFO     	 * (global step 14400: loss: 0.37050966918468475, lr: 1e-05
2023-12-21 06:20:24 INFO     	 * (global step 14450: loss: 0.27198024094104767, lr: 1e-05
2023-12-21 06:20:32 INFO     	 * (global step 14500: loss: 0.5431654453277588, lr: 1e-05
2023-12-21 06:20:40 INFO     	 * (global step 14550: loss: 0.2629178613424301, lr: 1e-05
2023-12-21 06:20:48 INFO     	 * (global step 14600: loss: 0.2508460506796837, lr: 1e-05
2023-12-21 06:20:56 INFO     	 * (global step 14650: loss: 0.32086844742298126, lr: 1e-05
2023-12-21 06:21:04 INFO     	 * (global step 14700: loss: 0.45645809173583984, lr: 1e-05
2023-12-21 06:21:12 INFO     	 * (global step 14750: loss: 0.30604827404022217, lr: 1e-05
2023-12-21 06:21:20 INFO     	 * (global step 14800: loss: 0.4186009168624878, lr: 1e-05
2023-12-21 06:21:28 INFO     	 * (global step 14850: loss: 0.34248149394989014, lr: 1e-05
2023-12-21 06:21:36 INFO     	 * (global step 14900: loss: 0.46420033276081085, lr: 1e-05
2023-12-21 06:21:44 INFO     	 * (global step 14950: loss: 0.3634193390607834, lr: 1e-05
2023-12-21 06:21:52 INFO     	 * (global step 15000: loss: 0.2813730388879776, lr: 1e-05
2023-12-21 06:22:00 INFO     	 * (global step 15050: loss: 0.2920657768845558, lr: 1e-05
2023-12-21 06:22:08 INFO     	 * (global step 15100: loss: 0.2830835059285164, lr: 1e-05
2023-12-21 06:22:17 INFO     	 * (global step 15150: loss: 0.5014548003673553, lr: 1e-05
2023-12-21 06:22:25 INFO     	 * (global step 15200: loss: 0.5211549699306488, lr: 1e-05
2023-12-21 06:22:33 INFO     	 * (global step 15250: loss: 0.5335464179515839, lr: 1e-05
2023-12-21 06:22:41 INFO     	 * (global step 15300: loss: 0.36774466931819916, lr: 1e-05
2023-12-21 06:22:49 INFO     	 * (global step 15350: loss: 0.3319927975535393, lr: 1e-05
2023-12-21 06:22:57 INFO     	 * (global step 15400: loss: 0.2706451117992401, lr: 1e-05
2023-12-21 06:23:05 INFO     	 * (global step 15450: loss: 0.3399914354085922, lr: 1e-05
2023-12-21 06:23:13 INFO     	 * (global step 15500: loss: 0.35608893632888794, lr: 1e-05
2023-12-21 06:23:21 INFO     	 * (global step 15550: loss: 0.2838602587580681, lr: 1e-05
2023-12-21 06:23:29 INFO     	 * (global step 15600: loss: 0.4558950960636139, lr: 1e-05
2023-12-21 06:23:37 INFO     	 * (global step 15650: loss: 0.3973078727722168, lr: 1e-05
2023-12-21 06:23:45 INFO     	 * (global step 15700: loss: 0.37257660925388336, lr: 1e-05
2023-12-21 06:23:53 INFO     	 * (global step 15750: loss: 0.43582966923713684, lr: 1e-05
2023-12-21 06:24:01 INFO     	 * (global step 15800: loss: 0.26157964766025543, lr: 1e-05
2023-12-21 06:24:09 INFO     	 * (global step 15850: loss: 0.40469804406166077, lr: 1e-05
2023-12-21 06:24:17 INFO     	 * (global step 15900: loss: 0.26010967791080475, lr: 1e-05
2023-12-21 06:24:25 INFO     	 * (global step 15950: loss: 0.36266475915908813, lr: 1e-05
2023-12-21 06:24:33 INFO     	 * (global step 16000: loss: 0.18777745217084885, lr: 1e-05
2023-12-21 06:24:41 INFO     	 * (global step 16050: loss: 0.4824414849281311, lr: 1e-05
2023-12-21 06:24:49 INFO     	 * (global step 16100: loss: 0.40711329877376556, lr: 1e-05
2023-12-21 06:24:57 INFO     	 * (global step 16150: loss: 0.2853944003582001, lr: 1e-05
2023-12-21 06:25:05 INFO     	 * (global step 16200: loss: 0.32059381902217865, lr: 1e-05
2023-12-21 06:25:13 INFO     	 * (global step 16250: loss: 0.3853626847267151, lr: 1e-05
2023-12-21 06:25:21 INFO     	 * (global step 16300: loss: 0.29076530784368515, lr: 1e-05
2023-12-21 06:25:29 INFO     	 * (global step 16350: loss: 0.35743340849876404, lr: 1e-05
2023-12-21 06:25:37 INFO     	 * (global step 16400: loss: 0.36017850041389465, lr: 1e-05
2023-12-21 06:25:45 INFO     	 * (global step 16450: loss: 0.3589743822813034, lr: 1e-05
2023-12-21 06:25:53 INFO     	 * (global step 16500: loss: 0.3424276113510132, lr: 1e-05
2023-12-21 06:26:01 INFO     	 * (global step 16550: loss: 0.38800448179244995, lr: 1e-05
2023-12-21 06:26:09 INFO     	 * (global step 16600: loss: 0.30423421412706375, lr: 1e-05
2023-12-21 06:26:17 INFO     	 * (global step 16650: loss: 0.35550108551979065, lr: 1e-05
2023-12-21 06:26:25 INFO     	 * (global step 16700: loss: 0.2599039897322655, lr: 1e-05
2023-12-21 06:26:33 INFO     	 * (global step 16750: loss: 0.1631110981106758, lr: 1e-05
2023-12-21 06:26:41 INFO     	 * (global step 16800: loss: 0.35334426164627075, lr: 1e-05
2023-12-21 06:26:49 INFO     	 * (global step 16850: loss: 0.41865672171115875, lr: 1e-05
2023-12-21 06:26:57 INFO     	 * (global step 16900: loss: 0.3120039701461792, lr: 1e-05
2023-12-21 06:27:05 INFO     	 * (global step 16950: loss: 0.390916109085083, lr: 1e-05
2023-12-21 06:27:13 INFO     	 * (global step 17000: loss: 0.29377777874469757, lr: 1e-05
2023-12-21 06:27:21 INFO     	 * (global step 17050: loss: 0.2428528070449829, lr: 1e-05
2023-12-21 06:27:30 INFO     	 * (global step 17100: loss: 0.3701193779706955, lr: 1e-05
2023-12-21 06:27:38 INFO     	 * (global step 17150: loss: 0.2941244915127754, lr: 1e-05
2023-12-21 06:27:46 INFO     	 * (global step 17200: loss: 0.3633961156010628, lr: 1e-05
2023-12-21 06:27:54 INFO     	 * (global step 17250: loss: 0.3326919972896576, lr: 1e-05
2023-12-21 06:28:02 INFO     	 * (global step 17300: loss: 0.2536608725786209, lr: 1e-05
2023-12-21 06:28:10 INFO     	 * (global step 17350: loss: 0.41532063484191895, lr: 1e-05
2023-12-21 06:28:18 INFO     	 * (global step 17400: loss: 0.2302032709121704, lr: 1e-05
2023-12-21 06:28:26 INFO     	 * (global step 17450: loss: 0.5040368959307671, lr: 1e-05
2023-12-21 06:28:34 INFO     	 * (global step 17500: loss: 0.3802638202905655, lr: 1e-05
2023-12-21 06:28:42 INFO     	 * (global step 17550: loss: 0.33344055712223053, lr: 1e-05
2023-12-21 06:28:50 INFO     	 * (global step 17600: loss: 0.33542588353157043, lr: 1e-05
2023-12-21 06:28:58 INFO     	 * (global step 17650: loss: 0.46349143981933594, lr: 1e-05
2023-12-21 06:29:06 INFO     	 * (global step 17700: loss: 0.30440889298915863, lr: 1e-05
2023-12-21 06:29:14 INFO     	 * (global step 17750: loss: 0.3917962908744812, lr: 1e-05
2023-12-21 06:29:22 INFO     	 * (global step 17800: loss: 0.536663681268692, lr: 1e-05
2023-12-21 06:29:30 INFO     	 * (global step 17850: loss: 0.25884605199098587, lr: 1e-05
2023-12-21 06:29:38 INFO     	 * (global step 17900: loss: 0.373471274971962, lr: 1e-05
2023-12-21 06:29:46 INFO     	 * (global step 17950: loss: 0.3237803280353546, lr: 1e-05
2023-12-21 06:29:54 INFO     	 * (global step 18000: loss: 0.35583028197288513, lr: 1e-05
2023-12-21 06:30:02 INFO     	 * (global step 18050: loss: 0.22917790710926056, lr: 1e-05
2023-12-21 06:30:10 INFO     	 * (global step 18100: loss: 0.3752148151397705, lr: 1e-05
2023-12-21 06:30:18 INFO     	 * (global step 18150: loss: 0.34151491522789, lr: 1e-05
2023-12-21 06:30:26 INFO     	 * (global step 18200: loss: 0.4449482411146164, lr: 1e-05
2023-12-21 06:30:34 INFO     	 * (global step 18250: loss: 0.2860836237668991, lr: 1e-05
2023-12-21 06:30:42 INFO     	 * (global step 18300: loss: 0.3113458752632141, lr: 1e-05
2023-12-21 06:30:50 INFO     	 * (global step 18350: loss: 0.3418331891298294, lr: 1e-05
2023-12-21 06:30:58 INFO     	 * (global step 18400: loss: 0.3426467925310135, lr: 1e-05
2023-12-21 06:31:06 INFO     	 * (global step 18450: loss: 0.37326309084892273, lr: 1e-05
2023-12-21 06:31:14 INFO     	 * (global step 18500: loss: 0.33043844997882843, lr: 1e-05
2023-12-21 06:31:22 INFO     	 * (global step 18550: loss: 0.40231090784072876, lr: 1e-05
2023-12-21 06:31:31 INFO     	 * (global step 18600: loss: 0.47538377344608307, lr: 1e-05
2023-12-21 06:31:39 INFO     	 * (global step 18650: loss: 0.626231387257576, lr: 1e-05
2023-12-21 06:31:47 INFO     	 * (global step 18700: loss: 0.4238465130329132, lr: 1e-05
2023-12-21 06:31:55 INFO     	 * (global step 18750: loss: 0.24172180891036987, lr: 1e-05
2023-12-21 06:32:03 INFO     	 * (global step 18800: loss: 0.5703385099768639, lr: 1e-05
2023-12-21 06:32:11 INFO     	 * (global step 18850: loss: 0.2897709012031555, lr: 1e-05
2023-12-21 06:32:18 INFO     [epoch 3/15] average loss: 0.349, lr: 1e-05
2023-12-21 06:32:18 INFO     saving model related files
2023-12-21 06:32:18 INFO     saving model
2023-12-21 06:32:18 INFO     saving tokenizer
2023-12-21 06:32:18 INFO     saving optimizer
2023-12-21 06:32:19 INFO     remove old optimizer files
2023-12-21 06:32:21 INFO     	 * (global step 18900: loss: 0.2943556532263756, lr: 1e-05
2023-12-21 06:32:29 INFO     	 * (global step 18950: loss: 0.31351424753665924, lr: 1e-05
2023-12-21 06:32:37 INFO     	 * (global step 19000: loss: 0.5931851118803024, lr: 1e-05
2023-12-21 06:32:45 INFO     	 * (global step 19050: loss: 0.4198019951581955, lr: 1e-05
2023-12-21 06:32:53 INFO     	 * (global step 19100: loss: 0.2692527621984482, lr: 1e-05
2023-12-21 06:33:01 INFO     	 * (global step 19150: loss: 0.25952891260385513, lr: 1e-05
2023-12-21 06:33:09 INFO     	 * (global step 19200: loss: 0.36599916219711304, lr: 1e-05
2023-12-21 06:33:17 INFO     	 * (global step 19250: loss: 0.26462098956108093, lr: 1e-05
2023-12-21 06:33:25 INFO     	 * (global step 19300: loss: 0.26242223381996155, lr: 1e-05
2023-12-21 06:33:33 INFO     	 * (global step 19350: loss: 0.22080720216035843, lr: 1e-05
2023-12-21 06:33:41 INFO     	 * (global step 19400: loss: 0.3444901555776596, lr: 1e-05
2023-12-21 06:33:49 INFO     	 * (global step 19450: loss: 0.21451113373041153, lr: 1e-05
2023-12-21 06:33:57 INFO     	 * (global step 19500: loss: 0.36794517934322357, lr: 1e-05
2023-12-21 06:34:05 INFO     	 * (global step 19550: loss: 0.35501720011234283, lr: 1e-05
2023-12-21 06:34:13 INFO     	 * (global step 19600: loss: 0.24646329134702682, lr: 1e-05
2023-12-21 06:34:21 INFO     	 * (global step 19650: loss: 0.610071063041687, lr: 1e-05
2023-12-21 06:34:29 INFO     	 * (global step 19700: loss: 0.32467710971832275, lr: 1e-05
2023-12-21 06:34:38 INFO     	 * (global step 19750: loss: 0.3194696754217148, lr: 1e-05
2023-12-21 06:34:46 INFO     	 * (global step 19800: loss: 0.29956139624118805, lr: 1e-05
2023-12-21 06:34:54 INFO     	 * (global step 19850: loss: 0.23403993248939514, lr: 1e-05
2023-12-21 06:35:02 INFO     	 * (global step 19900: loss: 0.3648403137922287, lr: 1e-05
2023-12-21 06:35:10 INFO     	 * (global step 19950: loss: 0.29788079112768173, lr: 1e-05
2023-12-21 06:35:18 INFO     	 * (global step 20000: loss: 0.23191243410110474, lr: 1e-05
2023-12-21 06:35:26 INFO     	 * (global step 20050: loss: 0.26500314474105835, lr: 1e-05
2023-12-21 06:35:34 INFO     	 * (global step 20100: loss: 0.4064589589834213, lr: 1e-05
2023-12-21 06:35:42 INFO     	 * (global step 20150: loss: 0.3131861314177513, lr: 1e-05
2023-12-21 06:35:50 INFO     	 * (global step 20200: loss: 0.3194909989833832, lr: 1e-05
2023-12-21 06:35:58 INFO     	 * (global step 20250: loss: 0.3365868031978607, lr: 1e-05
2023-12-21 06:36:06 INFO     	 * (global step 20300: loss: 0.35330332815647125, lr: 1e-05
2023-12-21 06:36:14 INFO     	 * (global step 20350: loss: 0.27445902675390244, lr: 1e-05
2023-12-21 06:36:22 INFO     	 * (global step 20400: loss: 0.2888701260089874, lr: 1e-05
2023-12-21 06:36:30 INFO     	 * (global step 20450: loss: 0.23999237269163132, lr: 1e-05
2023-12-21 06:36:38 INFO     	 * (global step 20500: loss: 0.2708466723561287, lr: 1e-05
2023-12-21 06:36:46 INFO     	 * (global step 20550: loss: 0.277143657207489, lr: 1e-05
2023-12-21 06:36:54 INFO     	 * (global step 20600: loss: 0.3165957108139992, lr: 1e-05
2023-12-21 06:37:02 INFO     	 * (global step 20650: loss: 0.4686806946992874, lr: 1e-05
2023-12-21 06:37:10 INFO     	 * (global step 20700: loss: 0.290608286857605, lr: 1e-05
2023-12-21 06:37:18 INFO     	 * (global step 20750: loss: 0.27790845185518265, lr: 1e-05
2023-12-21 06:37:26 INFO     	 * (global step 20800: loss: 0.37933699041604996, lr: 1e-05
2023-12-21 06:37:34 INFO     	 * (global step 20850: loss: 0.36279910802841187, lr: 1e-05
2023-12-21 06:37:42 INFO     	 * (global step 20900: loss: 0.36983467638492584, lr: 1e-05
2023-12-21 06:37:50 INFO     	 * (global step 20950: loss: 0.3385116159915924, lr: 1e-05
2023-12-21 06:37:58 INFO     	 * (global step 21000: loss: 0.5347755551338196, lr: 1e-05
2023-12-21 06:38:06 INFO     	 * (global step 21050: loss: 0.2964836657047272, lr: 1e-05
2023-12-21 06:38:14 INFO     	 * (global step 21100: loss: 0.2735094726085663, lr: 1e-05
2023-12-21 06:38:23 INFO     	 * (global step 21150: loss: 0.3283730447292328, lr: 1e-05
2023-12-21 06:38:31 INFO     	 * (global step 21200: loss: 0.39652736485004425, lr: 1e-05
2023-12-21 06:38:39 INFO     	 * (global step 21250: loss: 0.28546810150146484, lr: 1e-05
2023-12-21 06:38:47 INFO     	 * (global step 21300: loss: 0.35417354106903076, lr: 1e-05
2023-12-21 06:38:55 INFO     	 * (global step 21350: loss: 0.27331972122192383, lr: 1e-05
2023-12-21 06:39:03 INFO     	 * (global step 21400: loss: 0.2342095598578453, lr: 1e-05
2023-12-21 06:39:11 INFO     	 * (global step 21450: loss: 0.5609567910432816, lr: 1e-05
2023-12-21 06:39:19 INFO     	 * (global step 21500: loss: 0.29992326349020004, lr: 1e-05
2023-12-21 06:39:27 INFO     	 * (global step 21550: loss: 0.3365065008401871, lr: 1e-05
2023-12-21 06:39:35 INFO     	 * (global step 21600: loss: 0.4074452519416809, lr: 1e-05
2023-12-21 06:39:43 INFO     	 * (global step 21650: loss: 0.29963311553001404, lr: 1e-05
2023-12-21 06:39:51 INFO     	 * (global step 21700: loss: 0.22235393524169922, lr: 1e-05
2023-12-21 06:39:59 INFO     	 * (global step 21750: loss: 0.400631383061409, lr: 1e-05
2023-12-21 06:40:07 INFO     	 * (global step 21800: loss: 0.3623279854655266, lr: 1e-05
2023-12-21 06:40:15 INFO     	 * (global step 21850: loss: 0.4189707189798355, lr: 1e-05
2023-12-21 06:40:23 INFO     	 * (global step 21900: loss: 0.308410182595253, lr: 1e-05
2023-12-21 06:40:31 INFO     	 * (global step 21950: loss: 0.3408767729997635, lr: 1e-05
2023-12-21 06:40:39 INFO     	 * (global step 22000: loss: 0.34262777864933014, lr: 1e-05
2023-12-21 06:40:47 INFO     	 * (global step 22050: loss: 0.38379405438899994, lr: 1e-05
2023-12-21 06:40:55 INFO     	 * (global step 22100: loss: 0.493681862950325, lr: 1e-05
2023-12-21 06:41:03 INFO     	 * (global step 22150: loss: 0.49837230145931244, lr: 1e-05
2023-12-21 06:41:11 INFO     	 * (global step 22200: loss: 0.2339070364832878, lr: 1e-05
2023-12-21 06:41:20 INFO     	 * (global step 22250: loss: 0.3103254586458206, lr: 1e-05
2023-12-21 06:41:28 INFO     	 * (global step 22300: loss: 0.34976959228515625, lr: 1e-05
2023-12-21 06:41:36 INFO     	 * (global step 22350: loss: 0.39904579520225525, lr: 1e-05
2023-12-21 06:41:44 INFO     	 * (global step 22400: loss: 0.23885754495859146, lr: 1e-05
2023-12-21 06:41:52 INFO     	 * (global step 22450: loss: 0.4320440888404846, lr: 1e-05
2023-12-21 06:42:00 INFO     	 * (global step 22500: loss: 0.34083931148052216, lr: 1e-05
2023-12-21 06:42:08 INFO     	 * (global step 22550: loss: 0.31716251373291016, lr: 1e-05
2023-12-21 06:42:16 INFO     	 * (global step 22600: loss: 0.33114948868751526, lr: 1e-05
2023-12-21 06:42:24 INFO     	 * (global step 22650: loss: 0.19194107875227928, lr: 1e-05
2023-12-21 06:42:32 INFO     	 * (global step 22700: loss: 0.45707589387893677, lr: 1e-05
2023-12-21 06:42:40 INFO     	 * (global step 22750: loss: 0.37955641746520996, lr: 1e-05
2023-12-21 06:42:48 INFO     	 * (global step 22800: loss: 0.44351519644260406, lr: 1e-05
2023-12-21 06:42:56 INFO     	 * (global step 22850: loss: 0.36278636008501053, lr: 1e-05
2023-12-21 06:43:04 INFO     	 * (global step 22900: loss: 0.35921938717365265, lr: 1e-05
2023-12-21 06:43:12 INFO     	 * (global step 22950: loss: 0.25606220215559006, lr: 1e-05
2023-12-21 06:43:20 INFO     	 * (global step 23000: loss: 0.4249245375394821, lr: 1e-05
2023-12-21 06:43:28 INFO     	 * (global step 23050: loss: 0.5007382780313492, lr: 1e-05
2023-12-21 06:43:36 INFO     	 * (global step 23100: loss: 0.4017740339040756, lr: 1e-05
2023-12-21 06:43:44 INFO     	 * (global step 23150: loss: 0.3084557354450226, lr: 1e-05
2023-12-21 06:43:52 INFO     	 * (global step 23200: loss: 0.32428763806819916, lr: 1e-05
2023-12-21 06:44:00 INFO     	 * (global step 23250: loss: 0.18240900337696075, lr: 1e-05
2023-12-21 06:44:08 INFO     	 * (global step 23300: loss: 0.30184853076934814, lr: 1e-05
2023-12-21 06:44:16 INFO     	 * (global step 23350: loss: 0.3093997687101364, lr: 1e-05
2023-12-21 06:44:24 INFO     	 * (global step 23400: loss: 0.3780759572982788, lr: 1e-05
2023-12-21 06:44:32 INFO     	 * (global step 23450: loss: 0.4589563459157944, lr: 1e-05
2023-12-21 06:44:40 INFO     	 * (global step 23500: loss: 0.2831384465098381, lr: 1e-05
2023-12-21 06:44:48 INFO     	 * (global step 23550: loss: 0.3325956091284752, lr: 1e-05
2023-12-21 06:44:56 INFO     	 * (global step 23600: loss: 0.40246595442295074, lr: 1e-05
2023-12-21 06:44:58 INFO     [epoch 4/15] average loss: 0.34, lr: 1e-05
2023-12-21 06:44:58 INFO     saving model related files
2023-12-21 06:44:58 INFO     saving model
2023-12-21 06:44:59 INFO     saving tokenizer
2023-12-21 06:44:59 INFO     saving optimizer
2023-12-21 06:45:00 INFO     remove old optimizer files
2023-12-21 06:45:06 INFO     	 * (global step 23650: loss: 0.4950941503047943, lr: 1e-05
2023-12-21 06:45:14 INFO     	 * (global step 23700: loss: 0.28114327788352966, lr: 1e-05
2023-12-21 06:45:22 INFO     	 * (global step 23750: loss: 0.35622017830610275, lr: 1e-05
2023-12-21 06:45:30 INFO     	 * (global step 23800: loss: 0.2714245989918709, lr: 1e-05
2023-12-21 06:45:38 INFO     	 * (global step 23850: loss: 0.37746402621269226, lr: 1e-05
2023-12-21 06:45:46 INFO     	 * (global step 23900: loss: 0.29874470829963684, lr: 1e-05
2023-12-21 06:45:54 INFO     	 * (global step 23950: loss: 0.25878236442804337, lr: 1e-05
2023-12-21 06:46:02 INFO     	 * (global step 24000: loss: 0.3944942206144333, lr: 1e-05
2023-12-21 06:46:10 INFO     	 * (global step 24050: loss: 0.5355748534202576, lr: 1e-05
2023-12-21 06:46:18 INFO     	 * (global step 24100: loss: 0.30453649163246155, lr: 1e-05
2023-12-21 06:46:26 INFO     	 * (global step 24150: loss: 0.4105122685432434, lr: 1e-05
2023-12-21 06:46:34 INFO     	 * (global step 24200: loss: 0.3784554749727249, lr: 1e-05
2023-12-21 06:46:42 INFO     	 * (global step 24250: loss: 0.48107457160949707, lr: 1e-05
2023-12-21 06:46:50 INFO     	 * (global step 24300: loss: 0.31378471851348877, lr: 1e-05
2023-12-21 06:46:58 INFO     	 * (global step 24350: loss: 0.43371444940567017, lr: 1e-05
2023-12-21 06:47:06 INFO     	 * (global step 24400: loss: 0.3188198059797287, lr: 1e-05
2023-12-21 06:47:14 INFO     	 * (global step 24450: loss: 0.42087891697883606, lr: 1e-05
2023-12-21 06:47:22 INFO     	 * (global step 24500: loss: 0.2818243205547333, lr: 1e-05
2023-12-21 06:47:30 INFO     	 * (global step 24550: loss: 0.3984134644269943, lr: 1e-05
2023-12-21 06:47:38 INFO     	 * (global step 24600: loss: 0.3522990718483925, lr: 1e-05
2023-12-21 06:47:46 INFO     	 * (global step 24650: loss: 0.23357778787612915, lr: 1e-05
2023-12-21 06:47:55 INFO     	 * (global step 24700: loss: 0.3651922643184662, lr: 1e-05
2023-12-21 06:48:03 INFO     	 * (global step 24750: loss: 0.3397618681192398, lr: 1e-05
2023-12-21 06:48:11 INFO     	 * (global step 24800: loss: 0.3344765901565552, lr: 1e-05
2023-12-21 06:48:19 INFO     	 * (global step 24850: loss: 0.39074672013521194, lr: 1e-05
2023-12-21 06:48:27 INFO     	 * (global step 24900: loss: 0.3834517151117325, lr: 1e-05
2023-12-21 06:48:35 INFO     	 * (global step 24950: loss: 0.3451722711324692, lr: 1e-05
2023-12-21 06:48:43 INFO     	 * (global step 25000: loss: 0.3722255527973175, lr: 1e-05
2023-12-21 06:48:51 INFO     	 * (global step 25050: loss: 0.2089669555425644, lr: 1e-05
2023-12-21 06:48:59 INFO     	 * (global step 25100: loss: 0.4075656458735466, lr: 1e-05
2023-12-21 06:49:07 INFO     	 * (global step 25150: loss: 0.888655811548233, lr: 1e-05
2023-12-21 06:49:15 INFO     	 * (global step 25200: loss: 0.35902246832847595, lr: 1e-05
2023-12-21 06:49:23 INFO     	 * (global step 25250: loss: 0.2856258600950241, lr: 1e-05
2023-12-21 06:49:31 INFO     	 * (global step 25300: loss: 0.3673063665628433, lr: 1e-05
2023-12-21 06:49:39 INFO     	 * (global step 25350: loss: 0.31194040179252625, lr: 1e-05
2023-12-21 06:49:47 INFO     	 * (global step 25400: loss: 0.30444464832544327, lr: 1e-05
2023-12-21 06:49:55 INFO     	 * (global step 25450: loss: 0.29301296174526215, lr: 1e-05
2023-12-21 06:50:03 INFO     	 * (global step 25500: loss: 0.35756349563598633, lr: 1e-05
2023-12-21 06:50:11 INFO     	 * (global step 25550: loss: 0.3902410864830017, lr: 1e-05
2023-12-21 06:50:19 INFO     	 * (global step 25600: loss: 0.32167893648147583, lr: 1e-05
2023-12-21 06:50:27 INFO     	 * (global step 25650: loss: 0.2528231739997864, lr: 1e-05
2023-12-21 06:50:35 INFO     	 * (global step 25700: loss: 0.37121520936489105, lr: 1e-05
2023-12-21 06:50:43 INFO     	 * (global step 25750: loss: 0.2855533957481384, lr: 1e-05
2023-12-21 06:50:51 INFO     	 * (global step 25800: loss: 0.326894611120224, lr: 1e-05
2023-12-21 06:50:59 INFO     	 * (global step 25850: loss: 0.29482097923755646, lr: 1e-05
2023-12-21 06:51:07 INFO     	 * (global step 25900: loss: 0.6504318118095398, lr: 1e-05
2023-12-21 06:51:15 INFO     	 * (global step 25950: loss: 0.23345454782247543, lr: 1e-05
2023-12-21 06:51:23 INFO     	 * (global step 26000: loss: 0.38347336649894714, lr: 1e-05
2023-12-21 06:51:31 INFO     	 * (global step 26050: loss: 0.45115554332733154, lr: 1e-05
2023-12-21 06:51:39 INFO     	 * (global step 26100: loss: 0.3704894483089447, lr: 1e-05
2023-12-21 06:51:47 INFO     	 * (global step 26150: loss: 0.31782176345586777, lr: 1e-05
2023-12-21 06:51:55 INFO     	 * (global step 26200: loss: 0.48388147354125977, lr: 1e-05
2023-12-21 06:52:03 INFO     	 * (global step 26250: loss: 0.28940249979496, lr: 1e-05
2023-12-21 06:52:11 INFO     	 * (global step 26300: loss: 0.44381463527679443, lr: 1e-05
2023-12-21 06:52:19 INFO     	 * (global step 26350: loss: 0.2730284631252289, lr: 1e-05
2023-12-21 06:52:27 INFO     	 * (global step 26400: loss: 0.44380202889442444, lr: 1e-05
2023-12-21 06:52:35 INFO     	 * (global step 26450: loss: 0.2823799103498459, lr: 1e-05
2023-12-21 06:52:43 INFO     	 * (global step 26500: loss: 0.28665389120578766, lr: 1e-05
2023-12-21 06:52:51 INFO     	 * (global step 26550: loss: 0.3174048513174057, lr: 1e-05
2023-12-21 06:52:59 INFO     	 * (global step 26600: loss: 0.260578915476799, lr: 1e-05
2023-12-21 06:53:07 INFO     	 * (global step 26650: loss: 0.42353154718875885, lr: 1e-05
2023-12-21 06:53:15 INFO     	 * (global step 26700: loss: 0.1747417151927948, lr: 1e-05
2023-12-21 06:53:23 INFO     	 * (global step 26750: loss: 0.26337794959545135, lr: 1e-05
2023-12-21 06:53:31 INFO     	 * (global step 26800: loss: 0.3689161092042923, lr: 1e-05
2023-12-21 06:53:39 INFO     	 * (global step 26850: loss: 0.23817143589258194, lr: 1e-05
2023-12-21 06:53:47 INFO     	 * (global step 26900: loss: 0.2573239877820015, lr: 1e-05
2023-12-21 06:53:55 INFO     	 * (global step 26950: loss: 0.3402237296104431, lr: 1e-05
2023-12-21 06:54:03 INFO     	 * (global step 27000: loss: 0.347765788435936, lr: 1e-05
2023-12-21 06:54:11 INFO     	 * (global step 27050: loss: 0.21760465949773788, lr: 1e-05
2023-12-21 06:54:19 INFO     	 * (global step 27100: loss: 0.3204817771911621, lr: 1e-05
2023-12-21 06:54:27 INFO     	 * (global step 27150: loss: 0.37372957170009613, lr: 1e-05
2023-12-21 06:54:35 INFO     	 * (global step 27200: loss: 0.5530149787664413, lr: 1e-05
2023-12-21 06:54:43 INFO     	 * (global step 27250: loss: 0.28158268332481384, lr: 1e-05
2023-12-21 06:54:51 INFO     	 * (global step 27300: loss: 0.3342798054218292, lr: 1e-05
2023-12-21 06:54:59 INFO     	 * (global step 27350: loss: 0.2408095821738243, lr: 1e-05
2023-12-21 06:55:07 INFO     	 * (global step 27400: loss: 0.31942011415958405, lr: 1e-05
2023-12-21 06:55:15 INFO     	 * (global step 27450: loss: 0.3862350136041641, lr: 1e-05
2023-12-21 06:55:23 INFO     	 * (global step 27500: loss: 0.3534251004457474, lr: 1e-05
2023-12-21 06:55:31 INFO     	 * (global step 27550: loss: 0.4961894080042839, lr: 1e-05
2023-12-21 06:55:39 INFO     	 * (global step 27600: loss: 0.34368540346622467, lr: 1e-05
2023-12-21 06:55:47 INFO     	 * (global step 27650: loss: 0.3319627493619919, lr: 1e-05
2023-12-21 06:55:55 INFO     	 * (global step 27700: loss: 0.3565652370452881, lr: 1e-05
2023-12-21 06:56:03 INFO     	 * (global step 27750: loss: 0.2699286416172981, lr: 1e-05
2023-12-21 06:56:11 INFO     	 * (global step 27800: loss: 0.3163323253393173, lr: 1e-05
2023-12-21 06:56:19 INFO     	 * (global step 27850: loss: 0.4301009774208069, lr: 1e-05
2023-12-21 06:56:27 INFO     	 * (global step 27900: loss: 0.3071906045079231, lr: 1e-05
2023-12-21 06:56:35 INFO     	 * (global step 27950: loss: 0.49066823720932007, lr: 1e-05
2023-12-21 06:56:43 INFO     	 * (global step 28000: loss: 0.35658399015665054, lr: 1e-05
2023-12-21 06:56:51 INFO     	 * (global step 28050: loss: 0.30379311740398407, lr: 1e-05
2023-12-21 06:56:59 INFO     	 * (global step 28100: loss: 0.44759757816791534, lr: 1e-05
2023-12-21 06:57:07 INFO     	 * (global step 28150: loss: 0.4239516407251358, lr: 1e-05
2023-12-21 06:57:15 INFO     	 * (global step 28200: loss: 0.6058756709098816, lr: 1e-05
2023-12-21 06:57:22 INFO     	 * (global step 28250: loss: 0.23519019782543182, lr: 1e-05
2023-12-21 06:57:30 INFO     	 * (global step 28300: loss: 0.15927356109023094, lr: 1e-05
2023-12-21 06:57:37 INFO     [epoch 5/15] average loss: 0.333, lr: 1e-05
2023-12-21 06:57:37 INFO     saving model related files
2023-12-21 06:57:37 INFO     saving model
2023-12-21 06:57:37 INFO     saving tokenizer
2023-12-21 06:57:37 INFO     saving optimizer
2023-12-21 06:57:38 INFO     remove old optimizer files
2023-12-21 06:57:40 INFO     	 * (global step 28350: loss: 0.39721645414829254, lr: 1e-05
2023-12-21 06:57:48 INFO     	 * (global step 28400: loss: 0.3543238341808319, lr: 1e-05
2023-12-21 06:57:56 INFO     	 * (global step 28450: loss: 0.18392811715602875, lr: 1e-05
2023-12-21 06:58:04 INFO     	 * (global step 28500: loss: 0.376155748963356, lr: 1e-05
2023-12-21 06:58:12 INFO     	 * (global step 28550: loss: 0.19377800822257996, lr: 1e-05
2023-12-21 06:58:20 INFO     	 * (global step 28600: loss: 0.28234586119651794, lr: 1e-05
2023-12-21 06:58:28 INFO     	 * (global step 28650: loss: 0.3896543085575104, lr: 1e-05
2023-12-21 06:58:36 INFO     	 * (global step 28700: loss: 0.27175121009349823, lr: 1e-05
2023-12-21 06:58:44 INFO     	 * (global step 28750: loss: 0.2319985255599022, lr: 1e-05
2023-12-21 06:58:52 INFO     	 * (global step 28800: loss: 0.22789670526981354, lr: 1e-05
2023-12-21 06:59:00 INFO     	 * (global step 28850: loss: 0.33729569613933563, lr: 1e-05
2023-12-21 06:59:08 INFO     	 * (global step 28900: loss: 0.4187520891427994, lr: 1e-05
2023-12-21 06:59:16 INFO     	 * (global step 28950: loss: 0.21579555422067642, lr: 1e-05
2023-12-21 06:59:24 INFO     	 * (global step 29000: loss: 0.29007042944431305, lr: 1e-05
2023-12-21 06:59:32 INFO     	 * (global step 29050: loss: 0.2419605851173401, lr: 1e-05
2023-12-21 06:59:40 INFO     	 * (global step 29100: loss: 0.32002490758895874, lr: 1e-05
2023-12-21 06:59:48 INFO     	 * (global step 29150: loss: 0.3632368892431259, lr: 1e-05
2023-12-21 06:59:56 INFO     	 * (global step 29200: loss: 0.42822085320949554, lr: 1e-05
2023-12-21 07:00:04 INFO     	 * (global step 29250: loss: 0.2736848145723343, lr: 1e-05
2023-12-21 07:00:11 INFO     	 * (global step 29300: loss: 0.26295415312051773, lr: 1e-05
2023-12-21 07:00:19 INFO     	 * (global step 29350: loss: 0.3622194230556488, lr: 1e-05
2023-12-21 07:00:27 INFO     	 * (global step 29400: loss: 0.22669801115989685, lr: 1e-05
2023-12-21 07:00:35 INFO     	 * (global step 29450: loss: 0.34091879427433014, lr: 1e-05
2023-12-21 07:00:43 INFO     	 * (global step 29500: loss: 0.2493206486105919, lr: 1e-05
2023-12-21 07:00:51 INFO     	 * (global step 29550: loss: 0.28906723856925964, lr: 1e-05
2023-12-21 07:00:59 INFO     	 * (global step 29600: loss: 0.3570990562438965, lr: 1e-05
2023-12-21 07:01:07 INFO     	 * (global step 29650: loss: 0.25404244661331177, lr: 1e-05
2023-12-21 07:01:15 INFO     	 * (global step 29700: loss: 0.2801375687122345, lr: 1e-05
2023-12-21 07:01:23 INFO     	 * (global step 29750: loss: 0.27217044681310654, lr: 1e-05
2023-12-21 07:01:31 INFO     	 * (global step 29800: loss: 0.6973174214363098, lr: 1e-05
2023-12-21 07:01:39 INFO     	 * (global step 29850: loss: 0.25160447508096695, lr: 1e-05
2023-12-21 07:01:47 INFO     	 * (global step 29900: loss: 0.41232846677303314, lr: 1e-05
2023-12-21 07:01:55 INFO     	 * (global step 29950: loss: 0.3037226274609566, lr: 1e-05
2023-12-21 07:02:03 INFO     	 * (global step 30000: loss: 0.18680450320243835, lr: 1e-05
2023-12-21 07:02:11 INFO     	 * (global step 30050: loss: 0.2401222661137581, lr: 1e-05
2023-12-21 07:02:19 INFO     	 * (global step 30100: loss: 0.33365175127983093, lr: 1e-05
2023-12-21 07:02:27 INFO     	 * (global step 30150: loss: 0.3401799649000168, lr: 1e-05
2023-12-21 07:02:35 INFO     	 * (global step 30200: loss: 0.21098286658525467, lr: 1e-05
2023-12-21 07:02:43 INFO     	 * (global step 30250: loss: 0.33927109837532043, lr: 1e-05
2023-12-21 07:02:51 INFO     	 * (global step 30300: loss: 0.2306821644306183, lr: 1e-05
2023-12-21 07:02:59 INFO     	 * (global step 30350: loss: 0.2943100929260254, lr: 1e-05
2023-12-21 07:03:07 INFO     	 * (global step 30400: loss: 0.28252094984054565, lr: 1e-05
2023-12-21 07:03:16 INFO     	 * (global step 30450: loss: 0.3539661020040512, lr: 1e-05
2023-12-21 07:03:24 INFO     	 * (global step 30500: loss: 0.23923376947641373, lr: 1e-05
2023-12-21 07:03:32 INFO     	 * (global step 30550: loss: 0.2658728212118149, lr: 1e-05
2023-12-21 07:03:40 INFO     	 * (global step 30600: loss: 0.5652761608362198, lr: 1e-05
2023-12-21 07:03:48 INFO     	 * (global step 30650: loss: 0.2506033033132553, lr: 1e-05
2023-12-21 07:03:56 INFO     	 * (global step 30700: loss: 0.31250135600566864, lr: 1e-05
2023-12-21 07:04:03 INFO     	 * (global step 30750: loss: 0.2910194918513298, lr: 1e-05
2023-12-21 07:04:11 INFO     	 * (global step 30800: loss: 0.20741567015647888, lr: 1e-05
2023-12-21 07:04:19 INFO     	 * (global step 30850: loss: 0.23407763242721558, lr: 1e-05
2023-12-21 07:04:27 INFO     	 * (global step 30900: loss: 0.2885076403617859, lr: 1e-05
2023-12-21 07:04:35 INFO     	 * (global step 30950: loss: 0.4093802124261856, lr: 1e-05
2023-12-21 07:04:43 INFO     	 * (global step 31000: loss: 0.3235369920730591, lr: 1e-05
2023-12-21 07:04:51 INFO     	 * (global step 31050: loss: 0.30018018186092377, lr: 1e-05
2023-12-21 07:04:59 INFO     	 * (global step 31100: loss: 0.29338251054286957, lr: 1e-05
2023-12-21 07:05:07 INFO     	 * (global step 31150: loss: 0.27093957364559174, lr: 1e-05
2023-12-21 07:05:15 INFO     	 * (global step 31200: loss: 0.35675978660583496, lr: 1e-05
2023-12-21 07:05:23 INFO     	 * (global step 31250: loss: 0.2982502654194832, lr: 1e-05
2023-12-21 07:05:31 INFO     	 * (global step 31300: loss: 0.2690213546156883, lr: 1e-05
2023-12-21 07:05:39 INFO     	 * (global step 31350: loss: 0.40094031393527985, lr: 1e-05
2023-12-21 07:05:47 INFO     	 * (global step 31400: loss: 0.3229938894510269, lr: 1e-05
2023-12-21 07:05:55 INFO     	 * (global step 31450: loss: 0.32497353851795197, lr: 1e-05
2023-12-21 07:06:03 INFO     	 * (global step 31500: loss: 0.5497768223285675, lr: 1e-05
2023-12-21 07:06:11 INFO     	 * (global step 31550: loss: 0.2683757394552231, lr: 1e-05
2023-12-21 07:06:19 INFO     	 * (global step 31600: loss: 0.29766033589839935, lr: 1e-05
2023-12-21 07:06:27 INFO     	 * (global step 31650: loss: 0.22064197808504105, lr: 1e-05
2023-12-21 07:06:35 INFO     	 * (global step 31700: loss: 0.3433607667684555, lr: 1e-05
2023-12-21 07:06:43 INFO     	 * (global step 31750: loss: 0.4681508541107178, lr: 1e-05
2023-12-21 07:06:51 INFO     	 * (global step 31800: loss: 0.23081155866384506, lr: 1e-05
2023-12-21 07:06:59 INFO     	 * (global step 31850: loss: 0.3143806532025337, lr: 1e-05
2023-12-21 07:07:07 INFO     	 * (global step 31900: loss: 0.30081114172935486, lr: 1e-05
2023-12-21 07:07:15 INFO     	 * (global step 31950: loss: 0.26305273175239563, lr: 1e-05
2023-12-21 07:07:23 INFO     	 * (global step 32000: loss: 0.2582591623067856, lr: 1e-05
2023-12-21 07:07:31 INFO     	 * (global step 32050: loss: 0.36449022591114044, lr: 1e-05
2023-12-21 07:07:39 INFO     	 * (global step 32100: loss: 0.3437107652425766, lr: 1e-05
2023-12-21 07:07:47 INFO     	 * (global step 32150: loss: 0.4866834729909897, lr: 1e-05
2023-12-21 07:07:55 INFO     	 * (global step 32200: loss: 0.32429908215999603, lr: 1e-05
2023-12-21 07:08:03 INFO     	 * (global step 32250: loss: 0.22842595726251602, lr: 1e-05
2023-12-21 07:08:11 INFO     	 * (global step 32300: loss: 0.30767805874347687, lr: 1e-05
2023-12-21 07:08:19 INFO     	 * (global step 32350: loss: 0.3105514347553253, lr: 1e-05
2023-12-21 07:08:27 INFO     	 * (global step 32400: loss: 0.2298109233379364, lr: 1e-05
2023-12-21 07:08:35 INFO     	 * (global step 32450: loss: 0.2690374627709389, lr: 1e-05
2023-12-21 07:08:43 INFO     	 * (global step 32500: loss: 0.3491370975971222, lr: 1e-05
2023-12-21 07:08:51 INFO     	 * (global step 32550: loss: 0.31579283624887466, lr: 1e-05
2023-12-21 07:08:59 INFO     	 * (global step 32600: loss: 0.18136832863092422, lr: 1e-05
2023-12-21 07:09:07 INFO     	 * (global step 32650: loss: 0.502609446644783, lr: 1e-05
2023-12-21 07:09:15 INFO     	 * (global step 32700: loss: 0.26829536259174347, lr: 1e-05
2023-12-21 07:09:23 INFO     	 * (global step 32750: loss: 0.3663181737065315, lr: 1e-05
2023-12-21 07:09:31 INFO     	 * (global step 32800: loss: 0.2199251651763916, lr: 1e-05
2023-12-21 07:09:39 INFO     	 * (global step 32850: loss: 0.24296404421329498, lr: 1e-05
2023-12-21 07:09:47 INFO     	 * (global step 32900: loss: 0.4690602868795395, lr: 1e-05
2023-12-21 07:09:55 INFO     	 * (global step 32950: loss: 0.4416368007659912, lr: 1e-05
2023-12-21 07:10:03 INFO     	 * (global step 33000: loss: 0.26951203495264053, lr: 1e-05
2023-12-21 07:10:11 INFO     	 * (global step 33050: loss: 0.2781885415315628, lr: 1e-05
2023-12-21 07:10:13 INFO     [epoch 6/15] average loss: 0.327, lr: 1e-05
2023-12-21 07:10:13 INFO     saving model related files
2023-12-21 07:10:13 INFO     saving model
2023-12-21 07:10:13 INFO     saving tokenizer
2023-12-21 07:10:13 INFO     saving optimizer
2023-12-21 07:10:14 INFO     remove old optimizer files
2023-12-21 07:10:20 INFO     	 * (global step 33100: loss: 0.3186534196138382, lr: 1e-05
2023-12-21 07:10:28 INFO     	 * (global step 33150: loss: 0.352514311671257, lr: 1e-05
2023-12-21 07:10:36 INFO     	 * (global step 33200: loss: 0.3941141664981842, lr: 1e-05
2023-12-21 07:10:44 INFO     	 * (global step 33250: loss: 0.32089246064424515, lr: 1e-05
2023-12-21 07:10:52 INFO     	 * (global step 33300: loss: 0.27556179463863373, lr: 1e-05
2023-12-21 07:11:00 INFO     	 * (global step 33350: loss: 0.31974298506975174, lr: 1e-05
2023-12-21 07:11:08 INFO     	 * (global step 33400: loss: 0.2522039860486984, lr: 1e-05
2023-12-21 07:11:16 INFO     	 * (global step 33450: loss: 0.3135717958211899, lr: 1e-05
2023-12-21 07:11:24 INFO     	 * (global step 33500: loss: 0.2620355635881424, lr: 1e-05
2023-12-21 07:11:32 INFO     	 * (global step 33550: loss: 0.35292041301727295, lr: 1e-05
2023-12-21 07:11:40 INFO     	 * (global step 33600: loss: 0.2923809587955475, lr: 1e-05
2023-12-21 07:11:48 INFO     	 * (global step 33650: loss: 0.40385934710502625, lr: 1e-05
2023-12-21 07:11:56 INFO     	 * (global step 33700: loss: 0.24387817084789276, lr: 1e-05
2023-12-21 07:12:04 INFO     	 * (global step 33750: loss: 0.4643455147743225, lr: 1e-05
2023-12-21 07:12:12 INFO     	 * (global step 33800: loss: 0.39833807945251465, lr: 1e-05
2023-12-21 07:12:20 INFO     	 * (global step 33850: loss: 0.33615805208683014, lr: 1e-05
2023-12-21 07:12:28 INFO     	 * (global step 33900: loss: 0.329690158367157, lr: 1e-05
2023-12-21 07:12:36 INFO     	 * (global step 33950: loss: 0.29852503538131714, lr: 1e-05
2023-12-21 07:12:44 INFO     	 * (global step 34000: loss: 0.2438361719250679, lr: 1e-05
2023-12-21 07:12:52 INFO     	 * (global step 34050: loss: 0.36145608127117157, lr: 1e-05
2023-12-21 07:13:00 INFO     	 * (global step 34100: loss: 0.2951965481042862, lr: 1e-05
2023-12-21 07:13:08 INFO     	 * (global step 34150: loss: 0.5192602574825287, lr: 1e-05
2023-12-21 07:13:16 INFO     	 * (global step 34200: loss: 0.4118964374065399, lr: 1e-05
2023-12-21 07:13:24 INFO     	 * (global step 34250: loss: 0.33067844808101654, lr: 1e-05
2023-12-21 07:13:32 INFO     	 * (global step 34300: loss: 0.2973864674568176, lr: 1e-05
2023-12-21 07:13:40 INFO     	 * (global step 34350: loss: 0.19232956320047379, lr: 1e-05
2023-12-21 07:13:48 INFO     	 * (global step 34400: loss: 0.2764914557337761, lr: 1e-05
2023-12-21 07:13:56 INFO     	 * (global step 34450: loss: 0.375190794467926, lr: 1e-05
2023-12-21 07:14:03 INFO     	 * (global step 34500: loss: 0.29373185336589813, lr: 1e-05
2023-12-21 07:14:11 INFO     	 * (global step 34550: loss: 0.31420400738716125, lr: 1e-05
2023-12-21 07:14:19 INFO     	 * (global step 34600: loss: 0.2697261646389961, lr: 1e-05
2023-12-21 07:14:27 INFO     	 * (global step 34650: loss: 0.2566451132297516, lr: 1e-05
2023-12-21 07:14:35 INFO     	 * (global step 34700: loss: 0.3913523405790329, lr: 1e-05
2023-12-21 07:14:43 INFO     	 * (global step 34750: loss: 0.28189459443092346, lr: 1e-05
2023-12-21 07:14:51 INFO     	 * (global step 34800: loss: 0.23586077988147736, lr: 1e-05
2023-12-21 07:14:59 INFO     	 * (global step 34850: loss: 0.2905382663011551, lr: 1e-05
2023-12-21 07:15:07 INFO     	 * (global step 34900: loss: 0.2214047908782959, lr: 1e-05
2023-12-21 07:15:15 INFO     	 * (global step 34950: loss: 0.3132929503917694, lr: 1e-05
2023-12-21 07:15:23 INFO     	 * (global step 35000: loss: 0.32775282859802246, lr: 1e-05
2023-12-21 07:15:31 INFO     	 * (global step 35050: loss: 0.44792304933071136, lr: 1e-05
2023-12-21 07:15:39 INFO     	 * (global step 35100: loss: 0.4009234309196472, lr: 1e-05
2023-12-21 07:15:47 INFO     	 * (global step 35150: loss: 0.3820738196372986, lr: 1e-05
2023-12-21 07:15:55 INFO     	 * (global step 35200: loss: 0.3233814612030983, lr: 1e-05
2023-12-21 07:16:03 INFO     	 * (global step 35250: loss: 0.32532133162021637, lr: 1e-05
2023-12-21 07:16:11 INFO     	 * (global step 35300: loss: 0.382507361471653, lr: 1e-05
2023-12-21 07:16:19 INFO     	 * (global step 35350: loss: 0.25954075902700424, lr: 1e-05
2023-12-21 07:16:27 INFO     	 * (global step 35400: loss: 0.24513333290815353, lr: 1e-05
2023-12-21 07:16:35 INFO     	 * (global step 35450: loss: 0.3992239683866501, lr: 1e-05
2023-12-21 07:16:43 INFO     	 * (global step 35500: loss: 0.33542538434267044, lr: 1e-05
2023-12-21 07:16:51 INFO     	 * (global step 35550: loss: 0.20240860432386398, lr: 1e-05
2023-12-21 07:16:59 INFO     	 * (global step 35600: loss: 0.31243132054805756, lr: 1e-05
2023-12-21 07:17:07 INFO     	 * (global step 35650: loss: 0.35465317964553833, lr: 1e-05
2023-12-21 07:17:15 INFO     	 * (global step 35700: loss: 0.4107293635606766, lr: 1e-05
2023-12-21 07:17:23 INFO     	 * (global step 35750: loss: 0.2610737234354019, lr: 1e-05
2023-12-21 07:17:31 INFO     	 * (global step 35800: loss: 0.25254013389348984, lr: 1e-05
2023-12-21 07:17:39 INFO     	 * (global step 35850: loss: 0.2740221917629242, lr: 1e-05
2023-12-21 07:17:47 INFO     	 * (global step 35900: loss: 0.32434406876564026, lr: 1e-05
2023-12-21 07:17:55 INFO     	 * (global step 35950: loss: 0.47205010056495667, lr: 1e-05
2023-12-21 07:18:03 INFO     	 * (global step 36000: loss: 0.2611227184534073, lr: 1e-05
2023-12-21 07:18:11 INFO     	 * (global step 36050: loss: 0.11819364875555038, lr: 1e-05
2023-12-21 07:18:19 INFO     	 * (global step 36100: loss: 0.39339354634284973, lr: 1e-05
2023-12-21 07:18:27 INFO     	 * (global step 36150: loss: 0.5313991606235504, lr: 1e-05
2023-12-21 07:18:35 INFO     	 * (global step 36200: loss: 0.38864946365356445, lr: 1e-05
2023-12-21 07:18:42 INFO     	 * (global step 36250: loss: 0.25761476159095764, lr: 1e-05
2023-12-21 07:18:50 INFO     	 * (global step 36300: loss: 0.311894953250885, lr: 1e-05
2023-12-21 07:18:58 INFO     	 * (global step 36350: loss: 0.417075052857399, lr: 1e-05
2023-12-21 07:19:06 INFO     	 * (global step 36400: loss: 0.2804511487483978, lr: 1e-05
2023-12-21 07:19:14 INFO     	 * (global step 36450: loss: 0.24064332991838455, lr: 1e-05
2023-12-21 07:19:22 INFO     	 * (global step 36500: loss: 0.4488314390182495, lr: 1e-05
2023-12-21 07:19:30 INFO     	 * (global step 36550: loss: 0.2407732829451561, lr: 1e-05
2023-12-21 07:19:38 INFO     	 * (global step 36600: loss: 0.554976761341095, lr: 1e-05
2023-12-21 07:19:46 INFO     	 * (global step 36650: loss: 0.3114382326602936, lr: 1e-05
2023-12-21 07:19:54 INFO     	 * (global step 36700: loss: 0.2777753993868828, lr: 1e-05
2023-12-21 07:20:02 INFO     	 * (global step 36750: loss: 0.41950973868370056, lr: 1e-05
2023-12-21 07:20:10 INFO     	 * (global step 36800: loss: 0.2711172252893448, lr: 1e-05
2023-12-21 07:20:18 INFO     	 * (global step 36850: loss: 0.23633237183094025, lr: 1e-05
2023-12-21 07:20:26 INFO     	 * (global step 36900: loss: 0.34898194670677185, lr: 1e-05
2023-12-21 07:20:34 INFO     	 * (global step 36950: loss: 0.3410266935825348, lr: 1e-05
2023-12-21 07:20:42 INFO     	 * (global step 37000: loss: 0.34468311071395874, lr: 1e-05
2023-12-21 07:20:50 INFO     	 * (global step 37050: loss: 0.24186959862709045, lr: 1e-05
2023-12-21 07:20:58 INFO     	 * (global step 37100: loss: 0.3489697128534317, lr: 1e-05
2023-12-21 07:21:06 INFO     	 * (global step 37150: loss: 0.24399293959140778, lr: 1e-05
2023-12-21 07:21:14 INFO     	 * (global step 37200: loss: 0.5662044286727905, lr: 1e-05
2023-12-21 07:21:22 INFO     	 * (global step 37250: loss: 0.2376432716846466, lr: 1e-05
2023-12-21 07:21:30 INFO     	 * (global step 37300: loss: 0.31168539077043533, lr: 1e-05
2023-12-21 07:21:38 INFO     	 * (global step 37350: loss: 0.15344403311610222, lr: 1e-05
2023-12-21 07:21:46 INFO     	 * (global step 37400: loss: 0.29190436005592346, lr: 1e-05
2023-12-21 07:21:54 INFO     	 * (global step 37450: loss: 0.3804033398628235, lr: 1e-05
2023-12-21 07:22:02 INFO     	 * (global step 37500: loss: 0.31589844822883606, lr: 1e-05
2023-12-21 07:22:10 INFO     	 * (global step 37550: loss: 0.251175120472908, lr: 1e-05
2023-12-21 07:22:18 INFO     	 * (global step 37600: loss: 0.26299455761909485, lr: 1e-05
2023-12-21 07:22:26 INFO     	 * (global step 37650: loss: 0.185634046792984, lr: 1e-05
2023-12-21 07:22:34 INFO     	 * (global step 37700: loss: 0.28939059376716614, lr: 1e-05
2023-12-21 07:22:42 INFO     	 * (global step 37750: loss: 0.2009814977645874, lr: 1e-05
2023-12-21 07:22:47 INFO     [epoch 7/15] average loss: 0.322, lr: 1e-05
2023-12-21 07:22:47 INFO     saving model related files
2023-12-21 07:22:47 INFO     saving model
2023-12-21 07:22:48 INFO     saving tokenizer
2023-12-21 07:22:48 INFO     saving optimizer
2023-12-21 07:22:49 INFO     remove old optimizer files
2023-12-21 07:22:51 INFO     	 * (global step 37800: loss: 0.4176205098628998, lr: 1e-05
2023-12-21 07:22:59 INFO     	 * (global step 37850: loss: 0.2859983295202255, lr: 1e-05
2023-12-21 07:23:07 INFO     	 * (global step 37900: loss: 0.3163832426071167, lr: 1e-05
2023-12-21 07:23:15 INFO     	 * (global step 37950: loss: 0.31043099611997604, lr: 1e-05
2023-12-21 07:23:23 INFO     	 * (global step 38000: loss: 0.3746241182088852, lr: 1e-05
2023-12-21 07:23:31 INFO     	 * (global step 38050: loss: 0.30295248329639435, lr: 1e-05
2023-12-21 07:23:39 INFO     	 * (global step 38100: loss: 0.31525421887636185, lr: 1e-05
2023-12-21 07:23:47 INFO     	 * (global step 38150: loss: 0.32815317809581757, lr: 1e-05
2023-12-21 07:23:55 INFO     	 * (global step 38200: loss: 0.40235061943531036, lr: 1e-05
2023-12-21 07:24:03 INFO     	 * (global step 38250: loss: 0.3833465576171875, lr: 1e-05
2023-12-21 07:24:11 INFO     	 * (global step 38300: loss: 0.2279943749308586, lr: 1e-05
2023-12-21 07:24:19 INFO     	 * (global step 38350: loss: 0.3399975001811981, lr: 1e-05
2023-12-21 07:24:27 INFO     	 * (global step 38400: loss: 0.3678940236568451, lr: 1e-05
2023-12-21 07:24:35 INFO     	 * (global step 38450: loss: 0.37932321429252625, lr: 1e-05
2023-12-21 07:24:43 INFO     	 * (global step 38500: loss: 0.2664560526609421, lr: 1e-05
2023-12-21 07:24:51 INFO     	 * (global step 38550: loss: 0.3482109159231186, lr: 1e-05
2023-12-21 07:24:59 INFO     	 * (global step 38600: loss: 0.3724062442779541, lr: 1e-05
2023-12-21 07:25:07 INFO     	 * (global step 38650: loss: 0.16298790276050568, lr: 1e-05
2023-12-21 07:25:15 INFO     	 * (global step 38700: loss: 0.27386049181222916, lr: 1e-05
2023-12-21 07:25:23 INFO     	 * (global step 38750: loss: 0.3111532926559448, lr: 1e-05
2023-12-21 07:25:31 INFO     	 * (global step 38800: loss: 0.2457091584801674, lr: 1e-05
2023-12-21 07:25:39 INFO     	 * (global step 38850: loss: 0.46396908164024353, lr: 1e-05
2023-12-21 07:25:47 INFO     	 * (global step 38900: loss: 0.25748198479413986, lr: 1e-05
2023-12-21 07:25:55 INFO     	 * (global step 38950: loss: 0.26264993846416473, lr: 1e-05
2023-12-21 07:26:03 INFO     	 * (global step 39000: loss: 0.2891458794474602, lr: 1e-05
2023-12-21 07:26:11 INFO     	 * (global step 39050: loss: 0.3241334557533264, lr: 1e-05
2023-12-21 07:26:19 INFO     	 * (global step 39100: loss: 0.2662946507334709, lr: 1e-05
2023-12-21 07:26:27 INFO     	 * (global step 39150: loss: 0.21451126039028168, lr: 1e-05
2023-12-21 07:26:35 INFO     	 * (global step 39200: loss: 0.3255555331707001, lr: 1e-05
2023-12-21 07:26:43 INFO     	 * (global step 39250: loss: 0.28398558497428894, lr: 1e-05
2023-12-21 07:26:51 INFO     	 * (global step 39300: loss: 0.22205401957035065, lr: 1e-05
2023-12-21 07:26:59 INFO     	 * (global step 39350: loss: 0.24955643713474274, lr: 1e-05
2023-12-21 07:27:07 INFO     	 * (global step 39400: loss: 0.22312834858894348, lr: 1e-05
2023-12-21 07:27:15 INFO     	 * (global step 39450: loss: 0.2584720253944397, lr: 1e-05
2023-12-21 07:27:23 INFO     	 * (global step 39500: loss: 0.25378625094890594, lr: 1e-05
2023-12-21 07:27:31 INFO     	 * (global step 39550: loss: 0.23353469371795654, lr: 1e-05
2023-12-21 07:27:39 INFO     	 * (global step 39600: loss: 0.33849989622831345, lr: 1e-05
2023-12-21 07:27:47 INFO     	 * (global step 39650: loss: 0.27919338643550873, lr: 1e-05
2023-12-21 07:27:55 INFO     	 * (global step 39700: loss: 0.24400068074464798, lr: 1e-05
2023-12-21 07:28:02 INFO     	 * (global step 39750: loss: 0.3783397525548935, lr: 1e-05
2023-12-21 07:28:10 INFO     	 * (global step 39800: loss: 0.42142418026924133, lr: 1e-05
2023-12-21 07:28:18 INFO     	 * (global step 39850: loss: 0.3983116149902344, lr: 1e-05
2023-12-21 07:28:26 INFO     	 * (global step 39900: loss: 0.40891388058662415, lr: 1e-05
2023-12-21 07:28:34 INFO     	 * (global step 39950: loss: 0.42475274205207825, lr: 1e-05
2023-12-21 07:28:42 INFO     	 * (global step 40000: loss: 0.2510555759072304, lr: 1e-05
2023-12-21 07:28:50 INFO     	 * (global step 40050: loss: 0.27956147491931915, lr: 1e-05
2023-12-21 07:28:58 INFO     	 * (global step 40100: loss: 0.3211284875869751, lr: 1e-05
2023-12-21 07:29:06 INFO     	 * (global step 40150: loss: 0.25944481790065765, lr: 1e-05
2023-12-21 07:29:14 INFO     	 * (global step 40200: loss: 0.3240910470485687, lr: 1e-05
2023-12-21 07:29:22 INFO     	 * (global step 40250: loss: 0.3871510177850723, lr: 1e-05
2023-12-21 07:29:30 INFO     	 * (global step 40300: loss: 0.2970662862062454, lr: 1e-05
2023-12-21 07:29:38 INFO     	 * (global step 40350: loss: 0.3034994900226593, lr: 1e-05
2023-12-21 07:29:46 INFO     	 * (global step 40400: loss: 0.25560885667800903, lr: 1e-05
2023-12-21 07:29:54 INFO     	 * (global step 40450: loss: 0.23575995117425919, lr: 1e-05
2023-12-21 07:30:02 INFO     	 * (global step 40500: loss: 0.22116240113973618, lr: 1e-05
2023-12-21 07:30:10 INFO     	 * (global step 40550: loss: 0.27373241633176804, lr: 1e-05
2023-12-21 07:30:18 INFO     	 * (global step 40600: loss: 0.22338669002056122, lr: 1e-05
2023-12-21 07:30:26 INFO     	 * (global step 40650: loss: 0.34473367035388947, lr: 1e-05
2023-12-21 07:30:34 INFO     	 * (global step 40700: loss: 0.42756666243076324, lr: 1e-05
2023-12-21 07:30:42 INFO     	 * (global step 40750: loss: 0.2993316054344177, lr: 1e-05
2023-12-21 07:30:50 INFO     	 * (global step 40800: loss: 0.39050059020519257, lr: 1e-05
2023-12-21 07:30:58 INFO     	 * (global step 40850: loss: 0.32899952679872513, lr: 1e-05
2023-12-21 07:31:06 INFO     	 * (global step 40900: loss: 0.32406700402498245, lr: 1e-05
2023-12-21 07:31:14 INFO     	 * (global step 40950: loss: 0.24638381600379944, lr: 1e-05
2023-12-21 07:31:22 INFO     	 * (global step 41000: loss: 0.41353563219308853, lr: 1e-05
2023-12-21 07:31:30 INFO     	 * (global step 41050: loss: 0.27835826575756073, lr: 1e-05
2023-12-21 07:31:38 INFO     	 * (global step 41100: loss: 0.26610860228538513, lr: 1e-05
2023-12-21 07:31:46 INFO     	 * (global step 41150: loss: 0.28916536271572113, lr: 1e-05
2023-12-21 07:31:54 INFO     	 * (global step 41200: loss: 0.2884213775396347, lr: 1e-05
2023-12-21 07:32:02 INFO     	 * (global step 41250: loss: 0.21287419646978378, lr: 1e-05
2023-12-21 07:32:10 INFO     	 * (global step 41300: loss: 0.40550366044044495, lr: 1e-05
2023-12-21 07:32:18 INFO     	 * (global step 41350: loss: 0.2450670748949051, lr: 1e-05
2023-12-21 07:32:26 INFO     	 * (global step 41400: loss: 0.19989271461963654, lr: 1e-05
2023-12-21 07:32:34 INFO     	 * (global step 41450: loss: 0.3942039906978607, lr: 1e-05
2023-12-21 07:32:42 INFO     	 * (global step 41500: loss: 0.29842910915613174, lr: 1e-05
2023-12-21 07:32:50 INFO     	 * (global step 41550: loss: 0.33524008840322495, lr: 1e-05
2023-12-21 07:32:58 INFO     	 * (global step 41600: loss: 0.4111940264701843, lr: 1e-05
2023-12-21 07:33:06 INFO     	 * (global step 41650: loss: 0.29339204728603363, lr: 1e-05
2023-12-21 07:33:14 INFO     	 * (global step 41700: loss: 0.38633814454078674, lr: 1e-05
2023-12-21 07:33:22 INFO     	 * (global step 41750: loss: 0.19878830760717392, lr: 1e-05
2023-12-21 07:33:30 INFO     	 * (global step 41800: loss: 0.32265505194664, lr: 1e-05
2023-12-21 07:33:38 INFO     	 * (global step 41850: loss: 0.3567347228527069, lr: 1e-05
2023-12-21 07:33:46 INFO     	 * (global step 41900: loss: 0.36738668382167816, lr: 1e-05
2023-12-21 07:33:54 INFO     	 * (global step 41950: loss: 0.3394656777381897, lr: 1e-05
2023-12-21 07:34:02 INFO     	 * (global step 42000: loss: 0.22011437267065048, lr: 1e-05
2023-12-21 07:34:10 INFO     	 * (global step 42050: loss: 0.22533517330884933, lr: 1e-05
2023-12-21 07:34:18 INFO     	 * (global step 42100: loss: 0.3399377912282944, lr: 1e-05
2023-12-21 07:34:26 INFO     	 * (global step 42150: loss: 0.23273517936468124, lr: 1e-05
2023-12-21 07:34:34 INFO     	 * (global step 42200: loss: 0.3017733171582222, lr: 1e-05
2023-12-21 07:34:42 INFO     	 * (global step 42250: loss: 0.32546277344226837, lr: 1e-05
2023-12-21 07:34:50 INFO     	 * (global step 42300: loss: 0.2925695329904556, lr: 1e-05
2023-12-21 07:34:58 INFO     	 * (global step 42350: loss: 0.20765119791030884, lr: 1e-05
2023-12-21 07:35:06 INFO     	 * (global step 42400: loss: 0.3504660874605179, lr: 1e-05
2023-12-21 07:35:14 INFO     	 * (global step 42450: loss: 0.47021983563899994, lr: 1e-05
2023-12-21 07:35:22 INFO     	 * (global step 42500: loss: 0.2639806866645813, lr: 1e-05
2023-12-21 07:35:23 INFO     [epoch 8/15] average loss: 0.318, lr: 1e-05
2023-12-21 07:35:23 INFO     saving model related files
2023-12-21 07:35:23 INFO     saving model
2023-12-21 07:35:23 INFO     saving tokenizer
2023-12-21 07:35:24 INFO     saving optimizer
2023-12-21 07:35:24 INFO     remove old optimizer files
2023-12-21 07:35:31 INFO     	 * (global step 42550: loss: 0.3216257691383362, lr: 1e-05
2023-12-21 07:35:39 INFO     	 * (global step 42600: loss: 0.3363599330186844, lr: 1e-05
2023-12-21 07:35:47 INFO     	 * (global step 42650: loss: 0.36189571022987366, lr: 1e-05
2023-12-21 07:35:55 INFO     	 * (global step 42700: loss: 0.22236352413892746, lr: 1e-05
2023-12-21 07:36:03 INFO     	 * (global step 42750: loss: 0.3293147385120392, lr: 1e-05
2023-12-21 07:36:11 INFO     	 * (global step 42800: loss: 0.5890789031982422, lr: 1e-05
2023-12-21 07:36:19 INFO     	 * (global step 42850: loss: 0.25346149504184723, lr: 1e-05
2023-12-21 07:36:27 INFO     	 * (global step 42900: loss: 0.3348177671432495, lr: 1e-05
2023-12-21 07:36:35 INFO     	 * (global step 42950: loss: 0.3357994109392166, lr: 1e-05
2023-12-21 07:36:43 INFO     	 * (global step 43000: loss: 0.41029560565948486, lr: 1e-05
2023-12-21 07:36:51 INFO     	 * (global step 43050: loss: 0.2904299795627594, lr: 1e-05
2023-12-21 07:36:59 INFO     	 * (global step 43100: loss: 0.3602624088525772, lr: 1e-05
2023-12-21 07:37:07 INFO     	 * (global step 43150: loss: 0.3026631325483322, lr: 1e-05
2023-12-21 07:37:15 INFO     	 * (global step 43200: loss: 0.28749780356884, lr: 1e-05
2023-12-21 07:37:23 INFO     	 * (global step 43250: loss: 0.24865959584712982, lr: 1e-05
2023-12-21 07:37:31 INFO     	 * (global step 43300: loss: 0.5684331655502319, lr: 1e-05
2023-12-21 07:37:39 INFO     	 * (global step 43350: loss: 0.34853191673755646, lr: 1e-05
2023-12-21 07:37:47 INFO     	 * (global step 43400: loss: 0.26412368565797806, lr: 1e-05
2023-12-21 07:37:55 INFO     	 * (global step 43450: loss: 0.2847713753581047, lr: 1e-05
2023-12-21 07:38:03 INFO     	 * (global step 43500: loss: 0.45236213505268097, lr: 1e-05
2023-12-21 07:38:11 INFO     	 * (global step 43550: loss: 0.33791017532348633, lr: 1e-05
2023-12-21 07:38:19 INFO     	 * (global step 43600: loss: 0.3975478559732437, lr: 1e-05
2023-12-21 07:38:27 INFO     	 * (global step 43650: loss: 0.5274688005447388, lr: 1e-05
2023-12-21 07:38:35 INFO     	 * (global step 43700: loss: 0.28736652433872223, lr: 1e-05
2023-12-21 07:38:43 INFO     	 * (global step 43750: loss: 0.3516732454299927, lr: 1e-05
2023-12-21 07:38:51 INFO     	 * (global step 43800: loss: 0.268712654709816, lr: 1e-05
2023-12-21 07:38:59 INFO     	 * (global step 43850: loss: 0.30274464190006256, lr: 1e-05
2023-12-21 07:39:07 INFO     	 * (global step 43900: loss: 0.3934112638235092, lr: 1e-05
2023-12-21 07:39:15 INFO     	 * (global step 43950: loss: 0.23580166697502136, lr: 1e-05
2023-12-21 07:39:23 INFO     	 * (global step 44000: loss: 0.36191748082637787, lr: 1e-05
2023-12-21 07:39:31 INFO     	 * (global step 44050: loss: 0.30052438378334045, lr: 1e-05
2023-12-21 07:39:39 INFO     	 * (global step 44100: loss: 0.24782432615756989, lr: 1e-05
2023-12-21 07:39:47 INFO     	 * (global step 44150: loss: 0.41672752797603607, lr: 1e-05
2023-12-21 07:39:55 INFO     	 * (global step 44200: loss: 0.2124248817563057, lr: 1e-05
2023-12-21 07:40:03 INFO     	 * (global step 44250: loss: 0.3404236435890198, lr: 1e-05
2023-12-21 07:40:11 INFO     	 * (global step 44300: loss: 0.7336795777082443, lr: 1e-05
2023-12-21 07:40:19 INFO     	 * (global step 44350: loss: 0.2645478844642639, lr: 1e-05
2023-12-21 07:40:27 INFO     	 * (global step 44400: loss: 0.21399306505918503, lr: 1e-05
2023-12-21 07:40:35 INFO     	 * (global step 44450: loss: 0.38366083800792694, lr: 1e-05
2023-12-21 07:40:43 INFO     	 * (global step 44500: loss: 0.41692816466093063, lr: 1e-05
2023-12-21 07:40:51 INFO     	 * (global step 44550: loss: 0.17632747069001198, lr: 1e-05
2023-12-21 07:40:59 INFO     	 * (global step 44600: loss: 0.215827576816082, lr: 1e-05
2023-12-21 07:41:07 INFO     	 * (global step 44650: loss: 0.3861384838819504, lr: 1e-05
2023-12-21 07:41:15 INFO     	 * (global step 44700: loss: 0.27351805567741394, lr: 1e-05
2023-12-21 07:41:23 INFO     	 * (global step 44750: loss: 0.2282707840204239, lr: 1e-05
2023-12-21 07:41:31 INFO     	 * (global step 44800: loss: 0.28013429045677185, lr: 1e-05
2023-12-21 07:41:39 INFO     	 * (global step 44850: loss: 0.29767581075429916, lr: 1e-05
2023-12-21 07:41:47 INFO     	 * (global step 44900: loss: 0.24403317272663116, lr: 1e-05
2023-12-21 07:41:55 INFO     	 * (global step 44950: loss: 0.3180968686938286, lr: 1e-05
2023-12-21 07:42:03 INFO     	 * (global step 45000: loss: 0.2597707062959671, lr: 1e-05
2023-12-21 07:42:11 INFO     	 * (global step 45050: loss: 0.30089737474918365, lr: 1e-05
2023-12-21 07:42:19 INFO     	 * (global step 45100: loss: 0.36062680184841156, lr: 1e-05
2023-12-21 07:42:27 INFO     	 * (global step 45150: loss: 0.3840329349040985, lr: 1e-05
2023-12-21 07:42:35 INFO     	 * (global step 45200: loss: 0.22330265492200851, lr: 1e-05
2023-12-21 07:42:43 INFO     	 * (global step 45250: loss: 0.32547086477279663, lr: 1e-05
2023-12-21 07:42:51 INFO     	 * (global step 45300: loss: 0.24187958240509033, lr: 1e-05
2023-12-21 07:42:59 INFO     	 * (global step 45350: loss: 0.2367829978466034, lr: 1e-05
2023-12-21 07:43:07 INFO     	 * (global step 45400: loss: 0.27336485683918, lr: 1e-05
2023-12-21 07:43:15 INFO     	 * (global step 45450: loss: 0.2143252193927765, lr: 1e-05
2023-12-21 07:43:22 INFO     	 * (global step 45500: loss: 0.2389085292816162, lr: 1e-05
2023-12-21 07:43:30 INFO     	 * (global step 45550: loss: 0.31430794298648834, lr: 1e-05
2023-12-21 07:43:38 INFO     	 * (global step 45600: loss: 0.306920126080513, lr: 1e-05
2023-12-21 07:43:46 INFO     	 * (global step 45650: loss: 0.35220104455947876, lr: 1e-05
2023-12-21 07:43:54 INFO     	 * (global step 45700: loss: 0.266439750790596, lr: 1e-05
2023-12-21 07:44:02 INFO     	 * (global step 45750: loss: 0.49024058878421783, lr: 1e-05
2023-12-21 07:44:10 INFO     	 * (global step 45800: loss: 0.3168889135122299, lr: 1e-05
2023-12-21 07:44:18 INFO     	 * (global step 45850: loss: 0.3165690451860428, lr: 1e-05
2023-12-21 07:44:26 INFO     	 * (global step 45900: loss: 0.48024629056453705, lr: 1e-05
2023-12-21 07:44:34 INFO     	 * (global step 45950: loss: 0.37771813571453094, lr: 1e-05
2023-12-21 07:44:42 INFO     	 * (global step 46000: loss: 0.24411606043577194, lr: 1e-05
2023-12-21 07:44:50 INFO     	 * (global step 46050: loss: 0.27929164469242096, lr: 1e-05
2023-12-21 07:44:58 INFO     	 * (global step 46100: loss: 0.43428128957748413, lr: 1e-05
2023-12-21 07:45:06 INFO     	 * (global step 46150: loss: 0.20752469450235367, lr: 1e-05
2023-12-21 07:45:14 INFO     	 * (global step 46200: loss: 0.2044719234108925, lr: 1e-05
2023-12-21 07:45:22 INFO     	 * (global step 46250: loss: 0.27478037774562836, lr: 1e-05
2023-12-21 07:45:30 INFO     	 * (global step 46300: loss: 0.24964886158704758, lr: 1e-05
2023-12-21 07:45:38 INFO     	 * (global step 46350: loss: 0.31747375428676605, lr: 1e-05
2023-12-21 07:45:46 INFO     	 * (global step 46400: loss: 0.31509532034397125, lr: 1e-05
2023-12-21 07:45:54 INFO     	 * (global step 46450: loss: 0.38732556998729706, lr: 1e-05
2023-12-21 07:46:02 INFO     	 * (global step 46500: loss: 0.19104962050914764, lr: 1e-05
2023-12-21 07:46:10 INFO     	 * (global step 46550: loss: 0.6307331919670105, lr: 1e-05
2023-12-21 07:46:18 INFO     	 * (global step 46600: loss: 0.3421712964773178, lr: 1e-05
2023-12-21 07:46:26 INFO     	 * (global step 46650: loss: 0.39941397309303284, lr: 1e-05
2023-12-21 07:46:34 INFO     	 * (global step 46700: loss: 0.34621410071849823, lr: 1e-05
2023-12-21 07:46:42 INFO     	 * (global step 46750: loss: 0.333707332611084, lr: 1e-05
2023-12-21 07:46:50 INFO     	 * (global step 46800: loss: 0.20576130971312523, lr: 1e-05
2023-12-21 07:46:58 INFO     	 * (global step 46850: loss: 0.5236088335514069, lr: 1e-05
2023-12-21 07:47:06 INFO     	 * (global step 46900: loss: 0.2336743324995041, lr: 1e-05
2023-12-21 07:47:14 INFO     	 * (global step 46950: loss: 0.22787217050790787, lr: 1e-05
2023-12-21 07:47:22 INFO     	 * (global step 47000: loss: 0.41069352626800537, lr: 1e-05
2023-12-21 07:47:30 INFO     	 * (global step 47050: loss: 0.30716900527477264, lr: 1e-05
2023-12-21 07:47:38 INFO     	 * (global step 47100: loss: 0.2434585690498352, lr: 1e-05
2023-12-21 07:47:46 INFO     	 * (global step 47150: loss: 0.2706605941057205, lr: 1e-05
2023-12-21 07:47:54 INFO     	 * (global step 47200: loss: 0.2589821591973305, lr: 1e-05
2023-12-21 07:47:59 INFO     [epoch 9/15] average loss: 0.315, lr: 1e-05
2023-12-21 07:47:59 INFO     saving model related files
2023-12-21 07:47:59 INFO     saving model
2023-12-21 07:47:59 INFO     saving tokenizer
2023-12-21 07:47:59 INFO     saving optimizer
2023-12-21 07:48:00 INFO     remove old optimizer files
2023-12-21 07:48:00 INFO     complete training: model ckpt was saved at small_combined_trained_ckpt/model_oprhlh
2023-12-21 07:48:01 INFO     ## 1st RUN: Configuration 10/12 ##
2023-12-21 07:48:01 INFO     initialize model trainer
2023-12-21 07:48:01 INFO     initialize checkpoint at small_combined_trained_ckpt/model_vhyoja
2023-12-21 07:48:01 INFO     hyperparameters
2023-12-21 07:48:01 INFO     	 * dataset_path: StellarMilk/squad_newsqa
2023-12-21 07:48:01 INFO     	 * dataset_name: default
2023-12-21 07:48:01 INFO     	 * input_types: ['paragraph']
2023-12-21 07:48:01 INFO     	 * output_types: ['questions_answers']
2023-12-21 07:48:01 INFO     	 * prefix_types: ['qag']
2023-12-21 07:48:01 INFO     	 * model: t5-small
2023-12-21 07:48:01 INFO     	 * max_length: 512
2023-12-21 07:48:01 INFO     	 * max_length_output: 512
2023-12-21 07:48:01 INFO     	 * epoch: 15
2023-12-21 07:48:01 INFO     	 * batch: 2
2023-12-21 07:48:01 INFO     	 * lr: 1e-05
2023-12-21 07:48:01 INFO     	 * fp16: False
2023-12-21 07:48:01 INFO     	 * random_seed: 1
2023-12-21 07:48:01 INFO     	 * gradient_accumulation_steps: 4
2023-12-21 07:48:01 INFO     	 * label_smoothing: 0.0
2023-12-21 07:48:01 INFO     initialize checkpoint with t5-small
2023-12-21 07:48:02 INFO     use spaCy answer extraction model: positionrank
2023-12-21 07:48:02 INFO     Model `t5-small`
2023-12-21 07:48:02 INFO     	 * Num of GPU in use: 1
2023-12-21 07:48:02 INFO     	 * Prefix: True
2023-12-21 07:48:02 INFO     	 * Language: en (ignore at the training phase)
2023-12-21 07:48:02 INFO     dataset preprocessing
2023-12-21 07:48:04 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/squad_newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-21 07:48:06 INFO     start model training
2023-12-21 07:48:21 INFO     	 * (global step 50: loss: 4.9993051290512085, lr: 1e-05
2023-12-21 07:48:37 INFO     	 * (global step 100: loss: 2.6304880380630493, lr: 1e-05
2023-12-21 07:48:52 INFO     	 * (global step 150: loss: 2.11302986741066, lr: 1e-05
2023-12-21 07:49:08 INFO     	 * (global step 200: loss: 1.7670029699802399, lr: 1e-05
2023-12-21 07:49:23 INFO     	 * (global step 250: loss: 1.4231001138687134, lr: 1e-05
2023-12-21 07:49:39 INFO     	 * (global step 300: loss: 1.2368205040693283, lr: 1e-05
2023-12-21 07:49:54 INFO     	 * (global step 350: loss: 1.1538020074367523, lr: 1e-05
2023-12-21 07:50:10 INFO     	 * (global step 400: loss: 0.8391617238521576, lr: 1e-05
2023-12-21 07:50:26 INFO     	 * (global step 450: loss: 0.7780889421701431, lr: 1e-05
2023-12-21 07:50:41 INFO     	 * (global step 500: loss: 0.7121539413928986, lr: 1e-05
2023-12-21 07:50:56 INFO     	 * (global step 550: loss: 0.7944482564926147, lr: 1e-05
2023-12-21 07:51:12 INFO     	 * (global step 600: loss: 0.7913478761911392, lr: 1e-05
2023-12-21 07:51:27 INFO     	 * (global step 650: loss: 0.6261240690946579, lr: 1e-05
2023-12-21 07:51:43 INFO     	 * (global step 700: loss: 0.6075136810541153, lr: 1e-05
2023-12-21 07:51:58 INFO     	 * (global step 750: loss: 0.770898699760437, lr: 1e-05
2023-12-21 07:52:14 INFO     	 * (global step 800: loss: 0.6155637204647064, lr: 1e-05
2023-12-21 07:52:29 INFO     	 * (global step 850: loss: 0.600632332265377, lr: 1e-05
2023-12-21 07:52:45 INFO     	 * (global step 900: loss: 0.5872211158275604, lr: 1e-05
2023-12-21 07:53:00 INFO     	 * (global step 950: loss: 0.5892715379595757, lr: 1e-05
2023-12-21 07:53:16 INFO     	 * (global step 1000: loss: 0.576823778450489, lr: 1e-05
2023-12-21 07:53:31 INFO     	 * (global step 1050: loss: 0.5985307544469833, lr: 1e-05
2023-12-21 07:53:47 INFO     	 * (global step 1100: loss: 0.6072792708873749, lr: 1e-05
2023-12-21 07:54:03 INFO     	 * (global step 1150: loss: 0.8434543237090111, lr: 1e-05
2023-12-21 07:54:18 INFO     	 * (global step 1200: loss: 0.5258514061570168, lr: 1e-05
2023-12-21 07:54:34 INFO     	 * (global step 1250: loss: 0.4998392388224602, lr: 1e-05
2023-12-21 07:54:49 INFO     	 * (global step 1300: loss: 0.5548710003495216, lr: 1e-05
2023-12-21 07:55:05 INFO     	 * (global step 1350: loss: 0.4994741156697273, lr: 1e-05
2023-12-21 07:55:20 INFO     	 * (global step 1400: loss: 0.5944292545318604, lr: 1e-05
2023-12-21 07:55:36 INFO     	 * (global step 1450: loss: 0.5333956554532051, lr: 1e-05
2023-12-21 07:55:51 INFO     	 * (global step 1500: loss: 0.6335693895816803, lr: 1e-05
2023-12-21 07:56:07 INFO     	 * (global step 1550: loss: 0.45359544456005096, lr: 1e-05
2023-12-21 07:56:22 INFO     	 * (global step 1600: loss: 0.4516808018088341, lr: 1e-05
2023-12-21 07:56:38 INFO     	 * (global step 1650: loss: 0.5743836089968681, lr: 1e-05
2023-12-21 07:56:53 INFO     	 * (global step 1700: loss: 0.4498150870203972, lr: 1e-05
2023-12-21 07:57:09 INFO     	 * (global step 1750: loss: 0.42925315350294113, lr: 1e-05
2023-12-21 07:57:24 INFO     	 * (global step 1800: loss: 0.4449857324361801, lr: 1e-05
2023-12-21 07:57:40 INFO     	 * (global step 1850: loss: 0.3853289633989334, lr: 1e-05
2023-12-21 07:57:55 INFO     	 * (global step 1900: loss: 0.4881161004304886, lr: 1e-05
2023-12-21 07:58:11 INFO     	 * (global step 1950: loss: 0.44532080739736557, lr: 1e-05
2023-12-21 07:58:27 INFO     	 * (global step 2000: loss: 0.3879989758133888, lr: 1e-05
2023-12-21 07:58:42 INFO     	 * (global step 2050: loss: 0.41059695929288864, lr: 1e-05
2023-12-21 07:58:58 INFO     	 * (global step 2100: loss: 0.4086983725428581, lr: 1e-05
2023-12-21 07:59:13 INFO     	 * (global step 2150: loss: 0.5615677312016487, lr: 1e-05
2023-12-21 07:59:29 INFO     	 * (global step 2200: loss: 0.6007189527153969, lr: 1e-05
2023-12-21 07:59:44 INFO     	 * (global step 2250: loss: 0.40429677069187164, lr: 1e-05
2023-12-21 08:00:00 INFO     	 * (global step 2300: loss: 0.49800345301628113, lr: 1e-05
2023-12-21 08:00:15 INFO     	 * (global step 2350: loss: 0.479861356317997, lr: 1e-05
2023-12-21 08:00:19 INFO     [epoch 0/15] average loss: 0.917, lr: 1e-05
2023-12-21 08:00:19 INFO     saving model related files
2023-12-21 08:00:19 INFO     saving model
2023-12-21 08:00:19 INFO     saving tokenizer
2023-12-21 08:00:19 INFO     saving optimizer
2023-12-21 08:00:20 INFO     remove old optimizer files
2023-12-21 08:00:32 INFO     	 * (global step 2400: loss: 0.42973946779966354, lr: 1e-05
2023-12-21 08:00:48 INFO     	 * (global step 2450: loss: 0.4783950522542, lr: 1e-05
2023-12-21 08:01:03 INFO     	 * (global step 2500: loss: 0.500094436109066, lr: 1e-05
2023-12-21 08:01:19 INFO     	 * (global step 2550: loss: 0.435795146971941, lr: 1e-05
2023-12-21 08:01:34 INFO     	 * (global step 2600: loss: 0.42568351328372955, lr: 1e-05
2023-12-21 08:01:50 INFO     	 * (global step 2650: loss: 0.7038384974002838, lr: 1e-05
2023-12-21 08:02:05 INFO     	 * (global step 2700: loss: 0.5031572207808495, lr: 1e-05
2023-12-21 08:02:20 INFO     	 * (global step 2750: loss: 0.378230981528759, lr: 1e-05
2023-12-21 08:02:36 INFO     	 * (global step 2800: loss: 0.5800271704792976, lr: 1e-05
2023-12-21 08:02:51 INFO     	 * (global step 2850: loss: 0.573373943567276, lr: 1e-05
2023-12-21 08:03:07 INFO     	 * (global step 2900: loss: 0.5311386957764626, lr: 1e-05
2023-12-21 08:03:22 INFO     	 * (global step 2950: loss: 0.4331899434328079, lr: 1e-05
2023-12-21 08:03:38 INFO     	 * (global step 3000: loss: 0.4906816780567169, lr: 1e-05
2023-12-21 08:03:53 INFO     	 * (global step 3050: loss: 0.44736193120479584, lr: 1e-05
2023-12-21 08:04:09 INFO     	 * (global step 3100: loss: 0.43193238973617554, lr: 1e-05
2023-12-21 08:04:24 INFO     	 * (global step 3150: loss: 0.5998374745249748, lr: 1e-05
2023-12-21 08:04:40 INFO     	 * (global step 3200: loss: 0.39568790793418884, lr: 1e-05
2023-12-21 08:04:55 INFO     	 * (global step 3250: loss: 0.4740102291107178, lr: 1e-05
2023-12-21 08:05:10 INFO     	 * (global step 3300: loss: 0.401681587100029, lr: 1e-05
2023-12-21 08:05:26 INFO     	 * (global step 3350: loss: 0.39075034111738205, lr: 1e-05
2023-12-21 08:05:41 INFO     	 * (global step 3400: loss: 0.37084972113370895, lr: 1e-05
2023-12-21 08:05:57 INFO     	 * (global step 3450: loss: 0.4946783632040024, lr: 1e-05
2023-12-21 08:06:12 INFO     	 * (global step 3500: loss: 0.6415796652436256, lr: 1e-05
2023-12-21 08:06:28 INFO     	 * (global step 3550: loss: 0.3979099839925766, lr: 1e-05
2023-12-21 08:06:43 INFO     	 * (global step 3600: loss: 0.44487766921520233, lr: 1e-05
2023-12-21 08:06:59 INFO     	 * (global step 3650: loss: 0.391689658164978, lr: 1e-05
2023-12-21 08:07:14 INFO     	 * (global step 3700: loss: 0.44420280307531357, lr: 1e-05
2023-12-21 08:07:30 INFO     	 * (global step 3750: loss: 0.4206285700201988, lr: 1e-05
2023-12-21 08:07:45 INFO     	 * (global step 3800: loss: 0.3345048055052757, lr: 1e-05
2023-12-21 08:08:01 INFO     	 * (global step 3850: loss: 0.4148864448070526, lr: 1e-05
2023-12-21 08:08:16 INFO     	 * (global step 3900: loss: 0.34619829431176186, lr: 1e-05
2023-12-21 08:08:32 INFO     	 * (global step 3950: loss: 0.43357764929533005, lr: 1e-05
2023-12-21 08:08:47 INFO     	 * (global step 4000: loss: 0.36891451850533485, lr: 1e-05
2023-12-21 08:09:03 INFO     	 * (global step 4050: loss: 0.49513808637857437, lr: 1e-05
2023-12-21 08:09:18 INFO     	 * (global step 4100: loss: 0.39146876335144043, lr: 1e-05
2023-12-21 08:09:34 INFO     	 * (global step 4150: loss: 0.33108483999967575, lr: 1e-05
2023-12-21 08:09:49 INFO     	 * (global step 4200: loss: 0.4443097189068794, lr: 1e-05
2023-12-21 08:10:04 INFO     	 * (global step 4250: loss: 0.489250972867012, lr: 1e-05
2023-12-21 08:10:20 INFO     	 * (global step 4300: loss: 0.35474177077412605, lr: 1e-05
2023-12-21 08:10:35 INFO     	 * (global step 4350: loss: 0.3473157584667206, lr: 1e-05
2023-12-21 08:10:51 INFO     	 * (global step 4400: loss: 0.3612605631351471, lr: 1e-05
2023-12-21 08:11:06 INFO     	 * (global step 4450: loss: 0.33040232956409454, lr: 1e-05
2023-12-21 08:11:22 INFO     	 * (global step 4500: loss: 0.35560715198516846, lr: 1e-05
2023-12-21 08:11:37 INFO     	 * (global step 4550: loss: 0.4210636168718338, lr: 1e-05
2023-12-21 08:11:53 INFO     	 * (global step 4600: loss: 0.26572616398334503, lr: 1e-05
2023-12-21 08:12:08 INFO     	 * (global step 4650: loss: 0.4262065291404724, lr: 1e-05
2023-12-21 08:12:24 INFO     	 * (global step 4700: loss: 0.37484634667634964, lr: 1e-05
2023-12-21 08:12:31 INFO     [epoch 1/15] average loss: 0.431, lr: 1e-05
2023-12-21 08:12:31 INFO     saving model related files
2023-12-21 08:12:31 INFO     saving model
2023-12-21 08:12:31 INFO     saving tokenizer
2023-12-21 08:12:31 INFO     saving optimizer
2023-12-21 08:12:32 INFO     remove old optimizer files
2023-12-21 08:12:41 INFO     	 * (global step 4750: loss: 0.5721472054719925, lr: 1e-05
2023-12-21 08:12:57 INFO     	 * (global step 4800: loss: 0.3528956398367882, lr: 1e-05
2023-12-21 08:13:12 INFO     	 * (global step 4850: loss: 0.3146781884133816, lr: 1e-05
2023-12-21 08:13:28 INFO     	 * (global step 4900: loss: 0.3696332797408104, lr: 1e-05
2023-12-21 08:13:43 INFO     	 * (global step 4950: loss: 0.37488502264022827, lr: 1e-05
2023-12-21 08:13:59 INFO     	 * (global step 5000: loss: 0.6068108975887299, lr: 1e-05
2023-12-21 08:14:14 INFO     	 * (global step 5050: loss: 0.4551422595977783, lr: 1e-05
2023-12-21 08:14:30 INFO     	 * (global step 5100: loss: 0.3355177417397499, lr: 1e-05
2023-12-21 08:14:45 INFO     	 * (global step 5150: loss: 0.362757571041584, lr: 1e-05
2023-12-21 08:15:01 INFO     	 * (global step 5200: loss: 0.4507182836532593, lr: 1e-05
2023-12-21 08:15:16 INFO     	 * (global step 5250: loss: 0.38090649992227554, lr: 1e-05
2023-12-21 08:15:32 INFO     	 * (global step 5300: loss: 0.42069244384765625, lr: 1e-05
2023-12-21 08:15:47 INFO     	 * (global step 5350: loss: 0.3357563428580761, lr: 1e-05
2023-12-21 08:16:03 INFO     	 * (global step 5400: loss: 0.27975087985396385, lr: 1e-05
2023-12-21 08:16:18 INFO     	 * (global step 5450: loss: 0.5379257276654243, lr: 1e-05
2023-12-21 08:16:34 INFO     	 * (global step 5500: loss: 0.5324843898415565, lr: 1e-05
2023-12-21 08:16:49 INFO     	 * (global step 5550: loss: 0.3536023572087288, lr: 1e-05
2023-12-21 08:17:05 INFO     	 * (global step 5600: loss: 0.3131752349436283, lr: 1e-05
2023-12-21 08:17:20 INFO     	 * (global step 5650: loss: 0.31677621603012085, lr: 1e-05
2023-12-21 08:17:36 INFO     	 * (global step 5700: loss: 0.5301818549633026, lr: 1e-05
2023-12-21 08:17:52 INFO     	 * (global step 5750: loss: 0.41718681156635284, lr: 1e-05
2023-12-21 08:18:07 INFO     	 * (global step 5800: loss: 0.398206427693367, lr: 1e-05
2023-12-21 08:18:22 INFO     	 * (global step 5850: loss: 0.44670969992876053, lr: 1e-05
2023-12-21 08:18:38 INFO     	 * (global step 5900: loss: 0.39223887026309967, lr: 1e-05
2023-12-21 08:18:53 INFO     	 * (global step 5950: loss: 0.47255390137434006, lr: 1e-05
2023-12-21 08:19:09 INFO     	 * (global step 6000: loss: 0.4510406032204628, lr: 1e-05
2023-12-21 08:19:24 INFO     	 * (global step 6050: loss: 0.34412046521902084, lr: 1e-05
2023-12-21 08:19:40 INFO     	 * (global step 6100: loss: 0.4448418617248535, lr: 1e-05
2023-12-21 08:19:56 INFO     	 * (global step 6150: loss: 0.44977929443120956, lr: 1e-05
2023-12-21 08:20:11 INFO     	 * (global step 6200: loss: 0.36055710911750793, lr: 1e-05
2023-12-21 08:20:27 INFO     	 * (global step 6250: loss: 0.32144559919834137, lr: 1e-05
2023-12-21 08:20:42 INFO     	 * (global step 6300: loss: 0.44186070933938026, lr: 1e-05
2023-12-21 08:20:58 INFO     	 * (global step 6350: loss: 0.4592844396829605, lr: 1e-05
2023-12-21 08:21:13 INFO     	 * (global step 6400: loss: 0.33507247641682625, lr: 1e-05
2023-12-21 08:21:29 INFO     	 * (global step 6450: loss: 0.36730609089136124, lr: 1e-05
2023-12-21 08:21:44 INFO     	 * (global step 6500: loss: 0.36119256168603897, lr: 1e-05
2023-12-21 08:22:00 INFO     	 * (global step 6550: loss: 0.4986652471125126, lr: 1e-05
2023-12-21 08:22:15 INFO     	 * (global step 6600: loss: 0.4728501960635185, lr: 1e-05
2023-12-21 08:22:31 INFO     	 * (global step 6650: loss: 0.3719885125756264, lr: 1e-05
2023-12-21 08:22:46 INFO     	 * (global step 6700: loss: 0.3579346500337124, lr: 1e-05
2023-12-21 08:23:02 INFO     	 * (global step 6750: loss: 0.438174307346344, lr: 1e-05
2023-12-21 08:23:17 INFO     	 * (global step 6800: loss: 0.37145185470581055, lr: 1e-05
2023-12-21 08:23:33 INFO     	 * (global step 6850: loss: 0.3014301098883152, lr: 1e-05
2023-12-21 08:23:48 INFO     	 * (global step 6900: loss: 0.4584697559475899, lr: 1e-05
2023-12-21 08:24:04 INFO     	 * (global step 6950: loss: 0.3388475440442562, lr: 1e-05
2023-12-21 08:24:19 INFO     	 * (global step 7000: loss: 0.3495436757802963, lr: 1e-05
2023-12-21 08:24:35 INFO     	 * (global step 7050: loss: 0.33837833255529404, lr: 1e-05
2023-12-21 08:24:45 INFO     [epoch 2/15] average loss: 0.39, lr: 1e-05
2023-12-21 08:24:45 INFO     saving model related files
2023-12-21 08:24:45 INFO     saving model
2023-12-21 08:24:46 INFO     saving tokenizer
2023-12-21 08:24:46 INFO     saving optimizer
2023-12-21 08:24:47 INFO     remove old optimizer files
2023-12-21 08:24:53 INFO     	 * (global step 7100: loss: 0.322360597550869, lr: 1e-05
2023-12-21 08:25:08 INFO     	 * (global step 7150: loss: 0.5299533940851688, lr: 1e-05
2023-12-21 08:25:24 INFO     	 * (global step 7200: loss: 0.38477717339992523, lr: 1e-05
2023-12-21 08:25:39 INFO     	 * (global step 7250: loss: 0.2874959148466587, lr: 1e-05
2023-12-21 08:25:55 INFO     	 * (global step 7300: loss: 0.5089610740542412, lr: 1e-05
2023-12-21 08:26:10 INFO     	 * (global step 7350: loss: 0.32922685146331787, lr: 1e-05
2023-12-21 08:26:26 INFO     	 * (global step 7400: loss: 0.41146326810121536, lr: 1e-05
2023-12-21 08:26:41 INFO     	 * (global step 7450: loss: 0.32971489429473877, lr: 1e-05
2023-12-21 08:26:57 INFO     	 * (global step 7500: loss: 0.33974702283740044, lr: 1e-05
2023-12-21 08:27:12 INFO     	 * (global step 7550: loss: 0.39820289611816406, lr: 1e-05
2023-12-21 08:27:28 INFO     	 * (global step 7600: loss: 0.3778560981154442, lr: 1e-05
2023-12-21 08:27:43 INFO     	 * (global step 7650: loss: 0.33110034465789795, lr: 1e-05
2023-12-21 08:27:59 INFO     	 * (global step 7700: loss: 0.40512268245220184, lr: 1e-05
2023-12-21 08:28:14 INFO     	 * (global step 7750: loss: 0.3922514170408249, lr: 1e-05
2023-12-21 08:28:30 INFO     	 * (global step 7800: loss: 0.31798743084073067, lr: 1e-05
2023-12-21 08:28:45 INFO     	 * (global step 7850: loss: 0.28677405416965485, lr: 1e-05
2023-12-21 08:29:01 INFO     	 * (global step 7900: loss: 0.3076356127858162, lr: 1e-05
2023-12-21 08:29:16 INFO     	 * (global step 7950: loss: 0.2886856682598591, lr: 1e-05
2023-12-21 08:29:32 INFO     	 * (global step 8000: loss: 0.39200539886951447, lr: 1e-05
2023-12-21 08:29:48 INFO     	 * (global step 8050: loss: 0.31617090851068497, lr: 1e-05
2023-12-21 08:30:03 INFO     	 * (global step 8100: loss: 0.5203120820224285, lr: 1e-05
2023-12-21 08:30:19 INFO     	 * (global step 8150: loss: 0.3125578314065933, lr: 1e-05
2023-12-21 08:30:34 INFO     	 * (global step 8200: loss: 0.33990734070539474, lr: 1e-05
2023-12-21 08:30:50 INFO     	 * (global step 8250: loss: 0.36246348172426224, lr: 1e-05
2023-12-21 08:31:05 INFO     	 * (global step 8300: loss: 0.3245164752006531, lr: 1e-05
2023-12-21 08:31:21 INFO     	 * (global step 8350: loss: 0.4240028038620949, lr: 1e-05
2023-12-21 08:31:36 INFO     	 * (global step 8400: loss: 0.3574777990579605, lr: 1e-05
2023-12-21 08:31:52 INFO     	 * (global step 8450: loss: 0.3563239723443985, lr: 1e-05
2023-12-21 08:32:07 INFO     	 * (global step 8500: loss: 0.3719296008348465, lr: 1e-05
2023-12-21 08:32:23 INFO     	 * (global step 8550: loss: 0.2845746614038944, lr: 1e-05
2023-12-21 08:32:38 INFO     	 * (global step 8600: loss: 0.4217645898461342, lr: 1e-05
2023-12-21 08:32:54 INFO     	 * (global step 8650: loss: 0.503559909760952, lr: 1e-05
2023-12-21 08:33:09 INFO     	 * (global step 8700: loss: 0.4192178398370743, lr: 1e-05
2023-12-21 08:33:25 INFO     	 * (global step 8750: loss: 0.2808411829173565, lr: 1e-05
2023-12-21 08:33:40 INFO     	 * (global step 8800: loss: 0.3739582449197769, lr: 1e-05
2023-12-21 08:33:56 INFO     	 * (global step 8850: loss: 0.35417359694838524, lr: 1e-05
2023-12-21 08:34:12 INFO     	 * (global step 8900: loss: 0.28442056104540825, lr: 1e-05
2023-12-21 08:34:27 INFO     	 * (global step 8950: loss: 0.3467163071036339, lr: 1e-05
2023-12-21 08:34:43 INFO     	 * (global step 9000: loss: 0.3822757825255394, lr: 1e-05
2023-12-21 08:34:58 INFO     	 * (global step 9050: loss: 0.3431682512164116, lr: 1e-05
2023-12-21 08:35:14 INFO     	 * (global step 9100: loss: 0.36610250174999237, lr: 1e-05
2023-12-21 08:35:29 INFO     	 * (global step 9150: loss: 0.29526376724243164, lr: 1e-05
2023-12-21 08:35:45 INFO     	 * (global step 9200: loss: 0.2948102056980133, lr: 1e-05
2023-12-21 08:36:01 INFO     	 * (global step 9250: loss: 0.25004045478999615, lr: 1e-05
2023-12-21 08:36:16 INFO     	 * (global step 9300: loss: 0.28811322152614594, lr: 1e-05
2023-12-21 08:36:32 INFO     	 * (global step 9350: loss: 0.35872746631503105, lr: 1e-05
2023-12-21 08:36:47 INFO     	 * (global step 9400: loss: 0.4038036912679672, lr: 1e-05
2023-12-21 08:37:01 INFO     [epoch 3/15] average loss: 0.37, lr: 1e-05
2023-12-21 08:37:01 INFO     saving model related files
2023-12-21 08:37:01 INFO     saving model
2023-12-21 08:37:02 INFO     saving tokenizer
2023-12-21 08:37:02 INFO     saving optimizer
2023-12-21 08:37:03 INFO     remove old optimizer files
2023-12-21 08:37:05 INFO     	 * (global step 9450: loss: 0.282600961625576, lr: 1e-05
2023-12-21 08:37:20 INFO     	 * (global step 9500: loss: 0.4622882977128029, lr: 1e-05
2023-12-21 08:37:36 INFO     	 * (global step 9550: loss: 0.3519905135035515, lr: 1e-05
2023-12-21 08:37:51 INFO     	 * (global step 9600: loss: 0.44305192679166794, lr: 1e-05
2023-12-21 08:38:07 INFO     	 * (global step 9650: loss: 0.3521352484822273, lr: 1e-05
2023-12-21 08:38:22 INFO     	 * (global step 9700: loss: 0.34390465170145035, lr: 1e-05
2023-12-21 08:38:38 INFO     	 * (global step 9750: loss: 0.3268740773200989, lr: 1e-05
2023-12-21 08:38:53 INFO     	 * (global step 9800: loss: 0.3113832622766495, lr: 1e-05
2023-12-21 08:39:09 INFO     	 * (global step 9850: loss: 0.30964838340878487, lr: 1e-05
2023-12-21 08:39:24 INFO     	 * (global step 9900: loss: 0.41025160998106003, lr: 1e-05
2023-12-21 08:39:40 INFO     	 * (global step 9950: loss: 0.4603424668312073, lr: 1e-05
2023-12-21 08:39:55 INFO     	 * (global step 10000: loss: 0.32102803885936737, lr: 1e-05
2023-12-21 08:40:11 INFO     	 * (global step 10050: loss: 0.5183799043297768, lr: 1e-05
2023-12-21 08:40:26 INFO     	 * (global step 10100: loss: 0.2604031264781952, lr: 1e-05
2023-12-21 08:40:42 INFO     	 * (global step 10150: loss: 0.5082173123955727, lr: 1e-05
2023-12-21 08:40:57 INFO     	 * (global step 10200: loss: 0.4195276126265526, lr: 1e-05
2023-12-21 08:41:13 INFO     	 * (global step 10250: loss: 0.2483743131160736, lr: 1e-05
2023-12-21 08:41:28 INFO     	 * (global step 10300: loss: 0.3007972687482834, lr: 1e-05
2023-12-21 08:41:44 INFO     	 * (global step 10350: loss: 0.43231144547462463, lr: 1e-05
2023-12-21 08:41:59 INFO     	 * (global step 10400: loss: 0.26848386600613594, lr: 1e-05
2023-12-21 08:42:15 INFO     	 * (global step 10450: loss: 0.4518662467598915, lr: 1e-05
2023-12-21 08:42:31 INFO     	 * (global step 10500: loss: 0.4072785973548889, lr: 1e-05
2023-12-21 08:42:46 INFO     	 * (global step 10550: loss: 0.322633258998394, lr: 1e-05
2023-12-21 08:43:02 INFO     	 * (global step 10600: loss: 0.37647607177495956, lr: 1e-05
2023-12-21 08:43:17 INFO     	 * (global step 10650: loss: 0.3854203261435032, lr: 1e-05
2023-12-21 08:43:33 INFO     	 * (global step 10700: loss: 0.5266114845871925, lr: 1e-05
2023-12-21 08:43:48 INFO     	 * (global step 10750: loss: 0.580638200044632, lr: 1e-05
2023-12-21 08:44:04 INFO     	 * (global step 10800: loss: 0.3216850832104683, lr: 1e-05
2023-12-21 08:44:19 INFO     	 * (global step 10850: loss: 0.334880106151104, lr: 1e-05
2023-12-21 08:44:35 INFO     	 * (global step 10900: loss: 0.3670203760266304, lr: 1e-05
2023-12-21 08:44:50 INFO     	 * (global step 10950: loss: 0.2224162332713604, lr: 1e-05
2023-12-21 08:45:06 INFO     	 * (global step 11000: loss: 0.3407832384109497, lr: 1e-05
2023-12-21 08:45:21 INFO     	 * (global step 11050: loss: 0.38885604590177536, lr: 1e-05
2023-12-21 08:45:37 INFO     	 * (global step 11100: loss: 0.2779714986681938, lr: 1e-05
2023-12-21 08:45:52 INFO     	 * (global step 11150: loss: 0.30668892338871956, lr: 1e-05
2023-12-21 08:46:08 INFO     	 * (global step 11200: loss: 0.4034930467605591, lr: 1e-05
2023-12-21 08:46:23 INFO     	 * (global step 11250: loss: 0.451328806579113, lr: 1e-05
2023-12-21 08:46:39 INFO     	 * (global step 11300: loss: 0.3286783769726753, lr: 1e-05
2023-12-21 08:46:54 INFO     	 * (global step 11350: loss: 0.3629375472664833, lr: 1e-05
2023-12-21 08:47:10 INFO     	 * (global step 11400: loss: 0.29277805611491203, lr: 1e-05
2023-12-21 08:47:26 INFO     	 * (global step 11450: loss: 0.32079026103019714, lr: 1e-05
2023-12-21 08:47:41 INFO     	 * (global step 11500: loss: 0.2678952254354954, lr: 1e-05
2023-12-21 08:47:57 INFO     	 * (global step 11550: loss: 0.3582957349717617, lr: 1e-05
2023-12-21 08:48:12 INFO     	 * (global step 11600: loss: 0.4109279327094555, lr: 1e-05
2023-12-21 08:48:28 INFO     	 * (global step 11650: loss: 0.3742440938949585, lr: 1e-05
2023-12-21 08:48:43 INFO     	 * (global step 11700: loss: 0.3758402317762375, lr: 1e-05
2023-12-21 08:48:59 INFO     	 * (global step 11750: loss: 0.33127792924642563, lr: 1e-05
2023-12-21 08:49:14 INFO     	 * (global step 11800: loss: 0.41837237775325775, lr: 1e-05
2023-12-21 08:49:16 INFO     [epoch 4/15] average loss: 0.358, lr: 1e-05
2023-12-21 08:49:16 INFO     saving model related files
2023-12-21 08:49:16 INFO     saving model
2023-12-21 08:49:16 INFO     saving tokenizer
2023-12-21 08:49:16 INFO     saving optimizer
2023-12-21 08:49:17 INFO     remove old optimizer files
2023-12-21 08:49:31 INFO     	 * (global step 11850: loss: 0.4765969514846802, lr: 1e-05
2023-12-21 08:49:47 INFO     	 * (global step 11900: loss: 0.3762078210711479, lr: 1e-05
2023-12-21 08:50:02 INFO     	 * (global step 11950: loss: 0.32488778978586197, lr: 1e-05
2023-12-21 08:50:18 INFO     	 * (global step 12000: loss: 0.35501382127404213, lr: 1e-05
2023-12-21 08:50:33 INFO     	 * (global step 12050: loss: 0.3498517833650112, lr: 1e-05
2023-12-21 08:50:49 INFO     	 * (global step 12100: loss: 0.4250820651650429, lr: 1e-05
2023-12-21 08:51:04 INFO     	 * (global step 12150: loss: 0.43027475103735924, lr: 1e-05
2023-12-21 08:51:20 INFO     	 * (global step 12200: loss: 0.4307517185807228, lr: 1e-05
2023-12-21 08:51:35 INFO     	 * (global step 12250: loss: 0.2879149615764618, lr: 1e-05
2023-12-21 08:51:51 INFO     	 * (global step 12300: loss: 0.3774288445711136, lr: 1e-05
2023-12-21 08:52:07 INFO     	 * (global step 12350: loss: 0.289398867636919, lr: 1e-05
2023-12-21 08:52:22 INFO     	 * (global step 12400: loss: 0.3746553659439087, lr: 1e-05
2023-12-21 08:52:38 INFO     	 * (global step 12450: loss: 0.36955930665135384, lr: 1e-05
2023-12-21 08:52:53 INFO     	 * (global step 12500: loss: 0.27524491772055626, lr: 1e-05
2023-12-21 08:53:09 INFO     	 * (global step 12550: loss: 0.3772807717323303, lr: 1e-05
2023-12-21 08:53:24 INFO     	 * (global step 12600: loss: 0.40135519951581955, lr: 1e-05
2023-12-21 08:53:40 INFO     	 * (global step 12650: loss: 0.3420075848698616, lr: 1e-05
2023-12-21 08:53:55 INFO     	 * (global step 12700: loss: 0.3609168380498886, lr: 1e-05
2023-12-21 08:54:11 INFO     	 * (global step 12750: loss: 0.3333849497139454, lr: 1e-05
2023-12-21 08:54:26 INFO     	 * (global step 12800: loss: 0.3819800056517124, lr: 1e-05
2023-12-21 08:54:42 INFO     	 * (global step 12850: loss: 0.2484569549560547, lr: 1e-05
2023-12-21 08:54:57 INFO     	 * (global step 12900: loss: 0.3630107641220093, lr: 1e-05
2023-12-21 08:55:13 INFO     	 * (global step 12950: loss: 0.2854120284318924, lr: 1e-05
2023-12-21 08:55:28 INFO     	 * (global step 13000: loss: 0.42612699419260025, lr: 1e-05
2023-12-21 08:55:44 INFO     	 * (global step 13050: loss: 0.26653333753347397, lr: 1e-05
2023-12-21 08:55:59 INFO     	 * (global step 13100: loss: 0.34353170543909073, lr: 1e-05
2023-12-21 08:56:15 INFO     	 * (global step 13150: loss: 0.3118767887353897, lr: 1e-05
2023-12-21 08:56:30 INFO     	 * (global step 13200: loss: 0.27913954854011536, lr: 1e-05
2023-12-21 08:56:46 INFO     	 * (global step 13250: loss: 0.33859487995505333, lr: 1e-05
2023-12-21 08:57:01 INFO     	 * (global step 13300: loss: 0.4241417944431305, lr: 1e-05
2023-12-21 08:57:17 INFO     	 * (global step 13350: loss: 0.34183597937226295, lr: 1e-05
2023-12-21 08:57:32 INFO     	 * (global step 13400: loss: 0.3685329370200634, lr: 1e-05
2023-12-21 08:57:48 INFO     	 * (global step 13450: loss: 0.32387276738882065, lr: 1e-05
2023-12-21 08:58:04 INFO     	 * (global step 13500: loss: 0.27274438738822937, lr: 1e-05
2023-12-21 08:58:19 INFO     	 * (global step 13550: loss: 0.38735729455947876, lr: 1e-05
2023-12-21 08:58:35 INFO     	 * (global step 13600: loss: 0.3914475850760937, lr: 1e-05
2023-12-21 08:58:50 INFO     	 * (global step 13650: loss: 0.35766108334064484, lr: 1e-05
2023-12-21 08:59:06 INFO     	 * (global step 13700: loss: 0.41358959674835205, lr: 1e-05
2023-12-21 08:59:21 INFO     	 * (global step 13750: loss: 0.22365644946694374, lr: 1e-05
2023-12-21 08:59:37 INFO     	 * (global step 13800: loss: 0.3577532395720482, lr: 1e-05
2023-12-21 08:59:52 INFO     	 * (global step 13850: loss: 0.37686237692832947, lr: 1e-05
2023-12-21 09:00:08 INFO     	 * (global step 13900: loss: 0.3768468573689461, lr: 1e-05
2023-12-21 09:00:23 INFO     	 * (global step 13950: loss: 0.4365147054195404, lr: 1e-05
2023-12-21 09:00:39 INFO     	 * (global step 14000: loss: 0.3564700111746788, lr: 1e-05
2023-12-21 09:00:54 INFO     	 * (global step 14050: loss: 0.32340453192591667, lr: 1e-05
2023-12-21 09:01:10 INFO     	 * (global step 14100: loss: 0.4288737550377846, lr: 1e-05
2023-12-21 09:01:25 INFO     	 * (global step 14150: loss: 0.380720354616642, lr: 1e-05
2023-12-21 09:01:30 INFO     [epoch 5/15] average loss: 0.349, lr: 1e-05
2023-12-21 09:01:30 INFO     saving model related files
2023-12-21 09:01:30 INFO     saving model
2023-12-21 09:01:31 INFO     saving tokenizer
2023-12-21 09:01:31 INFO     saving optimizer
2023-12-21 09:01:32 INFO     remove old optimizer files
2023-12-21 09:01:42 INFO     	 * (global step 14200: loss: 0.39545178785920143, lr: 1e-05
2023-12-21 09:01:58 INFO     	 * (global step 14250: loss: 0.40650730580091476, lr: 1e-05
2023-12-21 09:02:13 INFO     	 * (global step 14300: loss: 0.3648250624537468, lr: 1e-05
2023-12-21 09:02:29 INFO     	 * (global step 14350: loss: 0.33321869373321533, lr: 1e-05
2023-12-21 09:02:44 INFO     	 * (global step 14400: loss: 0.4214532747864723, lr: 1e-05
2023-12-21 09:03:00 INFO     	 * (global step 14450: loss: 0.3012200631201267, lr: 1e-05
2023-12-21 09:03:15 INFO     	 * (global step 14500: loss: 0.44013237208127975, lr: 1e-05
2023-12-21 09:03:31 INFO     	 * (global step 14550: loss: 0.4076389744877815, lr: 1e-05
2023-12-21 09:03:46 INFO     	 * (global step 14600: loss: 0.32754790410399437, lr: 1e-05
2023-12-21 09:04:02 INFO     	 * (global step 14650: loss: 0.35394537448883057, lr: 1e-05
2023-12-21 09:04:18 INFO     	 * (global step 14700: loss: 0.2994518205523491, lr: 1e-05
2023-12-21 09:04:33 INFO     	 * (global step 14750: loss: 0.3825371339917183, lr: 1e-05
2023-12-21 09:04:49 INFO     	 * (global step 14800: loss: 0.3864148482680321, lr: 1e-05
2023-12-21 09:05:04 INFO     	 * (global step 14850: loss: 0.30775628983974457, lr: 1e-05
2023-12-21 09:05:20 INFO     	 * (global step 14900: loss: 0.287685614079237, lr: 1e-05
2023-12-21 09:05:35 INFO     	 * (global step 14950: loss: 0.44029367715120316, lr: 1e-05
2023-12-21 09:05:51 INFO     	 * (global step 15000: loss: 0.3962501585483551, lr: 1e-05
2023-12-21 09:06:06 INFO     	 * (global step 15050: loss: 0.35326574742794037, lr: 1e-05
2023-12-21 09:06:22 INFO     	 * (global step 15100: loss: 0.34413161873817444, lr: 1e-05
2023-12-21 09:06:37 INFO     	 * (global step 15150: loss: 0.26712873205542564, lr: 1e-05
2023-12-21 09:06:53 INFO     	 * (global step 15200: loss: 0.4275907836854458, lr: 1e-05
2023-12-21 09:07:08 INFO     	 * (global step 15250: loss: 0.23208773508667946, lr: 1e-05
2023-12-21 09:07:24 INFO     	 * (global step 15300: loss: 0.267328891903162, lr: 1e-05
2023-12-21 09:07:40 INFO     	 * (global step 15350: loss: 0.24962838739156723, lr: 1e-05
2023-12-21 09:07:55 INFO     	 * (global step 15400: loss: 0.3369436226785183, lr: 1e-05
2023-12-21 09:08:11 INFO     	 * (global step 15450: loss: 0.3337826728820801, lr: 1e-05
2023-12-21 09:08:26 INFO     	 * (global step 15500: loss: 0.3765576481819153, lr: 1e-05
2023-12-21 09:08:42 INFO     	 * (global step 15550: loss: 0.2792202942073345, lr: 1e-05
2023-12-21 09:08:57 INFO     	 * (global step 15600: loss: 0.307927493005991, lr: 1e-05
2023-12-21 09:09:13 INFO     	 * (global step 15650: loss: 0.3464125916361809, lr: 1e-05
2023-12-21 09:09:28 INFO     	 * (global step 15700: loss: 0.3022596798837185, lr: 1e-05
2023-12-21 09:09:43 INFO     	 * (global step 15750: loss: 0.34535063430666924, lr: 1e-05
2023-12-21 09:09:59 INFO     	 * (global step 15800: loss: 0.31950508058071136, lr: 1e-05
2023-12-21 09:10:14 INFO     	 * (global step 15850: loss: 0.39688678085803986, lr: 1e-05
2023-12-21 09:10:30 INFO     	 * (global step 15900: loss: 0.3619823679327965, lr: 1e-05
2023-12-21 09:10:45 INFO     	 * (global step 15950: loss: 0.2974989265203476, lr: 1e-05
2023-12-21 09:11:01 INFO     	 * (global step 16000: loss: 0.250683780759573, lr: 1e-05
2023-12-21 09:11:16 INFO     	 * (global step 16050: loss: 0.29460591822862625, lr: 1e-05
2023-12-21 09:11:32 INFO     	 * (global step 16100: loss: 0.35250476747751236, lr: 1e-05
2023-12-21 09:11:47 INFO     	 * (global step 16150: loss: 0.3381119780242443, lr: 1e-05
2023-12-21 09:12:03 INFO     	 * (global step 16200: loss: 0.2735123559832573, lr: 1e-05
2023-12-21 09:12:18 INFO     	 * (global step 16250: loss: 0.3031870648264885, lr: 1e-05
2023-12-21 09:12:34 INFO     	 * (global step 16300: loss: 0.32651861011981964, lr: 1e-05
2023-12-21 09:12:49 INFO     	 * (global step 16350: loss: 0.3640820160508156, lr: 1e-05
2023-12-21 09:13:05 INFO     	 * (global step 16400: loss: 0.2631087154150009, lr: 1e-05
2023-12-21 09:13:20 INFO     	 * (global step 16450: loss: 0.379646971821785, lr: 1e-05
2023-12-21 09:13:36 INFO     	 * (global step 16500: loss: 0.26138048991560936, lr: 1e-05
2023-12-21 09:13:44 INFO     [epoch 6/15] average loss: 0.342, lr: 1e-05
2023-12-21 09:13:44 INFO     saving model related files
2023-12-21 09:13:44 INFO     saving model
2023-12-21 09:13:45 INFO     saving tokenizer
2023-12-21 09:13:45 INFO     saving optimizer
2023-12-21 09:13:46 INFO     remove old optimizer files
2023-12-21 09:13:53 INFO     	 * (global step 16550: loss: 0.3870059922337532, lr: 1e-05
2023-12-21 09:14:08 INFO     	 * (global step 16600: loss: 0.2538530305027962, lr: 1e-05
2023-12-21 09:14:24 INFO     	 * (global step 16650: loss: 0.30177130177617073, lr: 1e-05
2023-12-21 09:14:39 INFO     	 * (global step 16700: loss: 0.3530140668153763, lr: 1e-05
2023-12-21 09:14:55 INFO     	 * (global step 16750: loss: 0.37439773231744766, lr: 1e-05
2023-12-21 09:15:10 INFO     	 * (global step 16800: loss: 0.3895663395524025, lr: 1e-05
2023-12-21 09:15:26 INFO     	 * (global step 16850: loss: 0.3198334276676178, lr: 1e-05
2023-12-21 09:15:41 INFO     	 * (global step 16900: loss: 0.29472240060567856, lr: 1e-05
2023-12-21 09:15:57 INFO     	 * (global step 16950: loss: 0.4273654744029045, lr: 1e-05
2023-12-21 09:16:13 INFO     	 * (global step 17000: loss: 0.2892494387924671, lr: 1e-05
2023-12-21 09:16:28 INFO     	 * (global step 17050: loss: 0.4437895342707634, lr: 1e-05
2023-12-21 09:16:44 INFO     	 * (global step 17100: loss: 0.5123838111758232, lr: 1e-05
2023-12-21 09:16:59 INFO     	 * (global step 17150: loss: 0.32195188477635384, lr: 1e-05
2023-12-21 09:17:15 INFO     	 * (global step 17200: loss: 0.2672388516366482, lr: 1e-05
2023-12-21 09:17:30 INFO     	 * (global step 17250: loss: 0.4169175699353218, lr: 1e-05
2023-12-21 09:17:46 INFO     	 * (global step 17300: loss: 0.3507951721549034, lr: 1e-05
2023-12-21 09:18:01 INFO     	 * (global step 17350: loss: 0.5295219495892525, lr: 1e-05
2023-12-21 09:18:17 INFO     	 * (global step 17400: loss: 0.3097448796033859, lr: 1e-05
2023-12-21 09:18:32 INFO     	 * (global step 17450: loss: 0.27925514802336693, lr: 1e-05
2023-12-21 09:18:48 INFO     	 * (global step 17500: loss: 0.3087363764643669, lr: 1e-05
2023-12-21 09:19:03 INFO     	 * (global step 17550: loss: 0.3074798509478569, lr: 1e-05
2023-12-21 09:19:19 INFO     	 * (global step 17600: loss: 0.3371589258313179, lr: 1e-05
2023-12-21 09:19:34 INFO     	 * (global step 17650: loss: 0.34371813386678696, lr: 1e-05
2023-12-21 09:19:50 INFO     	 * (global step 17700: loss: 0.39021166414022446, lr: 1e-05
2023-12-21 09:20:05 INFO     	 * (global step 17750: loss: 0.25688377767801285, lr: 1e-05
2023-12-21 09:20:21 INFO     	 * (global step 17800: loss: 0.29261817783117294, lr: 1e-05
2023-12-21 09:20:36 INFO     	 * (global step 17850: loss: 0.30900637060403824, lr: 1e-05
2023-12-21 09:20:52 INFO     	 * (global step 17900: loss: 0.30728384107351303, lr: 1e-05
2023-12-21 09:21:07 INFO     	 * (global step 17950: loss: 0.32928790897130966, lr: 1e-05
2023-12-21 09:21:23 INFO     	 * (global step 18000: loss: 0.3618398495018482, lr: 1e-05
2023-12-21 09:21:39 INFO     	 * (global step 18050: loss: 0.3371916711330414, lr: 1e-05
2023-12-21 09:21:54 INFO     	 * (global step 18100: loss: 0.28145553544163704, lr: 1e-05
2023-12-21 09:22:10 INFO     	 * (global step 18150: loss: 0.34119177609682083, lr: 1e-05
2023-12-21 09:22:25 INFO     	 * (global step 18200: loss: 0.337535809725523, lr: 1e-05
2023-12-21 09:22:41 INFO     	 * (global step 18250: loss: 0.2189142443239689, lr: 1e-05
2023-12-21 09:22:56 INFO     	 * (global step 18300: loss: 0.4064112976193428, lr: 1e-05
2023-12-21 09:23:12 INFO     	 * (global step 18350: loss: 0.47391320765018463, lr: 1e-05
2023-12-21 09:23:27 INFO     	 * (global step 18400: loss: 0.3030265346169472, lr: 1e-05
2023-12-21 09:23:43 INFO     	 * (global step 18450: loss: 0.24496162682771683, lr: 1e-05
2023-12-21 09:23:58 INFO     	 * (global step 18500: loss: 0.43225768208503723, lr: 1e-05
2023-12-21 09:24:14 INFO     	 * (global step 18550: loss: 0.21901191025972366, lr: 1e-05
2023-12-21 09:24:29 INFO     	 * (global step 18600: loss: 0.30531424283981323, lr: 1e-05
2023-12-21 09:24:45 INFO     	 * (global step 18650: loss: 0.2775244750082493, lr: 1e-05
2023-12-21 09:25:00 INFO     	 * (global step 18700: loss: 0.45296962559223175, lr: 1e-05
2023-12-21 09:25:16 INFO     	 * (global step 18750: loss: 0.5031965002417564, lr: 1e-05
2023-12-21 09:25:31 INFO     	 * (global step 18800: loss: 0.31467561051249504, lr: 1e-05
2023-12-21 09:25:47 INFO     	 * (global step 18850: loss: 0.22647752426564693, lr: 1e-05
2023-12-21 09:25:59 INFO     [epoch 7/15] average loss: 0.336, lr: 1e-05
2023-12-21 09:25:59 INFO     saving model related files
2023-12-21 09:25:59 INFO     saving model
2023-12-21 09:25:59 INFO     saving tokenizer
2023-12-21 09:25:59 INFO     saving optimizer
2023-12-21 09:26:00 INFO     remove old optimizer files
2023-12-21 09:26:04 INFO     	 * (global step 18900: loss: 0.3686882443726063, lr: 1e-05
2023-12-21 09:26:20 INFO     	 * (global step 18950: loss: 0.3763706646859646, lr: 1e-05
2023-12-21 09:26:35 INFO     	 * (global step 19000: loss: 0.30443913117051125, lr: 1e-05
2023-12-21 09:26:51 INFO     	 * (global step 19050: loss: 0.3404683656990528, lr: 1e-05
2023-12-21 09:27:06 INFO     	 * (global step 19100: loss: 0.30017613992094994, lr: 1e-05
2023-12-21 09:27:22 INFO     	 * (global step 19150: loss: 0.3317159488797188, lr: 1e-05
2023-12-21 09:27:37 INFO     	 * (global step 19200: loss: 0.36850372329354286, lr: 1e-05
2023-12-21 09:27:52 INFO     	 * (global step 19250: loss: 0.30924927443265915, lr: 1e-05
2023-12-21 09:28:08 INFO     	 * (global step 19300: loss: 0.2964922785758972, lr: 1e-05
2023-12-21 09:28:23 INFO     	 * (global step 19350: loss: 0.2958991080522537, lr: 1e-05
2023-12-21 09:28:39 INFO     	 * (global step 19400: loss: 0.3387377969920635, lr: 1e-05
2023-12-21 09:28:54 INFO     	 * (global step 19450: loss: 0.34091804176568985, lr: 1e-05
2023-12-21 09:29:10 INFO     	 * (global step 19500: loss: 0.48103588074445724, lr: 1e-05
2023-12-21 09:29:25 INFO     	 * (global step 19550: loss: 0.26470213755965233, lr: 1e-05
2023-12-21 09:29:41 INFO     	 * (global step 19600: loss: 0.2851356863975525, lr: 1e-05
2023-12-21 09:29:56 INFO     	 * (global step 19650: loss: 0.28996358811855316, lr: 1e-05
2023-12-21 09:30:12 INFO     	 * (global step 19700: loss: 0.41418566927313805, lr: 1e-05
2023-12-21 09:30:27 INFO     	 * (global step 19750: loss: 0.2671463266015053, lr: 1e-05
2023-12-21 09:30:43 INFO     	 * (global step 19800: loss: 0.3027101084589958, lr: 1e-05
2023-12-21 09:30:58 INFO     	 * (global step 19850: loss: 0.33017006143927574, lr: 1e-05
2023-12-21 09:31:14 INFO     	 * (global step 19900: loss: 0.29978660121560097, lr: 1e-05
2023-12-21 09:31:29 INFO     	 * (global step 19950: loss: 0.3408556133508682, lr: 1e-05
2023-12-21 09:31:45 INFO     	 * (global step 20000: loss: 0.2952405549585819, lr: 1e-05
2023-12-21 09:32:00 INFO     	 * (global step 20050: loss: 0.2792309783399105, lr: 1e-05
2023-12-21 09:32:16 INFO     	 * (global step 20100: loss: 0.3532078489661217, lr: 1e-05
2023-12-21 09:32:31 INFO     	 * (global step 20150: loss: 0.3314530998468399, lr: 1e-05
2023-12-21 09:32:47 INFO     	 * (global step 20200: loss: 0.5480127744376659, lr: 1e-05
2023-12-21 09:33:02 INFO     	 * (global step 20250: loss: 0.36478104814887047, lr: 1e-05
2023-12-21 09:33:18 INFO     	 * (global step 20300: loss: 0.349499624222517, lr: 1e-05
2023-12-21 09:33:33 INFO     	 * (global step 20350: loss: 0.33665086328983307, lr: 1e-05
2023-12-21 09:33:49 INFO     	 * (global step 20400: loss: 0.273059967905283, lr: 1e-05
2023-12-21 09:34:04 INFO     	 * (global step 20450: loss: 0.4169511944055557, lr: 1e-05
2023-12-21 09:34:20 INFO     	 * (global step 20500: loss: 0.25121450796723366, lr: 1e-05
2023-12-21 09:34:35 INFO     	 * (global step 20550: loss: 0.3449825905263424, lr: 1e-05
2023-12-21 09:34:51 INFO     	 * (global step 20600: loss: 0.36136142909526825, lr: 1e-05
2023-12-21 09:35:06 INFO     	 * (global step 20650: loss: 0.2972612492740154, lr: 1e-05
2023-12-21 09:35:22 INFO     	 * (global step 20700: loss: 0.33888356760144234, lr: 1e-05
2023-12-21 09:35:37 INFO     	 * (global step 20750: loss: 0.32968607544898987, lr: 1e-05
2023-12-21 09:35:53 INFO     	 * (global step 20800: loss: 0.36933116614818573, lr: 1e-05
2023-12-21 09:36:08 INFO     	 * (global step 20850: loss: 0.33948975801467896, lr: 1e-05
2023-12-21 09:36:24 INFO     	 * (global step 20900: loss: 0.385256826877594, lr: 1e-05
2023-12-21 09:36:39 INFO     	 * (global step 20950: loss: 0.33394739776849747, lr: 1e-05
2023-12-21 09:36:55 INFO     	 * (global step 21000: loss: 0.33762677758932114, lr: 1e-05
2023-12-21 09:37:10 INFO     	 * (global step 21050: loss: 0.2562604695558548, lr: 1e-05
2023-12-21 09:37:25 INFO     	 * (global step 21100: loss: 0.27502989023923874, lr: 1e-05
2023-12-21 09:37:41 INFO     	 * (global step 21150: loss: 0.2903888635337353, lr: 1e-05
2023-12-21 09:37:56 INFO     	 * (global step 21200: loss: 0.36324431747198105, lr: 1e-05
2023-12-21 09:38:12 INFO     [epoch 8/15] average loss: 0.332, lr: 1e-05
2023-12-21 09:38:12 INFO     saving model related files
2023-12-21 09:38:12 INFO     saving model
2023-12-21 09:38:12 INFO     saving tokenizer
2023-12-21 09:38:12 INFO     saving optimizer
2023-12-21 09:38:13 INFO     remove old optimizer files
2023-12-21 09:38:14 INFO     	 * (global step 21250: loss: 0.40066593885421753, lr: 1e-05
2023-12-21 09:38:29 INFO     	 * (global step 21300: loss: 0.2691076397895813, lr: 1e-05
2023-12-21 09:38:44 INFO     	 * (global step 21350: loss: 0.3182792440056801, lr: 1e-05
2023-12-21 09:39:00 INFO     	 * (global step 21400: loss: 0.38893166929483414, lr: 1e-05
2023-12-21 09:39:15 INFO     	 * (global step 21450: loss: 0.42712049186229706, lr: 1e-05
2023-12-21 09:39:31 INFO     	 * (global step 21500: loss: 0.30217182263731956, lr: 1e-05
2023-12-21 09:39:47 INFO     	 * (global step 21550: loss: 0.36010686680674553, lr: 1e-05
2023-12-21 09:40:02 INFO     	 * (global step 21600: loss: 0.2851356454193592, lr: 1e-05
2023-12-21 09:40:18 INFO     	 * (global step 21650: loss: 0.3355678543448448, lr: 1e-05
2023-12-21 09:40:33 INFO     	 * (global step 21700: loss: 0.31602322310209274, lr: 1e-05
2023-12-21 09:40:48 INFO     	 * (global step 21750: loss: 0.27100199460983276, lr: 1e-05
2023-12-21 09:41:04 INFO     	 * (global step 21800: loss: 0.47231266647577286, lr: 1e-05
2023-12-21 09:41:19 INFO     	 * (global step 21850: loss: 0.32111791893839836, lr: 1e-05
2023-12-21 09:41:35 INFO     	 * (global step 21900: loss: 0.4417842999100685, lr: 1e-05
2023-12-21 09:41:50 INFO     	 * (global step 21950: loss: 0.46421560645103455, lr: 1e-05
2023-12-21 09:42:06 INFO     	 * (global step 22000: loss: 0.3832695260643959, lr: 1e-05
2023-12-21 09:42:21 INFO     	 * (global step 22050: loss: 0.41889194399118423, lr: 1e-05
2023-12-21 09:42:37 INFO     	 * (global step 22100: loss: 0.2919619530439377, lr: 1e-05
2023-12-21 09:42:52 INFO     	 * (global step 22150: loss: 0.3135584220290184, lr: 1e-05
2023-12-21 09:43:08 INFO     	 * (global step 22200: loss: 0.22166017815470695, lr: 1e-05
2023-12-21 09:43:23 INFO     	 * (global step 22250: loss: 0.3524651378393173, lr: 1e-05
2023-12-21 09:43:39 INFO     	 * (global step 22300: loss: 0.32984118163585663, lr: 1e-05
2023-12-21 09:43:54 INFO     	 * (global step 22350: loss: 0.3749644607305527, lr: 1e-05
2023-12-21 09:44:10 INFO     	 * (global step 22400: loss: 0.3565455973148346, lr: 1e-05
2023-12-21 09:44:25 INFO     	 * (global step 22450: loss: 0.295319389551878, lr: 1e-05
2023-12-21 09:44:41 INFO     	 * (global step 22500: loss: 0.2725228685885668, lr: 1e-05
2023-12-21 09:44:56 INFO     	 * (global step 22550: loss: 0.35827746987342834, lr: 1e-05
2023-12-21 09:45:12 INFO     	 * (global step 22600: loss: 0.38871999830007553, lr: 1e-05
2023-12-21 09:45:27 INFO     	 * (global step 22650: loss: 0.32310228049755096, lr: 1e-05
2023-12-21 09:45:43 INFO     	 * (global step 22700: loss: 0.28550436347723007, lr: 1e-05
2023-12-21 09:45:58 INFO     	 * (global step 22750: loss: 0.36162514984607697, lr: 1e-05
2023-12-21 09:46:13 INFO     	 * (global step 22800: loss: 0.2886006124317646, lr: 1e-05
2023-12-21 09:46:29 INFO     	 * (global step 22850: loss: 0.34960925579071045, lr: 1e-05
2023-12-21 09:46:44 INFO     	 * (global step 22900: loss: 0.2875555083155632, lr: 1e-05
2023-12-21 09:47:00 INFO     	 * (global step 22950: loss: 0.35502907633781433, lr: 1e-05
2023-12-21 09:47:15 INFO     	 * (global step 23000: loss: 0.3244573399424553, lr: 1e-05
2023-12-21 09:47:31 INFO     	 * (global step 23050: loss: 0.2806906960904598, lr: 1e-05
2023-12-21 09:47:46 INFO     	 * (global step 23100: loss: 0.3893502354621887, lr: 1e-05
2023-12-21 09:48:02 INFO     	 * (global step 23150: loss: 0.3669709041714668, lr: 1e-05
2023-12-21 09:48:17 INFO     	 * (global step 23200: loss: 0.341853603720665, lr: 1e-05
2023-12-21 09:48:33 INFO     	 * (global step 23250: loss: 0.31248895823955536, lr: 1e-05
2023-12-21 09:48:48 INFO     	 * (global step 23300: loss: 0.27805686369538307, lr: 1e-05
2023-12-21 09:49:04 INFO     	 * (global step 23350: loss: 0.362852368503809, lr: 1e-05
2023-12-21 09:49:19 INFO     	 * (global step 23400: loss: 0.3649032637476921, lr: 1e-05
2023-12-21 09:49:35 INFO     	 * (global step 23450: loss: 0.3069284036755562, lr: 1e-05
2023-12-21 09:49:50 INFO     	 * (global step 23500: loss: 0.3050190396606922, lr: 1e-05
2023-12-21 09:50:06 INFO     	 * (global step 23550: loss: 0.3878449499607086, lr: 1e-05
2023-12-21 09:50:21 INFO     	 * (global step 23600: loss: 0.2744550108909607, lr: 1e-05
2023-12-21 09:50:24 INFO     [epoch 9/15] average loss: 0.328, lr: 1e-05
2023-12-21 09:50:24 INFO     saving model related files
2023-12-21 09:50:24 INFO     saving model
2023-12-21 09:50:25 INFO     saving tokenizer
2023-12-21 09:50:25 INFO     saving optimizer
2023-12-21 09:50:26 INFO     remove old optimizer files
2023-12-21 09:50:26 INFO     complete training: model ckpt was saved at small_combined_trained_ckpt/model_vhyoja
2023-12-21 09:50:26 INFO     ## 1st RUN: Configuration 11/12 ##
2023-12-21 09:50:26 INFO     initialize model trainer
2023-12-21 09:50:26 INFO     initialize checkpoint at small_combined_trained_ckpt/model_nrudfu
2023-12-21 09:50:26 INFO     hyperparameters
2023-12-21 09:50:26 INFO     	 * dataset_path: StellarMilk/squad_newsqa
2023-12-21 09:50:26 INFO     	 * dataset_name: default
2023-12-21 09:50:26 INFO     	 * input_types: ['paragraph']
2023-12-21 09:50:26 INFO     	 * output_types: ['questions_answers']
2023-12-21 09:50:26 INFO     	 * prefix_types: ['qag']
2023-12-21 09:50:26 INFO     	 * model: t5-small
2023-12-21 09:50:26 INFO     	 * max_length: 512
2023-12-21 09:50:26 INFO     	 * max_length_output: 512
2023-12-21 09:50:26 INFO     	 * epoch: 15
2023-12-21 09:50:26 INFO     	 * batch: 2
2023-12-21 09:50:26 INFO     	 * lr: 1e-05
2023-12-21 09:50:26 INFO     	 * fp16: False
2023-12-21 09:50:26 INFO     	 * random_seed: 1
2023-12-21 09:50:26 INFO     	 * gradient_accumulation_steps: 2
2023-12-21 09:50:26 INFO     	 * label_smoothing: 0.0
2023-12-21 09:50:26 INFO     initialize checkpoint with t5-small
2023-12-21 09:50:27 INFO     use spaCy answer extraction model: positionrank
2023-12-21 09:50:28 INFO     Model `t5-small`
2023-12-21 09:50:28 INFO     	 * Num of GPU in use: 1
2023-12-21 09:50:28 INFO     	 * Prefix: True
2023-12-21 09:50:28 INFO     	 * Language: en (ignore at the training phase)
2023-12-21 09:50:28 INFO     dataset preprocessing
2023-12-21 09:50:29 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/squad_newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-21 09:50:30 INFO     start model training
2023-12-21 09:50:38 INFO     	 * (global step 50: loss: 6.4232892990112305, lr: 1e-05
2023-12-21 09:50:46 INFO     	 * (global step 100: loss: 2.088875472545624, lr: 1e-05
2023-12-21 09:50:54 INFO     	 * (global step 150: loss: 1.9978147745132446, lr: 1e-05
2023-12-21 09:51:02 INFO     	 * (global step 200: loss: 1.637089192867279, lr: 1e-05
2023-12-21 09:51:10 INFO     	 * (global step 250: loss: 1.3043537735939026, lr: 1e-05
2023-12-21 09:51:18 INFO     	 * (global step 300: loss: 1.2349849939346313, lr: 1e-05
2023-12-21 09:51:26 INFO     	 * (global step 350: loss: 1.2145938873291016, lr: 1e-05
2023-12-21 09:51:34 INFO     	 * (global step 400: loss: 1.3204255104064941, lr: 1e-05
2023-12-21 09:51:42 INFO     	 * (global step 450: loss: 0.95344677567482, lr: 1e-05
2023-12-21 09:51:50 INFO     	 * (global step 500: loss: 0.9689079225063324, lr: 1e-05
2023-12-21 09:51:58 INFO     	 * (global step 550: loss: 1.0554715394973755, lr: 1e-05
2023-12-21 09:52:06 INFO     	 * (global step 600: loss: 0.6254541575908661, lr: 1e-05
2023-12-21 09:52:14 INFO     	 * (global step 650: loss: 0.6661103665828705, lr: 1e-05
2023-12-21 09:52:22 INFO     	 * (global step 700: loss: 0.8425436019897461, lr: 1e-05
2023-12-21 09:52:30 INFO     	 * (global step 750: loss: 0.6061383187770844, lr: 1e-05
2023-12-21 09:52:38 INFO     	 * (global step 800: loss: 0.6092453896999359, lr: 1e-05
2023-12-21 09:52:46 INFO     	 * (global step 850: loss: 0.4918735772371292, lr: 1e-05
2023-12-21 09:52:54 INFO     	 * (global step 900: loss: 0.6430191397666931, lr: 1e-05
2023-12-21 09:53:02 INFO     	 * (global step 950: loss: 0.5847509205341339, lr: 1e-05
2023-12-21 09:53:10 INFO     	 * (global step 1000: loss: 0.530880481004715, lr: 1e-05
2023-12-21 09:53:18 INFO     	 * (global step 1050: loss: 0.5511801838874817, lr: 1e-05
2023-12-21 09:53:26 INFO     	 * (global step 1100: loss: 0.7088213264942169, lr: 1e-05
2023-12-21 09:53:34 INFO     	 * (global step 1150: loss: 0.587779626250267, lr: 1e-05
2023-12-21 09:53:42 INFO     	 * (global step 1200: loss: 0.7090177237987518, lr: 1e-05
2023-12-21 09:53:50 INFO     	 * (global step 1250: loss: 0.6959192454814911, lr: 1e-05
2023-12-21 09:53:58 INFO     	 * (global step 1300: loss: 0.5356675982475281, lr: 1e-05
2023-12-21 09:54:06 INFO     	 * (global step 1350: loss: 0.6929364800453186, lr: 1e-05
2023-12-21 09:54:14 INFO     	 * (global step 1400: loss: 0.5955376327037811, lr: 1e-05
2023-12-21 09:54:22 INFO     	 * (global step 1450: loss: 0.7399693727493286, lr: 1e-05
2023-12-21 09:54:30 INFO     	 * (global step 1500: loss: 0.5671635270118713, lr: 1e-05
2023-12-21 09:54:38 INFO     	 * (global step 1550: loss: 0.479174941778183, lr: 1e-05
2023-12-21 09:54:46 INFO     	 * (global step 1600: loss: 0.5614080727100372, lr: 1e-05
2023-12-21 09:54:54 INFO     	 * (global step 1650: loss: 0.4835606962442398, lr: 1e-05
2023-12-21 09:55:02 INFO     	 * (global step 1700: loss: 0.5728875696659088, lr: 1e-05
2023-12-21 09:55:10 INFO     	 * (global step 1750: loss: 0.43463337421417236, lr: 1e-05
2023-12-21 09:55:18 INFO     	 * (global step 1800: loss: 0.4890117794275284, lr: 1e-05
2023-12-21 09:55:26 INFO     	 * (global step 1850: loss: 0.6798700839281082, lr: 1e-05
2023-12-21 09:55:34 INFO     	 * (global step 1900: loss: 0.3856533020734787, lr: 1e-05
2023-12-21 09:55:42 INFO     	 * (global step 1950: loss: 0.5006202757358551, lr: 1e-05
2023-12-21 09:55:50 INFO     	 * (global step 2000: loss: 0.4927281588315964, lr: 1e-05
2023-12-21 09:55:58 INFO     	 * (global step 2050: loss: 0.47885483503341675, lr: 1e-05
2023-12-21 09:56:06 INFO     	 * (global step 2100: loss: 0.482559397816658, lr: 1e-05
2023-12-21 09:56:14 INFO     	 * (global step 2150: loss: 0.5871424823999405, lr: 1e-05
2023-12-21 09:56:22 INFO     	 * (global step 2200: loss: 0.4970458447933197, lr: 1e-05
2023-12-21 09:56:30 INFO     	 * (global step 2250: loss: 0.541336327791214, lr: 1e-05
2023-12-21 09:56:38 INFO     	 * (global step 2300: loss: 0.5081181675195694, lr: 1e-05
2023-12-21 09:56:46 INFO     	 * (global step 2350: loss: 0.3977254182100296, lr: 1e-05
2023-12-21 09:56:54 INFO     	 * (global step 2400: loss: 0.49083784222602844, lr: 1e-05
2023-12-21 09:57:02 INFO     	 * (global step 2450: loss: 0.3835391253232956, lr: 1e-05
2023-12-21 09:57:10 INFO     	 * (global step 2500: loss: 0.4440813511610031, lr: 1e-05
2023-12-21 09:57:18 INFO     	 * (global step 2550: loss: 0.7948910892009735, lr: 1e-05
2023-12-21 09:57:26 INFO     	 * (global step 2600: loss: 0.5770972222089767, lr: 1e-05
2023-12-21 09:57:34 INFO     	 * (global step 2650: loss: 0.5697242021560669, lr: 1e-05
2023-12-21 09:57:42 INFO     	 * (global step 2700: loss: 0.3966704159975052, lr: 1e-05
2023-12-21 09:57:50 INFO     	 * (global step 2750: loss: 0.5407078564167023, lr: 1e-05
2023-12-21 09:57:58 INFO     	 * (global step 2800: loss: 0.513765960931778, lr: 1e-05
2023-12-21 09:58:06 INFO     	 * (global step 2850: loss: 0.3261433392763138, lr: 1e-05
2023-12-21 09:58:14 INFO     	 * (global step 2900: loss: 0.4823833703994751, lr: 1e-05
2023-12-21 09:58:22 INFO     	 * (global step 2950: loss: 0.28255966305732727, lr: 1e-05
2023-12-21 09:58:30 INFO     	 * (global step 3000: loss: 0.7039917707443237, lr: 1e-05
2023-12-21 09:58:38 INFO     	 * (global step 3050: loss: 0.4528759717941284, lr: 1e-05
2023-12-21 09:58:46 INFO     	 * (global step 3100: loss: 0.44925442337989807, lr: 1e-05
2023-12-21 09:58:54 INFO     	 * (global step 3150: loss: 0.43330471217632294, lr: 1e-05
2023-12-21 09:59:02 INFO     	 * (global step 3200: loss: 0.39464761316776276, lr: 1e-05
2023-12-21 09:59:10 INFO     	 * (global step 3250: loss: 0.4428756833076477, lr: 1e-05
2023-12-21 09:59:18 INFO     	 * (global step 3300: loss: 0.5975640416145325, lr: 1e-05
2023-12-21 09:59:25 INFO     	 * (global step 3350: loss: 0.40235063433647156, lr: 1e-05
2023-12-21 09:59:33 INFO     	 * (global step 3400: loss: 0.41888926923274994, lr: 1e-05
2023-12-21 09:59:41 INFO     	 * (global step 3450: loss: 0.5095836967229843, lr: 1e-05
2023-12-21 09:59:49 INFO     	 * (global step 3500: loss: 0.3873753845691681, lr: 1e-05
2023-12-21 09:59:57 INFO     	 * (global step 3550: loss: 0.6348569989204407, lr: 1e-05
2023-12-21 10:00:05 INFO     	 * (global step 3600: loss: 0.476227268576622, lr: 1e-05
2023-12-21 10:00:13 INFO     	 * (global step 3650: loss: 0.5212833732366562, lr: 1e-05
2023-12-21 10:00:21 INFO     	 * (global step 3700: loss: 0.40286439657211304, lr: 1e-05
2023-12-21 10:00:29 INFO     	 * (global step 3750: loss: 0.4148622304201126, lr: 1e-05
2023-12-21 10:00:37 INFO     	 * (global step 3800: loss: 0.3940749317407608, lr: 1e-05
2023-12-21 10:00:45 INFO     	 * (global step 3850: loss: 0.24359028041362762, lr: 1e-05
2023-12-21 10:00:53 INFO     	 * (global step 3900: loss: 0.4864963889122009, lr: 1e-05
2023-12-21 10:01:01 INFO     	 * (global step 3950: loss: 0.3276425451040268, lr: 1e-05
2023-12-21 10:01:09 INFO     	 * (global step 4000: loss: 0.30731500685214996, lr: 1e-05
2023-12-21 10:01:17 INFO     	 * (global step 4050: loss: 0.3139745593070984, lr: 1e-05
2023-12-21 10:01:25 INFO     	 * (global step 4100: loss: 0.41745994985103607, lr: 1e-05
2023-12-21 10:01:33 INFO     	 * (global step 4150: loss: 0.322064608335495, lr: 1e-05
2023-12-21 10:01:41 INFO     	 * (global step 4200: loss: 0.3584689646959305, lr: 1e-05
2023-12-21 10:01:49 INFO     	 * (global step 4250: loss: 0.4410828799009323, lr: 1e-05
2023-12-21 10:01:57 INFO     	 * (global step 4300: loss: 0.5171850323677063, lr: 1e-05
2023-12-21 10:02:05 INFO     	 * (global step 4350: loss: 0.34500741213560104, lr: 1e-05
2023-12-21 10:02:13 INFO     	 * (global step 4400: loss: 0.4605732262134552, lr: 1e-05
2023-12-21 10:02:21 INFO     	 * (global step 4450: loss: 0.47170720994472504, lr: 1e-05
2023-12-21 10:02:29 INFO     	 * (global step 4500: loss: 0.34172680974006653, lr: 1e-05
2023-12-21 10:02:37 INFO     	 * (global step 4550: loss: 0.3584465757012367, lr: 1e-05
2023-12-21 10:02:45 INFO     	 * (global step 4600: loss: 0.4129214882850647, lr: 1e-05
2023-12-21 10:02:53 INFO     	 * (global step 4650: loss: 0.45520950853824615, lr: 1e-05
2023-12-21 10:03:01 INFO     	 * (global step 4700: loss: 0.4120926856994629, lr: 1e-05
2023-12-21 10:03:05 INFO     [epoch 0/15] average loss: 0.719, lr: 1e-05
2023-12-21 10:03:05 INFO     saving model related files
2023-12-21 10:03:05 INFO     saving model
2023-12-21 10:03:05 INFO     saving tokenizer
2023-12-21 10:03:05 INFO     saving optimizer
2023-12-21 10:03:06 INFO     remove old optimizer files
2023-12-21 10:03:11 INFO     	 * (global step 4750: loss: 0.5252730846405029, lr: 1e-05
2023-12-21 10:03:19 INFO     	 * (global step 4800: loss: 0.3826598823070526, lr: 1e-05
2023-12-21 10:03:27 INFO     	 * (global step 4850: loss: 0.3365985304117203, lr: 1e-05
2023-12-21 10:03:35 INFO     	 * (global step 4900: loss: 0.4753500819206238, lr: 1e-05
2023-12-21 10:03:43 INFO     	 * (global step 4950: loss: 0.42259135842323303, lr: 1e-05
2023-12-21 10:03:51 INFO     	 * (global step 5000: loss: 0.2933473289012909, lr: 1e-05
2023-12-21 10:03:59 INFO     	 * (global step 5050: loss: 0.45100632309913635, lr: 1e-05
2023-12-21 10:04:07 INFO     	 * (global step 5100: loss: 0.4001987874507904, lr: 1e-05
2023-12-21 10:04:15 INFO     	 * (global step 5150: loss: 0.29196852445602417, lr: 1e-05
2023-12-21 10:04:23 INFO     	 * (global step 5200: loss: 0.3791603296995163, lr: 1e-05
2023-12-21 10:04:31 INFO     	 * (global step 5250: loss: 0.3164922147989273, lr: 1e-05
2023-12-21 10:04:39 INFO     	 * (global step 5300: loss: 0.411502480506897, lr: 1e-05
2023-12-21 10:04:47 INFO     	 * (global step 5350: loss: 0.5013992935419083, lr: 1e-05
2023-12-21 10:04:55 INFO     	 * (global step 5400: loss: 0.4608525484800339, lr: 1e-05
2023-12-21 10:05:03 INFO     	 * (global step 5450: loss: 0.774744987487793, lr: 1e-05
2023-12-21 10:05:11 INFO     	 * (global step 5500: loss: 0.3351118713617325, lr: 1e-05
2023-12-21 10:05:18 INFO     	 * (global step 5550: loss: 0.32587721943855286, lr: 1e-05
2023-12-21 10:05:26 INFO     	 * (global step 5600: loss: 0.5460112690925598, lr: 1e-05
2023-12-21 10:05:34 INFO     	 * (global step 5650: loss: 0.2639656066894531, lr: 1e-05
2023-12-21 10:05:42 INFO     	 * (global step 5700: loss: 0.371078759431839, lr: 1e-05
2023-12-21 10:05:50 INFO     	 * (global step 5750: loss: 0.4357324093580246, lr: 1e-05
2023-12-21 10:05:58 INFO     	 * (global step 5800: loss: 0.34023046493530273, lr: 1e-05
2023-12-21 10:06:06 INFO     	 * (global step 5850: loss: 0.3831135481595993, lr: 1e-05
2023-12-21 10:06:14 INFO     	 * (global step 5900: loss: 0.30881233513355255, lr: 1e-05
2023-12-21 10:06:22 INFO     	 * (global step 5950: loss: 0.4923167824745178, lr: 1e-05
2023-12-21 10:06:30 INFO     	 * (global step 6000: loss: 0.40480343997478485, lr: 1e-05
2023-12-21 10:06:38 INFO     	 * (global step 6050: loss: 0.3592247813940048, lr: 1e-05
2023-12-21 10:06:46 INFO     	 * (global step 6100: loss: 0.2991103529930115, lr: 1e-05
2023-12-21 10:06:54 INFO     	 * (global step 6150: loss: 0.3811469227075577, lr: 1e-05
2023-12-21 10:07:02 INFO     	 * (global step 6200: loss: 0.46398523449897766, lr: 1e-05
2023-12-21 10:07:10 INFO     	 * (global step 6250: loss: 0.4127163141965866, lr: 1e-05
2023-12-21 10:07:18 INFO     	 * (global step 6300: loss: 0.7506860494613647, lr: 1e-05
2023-12-21 10:07:26 INFO     	 * (global step 6350: loss: 0.23432444781064987, lr: 1e-05
2023-12-21 10:07:34 INFO     	 * (global step 6400: loss: 0.3578914999961853, lr: 1e-05
2023-12-21 10:07:42 INFO     	 * (global step 6450: loss: 0.314742736518383, lr: 1e-05
2023-12-21 10:07:50 INFO     	 * (global step 6500: loss: 0.3099888414144516, lr: 1e-05
2023-12-21 10:07:58 INFO     	 * (global step 6550: loss: 0.21539147943258286, lr: 1e-05
2023-12-21 10:08:06 INFO     	 * (global step 6600: loss: 0.37740960717201233, lr: 1e-05
2023-12-21 10:08:14 INFO     	 * (global step 6650: loss: 0.37159135937690735, lr: 1e-05
2023-12-21 10:08:22 INFO     	 * (global step 6700: loss: 0.3947765678167343, lr: 1e-05
2023-12-21 10:08:30 INFO     	 * (global step 6750: loss: 0.50481878221035, lr: 1e-05
2023-12-21 10:08:38 INFO     	 * (global step 6800: loss: 0.3558315634727478, lr: 1e-05
2023-12-21 10:08:46 INFO     	 * (global step 6850: loss: 0.7657380402088165, lr: 1e-05
2023-12-21 10:08:54 INFO     	 * (global step 6900: loss: 0.37421247363090515, lr: 1e-05
2023-12-21 10:09:02 INFO     	 * (global step 6950: loss: 0.32268884778022766, lr: 1e-05
2023-12-21 10:09:10 INFO     	 * (global step 7000: loss: 0.5277785211801529, lr: 1e-05
2023-12-21 10:09:18 INFO     	 * (global step 7050: loss: 0.37940774857997894, lr: 1e-05
2023-12-21 10:09:26 INFO     	 * (global step 7100: loss: 0.39832915365695953, lr: 1e-05
2023-12-21 10:09:34 INFO     	 * (global step 7150: loss: 0.3273954540491104, lr: 1e-05
2023-12-21 10:09:42 INFO     	 * (global step 7200: loss: 0.3378543108701706, lr: 1e-05
2023-12-21 10:09:50 INFO     	 * (global step 7250: loss: 0.373934268951416, lr: 1e-05
2023-12-21 10:09:58 INFO     	 * (global step 7300: loss: 0.36641743779182434, lr: 1e-05
2023-12-21 10:10:06 INFO     	 * (global step 7350: loss: 0.2461131066083908, lr: 1e-05
2023-12-21 10:10:14 INFO     	 * (global step 7400: loss: 0.5249754339456558, lr: 1e-05
2023-12-21 10:10:22 INFO     	 * (global step 7450: loss: 0.5196754783391953, lr: 1e-05
2023-12-21 10:10:30 INFO     	 * (global step 7500: loss: 0.3777723014354706, lr: 1e-05
2023-12-21 10:10:38 INFO     	 * (global step 7550: loss: 0.46013985574245453, lr: 1e-05
2023-12-21 10:10:46 INFO     	 * (global step 7600: loss: 0.28495535999536514, lr: 1e-05
2023-12-21 10:10:54 INFO     	 * (global step 7650: loss: 0.3200099617242813, lr: 1e-05
2023-12-21 10:11:02 INFO     	 * (global step 7700: loss: 0.426260307431221, lr: 1e-05
2023-12-21 10:11:10 INFO     	 * (global step 7750: loss: 0.3457949683070183, lr: 1e-05
2023-12-21 10:11:18 INFO     	 * (global step 7800: loss: 0.34596124291419983, lr: 1e-05
2023-12-21 10:11:26 INFO     	 * (global step 7850: loss: 0.3949660509824753, lr: 1e-05
2023-12-21 10:11:34 INFO     	 * (global step 7900: loss: 0.44138482213020325, lr: 1e-05
2023-12-21 10:11:42 INFO     	 * (global step 7950: loss: 0.4112684428691864, lr: 1e-05
2023-12-21 10:11:50 INFO     	 * (global step 8000: loss: 0.2848645895719528, lr: 1e-05
2023-12-21 10:11:58 INFO     	 * (global step 8050: loss: 0.4575423374772072, lr: 1e-05
2023-12-21 10:12:06 INFO     	 * (global step 8100: loss: 0.4904688894748688, lr: 1e-05
2023-12-21 10:12:14 INFO     	 * (global step 8150: loss: 0.5790349841117859, lr: 1e-05
2023-12-21 10:12:22 INFO     	 * (global step 8200: loss: 0.27849969267845154, lr: 1e-05
2023-12-21 10:12:30 INFO     	 * (global step 8250: loss: 0.5253859609365463, lr: 1e-05
2023-12-21 10:12:38 INFO     	 * (global step 8300: loss: 0.3256200700998306, lr: 1e-05
2023-12-21 10:12:46 INFO     	 * (global step 8350: loss: 0.517067477107048, lr: 1e-05
2023-12-21 10:12:54 INFO     	 * (global step 8400: loss: 0.33872365951538086, lr: 1e-05
2023-12-21 10:13:02 INFO     	 * (global step 8450: loss: 0.4266412854194641, lr: 1e-05
2023-12-21 10:13:10 INFO     	 * (global step 8500: loss: 0.5248731821775436, lr: 1e-05
2023-12-21 10:13:18 INFO     	 * (global step 8550: loss: 0.5584128499031067, lr: 1e-05
2023-12-21 10:13:26 INFO     	 * (global step 8600: loss: 0.23737812787294388, lr: 1e-05
2023-12-21 10:13:34 INFO     	 * (global step 8650: loss: 0.5043866485357285, lr: 1e-05
2023-12-21 10:13:42 INFO     	 * (global step 8700: loss: 0.2176009714603424, lr: 1e-05
2023-12-21 10:13:50 INFO     	 * (global step 8750: loss: 0.26461297273635864, lr: 1e-05
2023-12-21 10:13:58 INFO     	 * (global step 8800: loss: 0.33295978605747223, lr: 1e-05
2023-12-21 10:14:06 INFO     	 * (global step 8850: loss: 0.445552334189415, lr: 1e-05
2023-12-21 10:14:14 INFO     	 * (global step 8900: loss: 0.2871156930923462, lr: 1e-05
2023-12-21 10:14:22 INFO     	 * (global step 8950: loss: 0.2858666703104973, lr: 1e-05
2023-12-21 10:14:30 INFO     	 * (global step 9000: loss: 0.31017347425222397, lr: 1e-05
2023-12-21 10:14:38 INFO     	 * (global step 9050: loss: 0.5627783536911011, lr: 1e-05
2023-12-21 10:14:46 INFO     	 * (global step 9100: loss: 0.4700666218996048, lr: 1e-05
2023-12-21 10:14:54 INFO     	 * (global step 9150: loss: 0.3182470500469208, lr: 1e-05
2023-12-21 10:15:02 INFO     	 * (global step 9200: loss: 0.26757345348596573, lr: 1e-05
2023-12-21 10:15:10 INFO     	 * (global step 9250: loss: 0.6677846014499664, lr: 1e-05
2023-12-21 10:15:18 INFO     	 * (global step 9300: loss: 0.36962205171585083, lr: 1e-05
2023-12-21 10:15:26 INFO     	 * (global step 9350: loss: 0.45601484179496765, lr: 1e-05
2023-12-21 10:15:34 INFO     	 * (global step 9400: loss: 0.37825043499469757, lr: 1e-05
2023-12-21 10:15:41 INFO     [epoch 1/15] average loss: 0.391, lr: 1e-05
2023-12-21 10:15:41 INFO     saving model related files
2023-12-21 10:15:41 INFO     saving model
2023-12-21 10:15:41 INFO     saving tokenizer
2023-12-21 10:15:41 INFO     saving optimizer
2023-12-21 10:15:42 INFO     remove old optimizer files
2023-12-21 10:15:43 INFO     	 * (global step 9450: loss: 0.4531451612710953, lr: 1e-05
2023-12-21 10:15:51 INFO     	 * (global step 9500: loss: 0.3187858760356903, lr: 1e-05
2023-12-21 10:15:59 INFO     	 * (global step 9550: loss: 0.4380260407924652, lr: 1e-05
2023-12-21 10:16:07 INFO     	 * (global step 9600: loss: 0.3151218146085739, lr: 1e-05
2023-12-21 10:16:15 INFO     	 * (global step 9650: loss: 0.28942449390888214, lr: 1e-05
2023-12-21 10:16:23 INFO     	 * (global step 9700: loss: 0.2725445032119751, lr: 1e-05
2023-12-21 10:16:31 INFO     	 * (global step 9750: loss: 0.46202296018600464, lr: 1e-05
2023-12-21 10:16:39 INFO     	 * (global step 9800: loss: 0.2924950420856476, lr: 1e-05
2023-12-21 10:16:47 INFO     	 * (global step 9850: loss: 0.30788160115480423, lr: 1e-05
2023-12-21 10:16:55 INFO     	 * (global step 9900: loss: 0.23688708245754242, lr: 1e-05
2023-12-21 10:17:03 INFO     	 * (global step 9950: loss: 0.3545813113451004, lr: 1e-05
2023-12-21 10:17:11 INFO     	 * (global step 10000: loss: 0.2842072695493698, lr: 1e-05
2023-12-21 10:17:19 INFO     	 * (global step 10050: loss: 0.26206573843955994, lr: 1e-05
2023-12-21 10:17:27 INFO     	 * (global step 10100: loss: 0.3443772718310356, lr: 1e-05
2023-12-21 10:17:35 INFO     	 * (global step 10150: loss: 0.3436525762081146, lr: 1e-05
2023-12-21 10:17:43 INFO     	 * (global step 10200: loss: 0.34282585978507996, lr: 1e-05
2023-12-21 10:17:51 INFO     	 * (global step 10250: loss: 0.5632565766572952, lr: 1e-05
2023-12-21 10:17:59 INFO     	 * (global step 10300: loss: 0.4761319160461426, lr: 1e-05
2023-12-21 10:18:06 INFO     	 * (global step 10350: loss: 0.4567737728357315, lr: 1e-05
2023-12-21 10:18:14 INFO     	 * (global step 10400: loss: 0.36199021339416504, lr: 1e-05
2023-12-21 10:18:22 INFO     	 * (global step 10450: loss: 0.34009358286857605, lr: 1e-05
2023-12-21 10:18:30 INFO     	 * (global step 10500: loss: 0.3190390169620514, lr: 1e-05
2023-12-21 10:18:38 INFO     	 * (global step 10550: loss: 0.3804551213979721, lr: 1e-05
2023-12-21 10:18:46 INFO     	 * (global step 10600: loss: 0.35036584734916687, lr: 1e-05
2023-12-21 10:18:54 INFO     	 * (global step 10650: loss: 0.3857780247926712, lr: 1e-05
2023-12-21 10:19:02 INFO     	 * (global step 10700: loss: 0.309002161026001, lr: 1e-05
2023-12-21 10:19:10 INFO     	 * (global step 10750: loss: 0.4316401183605194, lr: 1e-05
2023-12-21 10:19:18 INFO     	 * (global step 10800: loss: 0.42713993787765503, lr: 1e-05
2023-12-21 10:19:26 INFO     	 * (global step 10850: loss: 0.3148060739040375, lr: 1e-05
2023-12-21 10:19:34 INFO     	 * (global step 10900: loss: 0.3625479191541672, lr: 1e-05
2023-12-21 10:19:42 INFO     	 * (global step 10950: loss: 0.34087763726711273, lr: 1e-05
2023-12-21 10:19:50 INFO     	 * (global step 11000: loss: 0.6141380071640015, lr: 1e-05
2023-12-21 10:19:58 INFO     	 * (global step 11050: loss: 0.3629700243473053, lr: 1e-05
2023-12-21 10:20:06 INFO     	 * (global step 11100: loss: 0.6240962892770767, lr: 1e-05
2023-12-21 10:20:14 INFO     	 * (global step 11150: loss: 0.37079788744449615, lr: 1e-05
2023-12-21 10:20:22 INFO     	 * (global step 11200: loss: 0.28299538791179657, lr: 1e-05
2023-12-21 10:20:30 INFO     	 * (global step 11250: loss: 0.2253548949956894, lr: 1e-05
2023-12-21 10:20:38 INFO     	 * (global step 11300: loss: 0.375461146235466, lr: 1e-05
2023-12-21 10:20:46 INFO     	 * (global step 11350: loss: 0.3206861764192581, lr: 1e-05
2023-12-21 10:20:54 INFO     	 * (global step 11400: loss: 0.26135334372520447, lr: 1e-05
2023-12-21 10:21:02 INFO     	 * (global step 11450: loss: 0.31975386291742325, lr: 1e-05
2023-12-21 10:21:10 INFO     	 * (global step 11500: loss: 0.2748119682073593, lr: 1e-05
2023-12-21 10:21:18 INFO     	 * (global step 11550: loss: 0.5134679824113846, lr: 1e-05
2023-12-21 10:21:26 INFO     	 * (global step 11600: loss: 0.4384906738996506, lr: 1e-05
2023-12-21 10:21:34 INFO     	 * (global step 11650: loss: 0.3082234859466553, lr: 1e-05
2023-12-21 10:21:42 INFO     	 * (global step 11700: loss: 0.5400221794843674, lr: 1e-05
2023-12-21 10:21:50 INFO     	 * (global step 11750: loss: 0.3063272684812546, lr: 1e-05
2023-12-21 10:21:58 INFO     	 * (global step 11800: loss: 0.23331131041049957, lr: 1e-05
2023-12-21 10:22:06 INFO     	 * (global step 11850: loss: 0.35633572190999985, lr: 1e-05
2023-12-21 10:22:14 INFO     	 * (global step 11900: loss: 0.3992500975728035, lr: 1e-05
2023-12-21 10:22:22 INFO     	 * (global step 11950: loss: 0.37297476828098297, lr: 1e-05
2023-12-21 10:22:30 INFO     	 * (global step 12000: loss: 0.3068213611841202, lr: 1e-05
2023-12-21 10:22:38 INFO     	 * (global step 12050: loss: 0.2923338860273361, lr: 1e-05
2023-12-21 10:22:46 INFO     	 * (global step 12100: loss: 0.3920630067586899, lr: 1e-05
2023-12-21 10:22:54 INFO     	 * (global step 12150: loss: 0.42482928931713104, lr: 1e-05
2023-12-21 10:23:02 INFO     	 * (global step 12200: loss: 0.3558291792869568, lr: 1e-05
2023-12-21 10:23:10 INFO     	 * (global step 12250: loss: 0.34330980479717255, lr: 1e-05
2023-12-21 10:23:18 INFO     	 * (global step 12300: loss: 0.29130419343709946, lr: 1e-05
2023-12-21 10:23:25 INFO     	 * (global step 12350: loss: 0.4097893089056015, lr: 1e-05
2023-12-21 10:23:33 INFO     	 * (global step 12400: loss: 0.7629024535417557, lr: 1e-05
2023-12-21 10:23:41 INFO     	 * (global step 12450: loss: 0.23997561633586884, lr: 1e-05
2023-12-21 10:23:49 INFO     	 * (global step 12500: loss: 0.4246405363082886, lr: 1e-05
2023-12-21 10:23:57 INFO     	 * (global step 12550: loss: 0.4225628077983856, lr: 1e-05
2023-12-21 10:24:05 INFO     	 * (global step 12600: loss: 0.5093813836574554, lr: 1e-05
2023-12-21 10:24:13 INFO     	 * (global step 12650: loss: 0.3590068519115448, lr: 1e-05
2023-12-21 10:24:21 INFO     	 * (global step 12700: loss: 0.29026374220848083, lr: 1e-05
2023-12-21 10:24:29 INFO     	 * (global step 12750: loss: 0.38864728808403015, lr: 1e-05
2023-12-21 10:24:37 INFO     	 * (global step 12800: loss: 0.30776625871658325, lr: 1e-05
2023-12-21 10:24:45 INFO     	 * (global step 12850: loss: 0.30008190870285034, lr: 1e-05
2023-12-21 10:24:53 INFO     	 * (global step 12900: loss: 0.3104304149746895, lr: 1e-05
2023-12-21 10:25:01 INFO     	 * (global step 12950: loss: 0.42208926379680634, lr: 1e-05
2023-12-21 10:25:09 INFO     	 * (global step 13000: loss: 0.46363532543182373, lr: 1e-05
2023-12-21 10:25:17 INFO     	 * (global step 13050: loss: 0.2941528260707855, lr: 1e-05
2023-12-21 10:25:25 INFO     	 * (global step 13100: loss: 0.3790799155831337, lr: 1e-05
2023-12-21 10:25:33 INFO     	 * (global step 13150: loss: 0.40459302067756653, lr: 1e-05
2023-12-21 10:25:41 INFO     	 * (global step 13200: loss: 0.33899249136447906, lr: 1e-05
2023-12-21 10:25:49 INFO     	 * (global step 13250: loss: 0.28635671734809875, lr: 1e-05
2023-12-21 10:25:57 INFO     	 * (global step 13300: loss: 0.4530213475227356, lr: 1e-05
2023-12-21 10:26:05 INFO     	 * (global step 13350: loss: 0.4654712975025177, lr: 1e-05
2023-12-21 10:26:13 INFO     	 * (global step 13400: loss: 0.2598615884780884, lr: 1e-05
2023-12-21 10:26:21 INFO     	 * (global step 13450: loss: 0.4025249183177948, lr: 1e-05
2023-12-21 10:26:29 INFO     	 * (global step 13500: loss: 0.30168531835079193, lr: 1e-05
2023-12-21 10:26:37 INFO     	 * (global step 13550: loss: 0.2460315153002739, lr: 1e-05
2023-12-21 10:26:45 INFO     	 * (global step 13600: loss: 0.6330205053091049, lr: 1e-05
2023-12-21 10:26:53 INFO     	 * (global step 13650: loss: 0.24796921759843826, lr: 1e-05
2023-12-21 10:27:01 INFO     	 * (global step 13700: loss: 0.3401351347565651, lr: 1e-05
2023-12-21 10:27:09 INFO     	 * (global step 13750: loss: 0.31551697850227356, lr: 1e-05
2023-12-21 10:27:17 INFO     	 * (global step 13800: loss: 0.4115038141608238, lr: 1e-05
2023-12-21 10:27:25 INFO     	 * (global step 13850: loss: 0.6039001792669296, lr: 1e-05
2023-12-21 10:27:33 INFO     	 * (global step 13900: loss: 0.26113925874233246, lr: 1e-05
2023-12-21 10:27:41 INFO     	 * (global step 13950: loss: 0.33231478929519653, lr: 1e-05
2023-12-21 10:27:49 INFO     	 * (global step 14000: loss: 0.41615724563598633, lr: 1e-05
2023-12-21 10:27:56 INFO     	 * (global step 14050: loss: 0.4007178544998169, lr: 1e-05
2023-12-21 10:28:04 INFO     	 * (global step 14100: loss: 0.27244819700717926, lr: 1e-05
2023-12-21 10:28:12 INFO     	 * (global step 14150: loss: 0.37675319612026215, lr: 1e-05
2023-12-21 10:28:16 INFO     [epoch 2/15] average loss: 0.363, lr: 1e-05
2023-12-21 10:28:16 INFO     saving model related files
2023-12-21 10:28:16 INFO     saving model
2023-12-21 10:28:16 INFO     saving tokenizer
2023-12-21 10:28:16 INFO     saving optimizer
2023-12-21 10:28:17 INFO     remove old optimizer files
2023-12-21 10:28:22 INFO     	 * (global step 14200: loss: 0.3711121082305908, lr: 1e-05
2023-12-21 10:28:30 INFO     	 * (global step 14250: loss: 0.3040050193667412, lr: 1e-05
2023-12-21 10:28:38 INFO     	 * (global step 14300: loss: 0.18648536503314972, lr: 1e-05
2023-12-21 10:28:46 INFO     	 * (global step 14350: loss: 0.3980124667286873, lr: 1e-05
2023-12-21 10:28:54 INFO     	 * (global step 14400: loss: 0.37050966918468475, lr: 1e-05
2023-12-21 10:29:02 INFO     	 * (global step 14450: loss: 0.27198024094104767, lr: 1e-05
2023-12-21 10:29:10 INFO     	 * (global step 14500: loss: 0.5431654453277588, lr: 1e-05
2023-12-21 10:29:18 INFO     	 * (global step 14550: loss: 0.2629178613424301, lr: 1e-05
2023-12-21 10:29:26 INFO     	 * (global step 14600: loss: 0.2508460506796837, lr: 1e-05
2023-12-21 10:29:34 INFO     	 * (global step 14650: loss: 0.32086844742298126, lr: 1e-05
2023-12-21 10:29:42 INFO     	 * (global step 14700: loss: 0.45645809173583984, lr: 1e-05
2023-12-21 10:29:50 INFO     	 * (global step 14750: loss: 0.30604827404022217, lr: 1e-05
2023-12-21 10:29:57 INFO     	 * (global step 14800: loss: 0.4186009168624878, lr: 1e-05
2023-12-21 10:30:05 INFO     	 * (global step 14850: loss: 0.34248149394989014, lr: 1e-05
2023-12-21 10:30:13 INFO     	 * (global step 14900: loss: 0.46420033276081085, lr: 1e-05
2023-12-21 10:30:21 INFO     	 * (global step 14950: loss: 0.3634193390607834, lr: 1e-05
2023-12-21 10:30:29 INFO     	 * (global step 15000: loss: 0.2813730388879776, lr: 1e-05
2023-12-21 10:30:37 INFO     	 * (global step 15050: loss: 0.2920657768845558, lr: 1e-05
2023-12-21 10:30:45 INFO     	 * (global step 15100: loss: 0.2830835059285164, lr: 1e-05
2023-12-21 10:30:53 INFO     	 * (global step 15150: loss: 0.5014548003673553, lr: 1e-05
2023-12-21 10:31:01 INFO     	 * (global step 15200: loss: 0.5211549699306488, lr: 1e-05
2023-12-21 10:31:09 INFO     	 * (global step 15250: loss: 0.5335464179515839, lr: 1e-05
2023-12-21 10:31:17 INFO     	 * (global step 15300: loss: 0.36774466931819916, lr: 1e-05
2023-12-21 10:31:25 INFO     	 * (global step 15350: loss: 0.3319927975535393, lr: 1e-05
2023-12-21 10:31:33 INFO     	 * (global step 15400: loss: 0.2706451117992401, lr: 1e-05
2023-12-21 10:31:41 INFO     	 * (global step 15450: loss: 0.3399914354085922, lr: 1e-05
2023-12-21 10:31:49 INFO     	 * (global step 15500: loss: 0.35608893632888794, lr: 1e-05
2023-12-21 10:31:57 INFO     	 * (global step 15550: loss: 0.2838602587580681, lr: 1e-05
2023-12-21 10:32:05 INFO     	 * (global step 15600: loss: 0.4558950960636139, lr: 1e-05
2023-12-21 10:32:13 INFO     	 * (global step 15650: loss: 0.3973078727722168, lr: 1e-05
2023-12-21 10:32:21 INFO     	 * (global step 15700: loss: 0.37257660925388336, lr: 1e-05
2023-12-21 10:32:29 INFO     	 * (global step 15750: loss: 0.43582966923713684, lr: 1e-05
2023-12-21 10:32:37 INFO     	 * (global step 15800: loss: 0.26157964766025543, lr: 1e-05
2023-12-21 10:32:45 INFO     	 * (global step 15850: loss: 0.40469804406166077, lr: 1e-05
2023-12-21 10:32:53 INFO     	 * (global step 15900: loss: 0.26010967791080475, lr: 1e-05
2023-12-21 10:33:01 INFO     	 * (global step 15950: loss: 0.36266475915908813, lr: 1e-05
2023-12-21 10:33:09 INFO     	 * (global step 16000: loss: 0.18777745217084885, lr: 1e-05
2023-12-21 10:33:17 INFO     	 * (global step 16050: loss: 0.4824414849281311, lr: 1e-05
2023-12-21 10:33:25 INFO     	 * (global step 16100: loss: 0.40711329877376556, lr: 1e-05
2023-12-21 10:33:33 INFO     	 * (global step 16150: loss: 0.2853944003582001, lr: 1e-05
2023-12-21 10:33:41 INFO     	 * (global step 16200: loss: 0.32059381902217865, lr: 1e-05
2023-12-21 10:33:49 INFO     	 * (global step 16250: loss: 0.3853626847267151, lr: 1e-05
2023-12-21 10:33:57 INFO     	 * (global step 16300: loss: 0.29076530784368515, lr: 1e-05
2023-12-21 10:34:05 INFO     	 * (global step 16350: loss: 0.35743340849876404, lr: 1e-05
2023-12-21 10:34:12 INFO     	 * (global step 16400: loss: 0.36017850041389465, lr: 1e-05
2023-12-21 10:34:20 INFO     	 * (global step 16450: loss: 0.3589743822813034, lr: 1e-05
2023-12-21 10:34:28 INFO     	 * (global step 16500: loss: 0.3424276113510132, lr: 1e-05
2023-12-21 10:34:36 INFO     	 * (global step 16550: loss: 0.38800448179244995, lr: 1e-05
2023-12-21 10:34:44 INFO     	 * (global step 16600: loss: 0.30423421412706375, lr: 1e-05
2023-12-21 10:34:52 INFO     	 * (global step 16650: loss: 0.35550108551979065, lr: 1e-05
2023-12-21 10:35:00 INFO     	 * (global step 16700: loss: 0.2599039897322655, lr: 1e-05
2023-12-21 10:35:08 INFO     	 * (global step 16750: loss: 0.1631110981106758, lr: 1e-05
2023-12-21 10:35:16 INFO     	 * (global step 16800: loss: 0.35334426164627075, lr: 1e-05
2023-12-21 10:35:24 INFO     	 * (global step 16850: loss: 0.41865672171115875, lr: 1e-05
2023-12-21 10:35:32 INFO     	 * (global step 16900: loss: 0.3120039701461792, lr: 1e-05
2023-12-21 10:35:40 INFO     	 * (global step 16950: loss: 0.390916109085083, lr: 1e-05
2023-12-21 10:35:48 INFO     	 * (global step 17000: loss: 0.29377777874469757, lr: 1e-05
2023-12-21 10:35:56 INFO     	 * (global step 17050: loss: 0.2428528070449829, lr: 1e-05
2023-12-21 10:36:04 INFO     	 * (global step 17100: loss: 0.3701193779706955, lr: 1e-05
2023-12-21 10:36:12 INFO     	 * (global step 17150: loss: 0.2941244915127754, lr: 1e-05
2023-12-21 10:36:20 INFO     	 * (global step 17200: loss: 0.3633961156010628, lr: 1e-05
2023-12-21 10:36:28 INFO     	 * (global step 17250: loss: 0.3326919972896576, lr: 1e-05
2023-12-21 10:36:36 INFO     	 * (global step 17300: loss: 0.2536608725786209, lr: 1e-05
2023-12-21 10:36:44 INFO     	 * (global step 17350: loss: 0.41532063484191895, lr: 1e-05
2023-12-21 10:36:52 INFO     	 * (global step 17400: loss: 0.2302032709121704, lr: 1e-05
2023-12-21 10:37:00 INFO     	 * (global step 17450: loss: 0.5040368959307671, lr: 1e-05
2023-12-21 10:37:08 INFO     	 * (global step 17500: loss: 0.3802638202905655, lr: 1e-05
2023-12-21 10:37:16 INFO     	 * (global step 17550: loss: 0.33344055712223053, lr: 1e-05
2023-12-21 10:37:24 INFO     	 * (global step 17600: loss: 0.33542588353157043, lr: 1e-05
2023-12-21 10:37:32 INFO     	 * (global step 17650: loss: 0.46349143981933594, lr: 1e-05
2023-12-21 10:37:40 INFO     	 * (global step 17700: loss: 0.30440889298915863, lr: 1e-05
2023-12-21 10:37:48 INFO     	 * (global step 17750: loss: 0.3917962908744812, lr: 1e-05
2023-12-21 10:37:56 INFO     	 * (global step 17800: loss: 0.536663681268692, lr: 1e-05
2023-12-21 10:38:04 INFO     	 * (global step 17850: loss: 0.25884605199098587, lr: 1e-05
2023-12-21 10:38:12 INFO     	 * (global step 17900: loss: 0.373471274971962, lr: 1e-05
2023-12-21 10:38:20 INFO     	 * (global step 17950: loss: 0.3237803280353546, lr: 1e-05
2023-12-21 10:38:28 INFO     	 * (global step 18000: loss: 0.35583028197288513, lr: 1e-05
2023-12-21 10:38:36 INFO     	 * (global step 18050: loss: 0.22917790710926056, lr: 1e-05
2023-12-21 10:38:44 INFO     	 * (global step 18100: loss: 0.3752148151397705, lr: 1e-05
2023-12-21 10:38:52 INFO     	 * (global step 18150: loss: 0.34151491522789, lr: 1e-05
2023-12-21 10:39:00 INFO     	 * (global step 18200: loss: 0.4449482411146164, lr: 1e-05
2023-12-21 10:39:08 INFO     	 * (global step 18250: loss: 0.2860836237668991, lr: 1e-05
2023-12-21 10:39:15 INFO     	 * (global step 18300: loss: 0.3113458752632141, lr: 1e-05
2023-12-21 10:39:23 INFO     	 * (global step 18350: loss: 0.3418331891298294, lr: 1e-05
2023-12-21 10:39:31 INFO     	 * (global step 18400: loss: 0.3426467925310135, lr: 1e-05
2023-12-21 10:39:39 INFO     	 * (global step 18450: loss: 0.37326309084892273, lr: 1e-05
2023-12-21 10:39:47 INFO     	 * (global step 18500: loss: 0.33043844997882843, lr: 1e-05
2023-12-21 10:39:55 INFO     	 * (global step 18550: loss: 0.40231090784072876, lr: 1e-05
2023-12-21 10:40:03 INFO     	 * (global step 18600: loss: 0.47538377344608307, lr: 1e-05
2023-12-21 10:40:11 INFO     	 * (global step 18650: loss: 0.626231387257576, lr: 1e-05
2023-12-21 10:40:19 INFO     	 * (global step 18700: loss: 0.4238465130329132, lr: 1e-05
2023-12-21 10:40:27 INFO     	 * (global step 18750: loss: 0.24172180891036987, lr: 1e-05
2023-12-21 10:40:35 INFO     	 * (global step 18800: loss: 0.5703385099768639, lr: 1e-05
2023-12-21 10:40:43 INFO     	 * (global step 18850: loss: 0.2897709012031555, lr: 1e-05
2023-12-21 10:40:50 INFO     [epoch 3/15] average loss: 0.349, lr: 1e-05
2023-12-21 10:40:50 INFO     saving model related files
2023-12-21 10:40:50 INFO     saving model
2023-12-21 10:40:50 INFO     saving tokenizer
2023-12-21 10:40:50 INFO     saving optimizer
2023-12-21 10:40:51 INFO     remove old optimizer files
2023-12-21 10:40:53 INFO     	 * (global step 18900: loss: 0.2943556532263756, lr: 1e-05
2023-12-21 10:41:01 INFO     	 * (global step 18950: loss: 0.31351424753665924, lr: 1e-05
2023-12-21 10:41:09 INFO     	 * (global step 19000: loss: 0.5931851118803024, lr: 1e-05
2023-12-21 10:41:17 INFO     	 * (global step 19050: loss: 0.4198019951581955, lr: 1e-05
2023-12-21 10:41:25 INFO     	 * (global step 19100: loss: 0.2692527621984482, lr: 1e-05
2023-12-21 10:41:33 INFO     	 * (global step 19150: loss: 0.25952891260385513, lr: 1e-05
2023-12-21 10:41:41 INFO     	 * (global step 19200: loss: 0.36599916219711304, lr: 1e-05
2023-12-21 10:41:49 INFO     	 * (global step 19250: loss: 0.26462098956108093, lr: 1e-05
2023-12-21 10:41:57 INFO     	 * (global step 19300: loss: 0.26242223381996155, lr: 1e-05
2023-12-21 10:42:05 INFO     	 * (global step 19350: loss: 0.22080720216035843, lr: 1e-05
2023-12-21 10:42:13 INFO     	 * (global step 19400: loss: 0.3444901555776596, lr: 1e-05
2023-12-21 10:42:21 INFO     	 * (global step 19450: loss: 0.21451113373041153, lr: 1e-05
2023-12-21 10:42:29 INFO     	 * (global step 19500: loss: 0.36794517934322357, lr: 1e-05
2023-12-21 10:42:37 INFO     	 * (global step 19550: loss: 0.35501720011234283, lr: 1e-05
2023-12-21 10:42:45 INFO     	 * (global step 19600: loss: 0.24646329134702682, lr: 1e-05
2023-12-21 10:42:53 INFO     	 * (global step 19650: loss: 0.610071063041687, lr: 1e-05
2023-12-21 10:43:01 INFO     	 * (global step 19700: loss: 0.32467710971832275, lr: 1e-05
2023-12-21 10:43:09 INFO     	 * (global step 19750: loss: 0.3194696754217148, lr: 1e-05
2023-12-21 10:43:17 INFO     	 * (global step 19800: loss: 0.29956139624118805, lr: 1e-05
2023-12-21 10:43:25 INFO     	 * (global step 19850: loss: 0.23403993248939514, lr: 1e-05
2023-12-21 10:43:33 INFO     	 * (global step 19900: loss: 0.3648403137922287, lr: 1e-05
2023-12-21 10:43:41 INFO     	 * (global step 19950: loss: 0.29788079112768173, lr: 1e-05
2023-12-21 10:43:49 INFO     	 * (global step 20000: loss: 0.23191243410110474, lr: 1e-05
2023-12-21 10:43:57 INFO     	 * (global step 20050: loss: 0.26500314474105835, lr: 1e-05
2023-12-21 10:44:05 INFO     	 * (global step 20100: loss: 0.4064589589834213, lr: 1e-05
2023-12-21 10:44:13 INFO     	 * (global step 20150: loss: 0.3131861314177513, lr: 1e-05
2023-12-21 10:44:21 INFO     	 * (global step 20200: loss: 0.3194909989833832, lr: 1e-05
2023-12-21 10:44:29 INFO     	 * (global step 20250: loss: 0.3365868031978607, lr: 1e-05
2023-12-21 10:44:37 INFO     	 * (global step 20300: loss: 0.35330332815647125, lr: 1e-05
2023-12-21 10:44:45 INFO     	 * (global step 20350: loss: 0.27445902675390244, lr: 1e-05
2023-12-21 10:44:53 INFO     	 * (global step 20400: loss: 0.2888701260089874, lr: 1e-05
2023-12-21 10:45:01 INFO     	 * (global step 20450: loss: 0.23999237269163132, lr: 1e-05
2023-12-21 10:45:09 INFO     	 * (global step 20500: loss: 0.2708466723561287, lr: 1e-05
2023-12-21 10:45:17 INFO     	 * (global step 20550: loss: 0.277143657207489, lr: 1e-05
2023-12-21 10:45:25 INFO     	 * (global step 20600: loss: 0.3165957108139992, lr: 1e-05
2023-12-21 10:45:33 INFO     	 * (global step 20650: loss: 0.4686806946992874, lr: 1e-05
2023-12-21 10:45:41 INFO     	 * (global step 20700: loss: 0.290608286857605, lr: 1e-05
2023-12-21 10:45:49 INFO     	 * (global step 20750: loss: 0.27790845185518265, lr: 1e-05
2023-12-21 10:45:57 INFO     	 * (global step 20800: loss: 0.37933699041604996, lr: 1e-05
2023-12-21 10:46:05 INFO     	 * (global step 20850: loss: 0.36279910802841187, lr: 1e-05
2023-12-21 10:46:13 INFO     	 * (global step 20900: loss: 0.36983467638492584, lr: 1e-05
2023-12-21 10:46:21 INFO     	 * (global step 20950: loss: 0.3385116159915924, lr: 1e-05
2023-12-21 10:46:29 INFO     	 * (global step 21000: loss: 0.5347755551338196, lr: 1e-05
2023-12-21 10:46:37 INFO     	 * (global step 21050: loss: 0.2964836657047272, lr: 1e-05
2023-12-21 10:46:45 INFO     	 * (global step 21100: loss: 0.2735094726085663, lr: 1e-05
2023-12-21 10:46:53 INFO     	 * (global step 21150: loss: 0.3283730447292328, lr: 1e-05
2023-12-21 10:47:01 INFO     	 * (global step 21200: loss: 0.39652736485004425, lr: 1e-05
2023-12-21 10:47:10 INFO     	 * (global step 21250: loss: 0.28546810150146484, lr: 1e-05
2023-12-21 10:47:18 INFO     	 * (global step 21300: loss: 0.35417354106903076, lr: 1e-05
2023-12-21 10:47:26 INFO     	 * (global step 21350: loss: 0.27331972122192383, lr: 1e-05
2023-12-21 10:47:34 INFO     	 * (global step 21400: loss: 0.2342095598578453, lr: 1e-05
2023-12-21 10:47:42 INFO     	 * (global step 21450: loss: 0.5609567910432816, lr: 1e-05
2023-12-21 10:47:50 INFO     	 * (global step 21500: loss: 0.29992326349020004, lr: 1e-05
2023-12-21 10:47:58 INFO     	 * (global step 21550: loss: 0.3365065008401871, lr: 1e-05
2023-12-21 10:48:06 INFO     	 * (global step 21600: loss: 0.4074452519416809, lr: 1e-05
2023-12-21 10:48:14 INFO     	 * (global step 21650: loss: 0.29963311553001404, lr: 1e-05
2023-12-21 10:48:22 INFO     	 * (global step 21700: loss: 0.22235393524169922, lr: 1e-05
2023-12-21 10:48:30 INFO     	 * (global step 21750: loss: 0.400631383061409, lr: 1e-05
2023-12-21 10:48:38 INFO     	 * (global step 21800: loss: 0.3623279854655266, lr: 1e-05
2023-12-21 10:48:46 INFO     	 * (global step 21850: loss: 0.4189707189798355, lr: 1e-05
2023-12-21 10:48:54 INFO     	 * (global step 21900: loss: 0.308410182595253, lr: 1e-05
2023-12-21 10:49:02 INFO     	 * (global step 21950: loss: 0.3408767729997635, lr: 1e-05
2023-12-21 10:49:10 INFO     	 * (global step 22000: loss: 0.34262777864933014, lr: 1e-05
2023-12-21 10:49:18 INFO     	 * (global step 22050: loss: 0.38379405438899994, lr: 1e-05
2023-12-21 10:49:26 INFO     	 * (global step 22100: loss: 0.493681862950325, lr: 1e-05
2023-12-21 10:49:34 INFO     	 * (global step 22150: loss: 0.49837230145931244, lr: 1e-05
2023-12-21 10:49:42 INFO     	 * (global step 22200: loss: 0.2339070364832878, lr: 1e-05
2023-12-21 10:49:50 INFO     	 * (global step 22250: loss: 0.3103254586458206, lr: 1e-05
2023-12-21 10:49:58 INFO     	 * (global step 22300: loss: 0.34976959228515625, lr: 1e-05
2023-12-21 10:50:06 INFO     	 * (global step 22350: loss: 0.39904579520225525, lr: 1e-05
2023-12-21 10:50:14 INFO     	 * (global step 22400: loss: 0.23885754495859146, lr: 1e-05
2023-12-21 10:50:22 INFO     	 * (global step 22450: loss: 0.4320440888404846, lr: 1e-05
2023-12-21 10:50:30 INFO     	 * (global step 22500: loss: 0.34083931148052216, lr: 1e-05
2023-12-21 10:50:39 INFO     	 * (global step 22550: loss: 0.31716251373291016, lr: 1e-05
2023-12-21 10:50:47 INFO     	 * (global step 22600: loss: 0.33114948868751526, lr: 1e-05
2023-12-21 10:50:55 INFO     	 * (global step 22650: loss: 0.19194107875227928, lr: 1e-05
2023-12-21 10:51:03 INFO     	 * (global step 22700: loss: 0.45707589387893677, lr: 1e-05
2023-12-21 10:51:11 INFO     	 * (global step 22750: loss: 0.37955641746520996, lr: 1e-05
2023-12-21 10:51:19 INFO     	 * (global step 22800: loss: 0.44351519644260406, lr: 1e-05
2023-12-21 10:51:27 INFO     	 * (global step 22850: loss: 0.36278636008501053, lr: 1e-05
2023-12-21 10:51:35 INFO     	 * (global step 22900: loss: 0.35921938717365265, lr: 1e-05
2023-12-21 10:51:43 INFO     	 * (global step 22950: loss: 0.25606220215559006, lr: 1e-05
2023-12-21 10:51:51 INFO     	 * (global step 23000: loss: 0.4249245375394821, lr: 1e-05
2023-12-21 10:51:59 INFO     	 * (global step 23050: loss: 0.5007382780313492, lr: 1e-05
2023-12-21 10:52:07 INFO     	 * (global step 23100: loss: 0.4017740339040756, lr: 1e-05
2023-12-21 10:52:15 INFO     	 * (global step 23150: loss: 0.3084557354450226, lr: 1e-05
2023-12-21 10:52:23 INFO     	 * (global step 23200: loss: 0.32428763806819916, lr: 1e-05
2023-12-21 10:52:31 INFO     	 * (global step 23250: loss: 0.18240900337696075, lr: 1e-05
2023-12-21 10:52:39 INFO     	 * (global step 23300: loss: 0.30184853076934814, lr: 1e-05
2023-12-21 10:52:47 INFO     	 * (global step 23350: loss: 0.3093997687101364, lr: 1e-05
2023-12-21 10:52:55 INFO     	 * (global step 23400: loss: 0.3780759572982788, lr: 1e-05
2023-12-21 10:53:03 INFO     	 * (global step 23450: loss: 0.4589563459157944, lr: 1e-05
2023-12-21 10:53:11 INFO     	 * (global step 23500: loss: 0.2831384465098381, lr: 1e-05
2023-12-21 10:53:19 INFO     	 * (global step 23550: loss: 0.3325956091284752, lr: 1e-05
2023-12-21 10:53:27 INFO     	 * (global step 23600: loss: 0.40246595442295074, lr: 1e-05
2023-12-21 10:53:30 INFO     [epoch 4/15] average loss: 0.34, lr: 1e-05
2023-12-21 10:53:30 INFO     saving model related files
2023-12-21 10:53:30 INFO     saving model
2023-12-21 10:53:30 INFO     saving tokenizer
2023-12-21 10:53:30 INFO     saving optimizer
2023-12-21 10:53:31 INFO     remove old optimizer files
2023-12-21 10:53:37 INFO     	 * (global step 23650: loss: 0.4950941503047943, lr: 1e-05
2023-12-21 10:53:45 INFO     	 * (global step 23700: loss: 0.28114327788352966, lr: 1e-05
2023-12-21 10:53:53 INFO     	 * (global step 23750: loss: 0.35622017830610275, lr: 1e-05
2023-12-21 10:54:01 INFO     	 * (global step 23800: loss: 0.2714245989918709, lr: 1e-05
2023-12-21 10:54:09 INFO     	 * (global step 23850: loss: 0.37746402621269226, lr: 1e-05
2023-12-21 10:54:17 INFO     	 * (global step 23900: loss: 0.29874470829963684, lr: 1e-05
2023-12-21 10:54:25 INFO     	 * (global step 23950: loss: 0.25878236442804337, lr: 1e-05
2023-12-21 10:54:33 INFO     	 * (global step 24000: loss: 0.3944942206144333, lr: 1e-05
2023-12-21 10:54:41 INFO     	 * (global step 24050: loss: 0.5355748534202576, lr: 1e-05
2023-12-21 10:54:49 INFO     	 * (global step 24100: loss: 0.30453649163246155, lr: 1e-05
2023-12-21 10:54:57 INFO     	 * (global step 24150: loss: 0.4105122685432434, lr: 1e-05
2023-12-21 10:55:05 INFO     	 * (global step 24200: loss: 0.3784554749727249, lr: 1e-05
2023-12-21 10:55:13 INFO     	 * (global step 24250: loss: 0.48107457160949707, lr: 1e-05
2023-12-21 10:55:21 INFO     	 * (global step 24300: loss: 0.31378471851348877, lr: 1e-05
2023-12-21 10:55:29 INFO     	 * (global step 24350: loss: 0.43371444940567017, lr: 1e-05
2023-12-21 10:55:37 INFO     	 * (global step 24400: loss: 0.3188198059797287, lr: 1e-05
2023-12-21 10:55:45 INFO     	 * (global step 24450: loss: 0.42087891697883606, lr: 1e-05
2023-12-21 10:55:53 INFO     	 * (global step 24500: loss: 0.2818243205547333, lr: 1e-05
2023-12-21 10:56:01 INFO     	 * (global step 24550: loss: 0.3984134644269943, lr: 1e-05
2023-12-21 10:56:09 INFO     	 * (global step 24600: loss: 0.3522990718483925, lr: 1e-05
2023-12-21 10:56:17 INFO     	 * (global step 24650: loss: 0.23357778787612915, lr: 1e-05
2023-12-21 10:56:25 INFO     	 * (global step 24700: loss: 0.3651922643184662, lr: 1e-05
2023-12-21 10:56:33 INFO     	 * (global step 24750: loss: 0.3397618681192398, lr: 1e-05
2023-12-21 10:56:41 INFO     	 * (global step 24800: loss: 0.3344765901565552, lr: 1e-05
2023-12-21 10:56:49 INFO     	 * (global step 24850: loss: 0.39074672013521194, lr: 1e-05
2023-12-21 10:56:57 INFO     	 * (global step 24900: loss: 0.3834517151117325, lr: 1e-05
2023-12-21 10:57:05 INFO     	 * (global step 24950: loss: 0.3451722711324692, lr: 1e-05
2023-12-21 10:57:13 INFO     	 * (global step 25000: loss: 0.3722255527973175, lr: 1e-05
2023-12-21 10:57:21 INFO     	 * (global step 25050: loss: 0.2089669555425644, lr: 1e-05
2023-12-21 10:57:29 INFO     	 * (global step 25100: loss: 0.4075656458735466, lr: 1e-05
2023-12-21 10:57:37 INFO     	 * (global step 25150: loss: 0.888655811548233, lr: 1e-05
2023-12-21 10:57:45 INFO     	 * (global step 25200: loss: 0.35902246832847595, lr: 1e-05
2023-12-21 10:57:53 INFO     	 * (global step 25250: loss: 0.2856258600950241, lr: 1e-05
2023-12-21 10:58:01 INFO     	 * (global step 25300: loss: 0.3673063665628433, lr: 1e-05
2023-12-21 10:58:09 INFO     	 * (global step 25350: loss: 0.31194040179252625, lr: 1e-05
2023-12-21 10:58:17 INFO     	 * (global step 25400: loss: 0.30444464832544327, lr: 1e-05
2023-12-21 10:58:25 INFO     	 * (global step 25450: loss: 0.29301296174526215, lr: 1e-05
2023-12-21 10:58:33 INFO     	 * (global step 25500: loss: 0.35756349563598633, lr: 1e-05
2023-12-21 10:58:41 INFO     	 * (global step 25550: loss: 0.3902410864830017, lr: 1e-05
2023-12-21 10:58:49 INFO     	 * (global step 25600: loss: 0.32167893648147583, lr: 1e-05
2023-12-21 10:58:57 INFO     	 * (global step 25650: loss: 0.2528231739997864, lr: 1e-05
2023-12-21 10:59:05 INFO     	 * (global step 25700: loss: 0.37121520936489105, lr: 1e-05
2023-12-21 10:59:13 INFO     	 * (global step 25750: loss: 0.2855533957481384, lr: 1e-05
2023-12-21 10:59:21 INFO     	 * (global step 25800: loss: 0.326894611120224, lr: 1e-05
2023-12-21 10:59:29 INFO     	 * (global step 25850: loss: 0.29482097923755646, lr: 1e-05
2023-12-21 10:59:37 INFO     	 * (global step 25900: loss: 0.6504318118095398, lr: 1e-05
2023-12-21 10:59:45 INFO     	 * (global step 25950: loss: 0.23345454782247543, lr: 1e-05
2023-12-21 10:59:54 INFO     	 * (global step 26000: loss: 0.38347336649894714, lr: 1e-05
2023-12-21 11:00:02 INFO     	 * (global step 26050: loss: 0.45115554332733154, lr: 1e-05
2023-12-21 11:00:10 INFO     	 * (global step 26100: loss: 0.3704894483089447, lr: 1e-05
2023-12-21 11:00:18 INFO     	 * (global step 26150: loss: 0.31782176345586777, lr: 1e-05
2023-12-21 11:00:26 INFO     	 * (global step 26200: loss: 0.48388147354125977, lr: 1e-05
2023-12-21 11:00:34 INFO     	 * (global step 26250: loss: 0.28940249979496, lr: 1e-05
2023-12-21 11:00:42 INFO     	 * (global step 26300: loss: 0.44381463527679443, lr: 1e-05
2023-12-21 11:00:50 INFO     	 * (global step 26350: loss: 0.2730284631252289, lr: 1e-05
2023-12-21 11:00:58 INFO     	 * (global step 26400: loss: 0.44380202889442444, lr: 1e-05
2023-12-21 11:01:06 INFO     	 * (global step 26450: loss: 0.2823799103498459, lr: 1e-05
2023-12-21 11:01:14 INFO     	 * (global step 26500: loss: 0.28665389120578766, lr: 1e-05
2023-12-21 11:01:22 INFO     	 * (global step 26550: loss: 0.3174048513174057, lr: 1e-05
2023-12-21 11:01:30 INFO     	 * (global step 26600: loss: 0.260578915476799, lr: 1e-05
2023-12-21 11:01:38 INFO     	 * (global step 26650: loss: 0.42353154718875885, lr: 1e-05
2023-12-21 11:01:46 INFO     	 * (global step 26700: loss: 0.1747417151927948, lr: 1e-05
2023-12-21 11:01:54 INFO     	 * (global step 26750: loss: 0.26337794959545135, lr: 1e-05
2023-12-21 11:02:02 INFO     	 * (global step 26800: loss: 0.3689161092042923, lr: 1e-05
2023-12-21 11:02:10 INFO     	 * (global step 26850: loss: 0.23817143589258194, lr: 1e-05
2023-12-21 11:02:18 INFO     	 * (global step 26900: loss: 0.2573239877820015, lr: 1e-05
2023-12-21 11:02:26 INFO     	 * (global step 26950: loss: 0.3402237296104431, lr: 1e-05
2023-12-21 11:02:34 INFO     	 * (global step 27000: loss: 0.347765788435936, lr: 1e-05
2023-12-21 11:02:42 INFO     	 * (global step 27050: loss: 0.21760465949773788, lr: 1e-05
2023-12-21 11:02:50 INFO     	 * (global step 27100: loss: 0.3204817771911621, lr: 1e-05
2023-12-21 11:02:58 INFO     	 * (global step 27150: loss: 0.37372957170009613, lr: 1e-05
2023-12-21 11:03:06 INFO     	 * (global step 27200: loss: 0.5530149787664413, lr: 1e-05
2023-12-21 11:03:14 INFO     	 * (global step 27250: loss: 0.28158268332481384, lr: 1e-05
2023-12-21 11:03:22 INFO     	 * (global step 27300: loss: 0.3342798054218292, lr: 1e-05
2023-12-21 11:03:30 INFO     	 * (global step 27350: loss: 0.2408095821738243, lr: 1e-05
2023-12-21 11:03:38 INFO     	 * (global step 27400: loss: 0.31942011415958405, lr: 1e-05
2023-12-21 11:03:46 INFO     	 * (global step 27450: loss: 0.3862350136041641, lr: 1e-05
2023-12-21 11:03:54 INFO     	 * (global step 27500: loss: 0.3534251004457474, lr: 1e-05
2023-12-21 11:04:02 INFO     	 * (global step 27550: loss: 0.4961894080042839, lr: 1e-05
2023-12-21 11:04:10 INFO     	 * (global step 27600: loss: 0.34368540346622467, lr: 1e-05
2023-12-21 11:04:18 INFO     	 * (global step 27650: loss: 0.3319627493619919, lr: 1e-05
2023-12-21 11:04:26 INFO     	 * (global step 27700: loss: 0.3565652370452881, lr: 1e-05
2023-12-21 11:04:34 INFO     	 * (global step 27750: loss: 0.2699286416172981, lr: 1e-05
2023-12-21 11:04:42 INFO     	 * (global step 27800: loss: 0.3163323253393173, lr: 1e-05
2023-12-21 11:04:50 INFO     	 * (global step 27850: loss: 0.4301009774208069, lr: 1e-05
2023-12-21 11:04:58 INFO     	 * (global step 27900: loss: 0.3071906045079231, lr: 1e-05
2023-12-21 11:05:06 INFO     	 * (global step 27950: loss: 0.49066823720932007, lr: 1e-05
2023-12-21 11:05:14 INFO     	 * (global step 28000: loss: 0.35658399015665054, lr: 1e-05
2023-12-21 11:05:22 INFO     	 * (global step 28050: loss: 0.30379311740398407, lr: 1e-05
2023-12-21 11:05:30 INFO     	 * (global step 28100: loss: 0.44759757816791534, lr: 1e-05
2023-12-21 11:05:38 INFO     	 * (global step 28150: loss: 0.4239516407251358, lr: 1e-05
2023-12-21 11:05:46 INFO     	 * (global step 28200: loss: 0.6058756709098816, lr: 1e-05
2023-12-21 11:05:54 INFO     	 * (global step 28250: loss: 0.23519019782543182, lr: 1e-05
2023-12-21 11:06:02 INFO     	 * (global step 28300: loss: 0.15927356109023094, lr: 1e-05
2023-12-21 11:06:08 INFO     [epoch 5/15] average loss: 0.333, lr: 1e-05
2023-12-21 11:06:08 INFO     saving model related files
2023-12-21 11:06:08 INFO     saving model
2023-12-21 11:06:09 INFO     saving tokenizer
2023-12-21 11:06:09 INFO     saving optimizer
2023-12-21 11:06:10 INFO     remove old optimizer files
2023-12-21 11:06:11 INFO     	 * (global step 28350: loss: 0.39721645414829254, lr: 1e-05
2023-12-21 11:06:20 INFO     	 * (global step 28400: loss: 0.3543238341808319, lr: 1e-05
2023-12-21 11:06:28 INFO     	 * (global step 28450: loss: 0.18392811715602875, lr: 1e-05
2023-12-21 11:06:36 INFO     	 * (global step 28500: loss: 0.376155748963356, lr: 1e-05
2023-12-21 11:06:44 INFO     	 * (global step 28550: loss: 0.19377800822257996, lr: 1e-05
2023-12-21 11:06:52 INFO     	 * (global step 28600: loss: 0.28234586119651794, lr: 1e-05
2023-12-21 11:07:00 INFO     	 * (global step 28650: loss: 0.3896543085575104, lr: 1e-05
2023-12-21 11:07:08 INFO     	 * (global step 28700: loss: 0.27175121009349823, lr: 1e-05
2023-12-21 11:07:16 INFO     	 * (global step 28750: loss: 0.2319985255599022, lr: 1e-05
2023-12-21 11:07:24 INFO     	 * (global step 28800: loss: 0.22789670526981354, lr: 1e-05
2023-12-21 11:07:32 INFO     	 * (global step 28850: loss: 0.33729569613933563, lr: 1e-05
2023-12-21 11:07:40 INFO     	 * (global step 28900: loss: 0.4187520891427994, lr: 1e-05
2023-12-21 11:07:48 INFO     	 * (global step 28950: loss: 0.21579555422067642, lr: 1e-05
2023-12-21 11:07:56 INFO     	 * (global step 29000: loss: 0.29007042944431305, lr: 1e-05
2023-12-21 11:08:04 INFO     	 * (global step 29050: loss: 0.2419605851173401, lr: 1e-05
2023-12-21 11:08:12 INFO     	 * (global step 29100: loss: 0.32002490758895874, lr: 1e-05
2023-12-21 11:08:20 INFO     	 * (global step 29150: loss: 0.3632368892431259, lr: 1e-05
2023-12-21 11:08:28 INFO     	 * (global step 29200: loss: 0.42822085320949554, lr: 1e-05
2023-12-21 11:08:36 INFO     	 * (global step 29250: loss: 0.2736848145723343, lr: 1e-05
2023-12-21 11:08:44 INFO     	 * (global step 29300: loss: 0.26295415312051773, lr: 1e-05
2023-12-21 11:08:52 INFO     	 * (global step 29350: loss: 0.3622194230556488, lr: 1e-05
2023-12-21 11:09:00 INFO     	 * (global step 29400: loss: 0.22669801115989685, lr: 1e-05
2023-12-21 11:09:08 INFO     	 * (global step 29450: loss: 0.34091879427433014, lr: 1e-05
2023-12-21 11:09:16 INFO     	 * (global step 29500: loss: 0.2493206486105919, lr: 1e-05
2023-12-21 11:09:24 INFO     	 * (global step 29550: loss: 0.28906723856925964, lr: 1e-05
2023-12-21 11:09:32 INFO     	 * (global step 29600: loss: 0.3570990562438965, lr: 1e-05
2023-12-21 11:09:40 INFO     	 * (global step 29650: loss: 0.25404244661331177, lr: 1e-05
2023-12-21 11:09:48 INFO     	 * (global step 29700: loss: 0.2801375687122345, lr: 1e-05
2023-12-21 11:09:56 INFO     	 * (global step 29750: loss: 0.27217044681310654, lr: 1e-05
2023-12-21 11:10:04 INFO     	 * (global step 29800: loss: 0.6973174214363098, lr: 1e-05
2023-12-21 11:10:12 INFO     	 * (global step 29850: loss: 0.25160447508096695, lr: 1e-05
2023-12-21 11:10:20 INFO     	 * (global step 29900: loss: 0.41232846677303314, lr: 1e-05
2023-12-21 11:10:28 INFO     	 * (global step 29950: loss: 0.3037226274609566, lr: 1e-05
2023-12-21 11:10:36 INFO     	 * (global step 30000: loss: 0.18680450320243835, lr: 1e-05
2023-12-21 11:10:44 INFO     	 * (global step 30050: loss: 0.2401222661137581, lr: 1e-05
2023-12-21 11:10:52 INFO     	 * (global step 30100: loss: 0.33365175127983093, lr: 1e-05
2023-12-21 11:11:00 INFO     	 * (global step 30150: loss: 0.3401799649000168, lr: 1e-05
2023-12-21 11:11:08 INFO     	 * (global step 30200: loss: 0.21098286658525467, lr: 1e-05
2023-12-21 11:11:16 INFO     	 * (global step 30250: loss: 0.33927109837532043, lr: 1e-05
2023-12-21 11:11:24 INFO     	 * (global step 30300: loss: 0.2306821644306183, lr: 1e-05
2023-12-21 11:11:32 INFO     	 * (global step 30350: loss: 0.2943100929260254, lr: 1e-05
2023-12-21 11:11:40 INFO     	 * (global step 30400: loss: 0.28252094984054565, lr: 1e-05
2023-12-21 11:11:48 INFO     	 * (global step 30450: loss: 0.3539661020040512, lr: 1e-05
2023-12-21 11:11:56 INFO     	 * (global step 30500: loss: 0.23923376947641373, lr: 1e-05
2023-12-21 11:12:05 INFO     	 * (global step 30550: loss: 0.2658728212118149, lr: 1e-05
2023-12-21 11:12:13 INFO     	 * (global step 30600: loss: 0.5652761608362198, lr: 1e-05
2023-12-21 11:12:21 INFO     	 * (global step 30650: loss: 0.2506033033132553, lr: 1e-05
2023-12-21 11:12:29 INFO     	 * (global step 30700: loss: 0.31250135600566864, lr: 1e-05
2023-12-21 11:12:37 INFO     	 * (global step 30750: loss: 0.2910194918513298, lr: 1e-05
2023-12-21 11:12:45 INFO     	 * (global step 30800: loss: 0.20741567015647888, lr: 1e-05
2023-12-21 11:12:53 INFO     	 * (global step 30850: loss: 0.23407763242721558, lr: 1e-05
2023-12-21 11:13:01 INFO     	 * (global step 30900: loss: 0.2885076403617859, lr: 1e-05
2023-12-21 11:13:09 INFO     	 * (global step 30950: loss: 0.4093802124261856, lr: 1e-05
2023-12-21 11:13:17 INFO     	 * (global step 31000: loss: 0.3235369920730591, lr: 1e-05
2023-12-21 11:13:25 INFO     	 * (global step 31050: loss: 0.30018018186092377, lr: 1e-05
2023-12-21 11:13:33 INFO     	 * (global step 31100: loss: 0.29338251054286957, lr: 1e-05
2023-12-21 11:13:41 INFO     	 * (global step 31150: loss: 0.27093957364559174, lr: 1e-05
2023-12-21 11:13:49 INFO     	 * (global step 31200: loss: 0.35675978660583496, lr: 1e-05
2023-12-21 11:13:57 INFO     	 * (global step 31250: loss: 0.2982502654194832, lr: 1e-05
2023-12-21 11:14:05 INFO     	 * (global step 31300: loss: 0.2690213546156883, lr: 1e-05
2023-12-21 11:14:13 INFO     	 * (global step 31350: loss: 0.40094031393527985, lr: 1e-05
2023-12-21 11:14:21 INFO     	 * (global step 31400: loss: 0.3229938894510269, lr: 1e-05
2023-12-21 11:14:29 INFO     	 * (global step 31450: loss: 0.32497353851795197, lr: 1e-05
2023-12-21 11:14:38 INFO     	 * (global step 31500: loss: 0.5497768223285675, lr: 1e-05
2023-12-21 11:14:46 INFO     	 * (global step 31550: loss: 0.2683757394552231, lr: 1e-05
2023-12-21 11:14:54 INFO     	 * (global step 31600: loss: 0.29766033589839935, lr: 1e-05
2023-12-21 11:15:02 INFO     	 * (global step 31650: loss: 0.22064197808504105, lr: 1e-05
2023-12-21 11:15:10 INFO     	 * (global step 31700: loss: 0.3433607667684555, lr: 1e-05
2023-12-21 11:15:18 INFO     	 * (global step 31750: loss: 0.4681508541107178, lr: 1e-05
2023-12-21 11:15:26 INFO     	 * (global step 31800: loss: 0.23081155866384506, lr: 1e-05
2023-12-21 11:15:34 INFO     	 * (global step 31850: loss: 0.3143806532025337, lr: 1e-05
2023-12-21 11:15:42 INFO     	 * (global step 31900: loss: 0.30081114172935486, lr: 1e-05
2023-12-21 11:15:50 INFO     	 * (global step 31950: loss: 0.26305273175239563, lr: 1e-05
2023-12-21 11:15:58 INFO     	 * (global step 32000: loss: 0.2582591623067856, lr: 1e-05
2023-12-21 11:16:06 INFO     	 * (global step 32050: loss: 0.36449022591114044, lr: 1e-05
2023-12-21 11:16:14 INFO     	 * (global step 32100: loss: 0.3437107652425766, lr: 1e-05
2023-12-21 11:16:22 INFO     	 * (global step 32150: loss: 0.4866834729909897, lr: 1e-05
2023-12-21 11:16:30 INFO     	 * (global step 32200: loss: 0.32429908215999603, lr: 1e-05
2023-12-21 11:16:38 INFO     	 * (global step 32250: loss: 0.22842595726251602, lr: 1e-05
2023-12-21 11:16:46 INFO     	 * (global step 32300: loss: 0.30767805874347687, lr: 1e-05
2023-12-21 11:16:54 INFO     	 * (global step 32350: loss: 0.3105514347553253, lr: 1e-05
2023-12-21 11:17:02 INFO     	 * (global step 32400: loss: 0.2298109233379364, lr: 1e-05
2023-12-21 11:17:10 INFO     	 * (global step 32450: loss: 0.2690374627709389, lr: 1e-05
2023-12-21 11:17:18 INFO     	 * (global step 32500: loss: 0.3491370975971222, lr: 1e-05
2023-12-21 11:17:26 INFO     	 * (global step 32550: loss: 0.31579283624887466, lr: 1e-05
2023-12-21 11:17:34 INFO     	 * (global step 32600: loss: 0.18136832863092422, lr: 1e-05
2023-12-21 11:17:42 INFO     	 * (global step 32650: loss: 0.502609446644783, lr: 1e-05
2023-12-21 11:17:50 INFO     	 * (global step 32700: loss: 0.26829536259174347, lr: 1e-05
2023-12-21 11:17:58 INFO     	 * (global step 32750: loss: 0.3663181737065315, lr: 1e-05
2023-12-21 11:18:06 INFO     	 * (global step 32800: loss: 0.2199251651763916, lr: 1e-05
2023-12-21 11:18:14 INFO     	 * (global step 32850: loss: 0.24296404421329498, lr: 1e-05
2023-12-21 11:18:22 INFO     	 * (global step 32900: loss: 0.4690602868795395, lr: 1e-05
2023-12-21 11:18:30 INFO     	 * (global step 32950: loss: 0.4416368007659912, lr: 1e-05
2023-12-21 11:18:39 INFO     	 * (global step 33000: loss: 0.26951203495264053, lr: 1e-05
2023-12-21 11:18:47 INFO     	 * (global step 33050: loss: 0.2781885415315628, lr: 1e-05
2023-12-21 11:18:49 INFO     [epoch 6/15] average loss: 0.327, lr: 1e-05
2023-12-21 11:18:49 INFO     saving model related files
2023-12-21 11:18:49 INFO     saving model
2023-12-21 11:18:49 INFO     saving tokenizer
2023-12-21 11:18:49 INFO     saving optimizer
2023-12-21 11:18:50 INFO     remove old optimizer files
2023-12-21 11:18:57 INFO     	 * (global step 33100: loss: 0.3186534196138382, lr: 1e-05
2023-12-21 11:19:05 INFO     	 * (global step 33150: loss: 0.352514311671257, lr: 1e-05
2023-12-21 11:19:13 INFO     	 * (global step 33200: loss: 0.3941141664981842, lr: 1e-05
2023-12-21 11:19:21 INFO     	 * (global step 33250: loss: 0.32089246064424515, lr: 1e-05
2023-12-21 11:19:29 INFO     	 * (global step 33300: loss: 0.27556179463863373, lr: 1e-05
2023-12-21 11:19:37 INFO     	 * (global step 33350: loss: 0.31974298506975174, lr: 1e-05
2023-12-21 11:19:45 INFO     	 * (global step 33400: loss: 0.2522039860486984, lr: 1e-05
2023-12-21 11:19:53 INFO     	 * (global step 33450: loss: 0.3135717958211899, lr: 1e-05
2023-12-21 11:20:01 INFO     	 * (global step 33500: loss: 0.2620355635881424, lr: 1e-05
2023-12-21 11:20:09 INFO     	 * (global step 33550: loss: 0.35292041301727295, lr: 1e-05
2023-12-21 11:20:18 INFO     	 * (global step 33600: loss: 0.2923809587955475, lr: 1e-05
2023-12-21 11:20:26 INFO     	 * (global step 33650: loss: 0.40385934710502625, lr: 1e-05
2023-12-21 11:20:34 INFO     	 * (global step 33700: loss: 0.24387817084789276, lr: 1e-05
2023-12-21 11:20:42 INFO     	 * (global step 33750: loss: 0.4643455147743225, lr: 1e-05
2023-12-21 11:20:50 INFO     	 * (global step 33800: loss: 0.39833807945251465, lr: 1e-05
2023-12-21 11:20:58 INFO     	 * (global step 33850: loss: 0.33615805208683014, lr: 1e-05
2023-12-21 11:21:06 INFO     	 * (global step 33900: loss: 0.329690158367157, lr: 1e-05
2023-12-21 11:21:14 INFO     	 * (global step 33950: loss: 0.29852503538131714, lr: 1e-05
2023-12-21 11:21:22 INFO     	 * (global step 34000: loss: 0.2438361719250679, lr: 1e-05
2023-12-21 11:21:31 INFO     	 * (global step 34050: loss: 0.36145608127117157, lr: 1e-05
2023-12-21 11:21:39 INFO     	 * (global step 34100: loss: 0.2951965481042862, lr: 1e-05
2023-12-21 11:21:47 INFO     	 * (global step 34150: loss: 0.5192602574825287, lr: 1e-05
2023-12-21 11:21:55 INFO     	 * (global step 34200: loss: 0.4118964374065399, lr: 1e-05
2023-12-21 11:22:03 INFO     	 * (global step 34250: loss: 0.33067844808101654, lr: 1e-05
2023-12-21 11:22:11 INFO     	 * (global step 34300: loss: 0.2973864674568176, lr: 1e-05
2023-12-21 11:22:19 INFO     	 * (global step 34350: loss: 0.19232956320047379, lr: 1e-05
2023-12-21 11:22:27 INFO     	 * (global step 34400: loss: 0.2764914557337761, lr: 1e-05
2023-12-21 11:22:35 INFO     	 * (global step 34450: loss: 0.375190794467926, lr: 1e-05
2023-12-21 11:22:43 INFO     	 * (global step 34500: loss: 0.29373185336589813, lr: 1e-05
2023-12-21 11:22:51 INFO     	 * (global step 34550: loss: 0.31420400738716125, lr: 1e-05
2023-12-21 11:22:59 INFO     	 * (global step 34600: loss: 0.2697261646389961, lr: 1e-05
2023-12-21 11:23:07 INFO     	 * (global step 34650: loss: 0.2566451132297516, lr: 1e-05
2023-12-21 11:23:15 INFO     	 * (global step 34700: loss: 0.3913523405790329, lr: 1e-05
2023-12-21 11:23:23 INFO     	 * (global step 34750: loss: 0.28189459443092346, lr: 1e-05
2023-12-21 11:23:31 INFO     	 * (global step 34800: loss: 0.23586077988147736, lr: 1e-05
2023-12-21 11:23:39 INFO     	 * (global step 34850: loss: 0.2905382663011551, lr: 1e-05
2023-12-21 11:23:47 INFO     	 * (global step 34900: loss: 0.2214047908782959, lr: 1e-05
2023-12-21 11:23:55 INFO     	 * (global step 34950: loss: 0.3132929503917694, lr: 1e-05
2023-12-21 11:24:03 INFO     	 * (global step 35000: loss: 0.32775282859802246, lr: 1e-05
2023-12-21 11:24:11 INFO     	 * (global step 35050: loss: 0.44792304933071136, lr: 1e-05
2023-12-21 11:24:19 INFO     	 * (global step 35100: loss: 0.4009234309196472, lr: 1e-05
2023-12-21 11:24:27 INFO     	 * (global step 35150: loss: 0.3820738196372986, lr: 1e-05
2023-12-21 11:24:35 INFO     	 * (global step 35200: loss: 0.3233814612030983, lr: 1e-05
2023-12-21 11:24:43 INFO     	 * (global step 35250: loss: 0.32532133162021637, lr: 1e-05
2023-12-21 11:24:51 INFO     	 * (global step 35300: loss: 0.382507361471653, lr: 1e-05
2023-12-21 11:24:59 INFO     	 * (global step 35350: loss: 0.25954075902700424, lr: 1e-05
2023-12-21 11:25:07 INFO     	 * (global step 35400: loss: 0.24513333290815353, lr: 1e-05
2023-12-21 11:25:15 INFO     	 * (global step 35450: loss: 0.3992239683866501, lr: 1e-05
2023-12-21 11:25:23 INFO     	 * (global step 35500: loss: 0.33542538434267044, lr: 1e-05
2023-12-21 11:25:31 INFO     	 * (global step 35550: loss: 0.20240860432386398, lr: 1e-05
2023-12-21 11:25:39 INFO     	 * (global step 35600: loss: 0.31243132054805756, lr: 1e-05
2023-12-21 11:25:47 INFO     	 * (global step 35650: loss: 0.35465317964553833, lr: 1e-05
2023-12-21 11:25:55 INFO     	 * (global step 35700: loss: 0.4107293635606766, lr: 1e-05
2023-12-21 11:26:03 INFO     	 * (global step 35750: loss: 0.2610737234354019, lr: 1e-05
2023-12-21 11:26:11 INFO     	 * (global step 35800: loss: 0.25254013389348984, lr: 1e-05
2023-12-21 11:26:19 INFO     	 * (global step 35850: loss: 0.2740221917629242, lr: 1e-05
2023-12-21 11:26:27 INFO     	 * (global step 35900: loss: 0.32434406876564026, lr: 1e-05
2023-12-21 11:26:35 INFO     	 * (global step 35950: loss: 0.47205010056495667, lr: 1e-05
2023-12-21 11:26:43 INFO     	 * (global step 36000: loss: 0.2611227184534073, lr: 1e-05
2023-12-21 11:26:51 INFO     	 * (global step 36050: loss: 0.11819364875555038, lr: 1e-05
2023-12-21 11:26:59 INFO     	 * (global step 36100: loss: 0.39339354634284973, lr: 1e-05
2023-12-21 11:27:07 INFO     	 * (global step 36150: loss: 0.5313991606235504, lr: 1e-05
2023-12-21 11:27:15 INFO     	 * (global step 36200: loss: 0.38864946365356445, lr: 1e-05
2023-12-21 11:27:23 INFO     	 * (global step 36250: loss: 0.25761476159095764, lr: 1e-05
2023-12-21 11:27:31 INFO     	 * (global step 36300: loss: 0.311894953250885, lr: 1e-05
2023-12-21 11:27:39 INFO     	 * (global step 36350: loss: 0.417075052857399, lr: 1e-05
2023-12-21 11:27:47 INFO     	 * (global step 36400: loss: 0.2804511487483978, lr: 1e-05
2023-12-21 11:27:55 INFO     	 * (global step 36450: loss: 0.24064332991838455, lr: 1e-05
2023-12-21 11:28:03 INFO     	 * (global step 36500: loss: 0.4488314390182495, lr: 1e-05
2023-12-21 11:28:11 INFO     	 * (global step 36550: loss: 0.2407732829451561, lr: 1e-05
2023-12-21 11:28:19 INFO     	 * (global step 36600: loss: 0.554976761341095, lr: 1e-05
2023-12-21 11:28:27 INFO     	 * (global step 36650: loss: 0.3114382326602936, lr: 1e-05
2023-12-21 11:28:35 INFO     	 * (global step 36700: loss: 0.2777753993868828, lr: 1e-05
2023-12-21 11:28:43 INFO     	 * (global step 36750: loss: 0.41950973868370056, lr: 1e-05
2023-12-21 11:28:51 INFO     	 * (global step 36800: loss: 0.2711172252893448, lr: 1e-05
2023-12-21 11:28:59 INFO     	 * (global step 36850: loss: 0.23633237183094025, lr: 1e-05
2023-12-21 11:29:07 INFO     	 * (global step 36900: loss: 0.34898194670677185, lr: 1e-05
2023-12-21 11:29:15 INFO     	 * (global step 36950: loss: 0.3410266935825348, lr: 1e-05
2023-12-21 11:29:23 INFO     	 * (global step 37000: loss: 0.34468311071395874, lr: 1e-05
2023-12-21 11:29:31 INFO     	 * (global step 37050: loss: 0.24186959862709045, lr: 1e-05
2023-12-21 11:29:39 INFO     	 * (global step 37100: loss: 0.3489697128534317, lr: 1e-05
2023-12-21 11:29:47 INFO     	 * (global step 37150: loss: 0.24399293959140778, lr: 1e-05
2023-12-21 11:29:55 INFO     	 * (global step 37200: loss: 0.5662044286727905, lr: 1e-05
2023-12-21 11:30:03 INFO     	 * (global step 37250: loss: 0.2376432716846466, lr: 1e-05
2023-12-21 11:30:11 INFO     	 * (global step 37300: loss: 0.31168539077043533, lr: 1e-05
2023-12-21 11:30:19 INFO     	 * (global step 37350: loss: 0.15344403311610222, lr: 1e-05
2023-12-21 11:30:27 INFO     	 * (global step 37400: loss: 0.29190436005592346, lr: 1e-05
2023-12-21 11:30:35 INFO     	 * (global step 37450: loss: 0.3804033398628235, lr: 1e-05
2023-12-21 11:30:43 INFO     	 * (global step 37500: loss: 0.31589844822883606, lr: 1e-05
2023-12-21 11:30:51 INFO     	 * (global step 37550: loss: 0.251175120472908, lr: 1e-05
2023-12-21 11:31:00 INFO     	 * (global step 37600: loss: 0.26299455761909485, lr: 1e-05
2023-12-21 11:31:08 INFO     	 * (global step 37650: loss: 0.185634046792984, lr: 1e-05
2023-12-21 11:31:16 INFO     	 * (global step 37700: loss: 0.28939059376716614, lr: 1e-05
2023-12-21 11:31:24 INFO     	 * (global step 37750: loss: 0.2009814977645874, lr: 1e-05
2023-12-21 11:31:29 INFO     [epoch 7/15] average loss: 0.322, lr: 1e-05
2023-12-21 11:31:29 INFO     saving model related files
2023-12-21 11:31:29 INFO     saving model
2023-12-21 11:31:30 INFO     saving tokenizer
2023-12-21 11:31:30 INFO     saving optimizer
2023-12-21 11:31:31 INFO     remove old optimizer files
2023-12-21 11:31:34 INFO     	 * (global step 37800: loss: 0.4176205098628998, lr: 1e-05
2023-12-21 11:31:42 INFO     	 * (global step 37850: loss: 0.2859983295202255, lr: 1e-05
2023-12-21 11:31:50 INFO     	 * (global step 37900: loss: 0.3163832426071167, lr: 1e-05
2023-12-21 11:31:58 INFO     	 * (global step 37950: loss: 0.31043099611997604, lr: 1e-05
2023-12-21 11:32:06 INFO     	 * (global step 38000: loss: 0.3746241182088852, lr: 1e-05
2023-12-21 11:32:14 INFO     	 * (global step 38050: loss: 0.30295248329639435, lr: 1e-05
2023-12-21 11:32:22 INFO     	 * (global step 38100: loss: 0.31525421887636185, lr: 1e-05
2023-12-21 11:32:30 INFO     	 * (global step 38150: loss: 0.32815317809581757, lr: 1e-05
2023-12-21 11:32:38 INFO     	 * (global step 38200: loss: 0.40235061943531036, lr: 1e-05
2023-12-21 11:32:46 INFO     	 * (global step 38250: loss: 0.3833465576171875, lr: 1e-05
2023-12-21 11:32:55 INFO     	 * (global step 38300: loss: 0.2279943749308586, lr: 1e-05
2023-12-21 11:33:03 INFO     	 * (global step 38350: loss: 0.3399975001811981, lr: 1e-05
2023-12-21 11:33:11 INFO     	 * (global step 38400: loss: 0.3678940236568451, lr: 1e-05
2023-12-21 11:33:20 INFO     	 * (global step 38450: loss: 0.37932321429252625, lr: 1e-05
2023-12-21 11:33:28 INFO     	 * (global step 38500: loss: 0.2664560526609421, lr: 1e-05
2023-12-21 11:33:36 INFO     	 * (global step 38550: loss: 0.3482109159231186, lr: 1e-05
2023-12-21 11:33:44 INFO     	 * (global step 38600: loss: 0.3724062442779541, lr: 1e-05
2023-12-21 11:33:52 INFO     	 * (global step 38650: loss: 0.16298790276050568, lr: 1e-05
2023-12-21 11:34:00 INFO     	 * (global step 38700: loss: 0.27386049181222916, lr: 1e-05
2023-12-21 11:34:08 INFO     	 * (global step 38750: loss: 0.3111532926559448, lr: 1e-05
2023-12-21 11:34:16 INFO     	 * (global step 38800: loss: 0.2457091584801674, lr: 1e-05
2023-12-21 11:34:25 INFO     	 * (global step 38850: loss: 0.46396908164024353, lr: 1e-05
2023-12-21 11:34:33 INFO     	 * (global step 38900: loss: 0.25748198479413986, lr: 1e-05
2023-12-21 11:34:41 INFO     	 * (global step 38950: loss: 0.26264993846416473, lr: 1e-05
2023-12-21 11:34:49 INFO     	 * (global step 39000: loss: 0.2891458794474602, lr: 1e-05
2023-12-21 11:34:57 INFO     	 * (global step 39050: loss: 0.3241334557533264, lr: 1e-05
2023-12-21 11:35:06 INFO     	 * (global step 39100: loss: 0.2662946507334709, lr: 1e-05
2023-12-21 11:35:14 INFO     	 * (global step 39150: loss: 0.21451126039028168, lr: 1e-05
2023-12-21 11:35:22 INFO     	 * (global step 39200: loss: 0.3255555331707001, lr: 1e-05
2023-12-21 11:35:30 INFO     	 * (global step 39250: loss: 0.28398558497428894, lr: 1e-05
2023-12-21 11:35:39 INFO     	 * (global step 39300: loss: 0.22205401957035065, lr: 1e-05
2023-12-21 11:35:47 INFO     	 * (global step 39350: loss: 0.24955643713474274, lr: 1e-05
2023-12-21 11:35:55 INFO     	 * (global step 39400: loss: 0.22312834858894348, lr: 1e-05
2023-12-21 11:36:04 INFO     	 * (global step 39450: loss: 0.2584720253944397, lr: 1e-05
2023-12-21 11:36:12 INFO     	 * (global step 39500: loss: 0.25378625094890594, lr: 1e-05
2023-12-21 11:36:20 INFO     	 * (global step 39550: loss: 0.23353469371795654, lr: 1e-05
2023-12-21 11:36:28 INFO     	 * (global step 39600: loss: 0.33849989622831345, lr: 1e-05
2023-12-21 11:36:37 INFO     	 * (global step 39650: loss: 0.27919338643550873, lr: 1e-05
2023-12-21 11:36:46 INFO     	 * (global step 39700: loss: 0.24400068074464798, lr: 1e-05
2023-12-21 11:36:54 INFO     	 * (global step 39750: loss: 0.3783397525548935, lr: 1e-05
2023-12-21 11:37:02 INFO     	 * (global step 39800: loss: 0.42142418026924133, lr: 1e-05
2023-12-21 11:37:11 INFO     	 * (global step 39850: loss: 0.3983116149902344, lr: 1e-05
2023-12-21 11:37:19 INFO     	 * (global step 39900: loss: 0.40891388058662415, lr: 1e-05
2023-12-21 11:37:27 INFO     	 * (global step 39950: loss: 0.42475274205207825, lr: 1e-05
2023-12-21 11:37:36 INFO     	 * (global step 40000: loss: 0.2510555759072304, lr: 1e-05
2023-12-21 11:37:44 INFO     	 * (global step 40050: loss: 0.27956147491931915, lr: 1e-05
2023-12-21 11:37:53 INFO     	 * (global step 40100: loss: 0.3211284875869751, lr: 1e-05
2023-12-21 11:38:01 INFO     	 * (global step 40150: loss: 0.25944481790065765, lr: 1e-05
2023-12-21 11:38:09 INFO     	 * (global step 40200: loss: 0.3240910470485687, lr: 1e-05
2023-12-21 11:38:18 INFO     	 * (global step 40250: loss: 0.3871510177850723, lr: 1e-05
2023-12-21 11:38:26 INFO     	 * (global step 40300: loss: 0.2970662862062454, lr: 1e-05
2023-12-21 11:38:35 INFO     	 * (global step 40350: loss: 0.3034994900226593, lr: 1e-05
2023-12-21 11:38:43 INFO     	 * (global step 40400: loss: 0.25560885667800903, lr: 1e-05
2023-12-21 11:38:51 INFO     	 * (global step 40450: loss: 0.23575995117425919, lr: 1e-05
2023-12-21 11:38:59 INFO     	 * (global step 40500: loss: 0.22116240113973618, lr: 1e-05
2023-12-21 11:39:08 INFO     	 * (global step 40550: loss: 0.27373241633176804, lr: 1e-05
2023-12-21 11:39:16 INFO     	 * (global step 40600: loss: 0.22338669002056122, lr: 1e-05
2023-12-21 11:39:24 INFO     	 * (global step 40650: loss: 0.34473367035388947, lr: 1e-05
2023-12-21 11:39:33 INFO     	 * (global step 40700: loss: 0.42756666243076324, lr: 1e-05
2023-12-21 11:39:42 INFO     	 * (global step 40750: loss: 0.2993316054344177, lr: 1e-05
2023-12-21 11:39:50 INFO     	 * (global step 40800: loss: 0.39050059020519257, lr: 1e-05
2023-12-21 11:39:58 INFO     	 * (global step 40850: loss: 0.32899952679872513, lr: 1e-05
2023-12-21 11:40:07 INFO     	 * (global step 40900: loss: 0.32406700402498245, lr: 1e-05
2023-12-21 11:40:15 INFO     	 * (global step 40950: loss: 0.24638381600379944, lr: 1e-05
2023-12-21 11:40:23 INFO     	 * (global step 41000: loss: 0.41353563219308853, lr: 1e-05
2023-12-21 11:40:31 INFO     	 * (global step 41050: loss: 0.27835826575756073, lr: 1e-05
2023-12-21 11:40:40 INFO     	 * (global step 41100: loss: 0.26610860228538513, lr: 1e-05
2023-12-21 11:40:48 INFO     	 * (global step 41150: loss: 0.28916536271572113, lr: 1e-05
2023-12-21 11:40:57 INFO     	 * (global step 41200: loss: 0.2884213775396347, lr: 1e-05
2023-12-21 11:41:05 INFO     	 * (global step 41250: loss: 0.21287419646978378, lr: 1e-05
2023-12-21 11:41:13 INFO     	 * (global step 41300: loss: 0.40550366044044495, lr: 1e-05
2023-12-21 11:41:22 INFO     	 * (global step 41350: loss: 0.2450670748949051, lr: 1e-05
2023-12-21 11:41:30 INFO     	 * (global step 41400: loss: 0.19989271461963654, lr: 1e-05
2023-12-21 11:41:38 INFO     	 * (global step 41450: loss: 0.3942039906978607, lr: 1e-05
2023-12-21 11:41:47 INFO     	 * (global step 41500: loss: 0.29842910915613174, lr: 1e-05
2023-12-21 11:41:55 INFO     	 * (global step 41550: loss: 0.33524008840322495, lr: 1e-05
2023-12-21 11:42:04 INFO     	 * (global step 41600: loss: 0.4111940264701843, lr: 1e-05
2023-12-21 11:42:12 INFO     	 * (global step 41650: loss: 0.29339204728603363, lr: 1e-05
2023-12-21 11:42:21 INFO     	 * (global step 41700: loss: 0.38633814454078674, lr: 1e-05
2023-12-21 11:42:29 INFO     	 * (global step 41750: loss: 0.19878830760717392, lr: 1e-05
2023-12-21 11:42:38 INFO     	 * (global step 41800: loss: 0.32265505194664, lr: 1e-05
2023-12-21 11:42:46 INFO     	 * (global step 41850: loss: 0.3567347228527069, lr: 1e-05
2023-12-21 11:42:54 INFO     	 * (global step 41900: loss: 0.36738668382167816, lr: 1e-05
2023-12-21 11:43:03 INFO     	 * (global step 41950: loss: 0.3394656777381897, lr: 1e-05
2023-12-21 11:43:11 INFO     	 * (global step 42000: loss: 0.22011437267065048, lr: 1e-05
2023-12-21 11:43:19 INFO     	 * (global step 42050: loss: 0.22533517330884933, lr: 1e-05
2023-12-21 11:43:28 INFO     	 * (global step 42100: loss: 0.3399377912282944, lr: 1e-05
2023-12-21 11:43:36 INFO     	 * (global step 42150: loss: 0.23273517936468124, lr: 1e-05
2023-12-21 11:43:44 INFO     	 * (global step 42200: loss: 0.3017733171582222, lr: 1e-05
2023-12-21 11:43:52 INFO     	 * (global step 42250: loss: 0.32546277344226837, lr: 1e-05
2023-12-21 11:44:01 INFO     	 * (global step 42300: loss: 0.2925695329904556, lr: 1e-05
2023-12-21 11:44:09 INFO     	 * (global step 42350: loss: 0.20765119791030884, lr: 1e-05
2023-12-21 11:44:17 INFO     	 * (global step 42400: loss: 0.3504660874605179, lr: 1e-05
2023-12-21 11:44:26 INFO     	 * (global step 42450: loss: 0.47021983563899994, lr: 1e-05
2023-12-21 11:44:34 INFO     	 * (global step 42500: loss: 0.2639806866645813, lr: 1e-05
2023-12-21 11:44:36 INFO     [epoch 8/15] average loss: 0.318, lr: 1e-05
2023-12-21 11:44:36 INFO     saving model related files
2023-12-21 11:44:36 INFO     saving model
2023-12-21 11:44:36 INFO     saving tokenizer
2023-12-21 11:44:36 INFO     saving optimizer
2023-12-21 11:44:37 INFO     remove old optimizer files
2023-12-21 11:44:45 INFO     	 * (global step 42550: loss: 0.3216257691383362, lr: 1e-05
2023-12-21 11:44:53 INFO     	 * (global step 42600: loss: 0.3363599330186844, lr: 1e-05
2023-12-21 11:45:02 INFO     	 * (global step 42650: loss: 0.36189571022987366, lr: 1e-05
2023-12-21 11:45:10 INFO     	 * (global step 42700: loss: 0.22236352413892746, lr: 1e-05
2023-12-21 11:45:19 INFO     	 * (global step 42750: loss: 0.3293147385120392, lr: 1e-05
2023-12-21 11:45:27 INFO     	 * (global step 42800: loss: 0.5890789031982422, lr: 1e-05
2023-12-21 11:45:35 INFO     	 * (global step 42850: loss: 0.25346149504184723, lr: 1e-05
2023-12-21 11:45:43 INFO     	 * (global step 42900: loss: 0.3348177671432495, lr: 1e-05
2023-12-21 11:45:52 INFO     	 * (global step 42950: loss: 0.3357994109392166, lr: 1e-05
2023-12-21 11:46:00 INFO     	 * (global step 43000: loss: 0.41029560565948486, lr: 1e-05
2023-12-21 11:46:09 INFO     	 * (global step 43050: loss: 0.2904299795627594, lr: 1e-05
2023-12-21 11:46:17 INFO     	 * (global step 43100: loss: 0.3602624088525772, lr: 1e-05
2023-12-21 11:46:25 INFO     	 * (global step 43150: loss: 0.3026631325483322, lr: 1e-05
2023-12-21 11:46:34 INFO     	 * (global step 43200: loss: 0.28749780356884, lr: 1e-05
2023-12-21 11:46:42 INFO     	 * (global step 43250: loss: 0.24865959584712982, lr: 1e-05
2023-12-21 11:46:51 INFO     	 * (global step 43300: loss: 0.5684331655502319, lr: 1e-05
2023-12-21 11:46:59 INFO     	 * (global step 43350: loss: 0.34853191673755646, lr: 1e-05
2023-12-21 11:47:07 INFO     	 * (global step 43400: loss: 0.26412368565797806, lr: 1e-05
2023-12-21 11:47:16 INFO     	 * (global step 43450: loss: 0.2847713753581047, lr: 1e-05
2023-12-21 11:47:25 INFO     	 * (global step 43500: loss: 0.45236213505268097, lr: 1e-05
2023-12-21 11:47:33 INFO     	 * (global step 43550: loss: 0.33791017532348633, lr: 1e-05
2023-12-21 11:47:42 INFO     	 * (global step 43600: loss: 0.3975478559732437, lr: 1e-05
2023-12-21 11:47:50 INFO     	 * (global step 43650: loss: 0.5274688005447388, lr: 1e-05
2023-12-21 11:47:59 INFO     	 * (global step 43700: loss: 0.28736652433872223, lr: 1e-05
2023-12-21 11:48:07 INFO     	 * (global step 43750: loss: 0.3516732454299927, lr: 1e-05
2023-12-21 11:48:16 INFO     	 * (global step 43800: loss: 0.268712654709816, lr: 1e-05
2023-12-21 11:48:24 INFO     	 * (global step 43850: loss: 0.30274464190006256, lr: 1e-05
2023-12-21 11:48:32 INFO     	 * (global step 43900: loss: 0.3934112638235092, lr: 1e-05
2023-12-21 11:48:41 INFO     	 * (global step 43950: loss: 0.23580166697502136, lr: 1e-05
2023-12-21 11:48:49 INFO     	 * (global step 44000: loss: 0.36191748082637787, lr: 1e-05
2023-12-21 11:48:57 INFO     	 * (global step 44050: loss: 0.30052438378334045, lr: 1e-05
2023-12-21 11:49:05 INFO     	 * (global step 44100: loss: 0.24782432615756989, lr: 1e-05
2023-12-21 11:49:13 INFO     	 * (global step 44150: loss: 0.41672752797603607, lr: 1e-05
2023-12-21 11:49:21 INFO     	 * (global step 44200: loss: 0.2124248817563057, lr: 1e-05
2023-12-21 11:49:30 INFO     	 * (global step 44250: loss: 0.3404236435890198, lr: 1e-05
2023-12-21 11:49:38 INFO     	 * (global step 44300: loss: 0.7336795777082443, lr: 1e-05
2023-12-21 11:49:46 INFO     	 * (global step 44350: loss: 0.2645478844642639, lr: 1e-05
2023-12-21 11:49:54 INFO     	 * (global step 44400: loss: 0.21399306505918503, lr: 1e-05
2023-12-21 11:50:02 INFO     	 * (global step 44450: loss: 0.38366083800792694, lr: 1e-05
2023-12-21 11:50:10 INFO     	 * (global step 44500: loss: 0.41692816466093063, lr: 1e-05
2023-12-21 11:50:18 INFO     	 * (global step 44550: loss: 0.17632747069001198, lr: 1e-05
2023-12-21 11:50:26 INFO     	 * (global step 44600: loss: 0.215827576816082, lr: 1e-05
2023-12-21 11:50:34 INFO     	 * (global step 44650: loss: 0.3861384838819504, lr: 1e-05
2023-12-21 11:50:42 INFO     	 * (global step 44700: loss: 0.27351805567741394, lr: 1e-05
2023-12-21 11:50:50 INFO     	 * (global step 44750: loss: 0.2282707840204239, lr: 1e-05
2023-12-21 11:50:58 INFO     	 * (global step 44800: loss: 0.28013429045677185, lr: 1e-05
2023-12-21 11:51:06 INFO     	 * (global step 44850: loss: 0.29767581075429916, lr: 1e-05
2023-12-21 11:51:14 INFO     	 * (global step 44900: loss: 0.24403317272663116, lr: 1e-05
2023-12-21 11:51:22 INFO     	 * (global step 44950: loss: 0.3180968686938286, lr: 1e-05
2023-12-21 11:51:30 INFO     	 * (global step 45000: loss: 0.2597707062959671, lr: 1e-05
2023-12-21 11:51:38 INFO     	 * (global step 45050: loss: 0.30089737474918365, lr: 1e-05
2023-12-21 11:51:46 INFO     	 * (global step 45100: loss: 0.36062680184841156, lr: 1e-05
2023-12-21 11:51:54 INFO     	 * (global step 45150: loss: 0.3840329349040985, lr: 1e-05
2023-12-21 11:52:02 INFO     	 * (global step 45200: loss: 0.22330265492200851, lr: 1e-05
2023-12-21 11:52:10 INFO     	 * (global step 45250: loss: 0.32547086477279663, lr: 1e-05
2023-12-21 11:52:18 INFO     	 * (global step 45300: loss: 0.24187958240509033, lr: 1e-05
2023-12-21 11:52:26 INFO     	 * (global step 45350: loss: 0.2367829978466034, lr: 1e-05
2023-12-21 11:52:34 INFO     	 * (global step 45400: loss: 0.27336485683918, lr: 1e-05
2023-12-21 11:52:42 INFO     	 * (global step 45450: loss: 0.2143252193927765, lr: 1e-05
2023-12-21 11:52:50 INFO     	 * (global step 45500: loss: 0.2389085292816162, lr: 1e-05
2023-12-21 11:52:58 INFO     	 * (global step 45550: loss: 0.31430794298648834, lr: 1e-05
2023-12-21 11:53:06 INFO     	 * (global step 45600: loss: 0.306920126080513, lr: 1e-05
2023-12-21 11:53:14 INFO     	 * (global step 45650: loss: 0.35220104455947876, lr: 1e-05
2023-12-21 11:53:22 INFO     	 * (global step 45700: loss: 0.266439750790596, lr: 1e-05
2023-12-21 11:53:30 INFO     	 * (global step 45750: loss: 0.49024058878421783, lr: 1e-05
2023-12-21 11:53:38 INFO     	 * (global step 45800: loss: 0.3168889135122299, lr: 1e-05
2023-12-21 11:53:46 INFO     	 * (global step 45850: loss: 0.3165690451860428, lr: 1e-05
2023-12-21 11:53:54 INFO     	 * (global step 45900: loss: 0.48024629056453705, lr: 1e-05
2023-12-21 11:54:02 INFO     	 * (global step 45950: loss: 0.37771813571453094, lr: 1e-05
2023-12-21 11:54:10 INFO     	 * (global step 46000: loss: 0.24411606043577194, lr: 1e-05
2023-12-21 11:54:18 INFO     	 * (global step 46050: loss: 0.27929164469242096, lr: 1e-05
2023-12-21 11:54:27 INFO     	 * (global step 46100: loss: 0.43428128957748413, lr: 1e-05
2023-12-21 11:54:35 INFO     	 * (global step 46150: loss: 0.20752469450235367, lr: 1e-05
2023-12-21 11:54:43 INFO     	 * (global step 46200: loss: 0.2044719234108925, lr: 1e-05
2023-12-21 11:54:51 INFO     	 * (global step 46250: loss: 0.27478037774562836, lr: 1e-05
2023-12-21 11:54:59 INFO     	 * (global step 46300: loss: 0.24964886158704758, lr: 1e-05
2023-12-21 11:55:07 INFO     	 * (global step 46350: loss: 0.31747375428676605, lr: 1e-05
2023-12-21 11:55:15 INFO     	 * (global step 46400: loss: 0.31509532034397125, lr: 1e-05
2023-12-21 11:55:24 INFO     	 * (global step 46450: loss: 0.38732556998729706, lr: 1e-05
2023-12-21 11:55:32 INFO     	 * (global step 46500: loss: 0.19104962050914764, lr: 1e-05
2023-12-21 11:55:40 INFO     	 * (global step 46550: loss: 0.6307331919670105, lr: 1e-05
2023-12-21 11:55:48 INFO     	 * (global step 46600: loss: 0.3421712964773178, lr: 1e-05
2023-12-21 11:55:56 INFO     	 * (global step 46650: loss: 0.39941397309303284, lr: 1e-05
2023-12-21 11:56:04 INFO     	 * (global step 46700: loss: 0.34621410071849823, lr: 1e-05
2023-12-21 11:56:13 INFO     	 * (global step 46750: loss: 0.333707332611084, lr: 1e-05
2023-12-21 11:56:21 INFO     	 * (global step 46800: loss: 0.20576130971312523, lr: 1e-05
2023-12-21 11:56:29 INFO     	 * (global step 46850: loss: 0.5236088335514069, lr: 1e-05
2023-12-21 11:56:37 INFO     	 * (global step 46900: loss: 0.2336743324995041, lr: 1e-05
2023-12-21 11:56:45 INFO     	 * (global step 46950: loss: 0.22787217050790787, lr: 1e-05
2023-12-21 11:56:53 INFO     	 * (global step 47000: loss: 0.41069352626800537, lr: 1e-05
2023-12-21 11:57:01 INFO     	 * (global step 47050: loss: 0.30716900527477264, lr: 1e-05
2023-12-21 11:57:10 INFO     	 * (global step 47100: loss: 0.2434585690498352, lr: 1e-05
2023-12-21 11:57:18 INFO     	 * (global step 47150: loss: 0.2706605941057205, lr: 1e-05
2023-12-21 11:57:26 INFO     	 * (global step 47200: loss: 0.2589821591973305, lr: 1e-05
2023-12-21 11:57:31 INFO     [epoch 9/15] average loss: 0.315, lr: 1e-05
2023-12-21 11:57:31 INFO     saving model related files
2023-12-21 11:57:31 INFO     saving model
2023-12-21 11:57:31 INFO     saving tokenizer
2023-12-21 11:57:31 INFO     saving optimizer
2023-12-21 11:57:32 INFO     remove old optimizer files
2023-12-21 11:57:32 INFO     complete training: model ckpt was saved at small_combined_trained_ckpt/model_nrudfu
2023-12-21 11:57:33 INFO     ## 1st RUN (EVAL): Configuration 0/12 ##
2023-12-21 11:57:41 INFO     use spaCy answer extraction model: positionrank
2023-12-21 11:57:41 INFO     Model `small_combined_trained_ckpt/model_lwtqag/epoch_10`
2023-12-21 11:57:41 INFO     	 * Num of GPU in use: 1
2023-12-21 11:57:41 INFO     	 * Prefix: True
2023-12-21 11:57:41 INFO     	 * Language: en (ignore at the training phase)
2023-12-21 11:57:43 INFO     encode all the data       : 3003
  0%|          | 0/3003 [00:00<?, ?it/s]  8%|â–Š         | 247/3003 [00:00<00:01, 2463.20it/s] 19%|â–ˆâ–‰        | 567/3003 [00:00<00:00, 2889.78it/s] 30%|â–ˆâ–ˆâ–‰       | 886/3003 [00:00<00:00, 3025.83it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1194/3003 [00:00<00:00, 3045.68it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1499/3003 [00:00<00:00, 2990.46it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1845/3003 [00:00<00:00, 3144.72it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2160/3003 [00:00<00:00, 3135.99it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2474/3003 [00:00<00:00, 2678.58it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2753/3003 [00:01<00:00, 1507.61it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2968/3003 [00:01<00:00, 1190.77it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3003/3003 [00:01<00:00, 1868.16it/s]
2023-12-21 11:57:45 INFO     after remove the overflow : 3003
2023-12-21 11:57:45 INFO     preprocessed feature is saved at /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-21 12:11:46 INFO     encode all the data       : 2641
  0%|          | 0/2641 [00:00<?, ?it/s]  8%|â–Š         | 215/2641 [00:00<00:01, 2141.84it/s] 18%|â–ˆâ–Š        | 473/2641 [00:00<00:00, 2397.10it/s] 28%|â–ˆâ–ˆâ–Š       | 728/2641 [00:00<00:00, 2466.44it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 975/2641 [00:00<00:00, 2464.29it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1236/2641 [00:00<00:00, 2515.76it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1498/2641 [00:00<00:00, 2546.70it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1765/2641 [00:00<00:00, 2584.48it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2024/2641 [00:00<00:00, 2584.31it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2283/2641 [00:01<00:00, 1558.69it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2487/2641 [00:01<00:00, 1195.86it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2641/2641 [00:01<00:00, 1633.67it/s]
2023-12-21 12:11:49 INFO     after remove the overflow : 2641
2023-12-21 12:11:49 INFO     preprocessed feature is saved at /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-21 12:24:40 INFO     	Bleu_1: 0.2343218562500167
2023-12-21 12:24:40 INFO     	Bleu_2: 0.13883772225183832
2023-12-21 12:24:40 INFO     	Bleu_3: 0.08474227891655456
2023-12-21 12:24:40 INFO     	Bleu_4: 0.05728467914464016
2023-12-21 12:24:41 INFO     	Bleu_1: 0.22249246703585396
2023-12-21 12:24:41 INFO     	Bleu_2: 0.12963799281070348
2023-12-21 12:24:41 INFO     	Bleu_3: 0.07804222520057927
2023-12-21 12:24:41 INFO     	Bleu_4: 0.05223781575954629
2023-12-21 12:24:41 INFO     ## 1st RUN (EVAL): Configuration 1/12 ##
2023-12-21 12:24:47 INFO     use spaCy answer extraction model: positionrank
2023-12-21 12:24:47 INFO     Model `small_combined_trained_ckpt/model_eszyci/epoch_10`
2023-12-21 12:24:47 INFO     	 * Num of GPU in use: 1
2023-12-21 12:24:47 INFO     	 * Prefix: True
2023-12-21 12:24:47 INFO     	 * Language: en (ignore at the training phase)
2023-12-21 12:24:48 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-21 12:39:26 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-21 12:52:35 INFO     	Bleu_1: 0.2265808450719373
2023-12-21 12:52:35 INFO     	Bleu_2: 0.13405651402475033
2023-12-21 12:52:35 INFO     	Bleu_3: 0.08155911957705717
2023-12-21 12:52:35 INFO     	Bleu_4: 0.05500427185236388
2023-12-21 12:52:36 INFO     	Bleu_1: 0.21672159645594474
2023-12-21 12:52:36 INFO     	Bleu_2: 0.12623956041269407
2023-12-21 12:52:36 INFO     	Bleu_3: 0.07577276415075297
2023-12-21 12:52:36 INFO     	Bleu_4: 0.05063105202161663
2023-12-21 12:52:36 INFO     ## 1st RUN (EVAL): Configuration 2/12 ##
2023-12-21 12:52:42 INFO     use spaCy answer extraction model: positionrank
2023-12-21 12:52:42 INFO     Model `small_combined_trained_ckpt/model_dpyopu/epoch_10`
2023-12-21 12:52:42 INFO     	 * Num of GPU in use: 1
2023-12-21 12:52:42 INFO     	 * Prefix: True
2023-12-21 12:52:42 INFO     	 * Language: en (ignore at the training phase)
2023-12-21 12:52:43 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-21 13:07:04 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-21 13:19:45 INFO     	Bleu_1: 0.2343218562500167
2023-12-21 13:19:45 INFO     	Bleu_2: 0.13883772225183832
2023-12-21 13:19:45 INFO     	Bleu_3: 0.08474227891655456
2023-12-21 13:19:45 INFO     	Bleu_4: 0.05728467914464016
2023-12-21 13:19:46 INFO     	Bleu_1: 0.22249246703585396
2023-12-21 13:19:46 INFO     	Bleu_2: 0.12963799281070348
2023-12-21 13:19:46 INFO     	Bleu_3: 0.07804222520057927
2023-12-21 13:19:46 INFO     	Bleu_4: 0.05223781575954629
2023-12-21 13:19:46 INFO     ## 1st RUN (EVAL): Configuration 3/12 ##
2023-12-21 13:19:52 INFO     use spaCy answer extraction model: positionrank
2023-12-21 13:19:52 INFO     Model `small_combined_trained_ckpt/model_mzgdpa/epoch_10`
2023-12-21 13:19:52 INFO     	 * Num of GPU in use: 1
2023-12-21 13:19:52 INFO     	 * Prefix: True
2023-12-21 13:19:52 INFO     	 * Language: en (ignore at the training phase)
2023-12-21 13:19:54 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-21 13:34:00 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-21 13:46:33 INFO     	Bleu_1: 0.2265808450719373
2023-12-21 13:46:33 INFO     	Bleu_2: 0.13405651402475033
2023-12-21 13:46:33 INFO     	Bleu_3: 0.08155911957705717
2023-12-21 13:46:33 INFO     	Bleu_4: 0.05500427185236388
2023-12-21 13:46:35 INFO     	Bleu_1: 0.21672159645594474
2023-12-21 13:46:35 INFO     	Bleu_2: 0.12623956041269407
2023-12-21 13:46:35 INFO     	Bleu_3: 0.07577276415075297
2023-12-21 13:46:35 INFO     	Bleu_4: 0.05063105202161663
2023-12-21 13:46:35 INFO     ## 1st RUN (EVAL): Configuration 4/12 ##
2023-12-21 13:46:41 INFO     use spaCy answer extraction model: positionrank
2023-12-21 13:46:41 INFO     Model `small_combined_trained_ckpt/model_mntyya/epoch_10`
2023-12-21 13:46:41 INFO     	 * Num of GPU in use: 1
2023-12-21 13:46:41 INFO     	 * Prefix: True
2023-12-21 13:46:41 INFO     	 * Language: en (ignore at the training phase)
2023-12-21 13:46:43 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-21 13:59:09 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-21 14:10:34 INFO     	Bleu_1: 0.24168875651526361
2023-12-21 14:10:34 INFO     	Bleu_2: 0.1420455806250694
2023-12-21 14:10:34 INFO     	Bleu_3: 0.08600099275419883
2023-12-21 14:10:34 INFO     	Bleu_4: 0.057894333626831364
2023-12-21 14:10:35 INFO     	Bleu_1: 0.22518524246533286
2023-12-21 14:10:35 INFO     	Bleu_2: 0.1305572586756236
2023-12-21 14:10:35 INFO     	Bleu_3: 0.07791401929391165
2023-12-21 14:10:35 INFO     	Bleu_4: 0.05194241113573892
2023-12-21 14:10:35 INFO     ## 1st RUN (EVAL): Configuration 5/12 ##
2023-12-21 14:10:41 INFO     use spaCy answer extraction model: positionrank
2023-12-21 14:10:41 INFO     Model `small_combined_trained_ckpt/model_woixzh/epoch_10`
2023-12-21 14:10:41 INFO     	 * Num of GPU in use: 1
2023-12-21 14:10:41 INFO     	 * Prefix: True
2023-12-21 14:10:41 INFO     	 * Language: en (ignore at the training phase)
2023-12-21 14:10:42 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-21 14:23:40 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-21 14:35:18 INFO     	Bleu_1: 0.23417563385480186
2023-12-21 14:35:18 INFO     	Bleu_2: 0.1377273996685953
2023-12-21 14:35:18 INFO     	Bleu_3: 0.08324367935501138
2023-12-21 14:35:18 INFO     	Bleu_4: 0.055892981537013414
2023-12-21 14:35:20 INFO     	Bleu_1: 0.22161809135945315
2023-12-21 14:35:20 INFO     	Bleu_2: 0.12921397010337413
2023-12-21 14:35:20 INFO     	Bleu_3: 0.07778433224473078
2023-12-21 14:35:20 INFO     	Bleu_4: 0.05218362783073054
2023-12-21 14:35:20 INFO     ## 1st RUN (EVAL): Configuration 6/12 ##
2023-12-21 14:35:25 INFO     use spaCy answer extraction model: positionrank
2023-12-21 14:35:25 INFO     Model `small_combined_trained_ckpt/model_sdkaaa/epoch_10`
2023-12-21 14:35:25 INFO     	 * Num of GPU in use: 1
2023-12-21 14:35:25 INFO     	 * Prefix: True
2023-12-21 14:35:25 INFO     	 * Language: en (ignore at the training phase)
2023-12-21 14:35:26 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-21 14:47:53 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-21 14:59:17 INFO     	Bleu_1: 0.24168875651526361
2023-12-21 14:59:17 INFO     	Bleu_2: 0.1420455806250694
2023-12-21 14:59:17 INFO     	Bleu_3: 0.08600099275419883
2023-12-21 14:59:17 INFO     	Bleu_4: 0.057894333626831364
2023-12-21 14:59:18 INFO     	Bleu_1: 0.22518524246533286
2023-12-21 14:59:18 INFO     	Bleu_2: 0.1305572586756236
2023-12-21 14:59:18 INFO     	Bleu_3: 0.07791401929391165
2023-12-21 14:59:18 INFO     	Bleu_4: 0.05194241113573892
2023-12-21 14:59:18 INFO     ## 1st RUN (EVAL): Configuration 7/12 ##
2023-12-21 14:59:24 INFO     use spaCy answer extraction model: positionrank
2023-12-21 14:59:24 INFO     Model `small_combined_trained_ckpt/model_uramvg/epoch_10`
2023-12-21 14:59:24 INFO     	 * Num of GPU in use: 1
2023-12-21 14:59:24 INFO     	 * Prefix: True
2023-12-21 14:59:24 INFO     	 * Language: en (ignore at the training phase)
2023-12-21 14:59:25 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-21 15:12:21 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-21 15:24:13 INFO     	Bleu_1: 0.23417563385480186
2023-12-21 15:24:13 INFO     	Bleu_2: 0.1377273996685953
2023-12-21 15:24:13 INFO     	Bleu_3: 0.08324367935501138
2023-12-21 15:24:13 INFO     	Bleu_4: 0.055892981537013414
2023-12-21 15:24:14 INFO     	Bleu_1: 0.22161809135945315
2023-12-21 15:24:14 INFO     	Bleu_2: 0.12921397010337413
2023-12-21 15:24:14 INFO     	Bleu_3: 0.07778433224473078
2023-12-21 15:24:14 INFO     	Bleu_4: 0.05218362783073054
2023-12-21 15:24:14 INFO     ## 1st RUN (EVAL): Configuration 8/12 ##
2023-12-21 15:24:19 INFO     use spaCy answer extraction model: positionrank
2023-12-21 15:24:19 INFO     Model `small_combined_trained_ckpt/model_nxaqhy/epoch_10`
2023-12-21 15:24:19 INFO     	 * Num of GPU in use: 1
2023-12-21 15:24:19 INFO     	 * Prefix: True
2023-12-21 15:24:19 INFO     	 * Language: en (ignore at the training phase)
2023-12-21 15:24:21 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-21 15:36:18 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-21 15:47:09 INFO     	Bleu_1: 0.2685764364353389
2023-12-21 15:47:09 INFO     	Bleu_2: 0.1545074715537855
2023-12-21 15:47:09 INFO     	Bleu_3: 0.0909072882557253
2023-12-21 15:47:09 INFO     	Bleu_4: 0.06025492648987384
2023-12-21 15:47:10 INFO     	Bleu_1: 0.2531069887775895
2023-12-21 15:47:10 INFO     	Bleu_2: 0.14344317833127945
2023-12-21 15:47:10 INFO     	Bleu_3: 0.08325636736188925
2023-12-21 15:47:10 INFO     	Bleu_4: 0.05467758611057263
2023-12-21 15:47:10 INFO     ## 1st RUN (EVAL): Configuration 9/12 ##
2023-12-21 15:47:16 INFO     use spaCy answer extraction model: positionrank
2023-12-21 15:47:16 INFO     Model `small_combined_trained_ckpt/model_oprhlh/epoch_10`
2023-12-21 15:47:16 INFO     	 * Num of GPU in use: 1
2023-12-21 15:47:16 INFO     	 * Prefix: True
2023-12-21 15:47:16 INFO     	 * Language: en (ignore at the training phase)
2023-12-21 15:47:17 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-21 15:59:13 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-21 16:10:19 INFO     	Bleu_1: 0.25750459326276093
2023-12-21 16:10:19 INFO     	Bleu_2: 0.14949415675022223
2023-12-21 16:10:19 INFO     	Bleu_3: 0.08899118123927179
2023-12-21 16:10:19 INFO     	Bleu_4: 0.05941041226773688
2023-12-21 16:10:20 INFO     	Bleu_1: 0.24294688569902867
2023-12-21 16:10:20 INFO     	Bleu_2: 0.1394139300352437
2023-12-21 16:10:20 INFO     	Bleu_3: 0.08221489085794145
2023-12-21 16:10:20 INFO     	Bleu_4: 0.05460141472713746
2023-12-21 16:10:20 INFO     ## 1st RUN (EVAL): Configuration 10/12 ##
2023-12-21 16:10:25 INFO     use spaCy answer extraction model: positionrank
2023-12-21 16:10:26 INFO     Model `small_combined_trained_ckpt/model_vhyoja/epoch_10`
2023-12-21 16:10:26 INFO     	 * Num of GPU in use: 1
2023-12-21 16:10:26 INFO     	 * Prefix: True
2023-12-21 16:10:26 INFO     	 * Language: en (ignore at the training phase)
2023-12-21 16:10:27 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-21 16:22:08 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-21 16:32:46 INFO     	Bleu_1: 0.2685764364353389
2023-12-21 16:32:46 INFO     	Bleu_2: 0.1545074715537855
2023-12-21 16:32:46 INFO     	Bleu_3: 0.0909072882557253
2023-12-21 16:32:46 INFO     	Bleu_4: 0.06025492648987384
2023-12-21 16:32:47 INFO     	Bleu_1: 0.2531069887775895
2023-12-21 16:32:47 INFO     	Bleu_2: 0.14344317833127945
2023-12-21 16:32:47 INFO     	Bleu_3: 0.08325636736188925
2023-12-21 16:32:47 INFO     	Bleu_4: 0.05467758611057263
2023-12-21 16:32:47 INFO     ## 1st RUN (EVAL): Configuration 11/12 ##
2023-12-21 16:32:53 INFO     use spaCy answer extraction model: positionrank
2023-12-21 16:32:54 INFO     Model `small_combined_trained_ckpt/model_nrudfu/epoch_10`
2023-12-21 16:32:54 INFO     	 * Num of GPU in use: 1
2023-12-21 16:32:54 INFO     	 * Prefix: True
2023-12-21 16:32:54 INFO     	 * Language: en (ignore at the training phase)
2023-12-21 16:32:55 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-21 16:44:48 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-21 16:55:54 INFO     	Bleu_1: 0.25750459326276093
2023-12-21 16:55:54 INFO     	Bleu_2: 0.14949415675022223
2023-12-21 16:55:54 INFO     	Bleu_3: 0.08899118123927179
2023-12-21 16:55:54 INFO     	Bleu_4: 0.05941041226773688
2023-12-21 16:55:55 INFO     	Bleu_1: 0.24294688569902867
2023-12-21 16:55:55 INFO     	Bleu_2: 0.1394139300352437
2023-12-21 16:55:55 INFO     	Bleu_3: 0.08221489085794145
2023-12-21 16:55:55 INFO     	Bleu_4: 0.05460141472713746
2023-12-21 16:55:55 INFO     1st RUN RESULTS (validation/Bleu_4)
2023-12-21 16:55:55 INFO     	 * rank: 0 | metric: 0.06 | model: small_combined_trained_ckpt/model_nxaqhy/epoch_10 |
2023-12-21 16:55:55 INFO     	 * rank: 1 | metric: 0.06 | model: small_combined_trained_ckpt/model_vhyoja/epoch_10 |
2023-12-21 16:55:55 INFO     	 * rank: 2 | metric: 0.059 | model: small_combined_trained_ckpt/model_oprhlh/epoch_10 |
2023-12-21 16:55:55 INFO     	 * rank: 3 | metric: 0.059 | model: small_combined_trained_ckpt/model_nrudfu/epoch_10 |
2023-12-21 16:55:55 INFO     	 * rank: 4 | metric: 0.058 | model: small_combined_trained_ckpt/model_mntyya/epoch_10 |
2023-12-21 16:55:55 INFO     	 * rank: 5 | metric: 0.058 | model: small_combined_trained_ckpt/model_sdkaaa/epoch_10 |
2023-12-21 16:55:55 INFO     	 * rank: 6 | metric: 0.057 | model: small_combined_trained_ckpt/model_lwtqag/epoch_10 |
2023-12-21 16:55:55 INFO     	 * rank: 7 | metric: 0.057 | model: small_combined_trained_ckpt/model_dpyopu/epoch_10 |
2023-12-21 16:55:55 INFO     	 * rank: 8 | metric: 0.056 | model: small_combined_trained_ckpt/model_woixzh/epoch_10 |
2023-12-21 16:55:55 INFO     	 * rank: 9 | metric: 0.056 | model: small_combined_trained_ckpt/model_uramvg/epoch_10 |
2023-12-21 16:55:55 INFO     	 * rank: 10 | metric: 0.055 | model: small_combined_trained_ckpt/model_eszyci/epoch_10 |
2023-12-21 16:55:55 INFO     	 * rank: 11 | metric: 0.055 | model: small_combined_trained_ckpt/model_mzgdpa/epoch_10 |
2023-12-21 16:55:55 INFO     ## 2nd RUN: Configuration 0/5: validation/Bleu_4 = 0.06025492648987384
2023-12-21 16:55:55 INFO     initialize model trainer
2023-12-21 16:55:55 INFO     load config from existing checkpoint at small_combined_trained_ckpt/model_nxaqhy
2023-12-21 16:55:55 INFO     hyperparameters
2023-12-21 16:55:55 INFO     	 * dataset_path: StellarMilk/squad_newsqa
2023-12-21 16:55:55 INFO     	 * dataset_name: default
2023-12-21 16:55:55 INFO     	 * input_types: ['paragraph']
2023-12-21 16:55:55 INFO     	 * output_types: ['questions_answers']
2023-12-21 16:55:55 INFO     	 * prefix_types: ['qag']
2023-12-21 16:55:55 INFO     	 * model: t5-small
2023-12-21 16:55:55 INFO     	 * max_length: 512
2023-12-21 16:55:55 INFO     	 * max_length_output: 512
2023-12-21 16:55:55 INFO     	 * epoch: 15
2023-12-21 16:55:55 INFO     	 * batch: 2
2023-12-21 16:55:55 INFO     	 * lr: 1e-05
2023-12-21 16:55:55 INFO     	 * fp16: False
2023-12-21 16:55:55 INFO     	 * random_seed: 1
2023-12-21 16:55:55 INFO     	 * gradient_accumulation_steps: 4
2023-12-21 16:55:55 INFO     	 * label_smoothing: 0.15
2023-12-21 16:55:55 INFO     load checkpoint from small_combined_trained_ckpt/model_nxaqhy/epoch_10
2023-12-21 16:55:56 INFO     use spaCy answer extraction model: positionrank
2023-12-21 16:55:56 INFO     Model `small_combined_trained_ckpt/model_nxaqhy/epoch_10`
2023-12-21 16:55:56 INFO     	 * Num of GPU in use: 1
2023-12-21 16:55:56 INFO     	 * Prefix: True
2023-12-21 16:55:56 INFO     	 * Language: en (ignore at the training phase)
2023-12-21 16:55:56 INFO     load optimizer from small_combined_trained_ckpt/model_nxaqhy/optimizers/optimizer.10.pt
2023-12-21 16:55:56 INFO     optimizer is loading on cuda
2023-12-21 16:56:01 INFO     dataset preprocessing
2023-12-21 16:56:02 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/squad_newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-21 16:56:04 INFO     start model training
2023-12-21 16:56:20 INFO     	 * (global step 50: loss: 3.163436233997345, lr: 1e-05
2023-12-21 16:56:36 INFO     	 * (global step 100: loss: 2.8110565543174744, lr: 1e-05
2023-12-21 16:56:52 INFO     	 * (global step 150: loss: 2.7333896160125732, lr: 1e-05
2023-12-21 16:57:08 INFO     	 * (global step 200: loss: 2.7830371260643005, lr: 1e-05
2023-12-21 16:57:24 INFO     	 * (global step 250: loss: 2.6858185529708862, lr: 1e-05
2023-12-21 16:57:40 INFO     	 * (global step 300: loss: 2.7866872549057007, lr: 1e-05
2023-12-21 16:57:57 INFO     	 * (global step 350: loss: 2.81090646982193, lr: 1e-05
2023-12-21 16:58:13 INFO     	 * (global step 400: loss: 2.6185497641563416, lr: 1e-05
2023-12-21 16:58:29 INFO     	 * (global step 450: loss: 2.592373788356781, lr: 1e-05
2023-12-21 16:58:45 INFO     	 * (global step 500: loss: 2.5279558897018433, lr: 1e-05
2023-12-21 16:59:01 INFO     	 * (global step 550: loss: 2.554102838039398, lr: 1e-05
2023-12-21 16:59:17 INFO     	 * (global step 600: loss: 2.6530637741088867, lr: 1e-05
2023-12-21 16:59:33 INFO     	 * (global step 650: loss: 2.5613905787467957, lr: 1e-05
2023-12-21 16:59:49 INFO     	 * (global step 700: loss: 2.557552397251129, lr: 1e-05
2023-12-21 17:00:05 INFO     	 * (global step 750: loss: 2.551556706428528, lr: 1e-05
2023-12-21 17:00:22 INFO     	 * (global step 800: loss: 2.6031957864761353, lr: 1e-05
2023-12-21 17:00:38 INFO     	 * (global step 850: loss: 2.4855313301086426, lr: 1e-05
2023-12-21 17:00:54 INFO     	 * (global step 900: loss: 2.4856476187705994, lr: 1e-05
2023-12-21 17:01:10 INFO     	 * (global step 950: loss: 2.534884035587311, lr: 1e-05
2023-12-21 17:01:26 INFO     	 * (global step 1000: loss: 2.5991570353507996, lr: 1e-05
2023-12-21 17:01:42 INFO     	 * (global step 1050: loss: 2.5454436540603638, lr: 1e-05
2023-12-21 17:01:58 INFO     	 * (global step 1100: loss: 2.661665439605713, lr: 1e-05
2023-12-21 17:02:14 INFO     	 * (global step 1150: loss: 2.624966323375702, lr: 1e-05
2023-12-21 17:02:31 INFO     	 * (global step 1200: loss: 2.5246699452400208, lr: 1e-05
2023-12-21 17:02:47 INFO     	 * (global step 1250: loss: 2.5431268215179443, lr: 1e-05
2023-12-21 17:03:03 INFO     	 * (global step 1300: loss: 2.4513277411460876, lr: 1e-05
2023-12-21 17:03:19 INFO     	 * (global step 1350: loss: 2.4857106804847717, lr: 1e-05
2023-12-21 17:03:35 INFO     	 * (global step 1400: loss: 2.4862871170043945, lr: 1e-05
2023-12-21 17:03:51 INFO     	 * (global step 1450: loss: 2.4727378487586975, lr: 1e-05
2023-12-21 17:04:07 INFO     	 * (global step 1500: loss: 2.5959782004356384, lr: 1e-05
2023-12-21 17:04:23 INFO     	 * (global step 1550: loss: 2.5615805983543396, lr: 1e-05
2023-12-21 17:04:39 INFO     	 * (global step 1600: loss: 2.463498532772064, lr: 1e-05
2023-12-21 17:04:56 INFO     	 * (global step 1650: loss: 2.419844686985016, lr: 1e-05
2023-12-21 17:05:12 INFO     	 * (global step 1700: loss: 2.4429659247398376, lr: 1e-05
2023-12-21 17:05:28 INFO     	 * (global step 1750: loss: 2.4797744154930115, lr: 1e-05
2023-12-21 17:05:44 INFO     	 * (global step 1800: loss: 2.4362229108810425, lr: 1e-05
2023-12-21 17:06:00 INFO     	 * (global step 1850: loss: 2.5044819712638855, lr: 1e-05
2023-12-21 17:06:16 INFO     	 * (global step 1900: loss: 2.423645555973053, lr: 1e-05
2023-12-21 17:06:32 INFO     	 * (global step 1950: loss: 2.3759178519248962, lr: 1e-05
2023-12-21 17:06:48 INFO     	 * (global step 2000: loss: 2.5619015097618103, lr: 1e-05
2023-12-21 17:07:04 INFO     	 * (global step 2050: loss: 2.399406850337982, lr: 1e-05
2023-12-21 17:07:21 INFO     	 * (global step 2100: loss: 2.4661190509796143, lr: 1e-05
2023-12-21 17:07:37 INFO     	 * (global step 2150: loss: 2.4803760051727295, lr: 1e-05
2023-12-21 17:07:53 INFO     	 * (global step 2200: loss: 2.537000358104706, lr: 1e-05
2023-12-21 17:08:09 INFO     	 * (global step 2250: loss: 2.432813286781311, lr: 1e-05
2023-12-21 17:08:25 INFO     	 * (global step 2300: loss: 2.4644035696983337, lr: 1e-05
2023-12-21 17:08:41 INFO     	 * (global step 2350: loss: 2.508734703063965, lr: 1e-05
2023-12-21 17:08:45 INFO     [epoch 10/15] average loss: 2.584, lr: 1e-05
2023-12-21 17:08:45 INFO     saving model related files
2023-12-21 17:08:45 INFO     saving model
2023-12-21 17:08:45 INFO     saving tokenizer
2023-12-21 17:08:46 INFO     saving optimizer
2023-12-21 17:08:46 INFO     remove old optimizer files
2023-12-21 17:08:59 INFO     	 * (global step 2400: loss: 2.435197412967682, lr: 1e-05
2023-12-21 17:09:15 INFO     	 * (global step 2450: loss: 2.4765904545783997, lr: 1e-05
2023-12-21 17:09:31 INFO     	 * (global step 2500: loss: 2.4305487275123596, lr: 1e-05
2023-12-21 17:09:47 INFO     	 * (global step 2550: loss: 2.4214544892311096, lr: 1e-05
2023-12-21 17:10:04 INFO     	 * (global step 2600: loss: 2.4472691416740417, lr: 1e-05
2023-12-21 17:10:20 INFO     	 * (global step 2650: loss: 2.417435884475708, lr: 1e-05
2023-12-21 17:10:36 INFO     	 * (global step 2700: loss: 2.4094767570495605, lr: 1e-05
2023-12-21 17:10:52 INFO     	 * (global step 2750: loss: 2.4164494276046753, lr: 1e-05
2023-12-21 17:11:08 INFO     	 * (global step 2800: loss: 2.508060038089752, lr: 1e-05
2023-12-21 17:11:24 INFO     	 * (global step 2850: loss: 2.500226616859436, lr: 1e-05
2023-12-21 17:11:40 INFO     	 * (global step 2900: loss: 2.4442213773727417, lr: 1e-05
2023-12-21 17:11:56 INFO     	 * (global step 2950: loss: 2.4020267128944397, lr: 1e-05
2023-12-21 17:12:12 INFO     	 * (global step 3000: loss: 2.337406873703003, lr: 1e-05
2023-12-21 17:12:29 INFO     	 * (global step 3050: loss: 2.3789570331573486, lr: 1e-05
2023-12-21 17:12:45 INFO     	 * (global step 3100: loss: 2.393254518508911, lr: 1e-05
2023-12-21 17:13:01 INFO     	 * (global step 3150: loss: 2.3851439356803894, lr: 1e-05
2023-12-21 17:13:17 INFO     	 * (global step 3200: loss: 2.4107717871665955, lr: 1e-05
2023-12-21 17:13:33 INFO     	 * (global step 3250: loss: 2.4105862379074097, lr: 1e-05
2023-12-21 17:13:49 INFO     	 * (global step 3300: loss: 2.3723782300949097, lr: 1e-05
2023-12-21 17:14:05 INFO     	 * (global step 3350: loss: 2.3397732377052307, lr: 1e-05
2023-12-21 17:14:22 INFO     	 * (global step 3400: loss: 2.4379043579101562, lr: 1e-05
2023-12-21 17:14:38 INFO     	 * (global step 3450: loss: 2.412994623184204, lr: 1e-05
2023-12-21 17:14:54 INFO     	 * (global step 3500: loss: 2.4544354677200317, lr: 1e-05
2023-12-21 17:15:10 INFO     	 * (global step 3550: loss: 2.3632596135139465, lr: 1e-05
2023-12-21 17:15:26 INFO     	 * (global step 3600: loss: 2.4394586086273193, lr: 1e-05
2023-12-21 17:15:42 INFO     	 * (global step 3650: loss: 2.3871043920516968, lr: 1e-05
2023-12-21 17:15:58 INFO     	 * (global step 3700: loss: 2.349556624889374, lr: 1e-05
2023-12-21 17:16:14 INFO     	 * (global step 3750: loss: 2.459659695625305, lr: 1e-05
2023-12-21 17:16:31 INFO     	 * (global step 3800: loss: 2.3842676281929016, lr: 1e-05
2023-12-21 17:16:47 INFO     	 * (global step 3850: loss: 2.4950695633888245, lr: 1e-05
2023-12-21 17:17:03 INFO     	 * (global step 3900: loss: 2.482613205909729, lr: 1e-05
2023-12-21 17:17:19 INFO     	 * (global step 3950: loss: 2.4494022130966187, lr: 1e-05
2023-12-21 17:17:35 INFO     	 * (global step 4000: loss: 2.3552140593528748, lr: 1e-05
2023-12-21 17:17:51 INFO     	 * (global step 4050: loss: 2.439439117908478, lr: 1e-05
2023-12-21 17:18:08 INFO     	 * (global step 4100: loss: 2.3505462408065796, lr: 1e-05
2023-12-21 17:18:24 INFO     	 * (global step 4150: loss: 2.3977288603782654, lr: 1e-05
2023-12-21 17:18:40 INFO     	 * (global step 4200: loss: 2.4378726482391357, lr: 1e-05
2023-12-21 17:18:56 INFO     	 * (global step 4250: loss: 2.398682475090027, lr: 1e-05
2023-12-21 17:19:12 INFO     	 * (global step 4300: loss: 2.4705655574798584, lr: 1e-05
2023-12-21 17:19:28 INFO     	 * (global step 4350: loss: 2.3448262810707092, lr: 1e-05
2023-12-21 17:19:44 INFO     	 * (global step 4400: loss: 2.421842038631439, lr: 1e-05
2023-12-21 17:20:01 INFO     	 * (global step 4450: loss: 2.463375508785248, lr: 1e-05
2023-12-21 17:20:17 INFO     	 * (global step 4500: loss: 2.3480125665664673, lr: 1e-05
2023-12-21 17:20:33 INFO     	 * (global step 4550: loss: 2.445663094520569, lr: 1e-05
2023-12-21 17:20:49 INFO     	 * (global step 4600: loss: 2.426469624042511, lr: 1e-05
2023-12-21 17:21:05 INFO     	 * (global step 4650: loss: 2.4180691838264465, lr: 1e-05
2023-12-21 17:21:22 INFO     	 * (global step 4700: loss: 2.3680489659309387, lr: 1e-05
2023-12-21 17:21:29 INFO     [epoch 11/15] average loss: 2.431, lr: 1e-05
2023-12-21 17:21:29 INFO     saving model related files
2023-12-21 17:21:29 INFO     saving model
2023-12-21 17:21:29 INFO     saving tokenizer
2023-12-21 17:21:29 INFO     saving optimizer
2023-12-21 17:21:30 INFO     remove old optimizer files
2023-12-21 17:21:39 INFO     	 * (global step 4750: loss: 2.3408851623535156, lr: 1e-05
2023-12-21 17:21:56 INFO     	 * (global step 4800: loss: 2.380894660949707, lr: 1e-05
2023-12-21 17:22:12 INFO     	 * (global step 4850: loss: 2.3916165828704834, lr: 1e-05
2023-12-21 17:22:28 INFO     	 * (global step 4900: loss: 2.4110703468322754, lr: 1e-05
2023-12-21 17:22:44 INFO     	 * (global step 4950: loss: 2.405950963497162, lr: 1e-05
2023-12-21 17:23:00 INFO     	 * (global step 5000: loss: 2.4997133016586304, lr: 1e-05
2023-12-21 17:23:16 INFO     	 * (global step 5050: loss: 2.4027547240257263, lr: 1e-05
2023-12-21 17:23:33 INFO     	 * (global step 5100: loss: 2.4438358545303345, lr: 1e-05
2023-12-21 17:23:49 INFO     	 * (global step 5150: loss: 2.353562295436859, lr: 1e-05
2023-12-21 17:24:05 INFO     	 * (global step 5200: loss: 2.3692315816879272, lr: 1e-05
2023-12-21 17:24:21 INFO     	 * (global step 5250: loss: 2.4706735610961914, lr: 1e-05
2023-12-21 17:24:37 INFO     	 * (global step 5300: loss: 2.3819767832756042, lr: 1e-05
2023-12-21 17:24:53 INFO     	 * (global step 5350: loss: 2.404676377773285, lr: 1e-05
2023-12-21 17:25:10 INFO     	 * (global step 5400: loss: 2.398128628730774, lr: 1e-05
2023-12-21 17:25:26 INFO     	 * (global step 5450: loss: 2.3843873143196106, lr: 1e-05
2023-12-21 17:25:42 INFO     	 * (global step 5500: loss: 2.3312251567840576, lr: 1e-05
2023-12-21 17:25:58 INFO     	 * (global step 5550: loss: 2.501607894897461, lr: 1e-05
2023-12-21 17:26:14 INFO     	 * (global step 5600: loss: 2.377010464668274, lr: 1e-05
2023-12-21 17:26:31 INFO     	 * (global step 5650: loss: 2.419195294380188, lr: 1e-05
2023-12-21 17:26:47 INFO     	 * (global step 5700: loss: 2.4771657586097717, lr: 1e-05
2023-12-21 17:27:03 INFO     	 * (global step 5750: loss: 2.383765459060669, lr: 1e-05
2023-12-21 17:27:19 INFO     	 * (global step 5800: loss: 2.364458739757538, lr: 1e-05
2023-12-21 17:27:36 INFO     	 * (global step 5850: loss: 2.354452610015869, lr: 1e-05
2023-12-21 17:27:52 INFO     	 * (global step 5900: loss: 2.382053256034851, lr: 1e-05
2023-12-21 17:28:08 INFO     	 * (global step 5950: loss: 2.4169604182243347, lr: 1e-05
2023-12-21 17:28:24 INFO     	 * (global step 6000: loss: 2.422467350959778, lr: 1e-05
2023-12-21 17:28:41 INFO     	 * (global step 6050: loss: 2.3544602394104004, lr: 1e-05
2023-12-21 17:28:57 INFO     	 * (global step 6100: loss: 2.3696426153182983, lr: 1e-05
2023-12-21 17:29:13 INFO     	 * (global step 6150: loss: 2.3264400362968445, lr: 1e-05
2023-12-21 17:29:30 INFO     	 * (global step 6200: loss: 2.424063742160797, lr: 1e-05
2023-12-21 17:29:46 INFO     	 * (global step 6250: loss: 2.4323363304138184, lr: 1e-05
2023-12-21 17:30:02 INFO     	 * (global step 6300: loss: 2.376890242099762, lr: 1e-05
2023-12-21 17:30:19 INFO     	 * (global step 6350: loss: 2.4341946244239807, lr: 1e-05
2023-12-21 17:30:35 INFO     	 * (global step 6400: loss: 2.4425002932548523, lr: 1e-05
2023-12-21 17:30:51 INFO     	 * (global step 6450: loss: 2.3792101740837097, lr: 1e-05
2023-12-21 17:31:08 INFO     	 * (global step 6500: loss: 2.3573442101478577, lr: 1e-05
2023-12-21 17:31:24 INFO     	 * (global step 6550: loss: 2.316538691520691, lr: 1e-05
2023-12-21 17:31:40 INFO     	 * (global step 6600: loss: 2.3066720962524414, lr: 1e-05
2023-12-21 17:31:56 INFO     	 * (global step 6650: loss: 2.470853090286255, lr: 1e-05
2023-12-21 17:32:12 INFO     	 * (global step 6700: loss: 2.4746848940849304, lr: 1e-05
2023-12-21 17:32:29 INFO     	 * (global step 6750: loss: 2.3747827410697937, lr: 1e-05
2023-12-21 17:32:45 INFO     	 * (global step 6800: loss: 2.4536343812942505, lr: 1e-05
2023-12-21 17:33:01 INFO     	 * (global step 6850: loss: 2.4365752935409546, lr: 1e-05
2023-12-21 17:33:17 INFO     	 * (global step 6900: loss: 2.3941412568092346, lr: 1e-05
2023-12-21 17:33:33 INFO     	 * (global step 6950: loss: 2.3812085390090942, lr: 1e-05
2023-12-21 17:33:49 INFO     	 * (global step 7000: loss: 2.3311312198638916, lr: 1e-05
2023-12-21 17:34:06 INFO     	 * (global step 7050: loss: 2.45425146818161, lr: 1e-05
2023-12-21 17:34:17 INFO     [epoch 12/15] average loss: 2.396, lr: 1e-05
2023-12-21 17:34:17 INFO     saving model related files
2023-12-21 17:34:17 INFO     saving model
2023-12-21 17:34:17 INFO     saving tokenizer
2023-12-21 17:34:17 INFO     saving optimizer
2023-12-21 17:34:18 INFO     remove old optimizer files
2023-12-21 17:34:24 INFO     	 * (global step 7100: loss: 2.341079533100128, lr: 1e-05
2023-12-21 17:34:40 INFO     	 * (global step 7150: loss: 2.3190703988075256, lr: 1e-05
2023-12-21 17:34:56 INFO     	 * (global step 7200: loss: 2.403584897518158, lr: 1e-05
2023-12-21 17:35:12 INFO     	 * (global step 7250: loss: 2.294541358947754, lr: 1e-05
2023-12-21 17:35:28 INFO     	 * (global step 7300: loss: 2.3518824577331543, lr: 1e-05
2023-12-21 17:35:44 INFO     	 * (global step 7350: loss: 2.390130937099457, lr: 1e-05
2023-12-21 17:36:01 INFO     	 * (global step 7400: loss: 2.3287264108657837, lr: 1e-05
2023-12-21 17:36:17 INFO     	 * (global step 7450: loss: 2.3436992168426514, lr: 1e-05
2023-12-21 17:36:33 INFO     	 * (global step 7500: loss: 2.335639774799347, lr: 1e-05
2023-12-21 17:36:49 INFO     	 * (global step 7550: loss: 2.3907803893089294, lr: 1e-05
2023-12-21 17:37:05 INFO     	 * (global step 7600: loss: 2.3626593947410583, lr: 1e-05
2023-12-21 17:37:21 INFO     	 * (global step 7650: loss: 2.403158962726593, lr: 1e-05
2023-12-21 17:37:38 INFO     	 * (global step 7700: loss: 2.2843950390815735, lr: 1e-05
2023-12-21 17:37:54 INFO     	 * (global step 7750: loss: 2.447566032409668, lr: 1e-05
2023-12-21 17:38:10 INFO     	 * (global step 7800: loss: 2.391613721847534, lr: 1e-05
2023-12-21 17:38:26 INFO     	 * (global step 7850: loss: 2.373903810977936, lr: 1e-05
2023-12-21 17:38:42 INFO     	 * (global step 7900: loss: 2.379298210144043, lr: 1e-05
2023-12-21 17:38:58 INFO     	 * (global step 7950: loss: 2.325822591781616, lr: 1e-05
2023-12-21 17:39:15 INFO     	 * (global step 8000: loss: 2.39714515209198, lr: 1e-05
2023-12-21 17:39:31 INFO     	 * (global step 8050: loss: 2.4487964510917664, lr: 1e-05
2023-12-21 17:39:47 INFO     	 * (global step 8100: loss: 2.3767404556274414, lr: 1e-05
2023-12-21 17:40:03 INFO     	 * (global step 8150: loss: 2.415639877319336, lr: 1e-05
2023-12-21 17:40:19 INFO     	 * (global step 8200: loss: 2.3358126282691956, lr: 1e-05
2023-12-21 17:40:36 INFO     	 * (global step 8250: loss: 2.377456307411194, lr: 1e-05
2023-12-21 17:40:52 INFO     	 * (global step 8300: loss: 2.397179961204529, lr: 1e-05
2023-12-21 17:41:08 INFO     	 * (global step 8350: loss: 2.304114043712616, lr: 1e-05
2023-12-21 17:41:24 INFO     	 * (global step 8400: loss: 2.4035030603408813, lr: 1e-05
2023-12-21 17:41:40 INFO     	 * (global step 8450: loss: 2.3310890197753906, lr: 1e-05
2023-12-21 17:41:56 INFO     	 * (global step 8500: loss: 2.355438232421875, lr: 1e-05
2023-12-21 17:42:13 INFO     	 * (global step 8550: loss: 2.318325161933899, lr: 1e-05
2023-12-21 17:42:29 INFO     	 * (global step 8600: loss: 2.325785756111145, lr: 1e-05
2023-12-21 17:42:45 INFO     	 * (global step 8650: loss: 2.3206032514572144, lr: 1e-05
2023-12-21 17:43:01 INFO     	 * (global step 8700: loss: 2.3488317728042603, lr: 1e-05
2023-12-21 17:43:17 INFO     	 * (global step 8750: loss: 2.373978078365326, lr: 1e-05
2023-12-21 17:43:33 INFO     	 * (global step 8800: loss: 2.310205042362213, lr: 1e-05
2023-12-21 17:43:50 INFO     	 * (global step 8850: loss: 2.3359676599502563, lr: 1e-05
2023-12-21 17:44:06 INFO     	 * (global step 8900: loss: 2.5267794132232666, lr: 1e-05
2023-12-21 17:44:22 INFO     	 * (global step 8950: loss: 2.34421443939209, lr: 1e-05
2023-12-21 17:44:38 INFO     	 * (global step 9000: loss: 2.385163366794586, lr: 1e-05
2023-12-21 17:44:54 INFO     	 * (global step 9050: loss: 2.3696160316467285, lr: 1e-05
2023-12-21 17:45:10 INFO     	 * (global step 9100: loss: 2.320582091808319, lr: 1e-05
2023-12-21 17:45:27 INFO     	 * (global step 9150: loss: 2.4137181639671326, lr: 1e-05
2023-12-21 17:45:43 INFO     	 * (global step 9200: loss: 2.357850432395935, lr: 1e-05
2023-12-21 17:45:59 INFO     	 * (global step 9250: loss: 2.3423966765403748, lr: 1e-05
2023-12-21 17:46:15 INFO     	 * (global step 9300: loss: 2.328430712223053, lr: 1e-05
2023-12-21 17:46:31 INFO     	 * (global step 9350: loss: 2.3714847564697266, lr: 1e-05
2023-12-21 17:46:47 INFO     	 * (global step 9400: loss: 2.276192009449005, lr: 1e-05
2023-12-21 17:47:02 INFO     [epoch 13/15] average loss: 2.378, lr: 1e-05
2023-12-21 17:47:02 INFO     saving model related files
2023-12-21 17:47:02 INFO     saving model
2023-12-21 17:47:02 INFO     saving tokenizer
2023-12-21 17:47:02 INFO     saving optimizer
2023-12-21 17:47:03 INFO     remove old optimizer files
2023-12-21 17:47:05 INFO     	 * (global step 9450: loss: 2.3046517968177795, lr: 1e-05
2023-12-21 17:47:21 INFO     	 * (global step 9500: loss: 2.4035561084747314, lr: 1e-05
2023-12-21 17:47:38 INFO     	 * (global step 9550: loss: 2.3713417053222656, lr: 1e-05
2023-12-21 17:47:54 INFO     	 * (global step 9600: loss: 2.352501332759857, lr: 1e-05
2023-12-21 17:48:10 INFO     	 * (global step 9650: loss: 2.4116218090057373, lr: 1e-05
2023-12-21 17:48:26 INFO     	 * (global step 9700: loss: 2.325056314468384, lr: 1e-05
2023-12-21 17:48:42 INFO     	 * (global step 9750: loss: 2.325982391834259, lr: 1e-05
2023-12-21 17:48:58 INFO     	 * (global step 9800: loss: 2.388784110546112, lr: 1e-05
2023-12-21 17:49:14 INFO     	 * (global step 9850: loss: 2.367715895175934, lr: 1e-05
2023-12-21 17:49:31 INFO     	 * (global step 9900: loss: 2.268375813961029, lr: 1e-05
2023-12-21 17:49:47 INFO     	 * (global step 9950: loss: 2.440406620502472, lr: 1e-05
2023-12-21 17:50:03 INFO     	 * (global step 10000: loss: 2.3921173214912415, lr: 1e-05
2023-12-21 17:50:19 INFO     	 * (global step 10050: loss: 2.3730106949806213, lr: 1e-05
2023-12-21 17:50:35 INFO     	 * (global step 10100: loss: 2.3656333684921265, lr: 1e-05
2023-12-21 17:50:51 INFO     	 * (global step 10150: loss: 2.3577149510383606, lr: 1e-05
2023-12-21 17:51:08 INFO     	 * (global step 10200: loss: 2.3626034259796143, lr: 1e-05
2023-12-21 17:51:24 INFO     	 * (global step 10250: loss: 2.3437732458114624, lr: 1e-05
2023-12-21 17:51:40 INFO     	 * (global step 10300: loss: 2.3037102818489075, lr: 1e-05
2023-12-21 17:51:56 INFO     	 * (global step 10350: loss: 2.3148539662361145, lr: 1e-05
2023-12-21 17:52:12 INFO     	 * (global step 10400: loss: 2.3218448758125305, lr: 1e-05
2023-12-21 17:52:28 INFO     	 * (global step 10450: loss: 2.3709280490875244, lr: 1e-05
2023-12-21 17:52:45 INFO     	 * (global step 10500: loss: 2.4227535128593445, lr: 1e-05
2023-12-21 17:53:01 INFO     	 * (global step 10550: loss: 2.3013578057289124, lr: 1e-05
2023-12-21 17:53:17 INFO     	 * (global step 10600: loss: 2.349176287651062, lr: 1e-05
2023-12-21 17:53:33 INFO     	 * (global step 10650: loss: 2.406423270702362, lr: 1e-05
2023-12-21 17:53:49 INFO     	 * (global step 10700: loss: 2.345553159713745, lr: 1e-05
2023-12-21 17:54:06 INFO     	 * (global step 10750: loss: 2.48165500164032, lr: 1e-05
2023-12-21 17:54:22 INFO     	 * (global step 10800: loss: 2.3585179448127747, lr: 1e-05
2023-12-21 17:54:38 INFO     	 * (global step 10850: loss: 2.352912187576294, lr: 1e-05
2023-12-21 17:54:54 INFO     	 * (global step 10900: loss: 2.286950409412384, lr: 1e-05
2023-12-21 17:55:10 INFO     	 * (global step 10950: loss: 2.3399038910865784, lr: 1e-05
2023-12-21 17:55:26 INFO     	 * (global step 11000: loss: 2.4393447637557983, lr: 1e-05
2023-12-21 17:55:43 INFO     	 * (global step 11050: loss: 2.3256115317344666, lr: 1e-05
2023-12-21 17:55:59 INFO     	 * (global step 11100: loss: 2.3703449964523315, lr: 1e-05
2023-12-21 17:56:15 INFO     	 * (global step 11150: loss: 2.330022633075714, lr: 1e-05
2023-12-21 17:56:31 INFO     	 * (global step 11200: loss: 2.415706992149353, lr: 1e-05
2023-12-21 17:56:47 INFO     	 * (global step 11250: loss: 2.37236887216568, lr: 1e-05
2023-12-21 17:57:03 INFO     	 * (global step 11300: loss: 2.3001370429992676, lr: 1e-05
2023-12-21 17:57:20 INFO     	 * (global step 11350: loss: 2.315801680088043, lr: 1e-05
2023-12-21 17:57:36 INFO     	 * (global step 11400: loss: 2.3318318128585815, lr: 1e-05
2023-12-21 17:57:52 INFO     	 * (global step 11450: loss: 2.34725159406662, lr: 1e-05
2023-12-21 17:58:08 INFO     	 * (global step 11500: loss: 2.386311888694763, lr: 1e-05
2023-12-21 17:58:24 INFO     	 * (global step 11550: loss: 2.3063517808914185, lr: 1e-05
2023-12-21 17:58:41 INFO     	 * (global step 11600: loss: 2.329198479652405, lr: 1e-05
2023-12-21 17:58:57 INFO     	 * (global step 11650: loss: 2.329430043697357, lr: 1e-05
2023-12-21 17:59:13 INFO     	 * (global step 11700: loss: 2.3930959701538086, lr: 1e-05
2023-12-21 17:59:29 INFO     	 * (global step 11750: loss: 2.357837677001953, lr: 1e-05
2023-12-21 17:59:45 INFO     	 * (global step 11800: loss: 2.330337703227997, lr: 1e-05
2023-12-21 17:59:47 INFO     [epoch 14/15] average loss: 2.366, lr: 1e-05
2023-12-21 17:59:47 INFO     saving model related files
2023-12-21 17:59:47 INFO     saving model
2023-12-21 17:59:48 INFO     saving tokenizer
2023-12-21 17:59:48 INFO     saving optimizer
2023-12-21 17:59:48 INFO     remove old optimizer files
2023-12-21 17:59:49 INFO     complete training: model ckpt was saved at small_combined_trained_ckpt/model_nxaqhy
2023-12-21 17:59:49 INFO     ## 2nd RUN: Configuration 1/5: validation/Bleu_4 = 0.06025492648987384
2023-12-21 17:59:49 INFO     initialize model trainer
2023-12-21 17:59:49 INFO     load config from existing checkpoint at small_combined_trained_ckpt/model_vhyoja
2023-12-21 17:59:49 INFO     hyperparameters
2023-12-21 17:59:49 INFO     	 * dataset_path: StellarMilk/squad_newsqa
2023-12-21 17:59:49 INFO     	 * dataset_name: default
2023-12-21 17:59:49 INFO     	 * input_types: ['paragraph']
2023-12-21 17:59:49 INFO     	 * output_types: ['questions_answers']
2023-12-21 17:59:49 INFO     	 * prefix_types: ['qag']
2023-12-21 17:59:49 INFO     	 * model: t5-small
2023-12-21 17:59:49 INFO     	 * max_length: 512
2023-12-21 17:59:49 INFO     	 * max_length_output: 512
2023-12-21 17:59:49 INFO     	 * epoch: 15
2023-12-21 17:59:49 INFO     	 * batch: 2
2023-12-21 17:59:49 INFO     	 * lr: 1e-05
2023-12-21 17:59:49 INFO     	 * fp16: False
2023-12-21 17:59:49 INFO     	 * random_seed: 1
2023-12-21 17:59:49 INFO     	 * gradient_accumulation_steps: 4
2023-12-21 17:59:49 INFO     	 * label_smoothing: 0.0
2023-12-21 17:59:49 INFO     load checkpoint from small_combined_trained_ckpt/model_vhyoja/epoch_10
2023-12-21 17:59:49 INFO     use spaCy answer extraction model: positionrank
2023-12-21 17:59:50 INFO     Model `small_combined_trained_ckpt/model_vhyoja/epoch_10`
2023-12-21 17:59:50 INFO     	 * Num of GPU in use: 1
2023-12-21 17:59:50 INFO     	 * Prefix: True
2023-12-21 17:59:50 INFO     	 * Language: en (ignore at the training phase)
2023-12-21 17:59:50 INFO     load optimizer from small_combined_trained_ckpt/model_vhyoja/optimizers/optimizer.10.pt
2023-12-21 17:59:50 INFO     optimizer is loading on cuda
2023-12-21 17:59:55 INFO     dataset preprocessing
2023-12-21 17:59:57 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/squad_newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-21 17:59:59 INFO     start model training
2023-12-21 18:00:14 INFO     	 * (global step 50: loss: 0.4053587540984154, lr: 1e-05
2023-12-21 18:00:30 INFO     	 * (global step 100: loss: 0.2355690337717533, lr: 1e-05
2023-12-21 18:00:46 INFO     	 * (global step 150: loss: 0.23185232281684875, lr: 1e-05
2023-12-21 18:01:02 INFO     	 * (global step 200: loss: 0.35982469469308853, lr: 1e-05
2023-12-21 18:01:18 INFO     	 * (global step 250: loss: 0.3052864372730255, lr: 1e-05
2023-12-21 18:01:34 INFO     	 * (global step 300: loss: 0.44485561549663544, lr: 1e-05
2023-12-21 18:01:49 INFO     	 * (global step 350: loss: 0.498496375977993, lr: 1e-05
2023-12-21 18:02:05 INFO     	 * (global step 400: loss: 0.30122906900942326, lr: 1e-05
2023-12-21 18:02:21 INFO     	 * (global step 450: loss: 0.2994834594428539, lr: 1e-05
2023-12-21 18:02:37 INFO     	 * (global step 500: loss: 0.24167215824127197, lr: 1e-05
2023-12-21 18:02:53 INFO     	 * (global step 550: loss: 0.2848575748503208, lr: 1e-05
2023-12-21 18:03:09 INFO     	 * (global step 600: loss: 0.3931567147374153, lr: 1e-05
2023-12-21 18:03:24 INFO     	 * (global step 650: loss: 0.3019511438906193, lr: 1e-05
2023-12-21 18:03:40 INFO     	 * (global step 700: loss: 0.3083919547498226, lr: 1e-05
2023-12-21 18:03:56 INFO     	 * (global step 750: loss: 0.3099367804825306, lr: 1e-05
2023-12-21 18:04:12 INFO     	 * (global step 800: loss: 0.3670526221394539, lr: 1e-05
2023-12-21 18:04:27 INFO     	 * (global step 850: loss: 0.25365466997027397, lr: 1e-05
2023-12-21 18:04:43 INFO     	 * (global step 900: loss: 0.26078157871961594, lr: 1e-05
2023-12-21 18:04:59 INFO     	 * (global step 950: loss: 0.3292487822473049, lr: 1e-05
2023-12-21 18:05:15 INFO     	 * (global step 1000: loss: 0.4022356644272804, lr: 1e-05
2023-12-21 18:05:30 INFO     	 * (global step 1050: loss: 0.3316323235630989, lr: 1e-05
2023-12-21 18:05:46 INFO     	 * (global step 1100: loss: 0.47601358592510223, lr: 1e-05
2023-12-21 18:06:02 INFO     	 * (global step 1150: loss: 0.4305102527141571, lr: 1e-05
2023-12-21 18:06:17 INFO     	 * (global step 1200: loss: 0.3294011875987053, lr: 1e-05
2023-12-21 18:06:33 INFO     	 * (global step 1250: loss: 0.34941035136580467, lr: 1e-05
2023-12-21 18:06:49 INFO     	 * (global step 1300: loss: 0.24972504936158657, lr: 1e-05
2023-12-21 18:07:05 INFO     	 * (global step 1350: loss: 0.2989465966820717, lr: 1e-05
2023-12-21 18:07:20 INFO     	 * (global step 1400: loss: 0.3054470047354698, lr: 1e-05
2023-12-21 18:07:36 INFO     	 * (global step 1450: loss: 0.29682131856679916, lr: 1e-05
2023-12-21 18:07:52 INFO     	 * (global step 1500: loss: 0.42916645109653473, lr: 1e-05
2023-12-21 18:08:08 INFO     	 * (global step 1550: loss: 0.3906903974711895, lr: 1e-05
2023-12-21 18:08:23 INFO     	 * (global step 1600: loss: 0.2969316765666008, lr: 1e-05
2023-12-21 18:08:39 INFO     	 * (global step 1650: loss: 0.24642132595181465, lr: 1e-05
2023-12-21 18:08:55 INFO     	 * (global step 1700: loss: 0.2838405668735504, lr: 1e-05
2023-12-21 18:09:11 INFO     	 * (global step 1750: loss: 0.3265194371342659, lr: 1e-05
2023-12-21 18:09:26 INFO     	 * (global step 1800: loss: 0.2656652517616749, lr: 1e-05
2023-12-21 18:09:42 INFO     	 * (global step 1850: loss: 0.3515174090862274, lr: 1e-05
2023-12-21 18:09:58 INFO     	 * (global step 1900: loss: 0.2526123598217964, lr: 1e-05
2023-12-21 18:10:13 INFO     	 * (global step 1950: loss: 0.21635321527719498, lr: 1e-05
2023-12-21 18:10:29 INFO     	 * (global step 2000: loss: 0.4133725054562092, lr: 1e-05
2023-12-21 18:10:45 INFO     	 * (global step 2050: loss: 0.2413819432258606, lr: 1e-05
2023-12-21 18:11:01 INFO     	 * (global step 2100: loss: 0.32520948350429535, lr: 1e-05
2023-12-21 18:11:16 INFO     	 * (global step 2150: loss: 0.336721308529377, lr: 1e-05
2023-12-21 18:11:32 INFO     	 * (global step 2200: loss: 0.39838196337223053, lr: 1e-05
2023-12-21 18:11:48 INFO     	 * (global step 2250: loss: 0.29100072756409645, lr: 1e-05
2023-12-21 18:12:04 INFO     	 * (global step 2300: loss: 0.32211050018668175, lr: 1e-05
2023-12-21 18:12:19 INFO     	 * (global step 2350: loss: 0.3666990250349045, lr: 1e-05
2023-12-21 18:12:23 INFO     [epoch 10/15] average loss: 0.324, lr: 1e-05
2023-12-21 18:12:23 INFO     saving model related files
2023-12-21 18:12:23 INFO     saving model
2023-12-21 18:12:24 INFO     saving tokenizer
2023-12-21 18:12:24 INFO     saving optimizer
2023-12-21 18:12:25 INFO     remove old optimizer files
2023-12-21 18:12:37 INFO     	 * (global step 2400: loss: 0.2871648296713829, lr: 1e-05
2023-12-21 18:12:53 INFO     	 * (global step 2450: loss: 0.3575098216533661, lr: 1e-05
2023-12-21 18:13:08 INFO     	 * (global step 2500: loss: 0.29665715247392654, lr: 1e-05
2023-12-21 18:13:24 INFO     	 * (global step 2550: loss: 0.2893773019313812, lr: 1e-05
2023-12-21 18:13:40 INFO     	 * (global step 2600: loss: 0.31880129128694534, lr: 1e-05
2023-12-21 18:13:56 INFO     	 * (global step 2650: loss: 0.27517762035131454, lr: 1e-05
2023-12-21 18:14:11 INFO     	 * (global step 2700: loss: 0.2758292481303215, lr: 1e-05
2023-12-21 18:14:27 INFO     	 * (global step 2750: loss: 0.2723366543650627, lr: 1e-05
2023-12-21 18:14:43 INFO     	 * (global step 2800: loss: 0.3963143453001976, lr: 1e-05
2023-12-21 18:14:59 INFO     	 * (global step 2850: loss: 0.3662421703338623, lr: 1e-05
2023-12-21 18:15:14 INFO     	 * (global step 2900: loss: 0.3200257420539856, lr: 1e-05
2023-12-21 18:15:30 INFO     	 * (global step 2950: loss: 0.27762670814991, lr: 1e-05
2023-12-21 18:15:46 INFO     	 * (global step 3000: loss: 0.20769164711236954, lr: 1e-05
2023-12-21 18:16:01 INFO     	 * (global step 3050: loss: 0.2606973312795162, lr: 1e-05
2023-12-21 18:16:17 INFO     	 * (global step 3100: loss: 0.2803051359951496, lr: 1e-05
2023-12-21 18:16:33 INFO     	 * (global step 3150: loss: 0.27121008932590485, lr: 1e-05
2023-12-21 18:16:49 INFO     	 * (global step 3200: loss: 0.29704268276691437, lr: 1e-05
2023-12-21 18:17:04 INFO     	 * (global step 3250: loss: 0.29998303204774857, lr: 1e-05
2023-12-21 18:17:20 INFO     	 * (global step 3300: loss: 0.2573983296751976, lr: 1e-05
2023-12-21 18:17:36 INFO     	 * (global step 3350: loss: 0.23674269020557404, lr: 1e-05
2023-12-21 18:17:52 INFO     	 * (global step 3400: loss: 0.32500134781003, lr: 1e-05
2023-12-21 18:18:07 INFO     	 * (global step 3450: loss: 0.28918885812163353, lr: 1e-05
2023-12-21 18:18:23 INFO     	 * (global step 3500: loss: 0.34902942180633545, lr: 1e-05
2023-12-21 18:18:39 INFO     	 * (global step 3550: loss: 0.24732240289449692, lr: 1e-05
2023-12-21 18:18:54 INFO     	 * (global step 3600: loss: 0.33985932543873787, lr: 1e-05
2023-12-21 18:19:10 INFO     	 * (global step 3650: loss: 0.2829713523387909, lr: 1e-05
2023-12-21 18:19:26 INFO     	 * (global step 3700: loss: 0.24576690793037415, lr: 1e-05
2023-12-21 18:19:41 INFO     	 * (global step 3750: loss: 0.353648416697979, lr: 1e-05
2023-12-21 18:19:57 INFO     	 * (global step 3800: loss: 0.2814747169613838, lr: 1e-05
2023-12-21 18:20:13 INFO     	 * (global step 3850: loss: 0.4063625745475292, lr: 1e-05
2023-12-21 18:20:29 INFO     	 * (global step 3900: loss: 0.3781202957034111, lr: 1e-05
2023-12-21 18:20:44 INFO     	 * (global step 3950: loss: 0.34888554736971855, lr: 1e-05
2023-12-21 18:21:00 INFO     	 * (global step 4000: loss: 0.2576140686869621, lr: 1e-05
2023-12-21 18:21:16 INFO     	 * (global step 4050: loss: 0.3452911600470543, lr: 1e-05
2023-12-21 18:21:31 INFO     	 * (global step 4100: loss: 0.2394883707165718, lr: 1e-05
2023-12-21 18:21:47 INFO     	 * (global step 4150: loss: 0.29711728543043137, lr: 1e-05
2023-12-21 18:22:03 INFO     	 * (global step 4200: loss: 0.3416404724121094, lr: 1e-05
2023-12-21 18:22:19 INFO     	 * (global step 4250: loss: 0.3010118193924427, lr: 1e-05
2023-12-21 18:22:34 INFO     	 * (global step 4300: loss: 0.36901022493839264, lr: 1e-05
2023-12-21 18:22:50 INFO     	 * (global step 4350: loss: 0.23889396712183952, lr: 1e-05
2023-12-21 18:23:05 INFO     	 * (global step 4400: loss: 0.3324446529150009, lr: 1e-05
2023-12-21 18:23:21 INFO     	 * (global step 4450: loss: 0.37362948805093765, lr: 1e-05
2023-12-21 18:23:37 INFO     	 * (global step 4500: loss: 0.2417655661702156, lr: 1e-05
2023-12-21 18:23:52 INFO     	 * (global step 4550: loss: 0.34492215514183044, lr: 1e-05
2023-12-21 18:24:08 INFO     	 * (global step 4600: loss: 0.3293670304119587, lr: 1e-05
2023-12-21 18:24:24 INFO     	 * (global step 4650: loss: 0.3260635808110237, lr: 1e-05
2023-12-21 18:24:39 INFO     	 * (global step 4700: loss: 0.2723260447382927, lr: 1e-05
2023-12-21 18:24:46 INFO     [epoch 11/15] average loss: 0.321, lr: 1e-05
2023-12-21 18:24:46 INFO     saving model related files
2023-12-21 18:24:46 INFO     saving model
2023-12-21 18:24:47 INFO     saving tokenizer
2023-12-21 18:24:47 INFO     saving optimizer
2023-12-21 18:24:48 INFO     remove old optimizer files
2023-12-21 18:24:57 INFO     	 * (global step 4750: loss: 0.24990084767341614, lr: 1e-05
2023-12-21 18:25:12 INFO     	 * (global step 4800: loss: 0.29137659817934036, lr: 1e-05
2023-12-21 18:25:28 INFO     	 * (global step 4850: loss: 0.30710092186927795, lr: 1e-05
2023-12-21 18:25:44 INFO     	 * (global step 4900: loss: 0.3169602118432522, lr: 1e-05
2023-12-21 18:25:59 INFO     	 * (global step 4950: loss: 0.3152010329067707, lr: 1e-05
2023-12-21 18:26:15 INFO     	 * (global step 5000: loss: 0.4052969701588154, lr: 1e-05
2023-12-21 18:26:30 INFO     	 * (global step 5050: loss: 0.30321889743208885, lr: 1e-05
2023-12-21 18:26:46 INFO     	 * (global step 5100: loss: 0.3674980476498604, lr: 1e-05
2023-12-21 18:27:02 INFO     	 * (global step 5150: loss: 0.26792194694280624, lr: 1e-05
2023-12-21 18:27:17 INFO     	 * (global step 5200: loss: 0.2761183902621269, lr: 1e-05
2023-12-21 18:27:33 INFO     	 * (global step 5250: loss: 0.39338554441928864, lr: 1e-05
2023-12-21 18:27:49 INFO     	 * (global step 5300: loss: 0.30073706805706024, lr: 1e-05
2023-12-21 18:28:04 INFO     	 * (global step 5350: loss: 0.33219563215970993, lr: 1e-05
2023-12-21 18:28:20 INFO     	 * (global step 5400: loss: 0.3200984373688698, lr: 1e-05
2023-12-21 18:28:36 INFO     	 * (global step 5450: loss: 0.30282871425151825, lr: 1e-05
2023-12-21 18:28:51 INFO     	 * (global step 5500: loss: 0.24529169499874115, lr: 1e-05
2023-12-21 18:29:07 INFO     	 * (global step 5550: loss: 0.42296629399061203, lr: 1e-05
2023-12-21 18:29:23 INFO     	 * (global step 5600: loss: 0.30133209750056267, lr: 1e-05
2023-12-21 18:29:38 INFO     	 * (global step 5650: loss: 0.3246492072939873, lr: 1e-05
2023-12-21 18:29:54 INFO     	 * (global step 5700: loss: 0.42157749459147453, lr: 1e-05
2023-12-21 18:30:10 INFO     	 * (global step 5750: loss: 0.2997319847345352, lr: 1e-05
2023-12-21 18:30:25 INFO     	 * (global step 5800: loss: 0.27950187772512436, lr: 1e-05
2023-12-21 18:30:41 INFO     	 * (global step 5850: loss: 0.28045564889907837, lr: 1e-05
2023-12-21 18:30:57 INFO     	 * (global step 5900: loss: 0.3084854185581207, lr: 1e-05
2023-12-21 18:31:12 INFO     	 * (global step 5950: loss: 0.35303857177495956, lr: 1e-05
2023-12-21 18:31:28 INFO     	 * (global step 6000: loss: 0.3412914276123047, lr: 1e-05
2023-12-21 18:31:44 INFO     	 * (global step 6050: loss: 0.2794850952923298, lr: 1e-05
2023-12-21 18:32:00 INFO     	 * (global step 6100: loss: 0.3000160716474056, lr: 1e-05
2023-12-21 18:32:15 INFO     	 * (global step 6150: loss: 0.24288562312722206, lr: 1e-05
2023-12-21 18:32:31 INFO     	 * (global step 6200: loss: 0.3594060614705086, lr: 1e-05
2023-12-21 18:32:47 INFO     	 * (global step 6250: loss: 0.35054149478673935, lr: 1e-05
2023-12-21 18:33:03 INFO     	 * (global step 6300: loss: 0.31317007541656494, lr: 1e-05
2023-12-21 18:33:18 INFO     	 * (global step 6350: loss: 0.3592723533511162, lr: 1e-05
2023-12-21 18:33:34 INFO     	 * (global step 6400: loss: 0.38425901532173157, lr: 1e-05
2023-12-21 18:33:50 INFO     	 * (global step 6450: loss: 0.3131552152335644, lr: 1e-05
2023-12-21 18:34:06 INFO     	 * (global step 6500: loss: 0.28697945922613144, lr: 1e-05
2023-12-21 18:34:21 INFO     	 * (global step 6550: loss: 0.24114014580845833, lr: 1e-05
2023-12-21 18:34:37 INFO     	 * (global step 6600: loss: 0.23706994950771332, lr: 1e-05
2023-12-21 18:34:53 INFO     	 * (global step 6650: loss: 0.406997375190258, lr: 1e-05
2023-12-21 18:35:08 INFO     	 * (global step 6700: loss: 0.4078957214951515, lr: 1e-05
2023-12-21 18:35:24 INFO     	 * (global step 6750: loss: 0.3092683106660843, lr: 1e-05
2023-12-21 18:35:40 INFO     	 * (global step 6800: loss: 0.38232460245490074, lr: 1e-05
2023-12-21 18:35:56 INFO     	 * (global step 6850: loss: 0.37168464809656143, lr: 1e-05
2023-12-21 18:36:11 INFO     	 * (global step 6900: loss: 0.3345445916056633, lr: 1e-05
2023-12-21 18:36:27 INFO     	 * (global step 6950: loss: 0.30886030942201614, lr: 1e-05
2023-12-21 18:36:43 INFO     	 * (global step 7000: loss: 0.2610602527856827, lr: 1e-05
2023-12-21 18:36:59 INFO     	 * (global step 7050: loss: 0.38489922136068344, lr: 1e-05
2023-12-21 18:37:09 INFO     [epoch 12/15] average loss: 0.318, lr: 1e-05
2023-12-21 18:37:09 INFO     saving model related files
2023-12-21 18:37:09 INFO     saving model
2023-12-21 18:37:10 INFO     saving tokenizer
2023-12-21 18:37:10 INFO     saving optimizer
2023-12-21 18:37:11 INFO     remove old optimizer files
2023-12-21 18:37:16 INFO     	 * (global step 7100: loss: 0.266740620136261, lr: 1e-05
2023-12-21 18:37:32 INFO     	 * (global step 7150: loss: 0.24637098982930183, lr: 1e-05
2023-12-21 18:37:47 INFO     	 * (global step 7200: loss: 0.3366806097328663, lr: 1e-05
2023-12-21 18:38:03 INFO     	 * (global step 7250: loss: 0.22786723263561726, lr: 1e-05
2023-12-21 18:38:19 INFO     	 * (global step 7300: loss: 0.2780091166496277, lr: 1e-05
2023-12-21 18:38:35 INFO     	 * (global step 7350: loss: 0.31526803970336914, lr: 1e-05
2023-12-21 18:38:50 INFO     	 * (global step 7400: loss: 0.2590811662375927, lr: 1e-05
2023-12-21 18:39:06 INFO     	 * (global step 7450: loss: 0.27170125395059586, lr: 1e-05
2023-12-21 18:39:22 INFO     	 * (global step 7500: loss: 0.2777238078415394, lr: 1e-05
2023-12-21 18:39:37 INFO     	 * (global step 7550: loss: 0.3332367055118084, lr: 1e-05
2023-12-21 18:39:53 INFO     	 * (global step 7600: loss: 0.29463161155581474, lr: 1e-05
2023-12-21 18:40:09 INFO     	 * (global step 7650: loss: 0.3456229865550995, lr: 1e-05
2023-12-21 18:40:25 INFO     	 * (global step 7700: loss: 0.22184520587325096, lr: 1e-05
2023-12-21 18:40:40 INFO     	 * (global step 7750: loss: 0.3718499541282654, lr: 1e-05
2023-12-21 18:40:56 INFO     	 * (global step 7800: loss: 0.3240188658237457, lr: 1e-05
2023-12-21 18:41:12 INFO     	 * (global step 7850: loss: 0.30494364351034164, lr: 1e-05
2023-12-21 18:41:27 INFO     	 * (global step 7900: loss: 0.32014353573322296, lr: 1e-05
2023-12-21 18:41:43 INFO     	 * (global step 7950: loss: 0.259513508528471, lr: 1e-05
2023-12-21 18:41:59 INFO     	 * (global step 8000: loss: 0.33545029908418655, lr: 1e-05
2023-12-21 18:42:15 INFO     	 * (global step 8050: loss: 0.4037531912326813, lr: 1e-05
2023-12-21 18:42:30 INFO     	 * (global step 8100: loss: 0.3126281648874283, lr: 1e-05
2023-12-21 18:42:46 INFO     	 * (global step 8150: loss: 0.3365071453154087, lr: 1e-05
2023-12-21 18:43:02 INFO     	 * (global step 8200: loss: 0.2680140808224678, lr: 1e-05
2023-12-21 18:43:17 INFO     	 * (global step 8250: loss: 0.3149488493800163, lr: 1e-05
2023-12-21 18:43:33 INFO     	 * (global step 8300: loss: 0.34215135127305984, lr: 1e-05
2023-12-21 18:43:49 INFO     	 * (global step 8350: loss: 0.2444612756371498, lr: 1e-05
2023-12-21 18:44:05 INFO     	 * (global step 8400: loss: 0.34370213747024536, lr: 1e-05
2023-12-21 18:44:20 INFO     	 * (global step 8450: loss: 0.26468950510025024, lr: 1e-05
2023-12-21 18:44:36 INFO     	 * (global step 8500: loss: 0.28842315822839737, lr: 1e-05
2023-12-21 18:44:52 INFO     	 * (global step 8550: loss: 0.24791186675429344, lr: 1e-05
2023-12-21 18:45:07 INFO     	 * (global step 8600: loss: 0.2647431567311287, lr: 1e-05
2023-12-21 18:45:23 INFO     	 * (global step 8650: loss: 0.2601734362542629, lr: 1e-05
2023-12-21 18:45:39 INFO     	 * (global step 8700: loss: 0.28111837431788445, lr: 1e-05
2023-12-21 18:45:55 INFO     	 * (global step 8750: loss: 0.30591659247875214, lr: 1e-05
2023-12-21 18:46:10 INFO     	 * (global step 8800: loss: 0.23982549831271172, lr: 1e-05
2023-12-21 18:46:26 INFO     	 * (global step 8850: loss: 0.27799495309591293, lr: 1e-05
2023-12-21 18:46:42 INFO     	 * (global step 8900: loss: 0.4779442176222801, lr: 1e-05
2023-12-21 18:46:57 INFO     	 * (global step 8950: loss: 0.2786385864019394, lr: 1e-05
2023-12-21 18:47:13 INFO     	 * (global step 9000: loss: 0.33013949170708656, lr: 1e-05
2023-12-21 18:47:29 INFO     	 * (global step 9050: loss: 0.30871311388909817, lr: 1e-05
2023-12-21 18:47:44 INFO     	 * (global step 9100: loss: 0.2594209350645542, lr: 1e-05
2023-12-21 18:48:00 INFO     	 * (global step 9150: loss: 0.359347190707922, lr: 1e-05
2023-12-21 18:48:15 INFO     	 * (global step 9200: loss: 0.2978021651506424, lr: 1e-05
2023-12-21 18:48:31 INFO     	 * (global step 9250: loss: 0.28979039192199707, lr: 1e-05
2023-12-21 18:48:47 INFO     	 * (global step 9300: loss: 0.26409905031323433, lr: 1e-05
2023-12-21 18:49:02 INFO     	 * (global step 9350: loss: 0.31964848190546036, lr: 1e-05
2023-12-21 18:49:18 INFO     	 * (global step 9400: loss: 0.20546817779541016, lr: 1e-05
2023-12-21 18:49:32 INFO     [epoch 13/15] average loss: 0.316, lr: 1e-05
2023-12-21 18:49:32 INFO     saving model related files
2023-12-21 18:49:32 INFO     saving model
2023-12-21 18:49:32 INFO     saving tokenizer
2023-12-21 18:49:32 INFO     saving optimizer
2023-12-21 18:49:33 INFO     remove old optimizer files
2023-12-21 18:49:35 INFO     	 * (global step 9450: loss: 0.24040802754461765, lr: 1e-05
2023-12-21 18:49:51 INFO     	 * (global step 9500: loss: 0.3537892885506153, lr: 1e-05
2023-12-21 18:50:06 INFO     	 * (global step 9550: loss: 0.32581962645053864, lr: 1e-05
2023-12-21 18:50:22 INFO     	 * (global step 9600: loss: 0.2903406620025635, lr: 1e-05
2023-12-21 18:50:37 INFO     	 * (global step 9650: loss: 0.36079179495573044, lr: 1e-05
2023-12-21 18:50:53 INFO     	 * (global step 9700: loss: 0.2689606323838234, lr: 1e-05
2023-12-21 18:51:09 INFO     	 * (global step 9750: loss: 0.2668823506683111, lr: 1e-05
2023-12-21 18:51:24 INFO     	 * (global step 9800: loss: 0.3442201800644398, lr: 1e-05
2023-12-21 18:51:40 INFO     	 * (global step 9850: loss: 0.31665506958961487, lr: 1e-05
2023-12-21 18:51:56 INFO     	 * (global step 9900: loss: 0.21255604922771454, lr: 1e-05
2023-12-21 18:52:11 INFO     	 * (global step 9950: loss: 0.389327697455883, lr: 1e-05
2023-12-21 18:52:27 INFO     	 * (global step 10000: loss: 0.347481157630682, lr: 1e-05
2023-12-21 18:52:42 INFO     	 * (global step 10050: loss: 0.3216991499066353, lr: 1e-05
2023-12-21 18:52:58 INFO     	 * (global step 10100: loss: 0.31168994307518005, lr: 1e-05
2023-12-21 18:53:14 INFO     	 * (global step 10150: loss: 0.3123488575220108, lr: 1e-05
2023-12-21 18:53:29 INFO     	 * (global step 10200: loss: 0.31047020852565765, lr: 1e-05
2023-12-21 18:53:45 INFO     	 * (global step 10250: loss: 0.29385398514568806, lr: 1e-05
2023-12-21 18:54:00 INFO     	 * (global step 10300: loss: 0.24529844895005226, lr: 1e-05
2023-12-21 18:54:16 INFO     	 * (global step 10350: loss: 0.2608756273984909, lr: 1e-05
2023-12-21 18:54:32 INFO     	 * (global step 10400: loss: 0.26330772042274475, lr: 1e-05
2023-12-21 18:54:48 INFO     	 * (global step 10450: loss: 0.3174465075135231, lr: 1e-05
2023-12-21 18:55:03 INFO     	 * (global step 10500: loss: 0.38138192519545555, lr: 1e-05
2023-12-21 18:55:19 INFO     	 * (global step 10550: loss: 0.23668941855430603, lr: 1e-05
2023-12-21 18:55:35 INFO     	 * (global step 10600: loss: 0.2920932099223137, lr: 1e-05
2023-12-21 18:55:51 INFO     	 * (global step 10650: loss: 0.3489866331219673, lr: 1e-05
2023-12-21 18:56:06 INFO     	 * (global step 10700: loss: 0.28781289234757423, lr: 1e-05
2023-12-21 18:56:22 INFO     	 * (global step 10750: loss: 0.43187448382377625, lr: 1e-05
2023-12-21 18:56:38 INFO     	 * (global step 10800: loss: 0.30503275617957115, lr: 1e-05
2023-12-21 18:56:54 INFO     	 * (global step 10850: loss: 0.29980097338557243, lr: 1e-05
2023-12-21 18:57:09 INFO     	 * (global step 10900: loss: 0.23831790313124657, lr: 1e-05
2023-12-21 18:57:25 INFO     	 * (global step 10950: loss: 0.28613854199647903, lr: 1e-05
2023-12-21 18:57:40 INFO     	 * (global step 11000: loss: 0.38696491718292236, lr: 1e-05
2023-12-21 18:57:56 INFO     	 * (global step 11050: loss: 0.2620004881173372, lr: 1e-05
2023-12-21 18:58:12 INFO     	 * (global step 11100: loss: 0.32876214757561684, lr: 1e-05
2023-12-21 18:58:27 INFO     	 * (global step 11150: loss: 0.27829332649707794, lr: 1e-05
2023-12-21 18:58:43 INFO     	 * (global step 11200: loss: 0.37260979413986206, lr: 1e-05
2023-12-21 18:58:58 INFO     	 * (global step 11250: loss: 0.3318726643919945, lr: 1e-05
2023-12-21 18:59:14 INFO     	 * (global step 11300: loss: 0.23763002082705498, lr: 1e-05
2023-12-21 18:59:30 INFO     	 * (global step 11350: loss: 0.25541068613529205, lr: 1e-05
2023-12-21 18:59:45 INFO     	 * (global step 11400: loss: 0.2711718678474426, lr: 1e-05
2023-12-21 19:00:01 INFO     	 * (global step 11450: loss: 0.29572852700948715, lr: 1e-05
2023-12-21 19:00:16 INFO     	 * (global step 11500: loss: 0.3333224877715111, lr: 1e-05
2023-12-21 19:00:32 INFO     	 * (global step 11550: loss: 0.24789877235889435, lr: 1e-05
2023-12-21 19:00:48 INFO     	 * (global step 11600: loss: 0.27420197799801826, lr: 1e-05
2023-12-21 19:01:03 INFO     	 * (global step 11650: loss: 0.28022661060094833, lr: 1e-05
2023-12-21 19:01:19 INFO     	 * (global step 11700: loss: 0.3355611823499203, lr: 1e-05
2023-12-21 19:01:35 INFO     	 * (global step 11750: loss: 0.3128858841955662, lr: 1e-05
2023-12-21 19:01:50 INFO     	 * (global step 11800: loss: 0.27479882165789604, lr: 1e-05
2023-12-21 19:01:52 INFO     [epoch 14/15] average loss: 0.313, lr: 1e-05
2023-12-21 19:01:52 INFO     saving model related files
2023-12-21 19:01:52 INFO     saving model
2023-12-21 19:01:52 INFO     saving tokenizer
2023-12-21 19:01:52 INFO     saving optimizer
2023-12-21 19:01:53 INFO     remove old optimizer files
2023-12-21 19:01:53 INFO     complete training: model ckpt was saved at small_combined_trained_ckpt/model_vhyoja
2023-12-21 19:01:54 INFO     ## 2nd RUN: Configuration 2/5: validation/Bleu_4 = 0.05941041226773688
2023-12-21 19:01:54 INFO     initialize model trainer
2023-12-21 19:01:54 INFO     load config from existing checkpoint at small_combined_trained_ckpt/model_oprhlh
2023-12-21 19:01:54 INFO     hyperparameters
2023-12-21 19:01:54 INFO     	 * dataset_path: StellarMilk/squad_newsqa
2023-12-21 19:01:54 INFO     	 * dataset_name: default
2023-12-21 19:01:54 INFO     	 * input_types: ['paragraph']
2023-12-21 19:01:54 INFO     	 * output_types: ['questions_answers']
2023-12-21 19:01:54 INFO     	 * prefix_types: ['qag']
2023-12-21 19:01:54 INFO     	 * model: t5-small
2023-12-21 19:01:54 INFO     	 * max_length: 512
2023-12-21 19:01:54 INFO     	 * max_length_output: 512
2023-12-21 19:01:54 INFO     	 * epoch: 15
2023-12-21 19:01:54 INFO     	 * batch: 2
2023-12-21 19:01:54 INFO     	 * lr: 1e-05
2023-12-21 19:01:54 INFO     	 * fp16: False
2023-12-21 19:01:54 INFO     	 * random_seed: 1
2023-12-21 19:01:54 INFO     	 * gradient_accumulation_steps: 2
2023-12-21 19:01:54 INFO     	 * label_smoothing: 0.15
2023-12-21 19:01:54 INFO     load checkpoint from small_combined_trained_ckpt/model_oprhlh/epoch_10
2023-12-21 19:01:54 INFO     use spaCy answer extraction model: positionrank
2023-12-21 19:01:55 INFO     Model `small_combined_trained_ckpt/model_oprhlh/epoch_10`
2023-12-21 19:01:55 INFO     	 * Num of GPU in use: 1
2023-12-21 19:01:55 INFO     	 * Prefix: True
2023-12-21 19:01:55 INFO     	 * Language: en (ignore at the training phase)
2023-12-21 19:01:55 INFO     load optimizer from small_combined_trained_ckpt/model_oprhlh/optimizers/optimizer.10.pt
2023-12-21 19:01:55 INFO     optimizer is loading on cuda
2023-12-21 19:02:02 INFO     dataset preprocessing
2023-12-21 19:02:08 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/squad_newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-21 19:02:10 INFO     start model training
2023-12-21 19:02:18 INFO     	 * (global step 50: loss: 3.085407018661499, lr: 1e-05
2023-12-21 19:02:27 INFO     	 * (global step 100: loss: 3.049100160598755, lr: 1e-05
2023-12-21 19:02:35 INFO     	 * (global step 150: loss: 3.1732308864593506, lr: 1e-05
2023-12-21 19:02:43 INFO     	 * (global step 200: loss: 2.6656993627548218, lr: 1e-05
2023-12-21 19:02:52 INFO     	 * (global step 250: loss: 2.734555721282959, lr: 1e-05
2023-12-21 19:03:00 INFO     	 * (global step 300: loss: 2.5867221355438232, lr: 1e-05
2023-12-21 19:03:08 INFO     	 * (global step 350: loss: 2.6641005277633667, lr: 1e-05
2023-12-21 19:03:17 INFO     	 * (global step 400: loss: 2.6949533224105835, lr: 1e-05
2023-12-21 19:03:25 INFO     	 * (global step 450: loss: 2.595083475112915, lr: 1e-05
2023-12-21 19:03:33 INFO     	 * (global step 500: loss: 2.616335153579712, lr: 1e-05
2023-12-21 19:03:42 INFO     	 * (global step 550: loss: 2.6097217798233032, lr: 1e-05
2023-12-21 19:03:50 INFO     	 * (global step 600: loss: 2.8036144971847534, lr: 1e-05
2023-12-21 19:03:59 INFO     	 * (global step 650: loss: 2.4962164163589478, lr: 1e-05
2023-12-21 19:04:07 INFO     	 * (global step 700: loss: 2.8547333478927612, lr: 1e-05
2023-12-21 19:04:15 INFO     	 * (global step 750: loss: 2.8064727783203125, lr: 1e-05
2023-12-21 19:04:24 INFO     	 * (global step 800: loss: 2.567174553871155, lr: 1e-05
2023-12-21 19:04:32 INFO     	 * (global step 850: loss: 2.540396809577942, lr: 1e-05
2023-12-21 19:04:40 INFO     	 * (global step 900: loss: 2.5164860486984253, lr: 1e-05
2023-12-21 19:04:49 INFO     	 * (global step 950: loss: 2.5722166299819946, lr: 1e-05
2023-12-21 19:04:57 INFO     	 * (global step 1000: loss: 2.465261936187744, lr: 1e-05
2023-12-21 19:05:06 INFO     	 * (global step 1050: loss: 2.4955719709396362, lr: 1e-05
2023-12-21 19:05:14 INFO     	 * (global step 1100: loss: 2.524364948272705, lr: 1e-05
2023-12-21 19:05:22 INFO     	 * (global step 1150: loss: 2.5362277030944824, lr: 1e-05
2023-12-21 19:05:31 INFO     	 * (global step 1200: loss: 2.5438170433044434, lr: 1e-05
2023-12-21 19:05:39 INFO     	 * (global step 1250: loss: 2.58445942401886, lr: 1e-05
2023-12-21 19:05:47 INFO     	 * (global step 1300: loss: 2.444317579269409, lr: 1e-05
2023-12-21 19:05:56 INFO     	 * (global step 1350: loss: 2.443000912666321, lr: 1e-05
2023-12-21 19:06:04 INFO     	 * (global step 1400: loss: 2.523653745651245, lr: 1e-05
2023-12-21 19:06:13 INFO     	 * (global step 1450: loss: 2.509291648864746, lr: 1e-05
2023-12-21 19:06:21 INFO     	 * (global step 1500: loss: 2.4630069732666016, lr: 1e-05
2023-12-21 19:06:29 INFO     	 * (global step 1550: loss: 2.4526865482330322, lr: 1e-05
2023-12-21 19:06:38 INFO     	 * (global step 1600: loss: 2.403369665145874, lr: 1e-05
2023-12-21 19:06:46 INFO     	 * (global step 1650: loss: 2.409881830215454, lr: 1e-05
2023-12-21 19:06:55 INFO     	 * (global step 1700: loss: 2.4320653676986694, lr: 1e-05
2023-12-21 19:07:03 INFO     	 * (global step 1750: loss: 2.548948884010315, lr: 1e-05
2023-12-21 19:07:11 INFO     	 * (global step 1800: loss: 2.4579278230667114, lr: 1e-05
2023-12-21 19:07:20 INFO     	 * (global step 1850: loss: 2.4456324577331543, lr: 1e-05
2023-12-21 19:07:28 INFO     	 * (global step 1900: loss: 2.434842586517334, lr: 1e-05
2023-12-21 19:07:36 INFO     	 * (global step 1950: loss: 2.6558032035827637, lr: 1e-05
2023-12-21 19:07:45 INFO     	 * (global step 2000: loss: 2.500872850418091, lr: 1e-05
2023-12-21 19:07:53 INFO     	 * (global step 2050: loss: 2.4341262578964233, lr: 1e-05
2023-12-21 19:08:02 INFO     	 * (global step 2100: loss: 2.4150233268737793, lr: 1e-05
2023-12-21 19:08:10 INFO     	 * (global step 2150: loss: 2.412126660346985, lr: 1e-05
2023-12-21 19:08:18 INFO     	 * (global step 2200: loss: 2.5139551162719727, lr: 1e-05
2023-12-21 19:08:27 INFO     	 * (global step 2250: loss: 2.4742707014083862, lr: 1e-05
2023-12-21 19:08:35 INFO     	 * (global step 2300: loss: 2.6488653421401978, lr: 1e-05
2023-12-21 19:08:43 INFO     	 * (global step 2350: loss: 2.4888516664505005, lr: 1e-05
2023-12-21 19:08:52 INFO     	 * (global step 2400: loss: 2.442848801612854, lr: 1e-05
2023-12-21 19:09:00 INFO     	 * (global step 2450: loss: 2.5191904306411743, lr: 1e-05
2023-12-21 19:09:09 INFO     	 * (global step 2500: loss: 2.586430311203003, lr: 1e-05
2023-12-21 19:09:17 INFO     	 * (global step 2550: loss: 2.4131667613983154, lr: 1e-05
2023-12-21 19:09:25 INFO     	 * (global step 2600: loss: 2.33700954914093, lr: 1e-05
2023-12-21 19:09:34 INFO     	 * (global step 2650: loss: 2.4339176416397095, lr: 1e-05
2023-12-21 19:09:42 INFO     	 * (global step 2700: loss: 2.4333317279815674, lr: 1e-05
2023-12-21 19:09:50 INFO     	 * (global step 2750: loss: 2.464591383934021, lr: 1e-05
2023-12-21 19:09:59 INFO     	 * (global step 2800: loss: 2.4105918407440186, lr: 1e-05
2023-12-21 19:10:07 INFO     	 * (global step 2850: loss: 2.4742772579193115, lr: 1e-05
2023-12-21 19:10:16 INFO     	 * (global step 2900: loss: 2.413349747657776, lr: 1e-05
2023-12-21 19:10:24 INFO     	 * (global step 2950: loss: 2.5203078985214233, lr: 1e-05
2023-12-21 19:10:32 INFO     	 * (global step 3000: loss: 2.4860646724700928, lr: 1e-05
2023-12-21 19:10:41 INFO     	 * (global step 3050: loss: 2.4136390686035156, lr: 1e-05
2023-12-21 19:10:49 INFO     	 * (global step 3100: loss: 2.6460633277893066, lr: 1e-05
2023-12-21 19:10:57 INFO     	 * (global step 3150: loss: 2.4624909162521362, lr: 1e-05
2023-12-21 19:11:06 INFO     	 * (global step 3200: loss: 2.385538697242737, lr: 1e-05
2023-12-21 19:11:14 INFO     	 * (global step 3250: loss: 2.4559388160705566, lr: 1e-05
2023-12-21 19:11:23 INFO     	 * (global step 3300: loss: 2.3713866472244263, lr: 1e-05
2023-12-21 19:11:31 INFO     	 * (global step 3350: loss: 2.439679741859436, lr: 1e-05
2023-12-21 19:11:39 INFO     	 * (global step 3400: loss: 2.3938238620758057, lr: 1e-05
2023-12-21 19:11:48 INFO     	 * (global step 3450: loss: 2.3813812732696533, lr: 1e-05
2023-12-21 19:11:56 INFO     	 * (global step 3500: loss: 2.4586448669433594, lr: 1e-05
2023-12-21 19:12:04 INFO     	 * (global step 3550: loss: 2.3766154050827026, lr: 1e-05
2023-12-21 19:12:13 INFO     	 * (global step 3600: loss: 2.382137179374695, lr: 1e-05
2023-12-21 19:12:21 INFO     	 * (global step 3650: loss: 2.42593252658844, lr: 1e-05
2023-12-21 19:12:30 INFO     	 * (global step 3700: loss: 2.466797113418579, lr: 1e-05
2023-12-21 19:12:38 INFO     	 * (global step 3750: loss: 2.459978699684143, lr: 1e-05
2023-12-21 19:12:46 INFO     	 * (global step 3800: loss: 2.4287678003311157, lr: 1e-05
2023-12-21 19:12:55 INFO     	 * (global step 3850: loss: 2.368511438369751, lr: 1e-05
2023-12-21 19:13:03 INFO     	 * (global step 3900: loss: 2.3331315517425537, lr: 1e-05
2023-12-21 19:13:11 INFO     	 * (global step 3950: loss: 2.4663853645324707, lr: 1e-05
2023-12-21 19:13:20 INFO     	 * (global step 4000: loss: 2.3431828022003174, lr: 1e-05
2023-12-21 19:13:28 INFO     	 * (global step 4050: loss: 2.5276639461517334, lr: 1e-05
2023-12-21 19:13:37 INFO     	 * (global step 4100: loss: 2.401659846305847, lr: 1e-05
2023-12-21 19:13:45 INFO     	 * (global step 4150: loss: 2.3750170469284058, lr: 1e-05
2023-12-21 19:13:53 INFO     	 * (global step 4200: loss: 2.3613585233688354, lr: 1e-05
2023-12-21 19:14:02 INFO     	 * (global step 4250: loss: 2.4503092765808105, lr: 1e-05
2023-12-21 19:14:10 INFO     	 * (global step 4300: loss: 2.490698218345642, lr: 1e-05
2023-12-21 19:14:18 INFO     	 * (global step 4350: loss: 2.478562593460083, lr: 1e-05
2023-12-21 19:14:27 INFO     	 * (global step 4400: loss: 2.372839331626892, lr: 1e-05
2023-12-21 19:14:35 INFO     	 * (global step 4450: loss: 2.5107048749923706, lr: 1e-05
2023-12-21 19:14:44 INFO     	 * (global step 4500: loss: 2.4086514711380005, lr: 1e-05
2023-12-21 19:14:52 INFO     	 * (global step 4550: loss: 2.717468738555908, lr: 1e-05
2023-12-21 19:15:00 INFO     	 * (global step 4600: loss: 2.4244284629821777, lr: 1e-05
2023-12-21 19:15:09 INFO     	 * (global step 4650: loss: 2.3046478033065796, lr: 1e-05
2023-12-21 19:15:17 INFO     	 * (global step 4700: loss: 2.4652222394943237, lr: 1e-05
2023-12-21 19:15:21 INFO     [epoch 10/15] average loss: 2.513, lr: 1e-05
2023-12-21 19:15:21 INFO     saving model related files
2023-12-21 19:15:21 INFO     saving model
2023-12-21 19:15:21 INFO     saving tokenizer
2023-12-21 19:15:22 INFO     saving optimizer
2023-12-21 19:15:22 INFO     remove old optimizer files
2023-12-21 19:15:27 INFO     	 * (global step 4750: loss: 2.348328948020935, lr: 1e-05
2023-12-21 19:15:36 INFO     	 * (global step 4800: loss: 2.359950065612793, lr: 1e-05
2023-12-21 19:15:44 INFO     	 * (global step 4850: loss: 2.4282647371292114, lr: 1e-05
2023-12-21 19:15:52 INFO     	 * (global step 4900: loss: 2.4903491735458374, lr: 1e-05
2023-12-21 19:16:01 INFO     	 * (global step 4950: loss: 2.458701252937317, lr: 1e-05
2023-12-21 19:16:09 INFO     	 * (global step 5000: loss: 2.4342548847198486, lr: 1e-05
2023-12-21 19:16:18 INFO     	 * (global step 5050: loss: 2.3694456815719604, lr: 1e-05
2023-12-21 19:16:26 INFO     	 * (global step 5100: loss: 2.3436708450317383, lr: 1e-05
2023-12-21 19:16:34 INFO     	 * (global step 5150: loss: 2.428295135498047, lr: 1e-05
2023-12-21 19:16:43 INFO     	 * (global step 5200: loss: 2.364461898803711, lr: 1e-05
2023-12-21 19:16:51 INFO     	 * (global step 5250: loss: 2.3784477710723877, lr: 1e-05
2023-12-21 19:16:59 INFO     	 * (global step 5300: loss: 2.413831114768982, lr: 1e-05
2023-12-21 19:17:08 INFO     	 * (global step 5350: loss: 2.404260754585266, lr: 1e-05
2023-12-21 19:17:16 INFO     	 * (global step 5400: loss: 2.325744867324829, lr: 1e-05
2023-12-21 19:17:25 INFO     	 * (global step 5450: loss: 2.5031256675720215, lr: 1e-05
2023-12-21 19:17:33 INFO     	 * (global step 5500: loss: 2.39453387260437, lr: 1e-05
2023-12-21 19:17:41 INFO     	 * (global step 5550: loss: 2.4170408248901367, lr: 1e-05
2023-12-21 19:17:50 INFO     	 * (global step 5600: loss: 2.541080355644226, lr: 1e-05
2023-12-21 19:17:58 INFO     	 * (global step 5650: loss: 2.3853554725646973, lr: 1e-05
2023-12-21 19:18:07 INFO     	 * (global step 5700: loss: 2.52053439617157, lr: 1e-05
2023-12-21 19:18:15 INFO     	 * (global step 5750: loss: 2.32500159740448, lr: 1e-05
2023-12-21 19:18:23 INFO     	 * (global step 5800: loss: 2.353046417236328, lr: 1e-05
2023-12-21 19:18:32 INFO     	 * (global step 5850: loss: 2.3370630741119385, lr: 1e-05
2023-12-21 19:18:40 INFO     	 * (global step 5900: loss: 2.368314743041992, lr: 1e-05
2023-12-21 19:18:48 INFO     	 * (global step 5950: loss: 2.347894072532654, lr: 1e-05
2023-12-21 19:18:57 INFO     	 * (global step 6000: loss: 2.2774463891983032, lr: 1e-05
2023-12-21 19:19:05 INFO     	 * (global step 6050: loss: 2.388778567314148, lr: 1e-05
2023-12-21 19:19:14 INFO     	 * (global step 6100: loss: 2.3348543643951416, lr: 1e-05
2023-12-21 19:19:22 INFO     	 * (global step 6150: loss: 2.431447386741638, lr: 1e-05
2023-12-21 19:19:30 INFO     	 * (global step 6200: loss: 2.3980443477630615, lr: 1e-05
2023-12-21 19:19:39 INFO     	 * (global step 6250: loss: 2.360278844833374, lr: 1e-05
2023-12-21 19:19:47 INFO     	 * (global step 6300: loss: 2.326059937477112, lr: 1e-05
2023-12-21 19:19:55 INFO     	 * (global step 6350: loss: 2.317578077316284, lr: 1e-05
2023-12-21 19:20:04 INFO     	 * (global step 6400: loss: 2.4644359350204468, lr: 1e-05
2023-12-21 19:20:12 INFO     	 * (global step 6450: loss: 2.4575799703598022, lr: 1e-05
2023-12-21 19:20:21 INFO     	 * (global step 6500: loss: 2.3067835569381714, lr: 1e-05
2023-12-21 19:20:29 INFO     	 * (global step 6550: loss: 2.337286949157715, lr: 1e-05
2023-12-21 19:20:37 INFO     	 * (global step 6600: loss: 2.294601321220398, lr: 1e-05
2023-12-21 19:20:46 INFO     	 * (global step 6650: loss: 2.5462048053741455, lr: 1e-05
2023-12-21 19:20:54 INFO     	 * (global step 6700: loss: 2.244491457939148, lr: 1e-05
2023-12-21 19:21:02 INFO     	 * (global step 6750: loss: 2.294462203979492, lr: 1e-05
2023-12-21 19:21:11 INFO     	 * (global step 6800: loss: 2.3922256231307983, lr: 1e-05
2023-12-21 19:21:19 INFO     	 * (global step 6850: loss: 2.3519020080566406, lr: 1e-05
2023-12-21 19:21:28 INFO     	 * (global step 6900: loss: 2.4194599390029907, lr: 1e-05
2023-12-21 19:21:36 INFO     	 * (global step 6950: loss: 2.35763156414032, lr: 1e-05
2023-12-21 19:21:44 INFO     	 * (global step 7000: loss: 2.4029654264450073, lr: 1e-05
2023-12-21 19:21:53 INFO     	 * (global step 7050: loss: 2.4177364110946655, lr: 1e-05
2023-12-21 19:22:01 INFO     	 * (global step 7100: loss: 2.2970162630081177, lr: 1e-05
2023-12-21 19:22:10 INFO     	 * (global step 7150: loss: 2.3724855184555054, lr: 1e-05
2023-12-21 19:22:18 INFO     	 * (global step 7200: loss: 2.478621006011963, lr: 1e-05
2023-12-21 19:22:26 INFO     	 * (global step 7250: loss: 2.4160048961639404, lr: 1e-05
2023-12-21 19:22:35 INFO     	 * (global step 7300: loss: 2.2956559658050537, lr: 1e-05
2023-12-21 19:22:43 INFO     	 * (global step 7350: loss: 2.403526186943054, lr: 1e-05
2023-12-21 19:22:51 INFO     	 * (global step 7400: loss: 2.2996160984039307, lr: 1e-05
2023-12-21 19:23:00 INFO     	 * (global step 7450: loss: 2.377173066139221, lr: 1e-05
2023-12-21 19:23:08 INFO     	 * (global step 7500: loss: 2.459131598472595, lr: 1e-05
2023-12-21 19:23:17 INFO     	 * (global step 7550: loss: 2.366198420524597, lr: 1e-05
2023-12-21 19:23:25 INFO     	 * (global step 7600: loss: 2.3588950634002686, lr: 1e-05
2023-12-21 19:23:33 INFO     	 * (global step 7650: loss: 2.3692944049835205, lr: 1e-05
2023-12-21 19:23:42 INFO     	 * (global step 7700: loss: 2.3596028089523315, lr: 1e-05
2023-12-21 19:23:50 INFO     	 * (global step 7750: loss: 2.361618757247925, lr: 1e-05
2023-12-21 19:23:58 INFO     	 * (global step 7800: loss: 2.4122644662857056, lr: 1e-05
2023-12-21 19:24:07 INFO     	 * (global step 7850: loss: 2.4234293699264526, lr: 1e-05
2023-12-21 19:24:15 INFO     	 * (global step 7900: loss: 2.42118763923645, lr: 1e-05
2023-12-21 19:24:24 INFO     	 * (global step 7950: loss: 2.3869293928146362, lr: 1e-05
2023-12-21 19:24:32 INFO     	 * (global step 8000: loss: 2.268555521965027, lr: 1e-05
2023-12-21 19:24:40 INFO     	 * (global step 8050: loss: 2.366472005844116, lr: 1e-05
2023-12-21 19:24:49 INFO     	 * (global step 8100: loss: 2.414202570915222, lr: 1e-05
2023-12-21 19:24:57 INFO     	 * (global step 8150: loss: 2.2486073970794678, lr: 1e-05
2023-12-21 19:25:06 INFO     	 * (global step 8200: loss: 2.2640124559402466, lr: 1e-05
2023-12-21 19:25:14 INFO     	 * (global step 8250: loss: 2.351094961166382, lr: 1e-05
2023-12-21 19:25:22 INFO     	 * (global step 8300: loss: 2.363190531730652, lr: 1e-05
2023-12-21 19:25:31 INFO     	 * (global step 8350: loss: 2.2623813152313232, lr: 1e-05
2023-12-21 19:25:39 INFO     	 * (global step 8400: loss: 2.455362558364868, lr: 1e-05
2023-12-21 19:25:47 INFO     	 * (global step 8450: loss: 2.3743847608566284, lr: 1e-05
2023-12-21 19:25:56 INFO     	 * (global step 8500: loss: 2.3188605308532715, lr: 1e-05
2023-12-21 19:26:04 INFO     	 * (global step 8550: loss: 2.4380284547805786, lr: 1e-05
2023-12-21 19:26:13 INFO     	 * (global step 8600: loss: 2.460254192352295, lr: 1e-05
2023-12-21 19:26:21 INFO     	 * (global step 8650: loss: 2.363955855369568, lr: 1e-05
2023-12-21 19:26:29 INFO     	 * (global step 8700: loss: 2.305243492126465, lr: 1e-05
2023-12-21 19:26:38 INFO     	 * (global step 8750: loss: 2.396575927734375, lr: 1e-05
2023-12-21 19:26:46 INFO     	 * (global step 8800: loss: 2.4880257844924927, lr: 1e-05
2023-12-21 19:26:55 INFO     	 * (global step 8850: loss: 2.3246792554855347, lr: 1e-05
2023-12-21 19:27:03 INFO     	 * (global step 8900: loss: 2.405118227005005, lr: 1e-05
2023-12-21 19:27:11 INFO     	 * (global step 8950: loss: 2.349536895751953, lr: 1e-05
2023-12-21 19:27:20 INFO     	 * (global step 9000: loss: 2.280537247657776, lr: 1e-05
2023-12-21 19:27:28 INFO     	 * (global step 9050: loss: 2.4138247966766357, lr: 1e-05
2023-12-21 19:27:36 INFO     	 * (global step 9100: loss: 2.3383913040161133, lr: 1e-05
2023-12-21 19:27:45 INFO     	 * (global step 9150: loss: 2.3054521083831787, lr: 1e-05
2023-12-21 19:27:53 INFO     	 * (global step 9200: loss: 2.403199076652527, lr: 1e-05
2023-12-21 19:28:02 INFO     	 * (global step 9250: loss: 2.376391649246216, lr: 1e-05
2023-12-21 19:28:10 INFO     	 * (global step 9300: loss: 2.431861639022827, lr: 1e-05
2023-12-21 19:28:18 INFO     	 * (global step 9350: loss: 2.4348113536834717, lr: 1e-05
2023-12-21 19:28:27 INFO     	 * (global step 9400: loss: 2.3476572036743164, lr: 1e-05
2023-12-21 19:28:34 INFO     [epoch 11/15] average loss: 2.388, lr: 1e-05
2023-12-21 19:28:34 INFO     saving model related files
2023-12-21 19:28:34 INFO     saving model
2023-12-21 19:28:35 INFO     saving tokenizer
2023-12-21 19:28:35 INFO     saving optimizer
2023-12-21 19:28:36 INFO     remove old optimizer files
2023-12-21 19:28:37 INFO     	 * (global step 9450: loss: 2.3033766746520996, lr: 1e-05
2023-12-21 19:28:45 INFO     	 * (global step 9500: loss: 2.2938857078552246, lr: 1e-05
2023-12-21 19:28:53 INFO     	 * (global step 9550: loss: 2.3992642164230347, lr: 1e-05
2023-12-21 19:29:02 INFO     	 * (global step 9600: loss: 2.4180599451065063, lr: 1e-05
2023-12-21 19:29:10 INFO     	 * (global step 9650: loss: 2.2684319019317627, lr: 1e-05
2023-12-21 19:29:18 INFO     	 * (global step 9700: loss: 2.2893980741500854, lr: 1e-05
2023-12-21 19:29:27 INFO     	 * (global step 9750: loss: 2.33467173576355, lr: 1e-05
2023-12-21 19:29:35 INFO     	 * (global step 9800: loss: 2.3661457300186157, lr: 1e-05
2023-12-21 19:29:44 INFO     	 * (global step 9850: loss: 2.563981771469116, lr: 1e-05
2023-12-21 19:29:52 INFO     	 * (global step 9900: loss: 2.2928963899612427, lr: 1e-05
2023-12-21 19:30:00 INFO     	 * (global step 9950: loss: 2.3887683153152466, lr: 1e-05
2023-12-21 19:30:09 INFO     	 * (global step 10000: loss: 2.3462764024734497, lr: 1e-05
2023-12-21 19:30:17 INFO     	 * (global step 10050: loss: 2.305179476737976, lr: 1e-05
2023-12-21 19:30:25 INFO     	 * (global step 10100: loss: 2.3092113733291626, lr: 1e-05
2023-12-21 19:30:34 INFO     	 * (global step 10150: loss: 2.2820241451263428, lr: 1e-05
2023-12-21 19:30:42 INFO     	 * (global step 10200: loss: 2.3119590282440186, lr: 1e-05
2023-12-21 19:30:51 INFO     	 * (global step 10250: loss: 2.3637558221817017, lr: 1e-05
2023-12-21 19:30:59 INFO     	 * (global step 10300: loss: 2.2451581954956055, lr: 1e-05
2023-12-21 19:31:07 INFO     	 * (global step 10350: loss: 2.511490225791931, lr: 1e-05
2023-12-21 19:31:16 INFO     	 * (global step 10400: loss: 2.4154343605041504, lr: 1e-05
2023-12-21 19:31:24 INFO     	 * (global step 10450: loss: 2.379593849182129, lr: 1e-05
2023-12-21 19:31:33 INFO     	 * (global step 10500: loss: 2.2455707788467407, lr: 1e-05
2023-12-21 19:31:41 INFO     	 * (global step 10550: loss: 2.4471436738967896, lr: 1e-05
2023-12-21 19:31:49 INFO     	 * (global step 10600: loss: 2.369774103164673, lr: 1e-05
2023-12-21 19:31:58 INFO     	 * (global step 10650: loss: 2.6113370656967163, lr: 1e-05
2023-12-21 19:32:06 INFO     	 * (global step 10700: loss: 2.48831844329834, lr: 1e-05
2023-12-21 19:32:14 INFO     	 * (global step 10750: loss: 2.441210627555847, lr: 1e-05
2023-12-21 19:32:23 INFO     	 * (global step 10800: loss: 2.3490407466888428, lr: 1e-05
2023-12-21 19:32:31 INFO     	 * (global step 10850: loss: 2.334093928337097, lr: 1e-05
2023-12-21 19:32:40 INFO     	 * (global step 10900: loss: 2.2482131719589233, lr: 1e-05
2023-12-21 19:32:48 INFO     	 * (global step 10950: loss: 2.5330843925476074, lr: 1e-05
2023-12-21 19:32:56 INFO     	 * (global step 11000: loss: 2.274287223815918, lr: 1e-05
2023-12-21 19:33:05 INFO     	 * (global step 11050: loss: 2.407793164253235, lr: 1e-05
2023-12-21 19:33:13 INFO     	 * (global step 11100: loss: 2.2810611724853516, lr: 1e-05
2023-12-21 19:33:21 INFO     	 * (global step 11150: loss: 2.5246535539627075, lr: 1e-05
2023-12-21 19:33:30 INFO     	 * (global step 11200: loss: 2.433392643928528, lr: 1e-05
2023-12-21 19:33:38 INFO     	 * (global step 11250: loss: 2.478492856025696, lr: 1e-05
2023-12-21 19:33:47 INFO     	 * (global step 11300: loss: 2.34087872505188, lr: 1e-05
2023-12-21 19:33:55 INFO     	 * (global step 11350: loss: 2.4031022787094116, lr: 1e-05
2023-12-21 19:34:03 INFO     	 * (global step 11400: loss: 2.4123693704605103, lr: 1e-05
2023-12-21 19:34:12 INFO     	 * (global step 11450: loss: 2.281943440437317, lr: 1e-05
2023-12-21 19:34:20 INFO     	 * (global step 11500: loss: 2.48531436920166, lr: 1e-05
2023-12-21 19:34:28 INFO     	 * (global step 11550: loss: 2.3645201921463013, lr: 1e-05
2023-12-21 19:34:37 INFO     	 * (global step 11600: loss: 2.605773448944092, lr: 1e-05
2023-12-21 19:34:45 INFO     	 * (global step 11650: loss: 2.506094217300415, lr: 1e-05
2023-12-21 19:34:54 INFO     	 * (global step 11700: loss: 2.3796766996383667, lr: 1e-05
2023-12-21 19:35:02 INFO     	 * (global step 11750: loss: 2.3095600605010986, lr: 1e-05
2023-12-21 19:35:10 INFO     	 * (global step 11800: loss: 2.317443370819092, lr: 1e-05
2023-12-21 19:35:19 INFO     	 * (global step 11850: loss: 2.417387366294861, lr: 1e-05
2023-12-21 19:35:27 INFO     	 * (global step 11900: loss: 2.501173496246338, lr: 1e-05
2023-12-21 19:35:36 INFO     	 * (global step 11950: loss: 2.294533371925354, lr: 1e-05
2023-12-21 19:35:44 INFO     	 * (global step 12000: loss: 2.311394691467285, lr: 1e-05
2023-12-21 19:35:52 INFO     	 * (global step 12050: loss: 2.3344266414642334, lr: 1e-05
2023-12-21 19:36:01 INFO     	 * (global step 12100: loss: 2.407689690589905, lr: 1e-05
2023-12-21 19:36:09 INFO     	 * (global step 12150: loss: 2.451091170310974, lr: 1e-05
2023-12-21 19:36:17 INFO     	 * (global step 12200: loss: 2.4299755096435547, lr: 1e-05
2023-12-21 19:36:26 INFO     	 * (global step 12250: loss: 2.297883629798889, lr: 1e-05
2023-12-21 19:36:34 INFO     	 * (global step 12300: loss: 2.3554494380950928, lr: 1e-05
2023-12-21 19:36:43 INFO     	 * (global step 12350: loss: 2.418770670890808, lr: 1e-05
2023-12-21 19:36:51 INFO     	 * (global step 12400: loss: 2.374346137046814, lr: 1e-05
2023-12-21 19:36:59 INFO     	 * (global step 12450: loss: 2.285289168357849, lr: 1e-05
2023-12-21 19:37:08 INFO     	 * (global step 12500: loss: 2.28728985786438, lr: 1e-05
2023-12-21 19:37:16 INFO     	 * (global step 12550: loss: 2.307421088218689, lr: 1e-05
2023-12-21 19:37:24 INFO     	 * (global step 12600: loss: 2.259443521499634, lr: 1e-05
2023-12-21 19:37:33 INFO     	 * (global step 12650: loss: 2.407333731651306, lr: 1e-05
2023-12-21 19:37:41 INFO     	 * (global step 12700: loss: 2.3111019134521484, lr: 1e-05
2023-12-21 19:37:50 INFO     	 * (global step 12750: loss: 2.422882556915283, lr: 1e-05
2023-12-21 19:37:58 INFO     	 * (global step 12800: loss: 2.4208223819732666, lr: 1e-05
2023-12-21 19:38:06 INFO     	 * (global step 12850: loss: 2.270700216293335, lr: 1e-05
2023-12-21 19:38:15 INFO     	 * (global step 12900: loss: 2.5015090703964233, lr: 1e-05
2023-12-21 19:38:23 INFO     	 * (global step 12950: loss: 2.3710598945617676, lr: 1e-05
2023-12-21 19:38:31 INFO     	 * (global step 13000: loss: 2.443716049194336, lr: 1e-05
2023-12-21 19:38:40 INFO     	 * (global step 13050: loss: 2.3386844396591187, lr: 1e-05
2023-12-21 19:38:48 INFO     	 * (global step 13100: loss: 2.2589462995529175, lr: 1e-05
2023-12-21 19:38:57 INFO     	 * (global step 13150: loss: 2.3674557209014893, lr: 1e-05
2023-12-21 19:39:05 INFO     	 * (global step 13200: loss: 2.340969443321228, lr: 1e-05
2023-12-21 19:39:13 INFO     	 * (global step 13250: loss: 2.4987540245056152, lr: 1e-05
2023-12-21 19:39:22 INFO     	 * (global step 13300: loss: 2.383580207824707, lr: 1e-05
2023-12-21 19:39:30 INFO     	 * (global step 13350: loss: 2.339720606803894, lr: 1e-05
2023-12-21 19:39:39 INFO     	 * (global step 13400: loss: 2.3125550746917725, lr: 1e-05
2023-12-21 19:39:47 INFO     	 * (global step 13450: loss: 2.479559540748596, lr: 1e-05
2023-12-21 19:39:55 INFO     	 * (global step 13500: loss: 2.310053825378418, lr: 1e-05
2023-12-21 19:40:04 INFO     	 * (global step 13550: loss: 2.287912964820862, lr: 1e-05
2023-12-21 19:40:12 INFO     	 * (global step 13600: loss: 2.3083072900772095, lr: 1e-05
2023-12-21 19:40:20 INFO     	 * (global step 13650: loss: 2.265350341796875, lr: 1e-05
2023-12-21 19:40:29 INFO     	 * (global step 13700: loss: 2.2823963165283203, lr: 1e-05
2023-12-21 19:40:37 INFO     	 * (global step 13750: loss: 2.2918591499328613, lr: 1e-05
2023-12-21 19:40:46 INFO     	 * (global step 13800: loss: 2.3265851736068726, lr: 1e-05
2023-12-21 19:40:54 INFO     	 * (global step 13850: loss: 2.369568705558777, lr: 1e-05
2023-12-21 19:41:02 INFO     	 * (global step 13900: loss: 2.3724050521850586, lr: 1e-05
2023-12-21 19:41:11 INFO     	 * (global step 13950: loss: 2.2475085258483887, lr: 1e-05
2023-12-21 19:41:19 INFO     	 * (global step 14000: loss: 2.3381389379501343, lr: 1e-05
2023-12-21 19:41:27 INFO     	 * (global step 14050: loss: 2.278839349746704, lr: 1e-05
2023-12-21 19:41:36 INFO     	 * (global step 14100: loss: 2.309826970100403, lr: 1e-05
2023-12-21 19:41:44 INFO     	 * (global step 14150: loss: 2.4270870685577393, lr: 1e-05
2023-12-21 19:41:47 INFO     [epoch 12/15] average loss: 2.363, lr: 1e-05
2023-12-21 19:41:47 INFO     saving model related files
2023-12-21 19:41:47 INFO     saving model
2023-12-21 19:41:48 INFO     saving tokenizer
2023-12-21 19:41:48 INFO     saving optimizer
2023-12-21 19:41:49 INFO     remove old optimizer files
2023-12-21 19:41:54 INFO     	 * (global step 14200: loss: 2.32378613948822, lr: 1e-05
2023-12-21 19:42:02 INFO     	 * (global step 14250: loss: 2.240193247795105, lr: 1e-05
2023-12-21 19:42:11 INFO     	 * (global step 14300: loss: 2.3539477586746216, lr: 1e-05
2023-12-21 19:42:19 INFO     	 * (global step 14350: loss: 2.276463747024536, lr: 1e-05
2023-12-21 19:42:28 INFO     	 * (global step 14400: loss: 2.3639503717422485, lr: 1e-05
2023-12-21 19:42:36 INFO     	 * (global step 14450: loss: 2.4359993934631348, lr: 1e-05
2023-12-21 19:42:44 INFO     	 * (global step 14500: loss: 2.3514822721481323, lr: 1e-05
2023-12-21 19:42:53 INFO     	 * (global step 14550: loss: 2.3109081983566284, lr: 1e-05
2023-12-21 19:43:01 INFO     	 * (global step 14600: loss: 2.3065338134765625, lr: 1e-05
2023-12-21 19:43:09 INFO     	 * (global step 14650: loss: 2.5745694637298584, lr: 1e-05
2023-12-21 19:43:18 INFO     	 * (global step 14700: loss: 2.436688184738159, lr: 1e-05
2023-12-21 19:43:26 INFO     	 * (global step 14750: loss: 2.367322087287903, lr: 1e-05
2023-12-21 19:43:35 INFO     	 * (global step 14800: loss: 2.3697547912597656, lr: 1e-05
2023-12-21 19:43:43 INFO     	 * (global step 14850: loss: 2.344020366668701, lr: 1e-05
2023-12-21 19:43:51 INFO     	 * (global step 14900: loss: 2.387920618057251, lr: 1e-05
2023-12-21 19:44:00 INFO     	 * (global step 14950: loss: 2.3966050148010254, lr: 1e-05
2023-12-21 19:44:08 INFO     	 * (global step 15000: loss: 2.235838294029236, lr: 1e-05
2023-12-21 19:44:16 INFO     	 * (global step 15050: loss: 2.357359290122986, lr: 1e-05
2023-12-21 19:44:25 INFO     	 * (global step 15100: loss: 2.2740743160247803, lr: 1e-05
2023-12-21 19:44:33 INFO     	 * (global step 15150: loss: 2.3694084882736206, lr: 1e-05
2023-12-21 19:44:42 INFO     	 * (global step 15200: loss: 2.5800429582595825, lr: 1e-05
2023-12-21 19:44:50 INFO     	 * (global step 15250: loss: 2.3234639167785645, lr: 1e-05
2023-12-21 19:44:58 INFO     	 * (global step 15300: loss: 2.4083590507507324, lr: 1e-05
2023-12-21 19:45:07 INFO     	 * (global step 15350: loss: 2.31303608417511, lr: 1e-05
2023-12-21 19:45:15 INFO     	 * (global step 15400: loss: 2.268790364265442, lr: 1e-05
2023-12-21 19:45:23 INFO     	 * (global step 15450: loss: 2.436446785926819, lr: 1e-05
2023-12-21 19:45:32 INFO     	 * (global step 15500: loss: 2.495694875717163, lr: 1e-05
2023-12-21 19:45:40 INFO     	 * (global step 15550: loss: 2.421736240386963, lr: 1e-05
2023-12-21 19:45:49 INFO     	 * (global step 15600: loss: 2.3449403047561646, lr: 1e-05
2023-12-21 19:45:57 INFO     	 * (global step 15650: loss: 2.387770175933838, lr: 1e-05
2023-12-21 19:46:05 INFO     	 * (global step 15700: loss: 2.5210800170898438, lr: 1e-05
2023-12-21 19:46:14 INFO     	 * (global step 15750: loss: 2.2951658964157104, lr: 1e-05
2023-12-21 19:46:22 INFO     	 * (global step 15800: loss: 2.384493827819824, lr: 1e-05
2023-12-21 19:46:30 INFO     	 * (global step 15850: loss: 2.4357659816741943, lr: 1e-05
2023-12-21 19:46:39 INFO     	 * (global step 15900: loss: 2.3941657543182373, lr: 1e-05
2023-12-21 19:46:47 INFO     	 * (global step 15950: loss: 2.33536696434021, lr: 1e-05
2023-12-21 19:46:56 INFO     	 * (global step 16000: loss: 2.3571226596832275, lr: 1e-05
2023-12-21 19:47:04 INFO     	 * (global step 16050: loss: 2.308091402053833, lr: 1e-05
2023-12-21 19:47:12 INFO     	 * (global step 16100: loss: 2.29746413230896, lr: 1e-05
2023-12-21 19:47:21 INFO     	 * (global step 16150: loss: 2.2604819536209106, lr: 1e-05
2023-12-21 19:47:29 INFO     	 * (global step 16200: loss: 2.4325950145721436, lr: 1e-05
2023-12-21 19:47:37 INFO     	 * (global step 16250: loss: 2.2760066986083984, lr: 1e-05
2023-12-21 19:47:46 INFO     	 * (global step 16300: loss: 2.308698534965515, lr: 1e-05
2023-12-21 19:47:54 INFO     	 * (global step 16350: loss: 2.3116238117218018, lr: 1e-05
2023-12-21 19:48:03 INFO     	 * (global step 16400: loss: 2.3814451694488525, lr: 1e-05
2023-12-21 19:48:11 INFO     	 * (global step 16450: loss: 2.3088937997817993, lr: 1e-05
2023-12-21 19:48:19 INFO     	 * (global step 16500: loss: 2.508632183074951, lr: 1e-05
2023-12-21 19:48:28 INFO     	 * (global step 16550: loss: 2.2957957983016968, lr: 1e-05
2023-12-21 19:48:36 INFO     	 * (global step 16600: loss: 2.566711664199829, lr: 1e-05
2023-12-21 19:48:45 INFO     	 * (global step 16650: loss: 2.4219608306884766, lr: 1e-05
2023-12-21 19:48:53 INFO     	 * (global step 16700: loss: 2.317860960960388, lr: 1e-05
2023-12-21 19:49:01 INFO     	 * (global step 16750: loss: 2.290185332298279, lr: 1e-05
2023-12-21 19:49:10 INFO     	 * (global step 16800: loss: 2.3368523120880127, lr: 1e-05
2023-12-21 19:49:18 INFO     	 * (global step 16850: loss: 2.2296117544174194, lr: 1e-05
2023-12-21 19:49:26 INFO     	 * (global step 16900: loss: 2.3212730884552, lr: 1e-05
2023-12-21 19:49:35 INFO     	 * (global step 16950: loss: 2.2687485218048096, lr: 1e-05
2023-12-21 19:49:43 INFO     	 * (global step 17000: loss: 2.375872850418091, lr: 1e-05
2023-12-21 19:49:52 INFO     	 * (global step 17050: loss: 2.26903760433197, lr: 1e-05
2023-12-21 19:50:00 INFO     	 * (global step 17100: loss: 2.366770386695862, lr: 1e-05
2023-12-21 19:50:08 INFO     	 * (global step 17150: loss: 2.325340151786804, lr: 1e-05
2023-12-21 19:50:17 INFO     	 * (global step 17200: loss: 2.883397698402405, lr: 1e-05
2023-12-21 19:50:25 INFO     	 * (global step 17250: loss: 2.2622194290161133, lr: 1e-05
2023-12-21 19:50:34 INFO     	 * (global step 17300: loss: 2.3296186923980713, lr: 1e-05
2023-12-21 19:50:42 INFO     	 * (global step 17350: loss: 2.4208494424819946, lr: 1e-05
2023-12-21 19:50:50 INFO     	 * (global step 17400: loss: 2.358490824699402, lr: 1e-05
2023-12-21 19:50:59 INFO     	 * (global step 17450: loss: 2.347295641899109, lr: 1e-05
2023-12-21 19:51:07 INFO     	 * (global step 17500: loss: 2.375999689102173, lr: 1e-05
2023-12-21 19:51:15 INFO     	 * (global step 17550: loss: 2.3636765480041504, lr: 1e-05
2023-12-21 19:51:24 INFO     	 * (global step 17600: loss: 2.3831675052642822, lr: 1e-05
2023-12-21 19:51:32 INFO     	 * (global step 17650: loss: 2.3107627630233765, lr: 1e-05
2023-12-21 19:51:41 INFO     	 * (global step 17700: loss: 2.218881607055664, lr: 1e-05
2023-12-21 19:51:49 INFO     	 * (global step 17750: loss: 2.429470181465149, lr: 1e-05
2023-12-21 19:51:57 INFO     	 * (global step 17800: loss: 2.339106798171997, lr: 1e-05
2023-12-21 19:52:06 INFO     	 * (global step 17850: loss: 2.306274175643921, lr: 1e-05
2023-12-21 19:52:14 INFO     	 * (global step 17900: loss: 2.442726731300354, lr: 1e-05
2023-12-21 19:52:22 INFO     	 * (global step 17950: loss: 2.3544671535491943, lr: 1e-05
2023-12-21 19:52:31 INFO     	 * (global step 18000: loss: 2.393900990486145, lr: 1e-05
2023-12-21 19:52:39 INFO     	 * (global step 18050: loss: 2.246267557144165, lr: 1e-05
2023-12-21 19:52:48 INFO     	 * (global step 18100: loss: 2.449146032333374, lr: 1e-05
2023-12-21 19:52:56 INFO     	 * (global step 18150: loss: 2.380691885948181, lr: 1e-05
2023-12-21 19:53:04 INFO     	 * (global step 18200: loss: 2.2534940242767334, lr: 1e-05
2023-12-21 19:53:13 INFO     	 * (global step 18250: loss: 2.3310863971710205, lr: 1e-05
2023-12-21 19:53:21 INFO     	 * (global step 18300: loss: 2.283698320388794, lr: 1e-05
2023-12-21 19:53:29 INFO     	 * (global step 18350: loss: 2.345017910003662, lr: 1e-05
2023-12-21 19:53:38 INFO     	 * (global step 18400: loss: 2.342425584793091, lr: 1e-05
2023-12-21 19:53:46 INFO     	 * (global step 18450: loss: 2.4756067991256714, lr: 1e-05
2023-12-21 19:53:55 INFO     	 * (global step 18500: loss: 2.303855776786804, lr: 1e-05
2023-12-21 19:54:03 INFO     	 * (global step 18550: loss: 2.2687747478485107, lr: 1e-05
2023-12-21 19:54:11 INFO     	 * (global step 18600: loss: 2.296594738960266, lr: 1e-05
2023-12-21 19:54:20 INFO     	 * (global step 18650: loss: 2.655881404876709, lr: 1e-05
2023-12-21 19:54:28 INFO     	 * (global step 18700: loss: 2.285579800605774, lr: 1e-05
2023-12-21 19:54:37 INFO     	 * (global step 18750: loss: 2.386809825897217, lr: 1e-05
2023-12-21 19:54:45 INFO     	 * (global step 18800: loss: 2.2698593139648438, lr: 1e-05
2023-12-21 19:54:53 INFO     	 * (global step 18850: loss: 2.5207176208496094, lr: 1e-05
2023-12-21 19:55:00 INFO     [epoch 13/15] average loss: 2.351, lr: 1e-05
2023-12-21 19:55:00 INFO     saving model related files
2023-12-21 19:55:00 INFO     saving model
2023-12-21 19:55:01 INFO     saving tokenizer
2023-12-21 19:55:01 INFO     saving optimizer
2023-12-21 19:55:02 INFO     remove old optimizer files
2023-12-21 19:55:03 INFO     	 * (global step 18900: loss: 2.4378750324249268, lr: 1e-05
2023-12-21 19:55:12 INFO     	 * (global step 18950: loss: 2.2799097299575806, lr: 1e-05
2023-12-21 19:55:20 INFO     	 * (global step 19000: loss: 2.33176052570343, lr: 1e-05
2023-12-21 19:55:28 INFO     	 * (global step 19050: loss: 2.4086493253707886, lr: 1e-05
2023-12-21 19:55:37 INFO     	 * (global step 19100: loss: 2.3695952892303467, lr: 1e-05
2023-12-21 19:55:45 INFO     	 * (global step 19150: loss: 2.5602245330810547, lr: 1e-05
2023-12-21 19:55:53 INFO     	 * (global step 19200: loss: 2.358660578727722, lr: 1e-05
2023-12-21 19:56:02 INFO     	 * (global step 19250: loss: 2.419453263282776, lr: 1e-05
2023-12-21 19:56:10 INFO     	 * (global step 19300: loss: 2.280116319656372, lr: 1e-05
2023-12-21 19:56:19 INFO     	 * (global step 19350: loss: 2.2695374488830566, lr: 1e-05
2023-12-21 19:56:27 INFO     	 * (global step 19400: loss: 2.351573348045349, lr: 1e-05
2023-12-21 19:56:35 INFO     	 * (global step 19450: loss: 2.375768780708313, lr: 1e-05
2023-12-21 19:56:44 INFO     	 * (global step 19500: loss: 2.434367299079895, lr: 1e-05
2023-12-21 19:56:52 INFO     	 * (global step 19550: loss: 2.4019588232040405, lr: 1e-05
2023-12-21 19:57:00 INFO     	 * (global step 19600: loss: 2.2804038524627686, lr: 1e-05
2023-12-21 19:57:09 INFO     	 * (global step 19650: loss: 2.3309870958328247, lr: 1e-05
2023-12-21 19:57:17 INFO     	 * (global step 19700: loss: 2.3734785318374634, lr: 1e-05
2023-12-21 19:57:25 INFO     	 * (global step 19750: loss: 2.5103758573532104, lr: 1e-05
2023-12-21 19:57:34 INFO     	 * (global step 19800: loss: 2.470211148262024, lr: 1e-05
2023-12-21 19:57:42 INFO     	 * (global step 19850: loss: 2.32719886302948, lr: 1e-05
2023-12-21 19:57:51 INFO     	 * (global step 19900: loss: 2.3183794021606445, lr: 1e-05
2023-12-21 19:57:59 INFO     	 * (global step 19950: loss: 2.3148738145828247, lr: 1e-05
2023-12-21 19:58:07 INFO     	 * (global step 20000: loss: 2.3058539628982544, lr: 1e-05
2023-12-21 19:58:16 INFO     	 * (global step 20050: loss: 2.3312532901763916, lr: 1e-05
2023-12-21 19:58:24 INFO     	 * (global step 20100: loss: 2.3669174909591675, lr: 1e-05
2023-12-21 19:58:32 INFO     	 * (global step 20150: loss: 2.237623929977417, lr: 1e-05
2023-12-21 19:58:41 INFO     	 * (global step 20200: loss: 2.3937548398971558, lr: 1e-05
2023-12-21 19:58:49 INFO     	 * (global step 20250: loss: 2.352620244026184, lr: 1e-05
2023-12-21 19:58:57 INFO     	 * (global step 20300: loss: 2.3026444911956787, lr: 1e-05
2023-12-21 19:59:06 INFO     	 * (global step 20350: loss: 2.3434988260269165, lr: 1e-05
2023-12-21 19:59:14 INFO     	 * (global step 20400: loss: 2.2607284784317017, lr: 1e-05
2023-12-21 19:59:22 INFO     	 * (global step 20450: loss: 2.393978714942932, lr: 1e-05
2023-12-21 19:59:31 INFO     	 * (global step 20500: loss: 2.4181700944900513, lr: 1e-05
2023-12-21 19:59:39 INFO     	 * (global step 20550: loss: 2.446658730506897, lr: 1e-05
2023-12-21 19:59:48 INFO     	 * (global step 20600: loss: 2.283674120903015, lr: 1e-05
2023-12-21 19:59:56 INFO     	 * (global step 20650: loss: 2.285431742668152, lr: 1e-05
2023-12-21 20:00:04 INFO     	 * (global step 20700: loss: 2.315469980239868, lr: 1e-05
2023-12-21 20:00:13 INFO     	 * (global step 20750: loss: 2.3048442602157593, lr: 1e-05
2023-12-21 20:00:21 INFO     	 * (global step 20800: loss: 2.3635311126708984, lr: 1e-05
2023-12-21 20:00:29 INFO     	 * (global step 20850: loss: 2.2905772924423218, lr: 1e-05
2023-12-21 20:00:38 INFO     	 * (global step 20900: loss: 2.2677429914474487, lr: 1e-05
2023-12-21 20:00:46 INFO     	 * (global step 20950: loss: 2.350937008857727, lr: 1e-05
2023-12-21 20:00:54 INFO     	 * (global step 21000: loss: 2.3876473903656006, lr: 1e-05
2023-12-21 20:01:03 INFO     	 * (global step 21050: loss: 2.319806694984436, lr: 1e-05
2023-12-21 20:01:11 INFO     	 * (global step 21100: loss: 2.290934681892395, lr: 1e-05
2023-12-21 20:01:19 INFO     	 * (global step 21150: loss: 2.4037115573883057, lr: 1e-05
2023-12-21 20:01:28 INFO     	 * (global step 21200: loss: 2.244289755821228, lr: 1e-05
2023-12-21 20:01:36 INFO     	 * (global step 21250: loss: 2.4430620670318604, lr: 1e-05
2023-12-21 20:01:45 INFO     	 * (global step 21300: loss: 2.486405849456787, lr: 1e-05
2023-12-21 20:01:53 INFO     	 * (global step 21350: loss: 2.3599112033843994, lr: 1e-05
2023-12-21 20:02:01 INFO     	 * (global step 21400: loss: 2.3863134384155273, lr: 1e-05
2023-12-21 20:02:10 INFO     	 * (global step 21450: loss: 2.211735725402832, lr: 1e-05
2023-12-21 20:02:18 INFO     	 * (global step 21500: loss: 2.247405767440796, lr: 1e-05
2023-12-21 20:02:26 INFO     	 * (global step 21550: loss: 2.333639144897461, lr: 1e-05
2023-12-21 20:02:35 INFO     	 * (global step 21600: loss: 2.318412184715271, lr: 1e-05
2023-12-21 20:02:43 INFO     	 * (global step 21650: loss: 2.4261890649795532, lr: 1e-05
2023-12-21 20:02:51 INFO     	 * (global step 21700: loss: 2.3909573554992676, lr: 1e-05
2023-12-21 20:03:00 INFO     	 * (global step 21750: loss: 2.358043074607849, lr: 1e-05
2023-12-21 20:03:08 INFO     	 * (global step 21800: loss: 2.41694974899292, lr: 1e-05
2023-12-21 20:03:17 INFO     	 * (global step 21850: loss: 2.304060697555542, lr: 1e-05
2023-12-21 20:03:25 INFO     	 * (global step 21900: loss: 2.324888229370117, lr: 1e-05
2023-12-21 20:03:33 INFO     	 * (global step 21950: loss: 2.3273712396621704, lr: 1e-05
2023-12-21 20:03:42 INFO     	 * (global step 22000: loss: 2.3055511713027954, lr: 1e-05
2023-12-21 20:03:50 INFO     	 * (global step 22050: loss: 2.307515263557434, lr: 1e-05
2023-12-21 20:03:58 INFO     	 * (global step 22100: loss: 2.2657952308654785, lr: 1e-05
2023-12-21 20:04:07 INFO     	 * (global step 22150: loss: 2.328111410140991, lr: 1e-05
2023-12-21 20:04:15 INFO     	 * (global step 22200: loss: 2.459811806678772, lr: 1e-05
2023-12-21 20:04:23 INFO     	 * (global step 22250: loss: 2.318947434425354, lr: 1e-05
2023-12-21 20:04:32 INFO     	 * (global step 22300: loss: 2.324973940849304, lr: 1e-05
2023-12-21 20:04:40 INFO     	 * (global step 22350: loss: 2.293289303779602, lr: 1e-05
2023-12-21 20:04:48 INFO     	 * (global step 22400: loss: 2.34784734249115, lr: 1e-05
2023-12-21 20:04:57 INFO     	 * (global step 22450: loss: 2.4087973833084106, lr: 1e-05
2023-12-21 20:05:05 INFO     	 * (global step 22500: loss: 2.2961000204086304, lr: 1e-05
2023-12-21 20:05:14 INFO     	 * (global step 22550: loss: 2.38240909576416, lr: 1e-05
2023-12-21 20:05:22 INFO     	 * (global step 22600: loss: 2.2948572635650635, lr: 1e-05
2023-12-21 20:05:30 INFO     	 * (global step 22650: loss: 2.3106693029403687, lr: 1e-05
2023-12-21 20:05:39 INFO     	 * (global step 22700: loss: 2.5844591856002808, lr: 1e-05
2023-12-21 20:05:47 INFO     	 * (global step 22750: loss: 2.2227123975753784, lr: 1e-05
2023-12-21 20:05:55 INFO     	 * (global step 22800: loss: 2.2500027418136597, lr: 1e-05
2023-12-21 20:06:04 INFO     	 * (global step 22850: loss: 2.3312660455703735, lr: 1e-05
2023-12-21 20:06:12 INFO     	 * (global step 22900: loss: 2.371075391769409, lr: 1e-05
2023-12-21 20:06:20 INFO     	 * (global step 22950: loss: 2.286412000656128, lr: 1e-05
2023-12-21 20:06:29 INFO     	 * (global step 23000: loss: 2.372465968132019, lr: 1e-05
2023-12-21 20:06:37 INFO     	 * (global step 23050: loss: 2.369311571121216, lr: 1e-05
2023-12-21 20:06:45 INFO     	 * (global step 23100: loss: 2.222314953804016, lr: 1e-05
2023-12-21 20:06:54 INFO     	 * (global step 23150: loss: 2.332334041595459, lr: 1e-05
2023-12-21 20:07:02 INFO     	 * (global step 23200: loss: 2.3150415420532227, lr: 1e-05
2023-12-21 20:07:11 INFO     	 * (global step 23250: loss: 2.546423077583313, lr: 1e-05
2023-12-21 20:07:19 INFO     	 * (global step 23300: loss: 2.319374680519104, lr: 1e-05
2023-12-21 20:07:27 INFO     	 * (global step 23350: loss: 2.346887946128845, lr: 1e-05
2023-12-21 20:07:36 INFO     	 * (global step 23400: loss: 2.460514187812805, lr: 1e-05
2023-12-21 20:07:44 INFO     	 * (global step 23450: loss: 2.290624499320984, lr: 1e-05
2023-12-21 20:07:52 INFO     	 * (global step 23500: loss: 2.267876386642456, lr: 1e-05
2023-12-21 20:08:01 INFO     	 * (global step 23550: loss: 2.2927862405776978, lr: 1e-05
2023-12-21 20:08:09 INFO     	 * (global step 23600: loss: 2.3444801568984985, lr: 1e-05
2023-12-21 20:08:12 INFO     [epoch 14/15] average loss: 2.343, lr: 1e-05
2023-12-21 20:08:12 INFO     saving model related files
2023-12-21 20:08:12 INFO     saving model
2023-12-21 20:08:12 INFO     saving tokenizer
2023-12-21 20:08:12 INFO     saving optimizer
2023-12-21 20:08:13 INFO     remove old optimizer files
2023-12-21 20:08:13 INFO     complete training: model ckpt was saved at small_combined_trained_ckpt/model_oprhlh
2023-12-21 20:08:14 INFO     ## 2nd RUN: Configuration 3/5: validation/Bleu_4 = 0.05941041226773688
2023-12-21 20:08:14 INFO     initialize model trainer
2023-12-21 20:08:14 INFO     load config from existing checkpoint at small_combined_trained_ckpt/model_nrudfu
2023-12-21 20:08:14 INFO     hyperparameters
2023-12-21 20:08:14 INFO     	 * dataset_path: StellarMilk/squad_newsqa
2023-12-21 20:08:14 INFO     	 * dataset_name: default
2023-12-21 20:08:14 INFO     	 * input_types: ['paragraph']
2023-12-21 20:08:14 INFO     	 * output_types: ['questions_answers']
2023-12-21 20:08:14 INFO     	 * prefix_types: ['qag']
2023-12-21 20:08:14 INFO     	 * model: t5-small
2023-12-21 20:08:14 INFO     	 * max_length: 512
2023-12-21 20:08:14 INFO     	 * max_length_output: 512
2023-12-21 20:08:14 INFO     	 * epoch: 15
2023-12-21 20:08:14 INFO     	 * batch: 2
2023-12-21 20:08:14 INFO     	 * lr: 1e-05
2023-12-21 20:08:14 INFO     	 * fp16: False
2023-12-21 20:08:14 INFO     	 * random_seed: 1
2023-12-21 20:08:14 INFO     	 * gradient_accumulation_steps: 2
2023-12-21 20:08:14 INFO     	 * label_smoothing: 0.0
2023-12-21 20:08:14 INFO     load checkpoint from small_combined_trained_ckpt/model_nrudfu/epoch_10
2023-12-21 20:08:14 INFO     use spaCy answer extraction model: positionrank
2023-12-21 20:08:14 INFO     Model `small_combined_trained_ckpt/model_nrudfu/epoch_10`
2023-12-21 20:08:14 INFO     	 * Num of GPU in use: 1
2023-12-21 20:08:14 INFO     	 * Prefix: True
2023-12-21 20:08:14 INFO     	 * Language: en (ignore at the training phase)
2023-12-21 20:08:14 INFO     load optimizer from small_combined_trained_ckpt/model_nrudfu/optimizers/optimizer.10.pt
2023-12-21 20:08:14 INFO     optimizer is loading on cuda
2023-12-21 20:08:19 INFO     dataset preprocessing
2023-12-21 20:08:20 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/squad_newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-21 20:08:22 INFO     start model training
2023-12-21 20:08:30 INFO     	 * (global step 50: loss: 0.2883327156305313, lr: 1e-05
2023-12-21 20:08:38 INFO     	 * (global step 100: loss: 0.5170982182025909, lr: 1e-05
2023-12-21 20:08:46 INFO     	 * (global step 150: loss: 0.7326374650001526, lr: 1e-05
2023-12-21 20:08:54 INFO     	 * (global step 200: loss: 0.20705212652683258, lr: 1e-05
2023-12-21 20:09:02 INFO     	 * (global step 250: loss: 0.36345726251602173, lr: 1e-05
2023-12-21 20:09:11 INFO     	 * (global step 300: loss: 0.19877207279205322, lr: 1e-05
2023-12-21 20:09:19 INFO     	 * (global step 350: loss: 0.2994225025177002, lr: 1e-05
2023-12-21 20:09:27 INFO     	 * (global step 400: loss: 0.3573366403579712, lr: 1e-05
2023-12-21 20:09:35 INFO     	 * (global step 450: loss: 0.2908024489879608, lr: 1e-05
2023-12-21 20:09:43 INFO     	 * (global step 500: loss: 0.3023063391447067, lr: 1e-05
2023-12-21 20:09:51 INFO     	 * (global step 550: loss: 0.3181644529104233, lr: 1e-05
2023-12-21 20:09:59 INFO     	 * (global step 600: loss: 0.5317128896713257, lr: 1e-05
2023-12-21 20:10:07 INFO     	 * (global step 650: loss: 0.2204066514968872, lr: 1e-05
2023-12-21 20:10:16 INFO     	 * (global step 700: loss: 0.6232596337795258, lr: 1e-05
2023-12-21 20:10:24 INFO     	 * (global step 750: loss: 0.5471574664115906, lr: 1e-05
2023-12-21 20:10:32 INFO     	 * (global step 800: loss: 0.30937354266643524, lr: 1e-05
2023-12-21 20:10:40 INFO     	 * (global step 850: loss: 0.3002883195877075, lr: 1e-05
2023-12-21 20:10:48 INFO     	 * (global step 900: loss: 0.2776383310556412, lr: 1e-05
2023-12-21 20:10:56 INFO     	 * (global step 950: loss: 0.3260408416390419, lr: 1e-05
2023-12-21 20:11:04 INFO     	 * (global step 1000: loss: 0.21626828610897064, lr: 1e-05
2023-12-21 20:11:13 INFO     	 * (global step 1050: loss: 0.26852893829345703, lr: 1e-05
2023-12-21 20:11:21 INFO     	 * (global step 1100: loss: 0.32326188683509827, lr: 1e-05
2023-12-21 20:11:29 INFO     	 * (global step 1150: loss: 0.30863963067531586, lr: 1e-05
2023-12-21 20:11:37 INFO     	 * (global step 1200: loss: 0.3329409509897232, lr: 1e-05
2023-12-21 20:11:45 INFO     	 * (global step 1250: loss: 0.3805752098560333, lr: 1e-05
2023-12-21 20:11:53 INFO     	 * (global step 1300: loss: 0.22939089685678482, lr: 1e-05
2023-12-21 20:12:01 INFO     	 * (global step 1350: loss: 0.24608854204416275, lr: 1e-05
2023-12-21 20:12:10 INFO     	 * (global step 1400: loss: 0.33774664252996445, lr: 1e-05
2023-12-21 20:12:18 INFO     	 * (global step 1450: loss: 0.32117465883493423, lr: 1e-05
2023-12-21 20:12:26 INFO     	 * (global step 1500: loss: 0.277731716632843, lr: 1e-05
2023-12-21 20:12:34 INFO     	 * (global step 1550: loss: 0.26376134902238846, lr: 1e-05
2023-12-21 20:12:42 INFO     	 * (global step 1600: loss: 0.20665930211544037, lr: 1e-05
2023-12-21 20:12:50 INFO     	 * (global step 1650: loss: 0.22421962022781372, lr: 1e-05
2023-12-21 20:12:58 INFO     	 * (global step 1700: loss: 0.24086406826972961, lr: 1e-05
2023-12-21 20:13:06 INFO     	 * (global step 1750: loss: 0.3876134604215622, lr: 1e-05
2023-12-21 20:13:15 INFO     	 * (global step 1800: loss: 0.30324722081422806, lr: 1e-05
2023-12-21 20:13:23 INFO     	 * (global step 1850: loss: 0.275519996881485, lr: 1e-05
2023-12-21 20:13:31 INFO     	 * (global step 1900: loss: 0.26895975321531296, lr: 1e-05
2023-12-21 20:13:39 INFO     	 * (global step 1950: loss: 0.502468153834343, lr: 1e-05
2023-12-21 20:13:47 INFO     	 * (global step 2000: loss: 0.36480771005153656, lr: 1e-05
2023-12-21 20:13:55 INFO     	 * (global step 2050: loss: 0.2681667357683182, lr: 1e-05
2023-12-21 20:14:03 INFO     	 * (global step 2100: loss: 0.2388756424188614, lr: 1e-05
2023-12-21 20:14:12 INFO     	 * (global step 2150: loss: 0.2675079330801964, lr: 1e-05
2023-12-21 20:14:20 INFO     	 * (global step 2200: loss: 0.3566644787788391, lr: 1e-05
2023-12-21 20:14:28 INFO     	 * (global step 2250: loss: 0.3182813674211502, lr: 1e-05
2023-12-21 20:14:36 INFO     	 * (global step 2300: loss: 0.48337210714817047, lr: 1e-05
2023-12-21 20:14:44 INFO     	 * (global step 2350: loss: 0.3262317329645157, lr: 1e-05
2023-12-21 20:14:52 INFO     	 * (global step 2400: loss: 0.29083800315856934, lr: 1e-05
2023-12-21 20:15:00 INFO     	 * (global step 2450: loss: 0.3469880223274231, lr: 1e-05
2023-12-21 20:15:09 INFO     	 * (global step 2500: loss: 0.4407767206430435, lr: 1e-05
2023-12-21 20:15:17 INFO     	 * (global step 2550: loss: 0.2599690780043602, lr: 1e-05
2023-12-21 20:15:25 INFO     	 * (global step 2600: loss: 0.18690283223986626, lr: 1e-05
2023-12-21 20:15:33 INFO     	 * (global step 2650: loss: 0.2981424182653427, lr: 1e-05
2023-12-21 20:15:41 INFO     	 * (global step 2700: loss: 0.2761536166071892, lr: 1e-05
2023-12-21 20:15:49 INFO     	 * (global step 2750: loss: 0.32836317270994186, lr: 1e-05
2023-12-21 20:15:57 INFO     	 * (global step 2800: loss: 0.28151895850896835, lr: 1e-05
2023-12-21 20:16:06 INFO     	 * (global step 2850: loss: 0.35289908945560455, lr: 1e-05
2023-12-21 20:16:14 INFO     	 * (global step 2900: loss: 0.29000235348939896, lr: 1e-05
2023-12-21 20:16:22 INFO     	 * (global step 2950: loss: 0.3900197744369507, lr: 1e-05
2023-12-21 20:16:30 INFO     	 * (global step 3000: loss: 0.3542613685131073, lr: 1e-05
2023-12-21 20:16:38 INFO     	 * (global step 3050: loss: 0.28157994896173477, lr: 1e-05
2023-12-21 20:16:46 INFO     	 * (global step 3100: loss: 0.5301373898983002, lr: 1e-05
2023-12-21 20:16:54 INFO     	 * (global step 3150: loss: 0.34648706018924713, lr: 1e-05
2023-12-21 20:17:02 INFO     	 * (global step 3200: loss: 0.26433782279491425, lr: 1e-05
2023-12-21 20:17:11 INFO     	 * (global step 3250: loss: 0.3245205581188202, lr: 1e-05
2023-12-21 20:17:19 INFO     	 * (global step 3300: loss: 0.2374948039650917, lr: 1e-05
2023-12-21 20:17:27 INFO     	 * (global step 3350: loss: 0.32697340846061707, lr: 1e-05
2023-12-21 20:17:35 INFO     	 * (global step 3400: loss: 0.2764061540365219, lr: 1e-05
2023-12-21 20:17:43 INFO     	 * (global step 3450: loss: 0.25730349868535995, lr: 1e-05
2023-12-21 20:17:51 INFO     	 * (global step 3500: loss: 0.34287287294864655, lr: 1e-05
2023-12-21 20:17:59 INFO     	 * (global step 3550: loss: 0.24341033399105072, lr: 1e-05
2023-12-21 20:18:08 INFO     	 * (global step 3600: loss: 0.2519862800836563, lr: 1e-05
2023-12-21 20:18:16 INFO     	 * (global step 3650: loss: 0.29460282623767853, lr: 1e-05
2023-12-21 20:18:24 INFO     	 * (global step 3700: loss: 0.3478948324918747, lr: 1e-05
2023-12-21 20:18:32 INFO     	 * (global step 3750: loss: 0.3552516996860504, lr: 1e-05
2023-12-21 20:18:40 INFO     	 * (global step 3800: loss: 0.3078150153160095, lr: 1e-05
2023-12-21 20:18:48 INFO     	 * (global step 3850: loss: 0.24099590629339218, lr: 1e-05
2023-12-21 20:18:56 INFO     	 * (global step 3900: loss: 0.21259502321481705, lr: 1e-05
2023-12-21 20:19:05 INFO     	 * (global step 3950: loss: 0.3562108427286148, lr: 1e-05
2023-12-21 20:19:13 INFO     	 * (global step 4000: loss: 0.2250305861234665, lr: 1e-05
2023-12-21 20:19:21 INFO     	 * (global step 4050: loss: 0.44168543815612793, lr: 1e-05
2023-12-21 20:19:29 INFO     	 * (global step 4100: loss: 0.2915881276130676, lr: 1e-05
2023-12-21 20:19:37 INFO     	 * (global step 4150: loss: 0.2701476737856865, lr: 1e-05
2023-12-21 20:19:45 INFO     	 * (global step 4200: loss: 0.257213719189167, lr: 1e-05
2023-12-21 20:19:53 INFO     	 * (global step 4250: loss: 0.34823253750801086, lr: 1e-05
2023-12-21 20:20:01 INFO     	 * (global step 4300: loss: 0.38432247936725616, lr: 1e-05
2023-12-21 20:20:10 INFO     	 * (global step 4350: loss: 0.386298805475235, lr: 1e-05
2023-12-21 20:20:18 INFO     	 * (global step 4400: loss: 0.2651318609714508, lr: 1e-05
2023-12-21 20:20:26 INFO     	 * (global step 4450: loss: 0.4239601641893387, lr: 1e-05
2023-12-21 20:20:34 INFO     	 * (global step 4500: loss: 0.31215915083885193, lr: 1e-05
2023-12-21 20:20:42 INFO     	 * (global step 4550: loss: 0.6294423788785934, lr: 1e-05
2023-12-21 20:20:50 INFO     	 * (global step 4600: loss: 0.3153024986386299, lr: 1e-05
2023-12-21 20:20:58 INFO     	 * (global step 4650: loss: 0.19642578065395355, lr: 1e-05
2023-12-21 20:21:07 INFO     	 * (global step 4700: loss: 0.3499157130718231, lr: 1e-05
2023-12-21 20:21:10 INFO     [epoch 10/15] average loss: 0.312, lr: 1e-05
2023-12-21 20:21:10 INFO     saving model related files
2023-12-21 20:21:10 INFO     saving model
2023-12-21 20:21:11 INFO     saving tokenizer
2023-12-21 20:21:11 INFO     saving optimizer
2023-12-21 20:21:12 INFO     remove old optimizer files
2023-12-21 20:21:17 INFO     	 * (global step 4750: loss: 0.2546452693641186, lr: 1e-05
2023-12-21 20:21:25 INFO     	 * (global step 4800: loss: 0.2437865138053894, lr: 1e-05
2023-12-21 20:21:33 INFO     	 * (global step 4850: loss: 0.33054426312446594, lr: 1e-05
2023-12-21 20:21:41 INFO     	 * (global step 4900: loss: 0.41679468750953674, lr: 1e-05
2023-12-21 20:21:49 INFO     	 * (global step 4950: loss: 0.37786000967025757, lr: 1e-05
2023-12-21 20:21:57 INFO     	 * (global step 5000: loss: 0.3406134098768234, lr: 1e-05
2023-12-21 20:22:05 INFO     	 * (global step 5050: loss: 0.2610781416296959, lr: 1e-05
2023-12-21 20:22:14 INFO     	 * (global step 5100: loss: 0.24001114815473557, lr: 1e-05
2023-12-21 20:22:22 INFO     	 * (global step 5150: loss: 0.34467029571533203, lr: 1e-05
2023-12-21 20:22:30 INFO     	 * (global step 5200: loss: 0.27198031544685364, lr: 1e-05
2023-12-21 20:22:38 INFO     	 * (global step 5250: loss: 0.28865183889865875, lr: 1e-05
2023-12-21 20:22:46 INFO     	 * (global step 5300: loss: 0.300245076417923, lr: 1e-05
2023-12-21 20:22:54 INFO     	 * (global step 5350: loss: 0.3037763684988022, lr: 1e-05
2023-12-21 20:23:02 INFO     	 * (global step 5400: loss: 0.22571046650409698, lr: 1e-05
2023-12-21 20:23:10 INFO     	 * (global step 5450: loss: 0.4251117706298828, lr: 1e-05
2023-12-21 20:23:19 INFO     	 * (global step 5500: loss: 0.2986687272787094, lr: 1e-05
2023-12-21 20:23:27 INFO     	 * (global step 5550: loss: 0.3216669261455536, lr: 1e-05
2023-12-21 20:23:35 INFO     	 * (global step 5600: loss: 0.4675559252500534, lr: 1e-05
2023-12-21 20:23:43 INFO     	 * (global step 5650: loss: 0.29377472400665283, lr: 1e-05
2023-12-21 20:23:51 INFO     	 * (global step 5700: loss: 0.4286736994981766, lr: 1e-05
2023-12-21 20:23:59 INFO     	 * (global step 5750: loss: 0.2415810525417328, lr: 1e-05
2023-12-21 20:24:07 INFO     	 * (global step 5800: loss: 0.24919243156909943, lr: 1e-05
2023-12-21 20:24:16 INFO     	 * (global step 5850: loss: 0.2537495642900467, lr: 1e-05
2023-12-21 20:24:24 INFO     	 * (global step 5900: loss: 0.26077131927013397, lr: 1e-05
2023-12-21 20:24:32 INFO     	 * (global step 5950: loss: 0.2607915699481964, lr: 1e-05
2023-12-21 20:24:40 INFO     	 * (global step 6000: loss: 0.18702350556850433, lr: 1e-05
2023-12-21 20:24:48 INFO     	 * (global step 6050: loss: 0.2965697646141052, lr: 1e-05
2023-12-21 20:24:56 INFO     	 * (global step 6100: loss: 0.24357278645038605, lr: 1e-05
2023-12-21 20:25:04 INFO     	 * (global step 6150: loss: 0.3372047543525696, lr: 1e-05
2023-12-21 20:25:12 INFO     	 * (global step 6200: loss: 0.3236192613840103, lr: 1e-05
2023-12-21 20:25:21 INFO     	 * (global step 6250: loss: 0.28008438646793365, lr: 1e-05
2023-12-21 20:25:29 INFO     	 * (global step 6300: loss: 0.2510441988706589, lr: 1e-05
2023-12-21 20:25:37 INFO     	 * (global step 6350: loss: 0.23327898979187012, lr: 1e-05
2023-12-21 20:25:45 INFO     	 * (global step 6400: loss: 0.39195071160793304, lr: 1e-05
2023-12-21 20:25:53 INFO     	 * (global step 6450: loss: 0.3820575177669525, lr: 1e-05
2023-12-21 20:26:01 INFO     	 * (global step 6500: loss: 0.2226559892296791, lr: 1e-05
2023-12-21 20:26:09 INFO     	 * (global step 6550: loss: 0.2549656257033348, lr: 1e-05
2023-12-21 20:26:17 INFO     	 * (global step 6600: loss: 0.2060612589120865, lr: 1e-05
2023-12-21 20:26:26 INFO     	 * (global step 6650: loss: 0.4728449881076813, lr: 1e-05
2023-12-21 20:26:34 INFO     	 * (global step 6700: loss: 0.18005943298339844, lr: 1e-05
2023-12-21 20:26:42 INFO     	 * (global step 6750: loss: 0.22413135319948196, lr: 1e-05
2023-12-21 20:26:50 INFO     	 * (global step 6800: loss: 0.313076913356781, lr: 1e-05
2023-12-21 20:26:58 INFO     	 * (global step 6850: loss: 0.279894083738327, lr: 1e-05
2023-12-21 20:27:06 INFO     	 * (global step 6900: loss: 0.3263052850961685, lr: 1e-05
2023-12-21 20:27:14 INFO     	 * (global step 6950: loss: 0.2777744382619858, lr: 1e-05
2023-12-21 20:27:23 INFO     	 * (global step 7000: loss: 0.3212354779243469, lr: 1e-05
2023-12-21 20:27:31 INFO     	 * (global step 7050: loss: 0.34079375863075256, lr: 1e-05
2023-12-21 20:27:39 INFO     	 * (global step 7100: loss: 0.20722149312496185, lr: 1e-05
2023-12-21 20:27:47 INFO     	 * (global step 7150: loss: 0.28625285625457764, lr: 1e-05
2023-12-21 20:27:55 INFO     	 * (global step 7200: loss: 0.4281785637140274, lr: 1e-05
2023-12-21 20:28:03 INFO     	 * (global step 7250: loss: 0.3288576081395149, lr: 1e-05
2023-12-21 20:28:11 INFO     	 * (global step 7300: loss: 0.21906069666147232, lr: 1e-05
2023-12-21 20:28:20 INFO     	 * (global step 7350: loss: 0.3262902498245239, lr: 1e-05
2023-12-21 20:28:28 INFO     	 * (global step 7400: loss: 0.20778018236160278, lr: 1e-05
2023-12-21 20:28:36 INFO     	 * (global step 7450: loss: 0.2900858074426651, lr: 1e-05
2023-12-21 20:28:44 INFO     	 * (global step 7500: loss: 0.3860335797071457, lr: 1e-05
2023-12-21 20:28:52 INFO     	 * (global step 7550: loss: 0.2854766696691513, lr: 1e-05
2023-12-21 20:29:00 INFO     	 * (global step 7600: loss: 0.29683294892311096, lr: 1e-05
2023-12-21 20:29:08 INFO     	 * (global step 7650: loss: 0.2956738546490669, lr: 1e-05
2023-12-21 20:29:17 INFO     	 * (global step 7700: loss: 0.286267027258873, lr: 1e-05
2023-12-21 20:29:25 INFO     	 * (global step 7750: loss: 0.2780182883143425, lr: 1e-05
2023-12-21 20:29:33 INFO     	 * (global step 7800: loss: 0.3476601541042328, lr: 1e-05
2023-12-21 20:29:41 INFO     	 * (global step 7850: loss: 0.35923174023628235, lr: 1e-05
2023-12-21 20:29:49 INFO     	 * (global step 7900: loss: 0.33795226365327835, lr: 1e-05
2023-12-21 20:29:57 INFO     	 * (global step 7950: loss: 0.31435947120189667, lr: 1e-05
2023-12-21 20:30:05 INFO     	 * (global step 8000: loss: 0.20125820487737656, lr: 1e-05
2023-12-21 20:30:14 INFO     	 * (global step 8050: loss: 0.30146364867687225, lr: 1e-05
2023-12-21 20:30:22 INFO     	 * (global step 8100: loss: 0.3511451482772827, lr: 1e-05
2023-12-21 20:30:30 INFO     	 * (global step 8150: loss: 0.17758020013570786, lr: 1e-05
2023-12-21 20:30:38 INFO     	 * (global step 8200: loss: 0.16555603593587875, lr: 1e-05
2023-12-21 20:30:46 INFO     	 * (global step 8250: loss: 0.2777382358908653, lr: 1e-05
2023-12-21 20:30:54 INFO     	 * (global step 8300: loss: 0.2881107032299042, lr: 1e-05
2023-12-21 20:31:02 INFO     	 * (global step 8350: loss: 0.18822287768125534, lr: 1e-05
2023-12-21 20:31:10 INFO     	 * (global step 8400: loss: 0.37738383561372757, lr: 1e-05
2023-12-21 20:31:19 INFO     	 * (global step 8450: loss: 0.3098675012588501, lr: 1e-05
2023-12-21 20:31:27 INFO     	 * (global step 8500: loss: 0.2494717314839363, lr: 1e-05
2023-12-21 20:31:35 INFO     	 * (global step 8550: loss: 0.3585376590490341, lr: 1e-05
2023-12-21 20:31:43 INFO     	 * (global step 8600: loss: 0.39473479986190796, lr: 1e-05
2023-12-21 20:31:51 INFO     	 * (global step 8650: loss: 0.3035358637571335, lr: 1e-05
2023-12-21 20:31:59 INFO     	 * (global step 8700: loss: 0.2330053746700287, lr: 1e-05
2023-12-21 20:32:07 INFO     	 * (global step 8750: loss: 0.3376249819993973, lr: 1e-05
2023-12-21 20:32:16 INFO     	 * (global step 8800: loss: 0.4315752238035202, lr: 1e-05
2023-12-21 20:32:24 INFO     	 * (global step 8850: loss: 0.2682376652956009, lr: 1e-05
2023-12-21 20:32:32 INFO     	 * (global step 8900: loss: 0.33066388964653015, lr: 1e-05
2023-12-21 20:32:40 INFO     	 * (global step 8950: loss: 0.27517086267471313, lr: 1e-05
2023-12-21 20:32:48 INFO     	 * (global step 9000: loss: 0.1988920047879219, lr: 1e-05
2023-12-21 20:32:56 INFO     	 * (global step 9050: loss: 0.3588062971830368, lr: 1e-05
2023-12-21 20:33:04 INFO     	 * (global step 9100: loss: 0.25616147369146347, lr: 1e-05
2023-12-21 20:33:13 INFO     	 * (global step 9150: loss: 0.23233310133218765, lr: 1e-05
2023-12-21 20:33:21 INFO     	 * (global step 9200: loss: 0.3332909494638443, lr: 1e-05
2023-12-21 20:33:29 INFO     	 * (global step 9250: loss: 0.3174060732126236, lr: 1e-05
2023-12-21 20:33:37 INFO     	 * (global step 9300: loss: 0.3592473864555359, lr: 1e-05
2023-12-21 20:33:45 INFO     	 * (global step 9350: loss: 0.36750054359436035, lr: 1e-05
2023-12-21 20:33:53 INFO     	 * (global step 9400: loss: 0.27937185764312744, lr: 1e-05
2023-12-21 20:34:01 INFO     [epoch 11/15] average loss: 0.309, lr: 1e-05
2023-12-21 20:34:01 INFO     saving model related files
2023-12-21 20:34:01 INFO     saving model
2023-12-21 20:34:01 INFO     saving tokenizer
2023-12-21 20:34:01 INFO     saving optimizer
2023-12-21 20:34:02 INFO     remove old optimizer files
2023-12-21 20:34:03 INFO     	 * (global step 9450: loss: 0.24618689715862274, lr: 1e-05
2023-12-21 20:34:11 INFO     	 * (global step 9500: loss: 0.2209075540304184, lr: 1e-05
2023-12-21 20:34:19 INFO     	 * (global step 9550: loss: 0.32574768364429474, lr: 1e-05
2023-12-21 20:34:27 INFO     	 * (global step 9600: loss: 0.3607102334499359, lr: 1e-05
2023-12-21 20:34:35 INFO     	 * (global step 9650: loss: 0.19330303370952606, lr: 1e-05
2023-12-21 20:34:44 INFO     	 * (global step 9700: loss: 0.2240702360868454, lr: 1e-05
2023-12-21 20:34:52 INFO     	 * (global step 9750: loss: 0.28562652319669724, lr: 1e-05
2023-12-21 20:35:00 INFO     	 * (global step 9800: loss: 0.28996752202510834, lr: 1e-05
2023-12-21 20:35:08 INFO     	 * (global step 9850: loss: 0.5109123438596725, lr: 1e-05
2023-12-21 20:35:16 INFO     	 * (global step 9900: loss: 0.2245187684893608, lr: 1e-05
2023-12-21 20:35:24 INFO     	 * (global step 9950: loss: 0.3133699595928192, lr: 1e-05
2023-12-21 20:35:33 INFO     	 * (global step 10000: loss: 0.27062248438596725, lr: 1e-05
2023-12-21 20:35:41 INFO     	 * (global step 10050: loss: 0.23684212565422058, lr: 1e-05
2023-12-21 20:35:49 INFO     	 * (global step 10100: loss: 0.23912085592746735, lr: 1e-05
2023-12-21 20:35:57 INFO     	 * (global step 10150: loss: 0.20052066445350647, lr: 1e-05
2023-12-21 20:36:05 INFO     	 * (global step 10200: loss: 0.2494978979229927, lr: 1e-05
2023-12-21 20:36:13 INFO     	 * (global step 10250: loss: 0.2959023267030716, lr: 1e-05
2023-12-21 20:36:22 INFO     	 * (global step 10300: loss: 0.18162622302770615, lr: 1e-05
2023-12-21 20:36:30 INFO     	 * (global step 10350: loss: 0.4623662084341049, lr: 1e-05
2023-12-21 20:36:38 INFO     	 * (global step 10400: loss: 0.3481217622756958, lr: 1e-05
2023-12-21 20:36:46 INFO     	 * (global step 10450: loss: 0.3172485679388046, lr: 1e-05
2023-12-21 20:36:54 INFO     	 * (global step 10500: loss: 0.18846890330314636, lr: 1e-05
2023-12-21 20:37:02 INFO     	 * (global step 10550: loss: 0.3970508575439453, lr: 1e-05
2023-12-21 20:37:10 INFO     	 * (global step 10600: loss: 0.30944718420505524, lr: 1e-05
2023-12-21 20:37:19 INFO     	 * (global step 10650: loss: 0.5507796108722687, lr: 1e-05
2023-12-21 20:37:27 INFO     	 * (global step 10700: loss: 0.4431825876235962, lr: 1e-05
2023-12-21 20:37:35 INFO     	 * (global step 10750: loss: 0.37598200142383575, lr: 1e-05
2023-12-21 20:37:43 INFO     	 * (global step 10800: loss: 0.28419240564107895, lr: 1e-05
2023-12-21 20:37:51 INFO     	 * (global step 10850: loss: 0.2703947499394417, lr: 1e-05
2023-12-21 20:37:59 INFO     	 * (global step 10900: loss: 0.18780482560396194, lr: 1e-05
2023-12-21 20:38:07 INFO     	 * (global step 10950: loss: 0.51117043197155, lr: 1e-05
2023-12-21 20:38:16 INFO     	 * (global step 11000: loss: 0.20973142981529236, lr: 1e-05
2023-12-21 20:38:24 INFO     	 * (global step 11050: loss: 0.3517516851425171, lr: 1e-05
2023-12-21 20:38:32 INFO     	 * (global step 11100: loss: 0.210844025015831, lr: 1e-05
2023-12-21 20:38:40 INFO     	 * (global step 11150: loss: 0.47658097743988037, lr: 1e-05
2023-12-21 20:38:48 INFO     	 * (global step 11200: loss: 0.39381974935531616, lr: 1e-05
2023-12-21 20:38:56 INFO     	 * (global step 11250: loss: 0.42167094349861145, lr: 1e-05
2023-12-21 20:39:04 INFO     	 * (global step 11300: loss: 0.2789386957883835, lr: 1e-05
2023-12-21 20:39:13 INFO     	 * (global step 11350: loss: 0.3514198362827301, lr: 1e-05
2023-12-21 20:39:21 INFO     	 * (global step 11400: loss: 0.3641497641801834, lr: 1e-05
2023-12-21 20:39:29 INFO     	 * (global step 11450: loss: 0.2225554808974266, lr: 1e-05
2023-12-21 20:39:37 INFO     	 * (global step 11500: loss: 0.4271553158760071, lr: 1e-05
2023-12-21 20:39:45 INFO     	 * (global step 11550: loss: 0.3172568380832672, lr: 1e-05
2023-12-21 20:39:53 INFO     	 * (global step 11600: loss: 0.5599033236503601, lr: 1e-05
2023-12-21 20:40:01 INFO     	 * (global step 11650: loss: 0.46581098437309265, lr: 1e-05
2023-12-21 20:40:10 INFO     	 * (global step 11700: loss: 0.32942357659339905, lr: 1e-05
2023-12-21 20:40:18 INFO     	 * (global step 11750: loss: 0.24777042865753174, lr: 1e-05
2023-12-21 20:40:26 INFO     	 * (global step 11800: loss: 0.26867977529764175, lr: 1e-05
2023-12-21 20:40:34 INFO     	 * (global step 11850: loss: 0.34787534177303314, lr: 1e-05
2023-12-21 20:40:42 INFO     	 * (global step 11900: loss: 0.4516284912824631, lr: 1e-05
2023-12-21 20:40:50 INFO     	 * (global step 11950: loss: 0.22458162158727646, lr: 1e-05
2023-12-21 20:40:59 INFO     	 * (global step 12000: loss: 0.253251776099205, lr: 1e-05
2023-12-21 20:41:07 INFO     	 * (global step 12050: loss: 0.2698267176747322, lr: 1e-05
2023-12-21 20:41:15 INFO     	 * (global step 12100: loss: 0.3656865805387497, lr: 1e-05
2023-12-21 20:41:23 INFO     	 * (global step 12150: loss: 0.39418722689151764, lr: 1e-05
2023-12-21 20:41:31 INFO     	 * (global step 12200: loss: 0.3786766529083252, lr: 1e-05
2023-12-21 20:41:39 INFO     	 * (global step 12250: loss: 0.23463232070207596, lr: 1e-05
2023-12-21 20:41:48 INFO     	 * (global step 12300: loss: 0.3070013076066971, lr: 1e-05
2023-12-21 20:41:56 INFO     	 * (global step 12350: loss: 0.35543185472488403, lr: 1e-05
2023-12-21 20:42:04 INFO     	 * (global step 12400: loss: 0.32615049183368683, lr: 1e-05
2023-12-21 20:42:12 INFO     	 * (global step 12450: loss: 0.23237460851669312, lr: 1e-05
2023-12-21 20:42:20 INFO     	 * (global step 12500: loss: 0.2145407423377037, lr: 1e-05
2023-12-21 20:42:28 INFO     	 * (global step 12550: loss: 0.24589361250400543, lr: 1e-05
2023-12-21 20:42:37 INFO     	 * (global step 12600: loss: 0.1902080625295639, lr: 1e-05
2023-12-21 20:42:45 INFO     	 * (global step 12650: loss: 0.34638285636901855, lr: 1e-05
2023-12-21 20:42:53 INFO     	 * (global step 12700: loss: 0.25673971325159073, lr: 1e-05
2023-12-21 20:43:01 INFO     	 * (global step 12750: loss: 0.3729401081800461, lr: 1e-05
2023-12-21 20:43:09 INFO     	 * (global step 12800: loss: 0.3647632896900177, lr: 1e-05
2023-12-21 20:43:17 INFO     	 * (global step 12850: loss: 0.21705978363752365, lr: 1e-05
2023-12-21 20:43:26 INFO     	 * (global step 12900: loss: 0.4740585684776306, lr: 1e-05
2023-12-21 20:43:34 INFO     	 * (global step 12950: loss: 0.3200511634349823, lr: 1e-05
2023-12-21 20:43:42 INFO     	 * (global step 13000: loss: 0.3943145275115967, lr: 1e-05
2023-12-21 20:43:50 INFO     	 * (global step 13050: loss: 0.29257503151893616, lr: 1e-05
2023-12-21 20:43:58 INFO     	 * (global step 13100: loss: 0.21060704439878464, lr: 1e-05
2023-12-21 20:44:06 INFO     	 * (global step 13150: loss: 0.3142143562436104, lr: 1e-05
2023-12-21 20:44:15 INFO     	 * (global step 13200: loss: 0.28568725287914276, lr: 1e-05
2023-12-21 20:44:23 INFO     	 * (global step 13250: loss: 0.45500072836875916, lr: 1e-05
2023-12-21 20:44:31 INFO     	 * (global step 13300: loss: 0.32356174290180206, lr: 1e-05
2023-12-21 20:44:39 INFO     	 * (global step 13350: loss: 0.2876894921064377, lr: 1e-05
2023-12-21 20:44:47 INFO     	 * (global step 13400: loss: 0.25073787569999695, lr: 1e-05
2023-12-21 20:44:55 INFO     	 * (global step 13450: loss: 0.4373581111431122, lr: 1e-05
2023-12-21 20:45:04 INFO     	 * (global step 13500: loss: 0.24308813363313675, lr: 1e-05
2023-12-21 20:45:12 INFO     	 * (global step 13550: loss: 0.24115298688411713, lr: 1e-05
2023-12-21 20:45:20 INFO     	 * (global step 13600: loss: 0.2583763748407364, lr: 1e-05
2023-12-21 20:45:28 INFO     	 * (global step 13650: loss: 0.19602412730455399, lr: 1e-05
2023-12-21 20:45:36 INFO     	 * (global step 13700: loss: 0.22867970168590546, lr: 1e-05
2023-12-21 20:45:44 INFO     	 * (global step 13750: loss: 0.23842032253742218, lr: 1e-05
2023-12-21 20:45:52 INFO     	 * (global step 13800: loss: 0.2599312290549278, lr: 1e-05
2023-12-21 20:46:01 INFO     	 * (global step 13850: loss: 0.31481389701366425, lr: 1e-05
2023-12-21 20:46:09 INFO     	 * (global step 13900: loss: 0.32015859335660934, lr: 1e-05
2023-12-21 20:46:17 INFO     	 * (global step 13950: loss: 0.19581251591444016, lr: 1e-05
2023-12-21 20:46:25 INFO     	 * (global step 14000: loss: 0.28802865743637085, lr: 1e-05
2023-12-21 20:46:33 INFO     	 * (global step 14050: loss: 0.22724174708127975, lr: 1e-05
2023-12-21 20:46:41 INFO     	 * (global step 14100: loss: 0.2530955672264099, lr: 1e-05
2023-12-21 20:46:50 INFO     	 * (global step 14150: loss: 0.38064493238925934, lr: 1e-05
2023-12-21 20:46:53 INFO     [epoch 12/15] average loss: 0.306, lr: 1e-05
2023-12-21 20:46:53 INFO     saving model related files
2023-12-21 20:46:53 INFO     saving model
2023-12-21 20:46:53 INFO     saving tokenizer
2023-12-21 20:46:53 INFO     saving optimizer
2023-12-21 20:46:54 INFO     remove old optimizer files
2023-12-21 20:46:59 INFO     	 * (global step 14200: loss: 0.2630401998758316, lr: 1e-05
2023-12-21 20:47:07 INFO     	 * (global step 14250: loss: 0.18112146854400635, lr: 1e-05
2023-12-21 20:47:16 INFO     	 * (global step 14300: loss: 0.31610289216041565, lr: 1e-05
2023-12-21 20:47:24 INFO     	 * (global step 14350: loss: 0.22809689491987228, lr: 1e-05
2023-12-21 20:47:32 INFO     	 * (global step 14400: loss: 0.29934316873550415, lr: 1e-05
2023-12-21 20:47:40 INFO     	 * (global step 14450: loss: 0.3920991122722626, lr: 1e-05
2023-12-21 20:47:48 INFO     	 * (global step 14500: loss: 0.29376718401908875, lr: 1e-05
2023-12-21 20:47:56 INFO     	 * (global step 14550: loss: 0.26355161517858505, lr: 1e-05
2023-12-21 20:48:04 INFO     	 * (global step 14600: loss: 0.24641413986682892, lr: 1e-05
2023-12-21 20:48:12 INFO     	 * (global step 14650: loss: 0.5419749766588211, lr: 1e-05
2023-12-21 20:48:21 INFO     	 * (global step 14700: loss: 0.3933830261230469, lr: 1e-05
2023-12-21 20:48:29 INFO     	 * (global step 14750: loss: 0.3402547240257263, lr: 1e-05
2023-12-21 20:48:37 INFO     	 * (global step 14800: loss: 0.3312970846891403, lr: 1e-05
2023-12-21 20:48:45 INFO     	 * (global step 14850: loss: 0.2924351617693901, lr: 1e-05
2023-12-21 20:48:53 INFO     	 * (global step 14900: loss: 0.3489533066749573, lr: 1e-05
2023-12-21 20:49:01 INFO     	 * (global step 14950: loss: 0.32929442822933197, lr: 1e-05
2023-12-21 20:49:09 INFO     	 * (global step 15000: loss: 0.17323265969753265, lr: 1e-05
2023-12-21 20:49:17 INFO     	 * (global step 15050: loss: 0.302653044462204, lr: 1e-05
2023-12-21 20:49:26 INFO     	 * (global step 15100: loss: 0.2385670468211174, lr: 1e-05
2023-12-21 20:49:34 INFO     	 * (global step 15150: loss: 0.32481782138347626, lr: 1e-05
2023-12-21 20:49:42 INFO     	 * (global step 15200: loss: 0.5551717132329941, lr: 1e-05
2023-12-21 20:49:50 INFO     	 * (global step 15250: loss: 0.2714804708957672, lr: 1e-05
2023-12-21 20:49:58 INFO     	 * (global step 15300: loss: 0.3651687353849411, lr: 1e-05
2023-12-21 20:50:06 INFO     	 * (global step 15350: loss: 0.26762886345386505, lr: 1e-05
2023-12-21 20:50:14 INFO     	 * (global step 15400: loss: 0.19903571903705597, lr: 1e-05
2023-12-21 20:50:23 INFO     	 * (global step 15450: loss: 0.409449502825737, lr: 1e-05
2023-12-21 20:50:31 INFO     	 * (global step 15500: loss: 0.44193805754184723, lr: 1e-05
2023-12-21 20:50:39 INFO     	 * (global step 15550: loss: 0.37684108316898346, lr: 1e-05
2023-12-21 20:50:47 INFO     	 * (global step 15600: loss: 0.2884864956140518, lr: 1e-05
2023-12-21 20:50:55 INFO     	 * (global step 15650: loss: 0.34318308532238007, lr: 1e-05
2023-12-21 20:51:03 INFO     	 * (global step 15700: loss: 0.49224869161844254, lr: 1e-05
2023-12-21 20:51:11 INFO     	 * (global step 15750: loss: 0.23329035937786102, lr: 1e-05
2023-12-21 20:51:19 INFO     	 * (global step 15800: loss: 0.35114310681819916, lr: 1e-05
2023-12-21 20:51:28 INFO     	 * (global step 15850: loss: 0.38995125889778137, lr: 1e-05
2023-12-21 20:51:36 INFO     	 * (global step 15900: loss: 0.34747999906539917, lr: 1e-05
2023-12-21 20:51:44 INFO     	 * (global step 15950: loss: 0.28116631507873535, lr: 1e-05
2023-12-21 20:51:52 INFO     	 * (global step 16000: loss: 0.3148564249277115, lr: 1e-05
2023-12-21 20:52:00 INFO     	 * (global step 16050: loss: 0.2737637907266617, lr: 1e-05
2023-12-21 20:52:08 INFO     	 * (global step 16100: loss: 0.23477131128311157, lr: 1e-05
2023-12-21 20:52:16 INFO     	 * (global step 16150: loss: 0.20093659311532974, lr: 1e-05
2023-12-21 20:52:24 INFO     	 * (global step 16200: loss: 0.40090957283973694, lr: 1e-05
2023-12-21 20:52:33 INFO     	 * (global step 16250: loss: 0.22669389098882675, lr: 1e-05
2023-12-21 20:52:41 INFO     	 * (global step 16300: loss: 0.2561779022216797, lr: 1e-05
2023-12-21 20:52:49 INFO     	 * (global step 16350: loss: 0.2612157315015793, lr: 1e-05
2023-12-21 20:52:57 INFO     	 * (global step 16400: loss: 0.34056033194065094, lr: 1e-05
2023-12-21 20:53:05 INFO     	 * (global step 16450: loss: 0.2557571977376938, lr: 1e-05
2023-12-21 20:53:13 INFO     	 * (global step 16500: loss: 0.44833941012620926, lr: 1e-05
2023-12-21 20:53:21 INFO     	 * (global step 16550: loss: 0.24234141409397125, lr: 1e-05
2023-12-21 20:53:29 INFO     	 * (global step 16600: loss: 0.5297496914863586, lr: 1e-05
2023-12-21 20:53:38 INFO     	 * (global step 16650: loss: 0.3591146469116211, lr: 1e-05
2023-12-21 20:53:46 INFO     	 * (global step 16700: loss: 0.27905210852622986, lr: 1e-05
2023-12-21 20:53:54 INFO     	 * (global step 16750: loss: 0.2345651090145111, lr: 1e-05
2023-12-21 20:54:02 INFO     	 * (global step 16800: loss: 0.29437483847141266, lr: 1e-05
2023-12-21 20:54:10 INFO     	 * (global step 16850: loss: 0.1809413619339466, lr: 1e-05
2023-12-21 20:54:18 INFO     	 * (global step 16900: loss: 0.27155404537916183, lr: 1e-05
2023-12-21 20:54:26 INFO     	 * (global step 16950: loss: 0.21123763918876648, lr: 1e-05
2023-12-21 20:54:35 INFO     	 * (global step 17000: loss: 0.3351808935403824, lr: 1e-05
2023-12-21 20:54:43 INFO     	 * (global step 17050: loss: 0.21572095900774002, lr: 1e-05
2023-12-21 20:54:51 INFO     	 * (global step 17100: loss: 0.31683582067489624, lr: 1e-05
2023-12-21 20:54:59 INFO     	 * (global step 17150: loss: 0.276968851685524, lr: 1e-05
2023-12-21 20:55:07 INFO     	 * (global step 17200: loss: 0.8510322868824005, lr: 1e-05
2023-12-21 20:55:15 INFO     	 * (global step 17250: loss: 0.20656805485486984, lr: 1e-05
2023-12-21 20:55:23 INFO     	 * (global step 17300: loss: 0.2797068655490875, lr: 1e-05
2023-12-21 20:55:32 INFO     	 * (global step 17350: loss: 0.38642843067646027, lr: 1e-05
2023-12-21 20:55:40 INFO     	 * (global step 17400: loss: 0.3211006075143814, lr: 1e-05
2023-12-21 20:55:48 INFO     	 * (global step 17450: loss: 0.3047635331749916, lr: 1e-05
2023-12-21 20:55:56 INFO     	 * (global step 17500: loss: 0.3147526979446411, lr: 1e-05
2023-12-21 20:56:04 INFO     	 * (global step 17550: loss: 0.31503862142562866, lr: 1e-05
2023-12-21 20:56:12 INFO     	 * (global step 17600: loss: 0.33917567878961563, lr: 1e-05
2023-12-21 20:56:20 INFO     	 * (global step 17650: loss: 0.2566302865743637, lr: 1e-05
2023-12-21 20:56:28 INFO     	 * (global step 17700: loss: 0.16447432339191437, lr: 1e-05
2023-12-21 20:56:37 INFO     	 * (global step 17750: loss: 0.39244334399700165, lr: 1e-05
2023-12-21 20:56:45 INFO     	 * (global step 17800: loss: 0.2769359350204468, lr: 1e-05
2023-12-21 20:56:53 INFO     	 * (global step 17850: loss: 0.25595444440841675, lr: 1e-05
2023-12-21 20:57:01 INFO     	 * (global step 17900: loss: 0.39260217547416687, lr: 1e-05
2023-12-21 20:57:09 INFO     	 * (global step 17950: loss: 0.313823938369751, lr: 1e-05
2023-12-21 20:57:17 INFO     	 * (global step 18000: loss: 0.3631269484758377, lr: 1e-05
2023-12-21 20:57:25 INFO     	 * (global step 18050: loss: 0.19428668916225433, lr: 1e-05
2023-12-21 20:57:34 INFO     	 * (global step 18100: loss: 0.41984397172927856, lr: 1e-05
2023-12-21 20:57:42 INFO     	 * (global step 18150: loss: 0.3396177887916565, lr: 1e-05
2023-12-21 20:57:50 INFO     	 * (global step 18200: loss: 0.19516010582447052, lr: 1e-05
2023-12-21 20:57:58 INFO     	 * (global step 18250: loss: 0.2859206721186638, lr: 1e-05
2023-12-21 20:58:06 INFO     	 * (global step 18300: loss: 0.23157546669244766, lr: 1e-05
2023-12-21 20:58:14 INFO     	 * (global step 18350: loss: 0.3084067106246948, lr: 1e-05
2023-12-21 20:58:22 INFO     	 * (global step 18400: loss: 0.3005395531654358, lr: 1e-05
2023-12-21 20:58:30 INFO     	 * (global step 18450: loss: 0.44792789220809937, lr: 1e-05
2023-12-21 20:58:39 INFO     	 * (global step 18500: loss: 0.25816479325294495, lr: 1e-05
2023-12-21 20:58:47 INFO     	 * (global step 18550: loss: 0.2091047614812851, lr: 1e-05
2023-12-21 20:58:55 INFO     	 * (global step 18600: loss: 0.2496393546462059, lr: 1e-05
2023-12-21 20:59:03 INFO     	 * (global step 18650: loss: 0.629041001200676, lr: 1e-05
2023-12-21 20:59:11 INFO     	 * (global step 18700: loss: 0.24006786197423935, lr: 1e-05
2023-12-21 20:59:19 INFO     	 * (global step 18750: loss: 0.3379637971520424, lr: 1e-05
2023-12-21 20:59:27 INFO     	 * (global step 18800: loss: 0.20781607180833817, lr: 1e-05
2023-12-21 20:59:36 INFO     	 * (global step 18850: loss: 0.4699541851878166, lr: 1e-05
2023-12-21 20:59:42 INFO     [epoch 13/15] average loss: 0.304, lr: 1e-05
2023-12-21 20:59:42 INFO     saving model related files
2023-12-21 20:59:42 INFO     saving model
2023-12-21 20:59:43 INFO     saving tokenizer
2023-12-21 20:59:43 INFO     saving optimizer
2023-12-21 20:59:44 INFO     remove old optimizer files
2023-12-21 20:59:45 INFO     	 * (global step 18900: loss: 0.3951118290424347, lr: 1e-05
2023-12-21 20:59:53 INFO     	 * (global step 18950: loss: 0.23620415478944778, lr: 1e-05
2023-12-21 21:00:01 INFO     	 * (global step 19000: loss: 0.2964606285095215, lr: 1e-05
2023-12-21 21:00:10 INFO     	 * (global step 19050: loss: 0.3540637046098709, lr: 1e-05
2023-12-21 21:00:18 INFO     	 * (global step 19100: loss: 0.33197783678770065, lr: 1e-05
2023-12-21 21:00:26 INFO     	 * (global step 19150: loss: 0.547315925359726, lr: 1e-05
2023-12-21 21:00:34 INFO     	 * (global step 19200: loss: 0.3063795268535614, lr: 1e-05
2023-12-21 21:00:42 INFO     	 * (global step 19250: loss: 0.3830147534608841, lr: 1e-05
2023-12-21 21:00:50 INFO     	 * (global step 19300: loss: 0.21851486712694168, lr: 1e-05
2023-12-21 21:00:58 INFO     	 * (global step 19350: loss: 0.21980556845664978, lr: 1e-05
2023-12-21 21:01:07 INFO     	 * (global step 19400: loss: 0.30632245540618896, lr: 1e-05
2023-12-21 21:01:15 INFO     	 * (global step 19450: loss: 0.34133435785770416, lr: 1e-05
2023-12-21 21:01:23 INFO     	 * (global step 19500: loss: 0.4081543833017349, lr: 1e-05
2023-12-21 21:01:31 INFO     	 * (global step 19550: loss: 0.35433492064476013, lr: 1e-05
2023-12-21 21:01:39 INFO     	 * (global step 19600: loss: 0.23888713866472244, lr: 1e-05
2023-12-21 21:01:47 INFO     	 * (global step 19650: loss: 0.28126946091651917, lr: 1e-05
2023-12-21 21:01:55 INFO     	 * (global step 19700: loss: 0.337026447057724, lr: 1e-05
2023-12-21 21:02:03 INFO     	 * (global step 19750: loss: 0.4713188707828522, lr: 1e-05
2023-12-21 21:02:12 INFO     	 * (global step 19800: loss: 0.4400195777416229, lr: 1e-05
2023-12-21 21:02:20 INFO     	 * (global step 19850: loss: 0.2833510488271713, lr: 1e-05
2023-12-21 21:02:28 INFO     	 * (global step 19900: loss: 0.2580000162124634, lr: 1e-05
2023-12-21 21:02:36 INFO     	 * (global step 19950: loss: 0.2713301554322243, lr: 1e-05
2023-12-21 21:02:44 INFO     	 * (global step 20000: loss: 0.25766152143478394, lr: 1e-05
2023-12-21 21:02:52 INFO     	 * (global step 20050: loss: 0.29022668302059174, lr: 1e-05
2023-12-21 21:03:00 INFO     	 * (global step 20100: loss: 0.3189409524202347, lr: 1e-05
2023-12-21 21:03:09 INFO     	 * (global step 20150: loss: 0.18325383216142654, lr: 1e-05
2023-12-21 21:03:17 INFO     	 * (global step 20200: loss: 0.35292646288871765, lr: 1e-05
2023-12-21 21:03:25 INFO     	 * (global step 20250: loss: 0.32159651070833206, lr: 1e-05
2023-12-21 21:03:33 INFO     	 * (global step 20300: loss: 0.2604183629155159, lr: 1e-05
2023-12-21 21:03:41 INFO     	 * (global step 20350: loss: 0.2953631579875946, lr: 1e-05
2023-12-21 21:03:49 INFO     	 * (global step 20400: loss: 0.21455537527799606, lr: 1e-05
2023-12-21 21:03:57 INFO     	 * (global step 20450: loss: 0.3574589490890503, lr: 1e-05
2023-12-21 21:04:05 INFO     	 * (global step 20500: loss: 0.37416204810142517, lr: 1e-05
2023-12-21 21:04:14 INFO     	 * (global step 20550: loss: 0.4091319888830185, lr: 1e-05
2023-12-21 21:04:22 INFO     	 * (global step 20600: loss: 0.22978366166353226, lr: 1e-05
2023-12-21 21:04:30 INFO     	 * (global step 20650: loss: 0.24206025898456573, lr: 1e-05
2023-12-21 21:04:38 INFO     	 * (global step 20700: loss: 0.25651127099990845, lr: 1e-05
2023-12-21 21:04:46 INFO     	 * (global step 20750: loss: 0.252369225025177, lr: 1e-05
2023-12-21 21:04:54 INFO     	 * (global step 20800: loss: 0.3272375911474228, lr: 1e-05
2023-12-21 21:05:02 INFO     	 * (global step 20850: loss: 0.23950771242380142, lr: 1e-05
2023-12-21 21:05:10 INFO     	 * (global step 20900: loss: 0.20346897840499878, lr: 1e-05
2023-12-21 21:05:19 INFO     	 * (global step 20950: loss: 0.3016905188560486, lr: 1e-05
2023-12-21 21:05:27 INFO     	 * (global step 21000: loss: 0.3462152034044266, lr: 1e-05
2023-12-21 21:05:35 INFO     	 * (global step 21050: loss: 0.26869793236255646, lr: 1e-05
2023-12-21 21:05:43 INFO     	 * (global step 21100: loss: 0.2408922165632248, lr: 1e-05
2023-12-21 21:05:51 INFO     	 * (global step 21150: loss: 0.361418753862381, lr: 1e-05
2023-12-21 21:05:59 INFO     	 * (global step 21200: loss: 0.19161579757928848, lr: 1e-05
2023-12-21 21:06:07 INFO     	 * (global step 21250: loss: 0.4147112965583801, lr: 1e-05
2023-12-21 21:06:16 INFO     	 * (global step 21300: loss: 0.4668813496828079, lr: 1e-05
2023-12-21 21:06:24 INFO     	 * (global step 21350: loss: 0.315059632062912, lr: 1e-05
2023-12-21 21:06:32 INFO     	 * (global step 21400: loss: 0.3568250760436058, lr: 1e-05
2023-12-21 21:06:40 INFO     	 * (global step 21450: loss: 0.15931299328804016, lr: 1e-05
2023-12-21 21:06:48 INFO     	 * (global step 21500: loss: 0.20743854343891144, lr: 1e-05
2023-12-21 21:06:56 INFO     	 * (global step 21550: loss: 0.29746701568365097, lr: 1e-05
2023-12-21 21:07:04 INFO     	 * (global step 21600: loss: 0.28352949023246765, lr: 1e-05
2023-12-21 21:07:12 INFO     	 * (global step 21650: loss: 0.3909101039171219, lr: 1e-05
2023-12-21 21:07:21 INFO     	 * (global step 21700: loss: 0.3558960109949112, lr: 1e-05
2023-12-21 21:07:29 INFO     	 * (global step 21750: loss: 0.31946365535259247, lr: 1e-05
2023-12-21 21:07:37 INFO     	 * (global step 21800: loss: 0.3905126601457596, lr: 1e-05
2023-12-21 21:07:45 INFO     	 * (global step 21850: loss: 0.2583888992667198, lr: 1e-05
2023-12-21 21:07:53 INFO     	 * (global step 21900: loss: 0.28287434577941895, lr: 1e-05
2023-12-21 21:08:01 INFO     	 * (global step 21950: loss: 0.28481265902519226, lr: 1e-05
2023-12-21 21:08:09 INFO     	 * (global step 22000: loss: 0.26098428666591644, lr: 1e-05
2023-12-21 21:08:18 INFO     	 * (global step 22050: loss: 0.2585350424051285, lr: 1e-05
2023-12-21 21:08:26 INFO     	 * (global step 22100: loss: 0.2200734168291092, lr: 1e-05
2023-12-21 21:08:34 INFO     	 * (global step 22150: loss: 0.2901875823736191, lr: 1e-05
2023-12-21 21:08:42 INFO     	 * (global step 22200: loss: 0.42887015640735626, lr: 1e-05
2023-12-21 21:08:50 INFO     	 * (global step 22250: loss: 0.2769114077091217, lr: 1e-05
2023-12-21 21:08:58 INFO     	 * (global step 22300: loss: 0.2884844243526459, lr: 1e-05
2023-12-21 21:09:06 INFO     	 * (global step 22350: loss: 0.25108925998210907, lr: 1e-05
2023-12-21 21:09:14 INFO     	 * (global step 22400: loss: 0.3135145455598831, lr: 1e-05
2023-12-21 21:09:23 INFO     	 * (global step 22450: loss: 0.37377631664276123, lr: 1e-05
2023-12-21 21:09:31 INFO     	 * (global step 22500: loss: 0.24446267634630203, lr: 1e-05
2023-12-21 21:09:39 INFO     	 * (global step 22550: loss: 0.34741178154945374, lr: 1e-05
2023-12-21 21:09:47 INFO     	 * (global step 22600: loss: 0.24521008878946304, lr: 1e-05
2023-12-21 21:09:55 INFO     	 * (global step 22650: loss: 0.2828790098428726, lr: 1e-05
2023-12-21 21:10:03 INFO     	 * (global step 22700: loss: 0.5642656236886978, lr: 1e-05
2023-12-21 21:10:11 INFO     	 * (global step 22750: loss: 0.16848113387823105, lr: 1e-05
2023-12-21 21:10:20 INFO     	 * (global step 22800: loss: 0.1985718235373497, lr: 1e-05
2023-12-21 21:10:28 INFO     	 * (global step 22850: loss: 0.2868664413690567, lr: 1e-05
2023-12-21 21:10:36 INFO     	 * (global step 22900: loss: 0.33048491179943085, lr: 1e-05
2023-12-21 21:10:44 INFO     	 * (global step 22950: loss: 0.24455353617668152, lr: 1e-05
2023-12-21 21:10:52 INFO     	 * (global step 23000: loss: 0.34412984549999237, lr: 1e-05
2023-12-21 21:11:00 INFO     	 * (global step 23050: loss: 0.33011117577552795, lr: 1e-05
2023-12-21 21:11:08 INFO     	 * (global step 23100: loss: 0.17124247550964355, lr: 1e-05
2023-12-21 21:11:17 INFO     	 * (global step 23150: loss: 0.28133177757263184, lr: 1e-05
2023-12-21 21:11:25 INFO     	 * (global step 23200: loss: 0.2798932045698166, lr: 1e-05
2023-12-21 21:11:33 INFO     	 * (global step 23250: loss: 0.5219472646713257, lr: 1e-05
2023-12-21 21:11:41 INFO     	 * (global step 23300: loss: 0.2680741846561432, lr: 1e-05
2023-12-21 21:11:49 INFO     	 * (global step 23350: loss: 0.30161847174167633, lr: 1e-05
2023-12-21 21:11:57 INFO     	 * (global step 23400: loss: 0.4342384412884712, lr: 1e-05
2023-12-21 21:12:05 INFO     	 * (global step 23450: loss: 0.2461441308259964, lr: 1e-05
2023-12-21 21:12:13 INFO     	 * (global step 23500: loss: 0.2085704505443573, lr: 1e-05
2023-12-21 21:12:22 INFO     	 * (global step 23550: loss: 0.23760437965393066, lr: 1e-05
2023-12-21 21:12:30 INFO     	 * (global step 23600: loss: 0.3133333772420883, lr: 1e-05
2023-12-21 21:12:32 INFO     [epoch 14/15] average loss: 0.301, lr: 1e-05
2023-12-21 21:12:32 INFO     saving model related files
2023-12-21 21:12:32 INFO     saving model
2023-12-21 21:12:33 INFO     saving tokenizer
2023-12-21 21:12:33 INFO     saving optimizer
2023-12-21 21:12:34 INFO     remove old optimizer files
2023-12-21 21:12:34 INFO     complete training: model ckpt was saved at small_combined_trained_ckpt/model_nrudfu
2023-12-21 21:12:34 INFO     ## 2nd RUN: Configuration 4/5: validation/Bleu_4 = 0.057894333626831364
2023-12-21 21:12:34 INFO     initialize model trainer
2023-12-21 21:12:34 INFO     load config from existing checkpoint at small_combined_trained_ckpt/model_mntyya
2023-12-21 21:12:34 INFO     hyperparameters
2023-12-21 21:12:34 INFO     	 * dataset_path: StellarMilk/squad_newsqa
2023-12-21 21:12:34 INFO     	 * dataset_name: default
2023-12-21 21:12:34 INFO     	 * input_types: ['paragraph']
2023-12-21 21:12:34 INFO     	 * output_types: ['questions_answers']
2023-12-21 21:12:34 INFO     	 * prefix_types: ['qag']
2023-12-21 21:12:34 INFO     	 * model: t5-small
2023-12-21 21:12:34 INFO     	 * max_length: 512
2023-12-21 21:12:34 INFO     	 * max_length_output: 512
2023-12-21 21:12:34 INFO     	 * epoch: 15
2023-12-21 21:12:34 INFO     	 * batch: 2
2023-12-21 21:12:34 INFO     	 * lr: 5e-05
2023-12-21 21:12:34 INFO     	 * fp16: False
2023-12-21 21:12:34 INFO     	 * random_seed: 1
2023-12-21 21:12:34 INFO     	 * gradient_accumulation_steps: 4
2023-12-21 21:12:34 INFO     	 * label_smoothing: 0.15
2023-12-21 21:12:34 INFO     load checkpoint from small_combined_trained_ckpt/model_mntyya/epoch_10
2023-12-21 21:12:35 INFO     use spaCy answer extraction model: positionrank
2023-12-21 21:12:35 INFO     Model `small_combined_trained_ckpt/model_mntyya/epoch_10`
2023-12-21 21:12:35 INFO     	 * Num of GPU in use: 1
2023-12-21 21:12:35 INFO     	 * Prefix: True
2023-12-21 21:12:35 INFO     	 * Language: en (ignore at the training phase)
2023-12-21 21:12:35 INFO     load optimizer from small_combined_trained_ckpt/model_mntyya/optimizers/optimizer.10.pt
2023-12-21 21:12:35 INFO     optimizer is loading on cuda
2023-12-21 21:12:48 INFO     dataset preprocessing
2023-12-21 21:12:50 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featureStellarMilk/squad_newsqa/t5-small.512.512.paragraph.questions_answers.train.qag.pkl
2023-12-21 21:12:52 INFO     start model training
2023-12-21 21:13:08 INFO     	 * (global step 50: loss: 2.721604585647583, lr: 5e-05
2023-12-21 21:13:24 INFO     	 * (global step 100: loss: 2.481350600719452, lr: 5e-05
2023-12-21 21:13:40 INFO     	 * (global step 150: loss: 2.4316834211349487, lr: 5e-05
2023-12-21 21:13:56 INFO     	 * (global step 200: loss: 2.5168047547340393, lr: 5e-05
2023-12-21 21:14:12 INFO     	 * (global step 250: loss: 2.4594168066978455, lr: 5e-05
2023-12-21 21:14:28 INFO     	 * (global step 300: loss: 2.5513628125190735, lr: 5e-05
2023-12-21 21:14:44 INFO     	 * (global step 350: loss: 2.5770131945610046, lr: 5e-05
2023-12-21 21:15:01 INFO     	 * (global step 400: loss: 2.4193949699401855, lr: 5e-05
2023-12-21 21:15:17 INFO     	 * (global step 450: loss: 2.398806869983673, lr: 5e-05
2023-12-21 21:15:33 INFO     	 * (global step 500: loss: 2.349174380302429, lr: 5e-05
2023-12-21 21:15:49 INFO     	 * (global step 550: loss: 2.3761879205703735, lr: 5e-05
2023-12-21 21:16:05 INFO     	 * (global step 600: loss: 2.4551007747650146, lr: 5e-05
2023-12-21 21:16:21 INFO     	 * (global step 650: loss: 2.3799495100975037, lr: 5e-05
2023-12-21 21:16:38 INFO     	 * (global step 700: loss: 2.384131610393524, lr: 5e-05
2023-12-21 21:16:54 INFO     	 * (global step 750: loss: 2.374323070049286, lr: 5e-05
2023-12-21 21:17:10 INFO     	 * (global step 800: loss: 2.424184501171112, lr: 5e-05
2023-12-21 21:17:26 INFO     	 * (global step 850: loss: 2.3326225876808167, lr: 5e-05
2023-12-21 21:17:42 INFO     	 * (global step 900: loss: 2.332994222640991, lr: 5e-05
2023-12-21 21:17:59 INFO     	 * (global step 950: loss: 2.3763442635536194, lr: 5e-05
2023-12-21 21:18:15 INFO     	 * (global step 1000: loss: 2.437780976295471, lr: 5e-05
2023-12-21 21:18:31 INFO     	 * (global step 1050: loss: 2.3870627880096436, lr: 5e-05
2023-12-21 21:18:47 INFO     	 * (global step 1100: loss: 2.4886212944984436, lr: 5e-05
2023-12-21 21:19:03 INFO     	 * (global step 1150: loss: 2.456243395805359, lr: 5e-05
2023-12-21 21:19:19 INFO     	 * (global step 1200: loss: 2.3820531964302063, lr: 5e-05
2023-12-21 21:19:36 INFO     	 * (global step 1250: loss: 2.391089141368866, lr: 5e-05
2023-12-21 21:19:52 INFO     	 * (global step 1300: loss: 2.3139257431030273, lr: 5e-05
2023-12-21 21:20:08 INFO     	 * (global step 1350: loss: 2.3406640887260437, lr: 5e-05
2023-12-21 21:20:24 INFO     	 * (global step 1400: loss: 2.3426249027252197, lr: 5e-05
2023-12-21 21:20:40 INFO     	 * (global step 1450: loss: 2.336884021759033, lr: 5e-05
2023-12-21 21:20:56 INFO     	 * (global step 1500: loss: 2.443341374397278, lr: 5e-05
2023-12-21 21:21:13 INFO     	 * (global step 1550: loss: 2.4119144678115845, lr: 5e-05
2023-12-21 21:21:29 INFO     	 * (global step 1600: loss: 2.3252679109573364, lr: 5e-05
2023-12-21 21:21:45 INFO     	 * (global step 1650: loss: 2.295409858226776, lr: 5e-05
2023-12-21 21:22:01 INFO     	 * (global step 1700: loss: 2.3200871348381042, lr: 5e-05
2023-12-21 21:22:17 INFO     	 * (global step 1750: loss: 2.3531784415245056, lr: 5e-05
2023-12-21 21:22:34 INFO     	 * (global step 1800: loss: 2.3002742528915405, lr: 5e-05
2023-12-21 21:22:50 INFO     	 * (global step 1850: loss: 2.3767674565315247, lr: 5e-05
2023-12-21 21:23:06 INFO     	 * (global step 1900: loss: 2.2988321781158447, lr: 5e-05
2023-12-21 21:23:22 INFO     	 * (global step 1950: loss: 2.2576582431793213, lr: 5e-05
2023-12-21 21:23:38 INFO     	 * (global step 2000: loss: 2.423351466655731, lr: 5e-05
2023-12-21 21:23:54 INFO     	 * (global step 2050: loss: 2.2851721048355103, lr: 5e-05
2023-12-21 21:24:10 INFO     	 * (global step 2100: loss: 2.339824676513672, lr: 5e-05
2023-12-21 21:24:27 INFO     	 * (global step 2150: loss: 2.3655250668525696, lr: 5e-05
2023-12-21 21:24:43 INFO     	 * (global step 2200: loss: 2.4000503420829773, lr: 5e-05
2023-12-21 21:24:59 INFO     	 * (global step 2250: loss: 2.313357174396515, lr: 5e-05
2023-12-21 21:25:15 INFO     	 * (global step 2300: loss: 2.3423717617988586, lr: 5e-05
2023-12-21 21:25:31 INFO     	 * (global step 2350: loss: 2.3809534311294556, lr: 5e-05
2023-12-21 21:25:35 INFO     [epoch 10/15] average loss: 2.407, lr: 5e-05
2023-12-21 21:25:35 INFO     saving model related files
2023-12-21 21:25:35 INFO     saving model
2023-12-21 21:25:35 INFO     saving tokenizer
2023-12-21 21:25:35 INFO     saving optimizer
2023-12-21 21:25:36 INFO     remove old optimizer files
2023-12-21 21:25:49 INFO     	 * (global step 2400: loss: 2.321241319179535, lr: 5e-05
2023-12-21 21:26:05 INFO     	 * (global step 2450: loss: 2.3729341626167297, lr: 5e-05
2023-12-21 21:26:21 INFO     	 * (global step 2500: loss: 2.318648397922516, lr: 5e-05
2023-12-21 21:26:37 INFO     	 * (global step 2550: loss: 2.3132311701774597, lr: 5e-05
2023-12-21 21:26:53 INFO     	 * (global step 2600: loss: 2.338656187057495, lr: 5e-05
2023-12-21 21:27:09 INFO     	 * (global step 2650: loss: 2.308809816837311, lr: 5e-05
2023-12-21 21:27:26 INFO     	 * (global step 2700: loss: 2.2984304428100586, lr: 5e-05
2023-12-21 21:27:42 INFO     	 * (global step 2750: loss: 2.3091304302215576, lr: 5e-05
2023-12-21 21:27:58 INFO     	 * (global step 2800: loss: 2.389340043067932, lr: 5e-05
2023-12-21 21:28:14 INFO     	 * (global step 2850: loss: 2.379833459854126, lr: 5e-05
2023-12-21 21:28:30 INFO     	 * (global step 2900: loss: 2.3339340686798096, lr: 5e-05
2023-12-21 21:28:46 INFO     	 * (global step 2950: loss: 2.304536461830139, lr: 5e-05
2023-12-21 21:29:02 INFO     	 * (global step 3000: loss: 2.2387084364891052, lr: 5e-05
2023-12-21 21:29:19 INFO     	 * (global step 3050: loss: 2.2778276801109314, lr: 5e-05
2023-12-21 21:29:35 INFO     	 * (global step 3100: loss: 2.293625295162201, lr: 5e-05
2023-12-21 21:29:51 INFO     	 * (global step 3150: loss: 2.293190062046051, lr: 5e-05
2023-12-21 21:30:07 INFO     	 * (global step 3200: loss: 2.3128395676612854, lr: 5e-05
2023-12-21 21:30:23 INFO     	 * (global step 3250: loss: 2.3075523376464844, lr: 5e-05
2023-12-21 21:30:39 INFO     	 * (global step 3300: loss: 2.2703136801719666, lr: 5e-05
2023-12-21 21:30:55 INFO     	 * (global step 3350: loss: 2.2545202374458313, lr: 5e-05
2023-12-21 21:31:12 INFO     	 * (global step 3400: loss: 2.3366777896881104, lr: 5e-05
2023-12-21 21:31:28 INFO     	 * (global step 3450: loss: 2.3196215629577637, lr: 5e-05
2023-12-21 21:31:44 INFO     	 * (global step 3500: loss: 2.3601200580596924, lr: 5e-05
2023-12-21 21:32:00 INFO     	 * (global step 3550: loss: 2.267424464225769, lr: 5e-05
2023-12-21 21:32:16 INFO     	 * (global step 3600: loss: 2.3463104367256165, lr: 5e-05
2023-12-21 21:32:32 INFO     	 * (global step 3650: loss: 2.2892016172409058, lr: 5e-05
2023-12-21 21:32:48 INFO     	 * (global step 3700: loss: 2.2680200934410095, lr: 5e-05
2023-12-21 21:33:05 INFO     	 * (global step 3750: loss: 2.3546127676963806, lr: 5e-05
2023-12-21 21:33:21 INFO     	 * (global step 3800: loss: 2.2932400703430176, lr: 5e-05
2023-12-21 21:33:37 INFO     	 * (global step 3850: loss: 2.396428942680359, lr: 5e-05
2023-12-21 21:33:53 INFO     	 * (global step 3900: loss: 2.3757463097572327, lr: 5e-05
2023-12-21 21:34:09 INFO     	 * (global step 3950: loss: 2.350040853023529, lr: 5e-05
2023-12-21 21:34:25 INFO     	 * (global step 4000: loss: 2.274588167667389, lr: 5e-05
2023-12-21 21:34:41 INFO     	 * (global step 4050: loss: 2.3443098664283752, lr: 5e-05
2023-12-21 21:34:58 INFO     	 * (global step 4100: loss: 2.260203182697296, lr: 5e-05
2023-12-21 21:35:14 INFO     	 * (global step 4150: loss: 2.3125763535499573, lr: 5e-05
2023-12-21 21:35:30 INFO     	 * (global step 4200: loss: 2.3411501049995422, lr: 5e-05
2023-12-21 21:35:46 INFO     	 * (global step 4250: loss: 2.311374068260193, lr: 5e-05
2023-12-21 21:36:02 INFO     	 * (global step 4300: loss: 2.3756527304649353, lr: 5e-05
2023-12-21 21:36:18 INFO     	 * (global step 4350: loss: 2.2559755444526672, lr: 5e-05
2023-12-21 21:36:34 INFO     	 * (global step 4400: loss: 2.323924720287323, lr: 5e-05
2023-12-21 21:36:51 INFO     	 * (global step 4450: loss: 2.3720725178718567, lr: 5e-05
2023-12-21 21:37:07 INFO     	 * (global step 4500: loss: 2.2614076137542725, lr: 5e-05
2023-12-21 21:37:23 INFO     	 * (global step 4550: loss: 2.3437968492507935, lr: 5e-05
2023-12-21 21:37:39 INFO     	 * (global step 4600: loss: 2.324884355068207, lr: 5e-05
2023-12-21 21:37:55 INFO     	 * (global step 4650: loss: 2.326090693473816, lr: 5e-05
2023-12-21 21:38:11 INFO     	 * (global step 4700: loss: 2.2794366478919983, lr: 5e-05
2023-12-21 21:38:19 INFO     [epoch 11/15] average loss: 2.33, lr: 5e-05
2023-12-21 21:38:19 INFO     saving model related files
2023-12-21 21:38:19 INFO     saving model
2023-12-21 21:38:19 INFO     saving tokenizer
2023-12-21 21:38:19 INFO     saving optimizer
2023-12-21 21:38:20 INFO     remove old optimizer files
2023-12-21 21:38:29 INFO     	 * (global step 4750: loss: 2.2617727518081665, lr: 5e-05
2023-12-21 21:38:45 INFO     	 * (global step 4800: loss: 2.290561616420746, lr: 5e-05
2023-12-21 21:39:01 INFO     	 * (global step 4850: loss: 2.309476852416992, lr: 5e-05
2023-12-21 21:39:18 INFO     	 * (global step 4900: loss: 2.329720973968506, lr: 5e-05
2023-12-21 21:39:34 INFO     	 * (global step 4950: loss: 2.3167710304260254, lr: 5e-05
2023-12-21 21:39:50 INFO     	 * (global step 5000: loss: 2.38386332988739, lr: 5e-05
2023-12-21 21:40:06 INFO     	 * (global step 5050: loss: 2.3152981996536255, lr: 5e-05
2023-12-21 21:40:22 INFO     	 * (global step 5100: loss: 2.347793459892273, lr: 5e-05
2023-12-21 21:40:38 INFO     	 * (global step 5150: loss: 2.27359539270401, lr: 5e-05
2023-12-21 21:40:55 INFO     	 * (global step 5200: loss: 2.2925283312797546, lr: 5e-05
2023-12-21 21:41:11 INFO     	 * (global step 5250: loss: 2.376494824886322, lr: 5e-05
2023-12-21 21:41:27 INFO     	 * (global step 5300: loss: 2.292792558670044, lr: 5e-05
2023-12-21 21:41:43 INFO     	 * (global step 5350: loss: 2.3235809803009033, lr: 5e-05
2023-12-21 21:41:59 INFO     	 * (global step 5400: loss: 2.3194068670272827, lr: 5e-05
2023-12-21 21:42:15 INFO     	 * (global step 5450: loss: 2.305673837661743, lr: 5e-05
2023-12-21 21:42:31 INFO     	 * (global step 5500: loss: 2.2533681392669678, lr: 5e-05
2023-12-21 21:42:48 INFO     	 * (global step 5550: loss: 2.406464457511902, lr: 5e-05
2023-12-21 21:43:04 INFO     	 * (global step 5600: loss: 2.299550235271454, lr: 5e-05
2023-12-21 21:43:20 INFO     	 * (global step 5650: loss: 2.328514814376831, lr: 5e-05
2023-12-21 21:43:36 INFO     	 * (global step 5700: loss: 2.3847931027412415, lr: 5e-05
2023-12-21 21:43:52 INFO     	 * (global step 5750: loss: 2.2965235710144043, lr: 5e-05
2023-12-21 21:44:08 INFO     	 * (global step 5800: loss: 2.288240432739258, lr: 5e-05
2023-12-21 21:44:25 INFO     	 * (global step 5850: loss: 2.2762245535850525, lr: 5e-05
2023-12-21 21:44:41 INFO     	 * (global step 5900: loss: 2.303987979888916, lr: 5e-05
2023-12-21 21:44:57 INFO     	 * (global step 5950: loss: 2.342306077480316, lr: 5e-05
2023-12-21 21:45:13 INFO     	 * (global step 6000: loss: 2.3350472450256348, lr: 5e-05
2023-12-21 21:45:29 INFO     	 * (global step 6050: loss: 2.2778024077415466, lr: 5e-05
2023-12-21 21:45:45 INFO     	 * (global step 6100: loss: 2.297673761844635, lr: 5e-05
2023-12-21 21:46:02 INFO     	 * (global step 6150: loss: 2.2528034448623657, lr: 5e-05
2023-12-21 21:46:18 INFO     	 * (global step 6200: loss: 2.346261441707611, lr: 5e-05
2023-12-21 21:46:34 INFO     	 * (global step 6250: loss: 2.3488240242004395, lr: 5e-05
2023-12-21 21:46:50 INFO     	 * (global step 6300: loss: 2.3072672486305237, lr: 5e-05
2023-12-21 21:47:06 INFO     	 * (global step 6350: loss: 2.3492379784584045, lr: 5e-05
2023-12-21 21:47:22 INFO     	 * (global step 6400: loss: 2.353469431400299, lr: 5e-05
2023-12-21 21:47:39 INFO     	 * (global step 6450: loss: 2.303686022758484, lr: 5e-05
2023-12-21 21:47:55 INFO     	 * (global step 6500: loss: 2.286515235900879, lr: 5e-05
2023-12-21 21:48:11 INFO     	 * (global step 6550: loss: 2.2461220026016235, lr: 5e-05
2023-12-21 21:48:27 INFO     	 * (global step 6600: loss: 2.2465165853500366, lr: 5e-05
2023-12-21 21:48:43 INFO     	 * (global step 6650: loss: 2.3828938007354736, lr: 5e-05
2023-12-21 21:48:59 INFO     	 * (global step 6700: loss: 2.38239061832428, lr: 5e-05
2023-12-21 21:49:16 INFO     	 * (global step 6750: loss: 2.300640881061554, lr: 5e-05
2023-12-21 21:49:32 INFO     	 * (global step 6800: loss: 2.3575181365013123, lr: 5e-05
2023-12-21 21:49:48 INFO     	 * (global step 6850: loss: 2.3453935384750366, lr: 5e-05
2023-12-21 21:50:04 INFO     	 * (global step 6900: loss: 2.315030872821808, lr: 5e-05
2023-12-21 21:50:20 INFO     	 * (global step 6950: loss: 2.3036235570907593, lr: 5e-05
2023-12-21 21:50:36 INFO     	 * (global step 7000: loss: 2.2649149894714355, lr: 5e-05
2023-12-21 21:50:53 INFO     	 * (global step 7050: loss: 2.3699756860733032, lr: 5e-05
2023-12-21 21:51:03 INFO     [epoch 12/15] average loss: 2.314, lr: 5e-05
2023-12-21 21:51:03 INFO     saving model related files
2023-12-21 21:51:03 INFO     saving model
2023-12-21 21:51:04 INFO     saving tokenizer
2023-12-21 21:51:04 INFO     saving optimizer
2023-12-21 21:51:05 INFO     remove old optimizer files
2023-12-21 21:51:10 INFO     	 * (global step 7100: loss: 2.2663873434066772, lr: 5e-05
2023-12-21 21:51:27 INFO     	 * (global step 7150: loss: 2.244051516056061, lr: 5e-05
2023-12-21 21:51:43 INFO     	 * (global step 7200: loss: 2.321808338165283, lr: 5e-05
2023-12-21 21:51:59 INFO     	 * (global step 7250: loss: 2.2287829518318176, lr: 5e-05
2023-12-21 21:52:15 INFO     	 * (global step 7300: loss: 2.27484130859375, lr: 5e-05
2023-12-21 21:52:31 INFO     	 * (global step 7350: loss: 2.308659851551056, lr: 5e-05
2023-12-21 21:52:47 INFO     	 * (global step 7400: loss: 2.2642211318016052, lr: 5e-05
2023-12-21 21:53:04 INFO     	 * (global step 7450: loss: 2.2628350853919983, lr: 5e-05
2023-12-21 21:53:20 INFO     	 * (global step 7500: loss: 2.2683854699134827, lr: 5e-05
2023-12-21 21:53:36 INFO     	 * (global step 7550: loss: 2.32144695520401, lr: 5e-05
2023-12-21 21:53:52 INFO     	 * (global step 7600: loss: 2.284098267555237, lr: 5e-05
2023-12-21 21:54:08 INFO     	 * (global step 7650: loss: 2.328302562236786, lr: 5e-05
2023-12-21 21:54:24 INFO     	 * (global step 7700: loss: 2.22838693857193, lr: 5e-05
2023-12-21 21:54:41 INFO     	 * (global step 7750: loss: 2.355374038219452, lr: 5e-05
2023-12-21 21:54:57 INFO     	 * (global step 7800: loss: 2.3096725940704346, lr: 5e-05
2023-12-21 21:55:13 INFO     	 * (global step 7850: loss: 2.303014576435089, lr: 5e-05
2023-12-21 21:55:29 INFO     	 * (global step 7900: loss: 2.314377725124359, lr: 5e-05
2023-12-21 21:55:45 INFO     	 * (global step 7950: loss: 2.2586730122566223, lr: 5e-05
2023-12-21 21:56:01 INFO     	 * (global step 8000: loss: 2.321480095386505, lr: 5e-05
2023-12-21 21:56:18 INFO     	 * (global step 8050: loss: 2.3702405095100403, lr: 5e-05
2023-12-21 21:56:34 INFO     	 * (global step 8100: loss: 2.3023264408111572, lr: 5e-05
2023-12-21 21:56:50 INFO     	 * (global step 8150: loss: 2.320148468017578, lr: 5e-05
2023-12-21 21:57:06 INFO     	 * (global step 8200: loss: 2.260461390018463, lr: 5e-05
2023-12-21 21:57:22 INFO     	 * (global step 8250: loss: 2.3060632944107056, lr: 5e-05
2023-12-21 21:57:38 INFO     	 * (global step 8300: loss: 2.319254755973816, lr: 5e-05
2023-12-21 21:57:55 INFO     	 * (global step 8350: loss: 2.244719922542572, lr: 5e-05
2023-12-21 21:58:11 INFO     	 * (global step 8400: loss: 2.3275137543678284, lr: 5e-05
2023-12-21 21:58:27 INFO     	 * (global step 8450: loss: 2.262376844882965, lr: 5e-05
2023-12-21 21:58:43 INFO     	 * (global step 8500: loss: 2.277645528316498, lr: 5e-05
2023-12-21 21:58:59 INFO     	 * (global step 8550: loss: 2.2484973669052124, lr: 5e-05
2023-12-21 21:59:15 INFO     	 * (global step 8600: loss: 2.260773241519928, lr: 5e-05
2023-12-21 21:59:32 INFO     	 * (global step 8650: loss: 2.258524239063263, lr: 5e-05
2023-12-21 21:59:48 INFO     	 * (global step 8700: loss: 2.278208374977112, lr: 5e-05
2023-12-21 22:00:04 INFO     	 * (global step 8750: loss: 2.2963988184928894, lr: 5e-05
2023-12-21 22:00:20 INFO     	 * (global step 8800: loss: 2.2463207840919495, lr: 5e-05
2023-12-21 22:00:36 INFO     	 * (global step 8850: loss: 2.2677680253982544, lr: 5e-05
2023-12-21 22:00:52 INFO     	 * (global step 8900: loss: 2.4286123514175415, lr: 5e-05
2023-12-21 22:01:09 INFO     	 * (global step 8950: loss: 2.266850173473358, lr: 5e-05
2023-12-21 22:01:25 INFO     	 * (global step 9000: loss: 2.3074910044670105, lr: 5e-05
2023-12-21 22:01:41 INFO     	 * (global step 9050: loss: 2.293973445892334, lr: 5e-05
2023-12-21 22:01:57 INFO     	 * (global step 9100: loss: 2.2628212571144104, lr: 5e-05
2023-12-21 22:02:13 INFO     	 * (global step 9150: loss: 2.335495412349701, lr: 5e-05
2023-12-21 22:02:29 INFO     	 * (global step 9200: loss: 2.281464457511902, lr: 5e-05
2023-12-21 22:02:46 INFO     	 * (global step 9250: loss: 2.2742477655410767, lr: 5e-05
2023-12-21 22:03:02 INFO     	 * (global step 9300: loss: 2.2656718492507935, lr: 5e-05
2023-12-21 22:03:18 INFO     	 * (global step 9350: loss: 2.294195234775543, lr: 5e-05
2023-12-21 22:03:34 INFO     	 * (global step 9400: loss: 2.215542256832123, lr: 5e-05
2023-12-21 22:03:48 INFO     [epoch 13/15] average loss: 2.303, lr: 5e-05
2023-12-21 22:03:48 INFO     saving model related files
2023-12-21 22:03:48 INFO     saving model
2023-12-21 22:03:49 INFO     saving tokenizer
2023-12-21 22:03:49 INFO     saving optimizer
2023-12-21 22:03:50 INFO     remove old optimizer files
2023-12-21 22:03:52 INFO     	 * (global step 9450: loss: 2.241226315498352, lr: 5e-05
2023-12-21 22:04:08 INFO     	 * (global step 9500: loss: 2.3265450596809387, lr: 5e-05
2023-12-21 22:04:24 INFO     	 * (global step 9550: loss: 2.3089285492897034, lr: 5e-05
2023-12-21 22:04:40 INFO     	 * (global step 9600: loss: 2.281845450401306, lr: 5e-05
2023-12-21 22:04:57 INFO     	 * (global step 9650: loss: 2.3339669704437256, lr: 5e-05
2023-12-21 22:05:13 INFO     	 * (global step 9700: loss: 2.257552683353424, lr: 5e-05
2023-12-21 22:05:29 INFO     	 * (global step 9750: loss: 2.2630706429481506, lr: 5e-05
2023-12-21 22:05:45 INFO     	 * (global step 9800: loss: 2.3228893280029297, lr: 5e-05
2023-12-21 22:06:01 INFO     	 * (global step 9850: loss: 2.2942569851875305, lr: 5e-05
2023-12-21 22:06:17 INFO     	 * (global step 9900: loss: 2.208533763885498, lr: 5e-05
2023-12-21 22:06:34 INFO     	 * (global step 9950: loss: 2.357332944869995, lr: 5e-05
2023-12-21 22:06:50 INFO     	 * (global step 10000: loss: 2.3227733373641968, lr: 5e-05
2023-12-21 22:07:06 INFO     	 * (global step 10050: loss: 2.3044018149375916, lr: 5e-05
2023-12-21 22:07:22 INFO     	 * (global step 10100: loss: 2.2979193925857544, lr: 5e-05
2023-12-21 22:07:38 INFO     	 * (global step 10150: loss: 2.2857689261436462, lr: 5e-05
2023-12-21 22:07:54 INFO     	 * (global step 10200: loss: 2.294910490512848, lr: 5e-05
2023-12-21 22:08:11 INFO     	 * (global step 10250: loss: 2.2737651467323303, lr: 5e-05
2023-12-21 22:08:27 INFO     	 * (global step 10300: loss: 2.248199760913849, lr: 5e-05
2023-12-21 22:08:43 INFO     	 * (global step 10350: loss: 2.251876473426819, lr: 5e-05
2023-12-21 22:08:59 INFO     	 * (global step 10400: loss: 2.2570783495903015, lr: 5e-05
2023-12-21 22:09:15 INFO     	 * (global step 10450: loss: 2.302933990955353, lr: 5e-05
2023-12-21 22:09:31 INFO     	 * (global step 10500: loss: 2.350143849849701, lr: 5e-05
2023-12-21 22:09:48 INFO     	 * (global step 10550: loss: 2.231333553791046, lr: 5e-05
2023-12-21 22:10:04 INFO     	 * (global step 10600: loss: 2.275244355201721, lr: 5e-05
2023-12-21 22:10:20 INFO     	 * (global step 10650: loss: 2.3203471899032593, lr: 5e-05
2023-12-21 22:10:36 INFO     	 * (global step 10700: loss: 2.27643883228302, lr: 5e-05
2023-12-21 22:10:52 INFO     	 * (global step 10750: loss: 2.3960695266723633, lr: 5e-05
2023-12-21 22:11:08 INFO     	 * (global step 10800: loss: 2.294080972671509, lr: 5e-05
2023-12-21 22:11:25 INFO     	 * (global step 10850: loss: 2.276611626148224, lr: 5e-05
2023-12-21 22:11:41 INFO     	 * (global step 10900: loss: 2.230944573879242, lr: 5e-05
2023-12-21 22:11:57 INFO     	 * (global step 10950: loss: 2.273975670337677, lr: 5e-05
2023-12-21 22:12:13 INFO     	 * (global step 11000: loss: 2.3561302423477173, lr: 5e-05
2023-12-21 22:12:29 INFO     	 * (global step 11050: loss: 2.252686858177185, lr: 5e-05
2023-12-21 22:12:45 INFO     	 * (global step 11100: loss: 2.303104519844055, lr: 5e-05
2023-12-21 22:13:01 INFO     	 * (global step 11150: loss: 2.265570282936096, lr: 5e-05
2023-12-21 22:13:18 INFO     	 * (global step 11200: loss: 2.346904158592224, lr: 5e-05
2023-12-21 22:13:34 INFO     	 * (global step 11250: loss: 2.304901123046875, lr: 5e-05
2023-12-21 22:13:50 INFO     	 * (global step 11300: loss: 2.233261466026306, lr: 5e-05
2023-12-21 22:14:06 INFO     	 * (global step 11350: loss: 2.2537370324134827, lr: 5e-05
2023-12-21 22:14:22 INFO     	 * (global step 11400: loss: 2.268260419368744, lr: 5e-05
2023-12-21 22:14:38 INFO     	 * (global step 11450: loss: 2.281842827796936, lr: 5e-05
2023-12-21 22:14:55 INFO     	 * (global step 11500: loss: 2.314609408378601, lr: 5e-05
2023-12-21 22:15:11 INFO     	 * (global step 11550: loss: 2.235931158065796, lr: 5e-05
2023-12-21 22:15:27 INFO     	 * (global step 11600: loss: 2.2683823704719543, lr: 5e-05
2023-12-21 22:15:43 INFO     	 * (global step 11650: loss: 2.2660117745399475, lr: 5e-05
2023-12-21 22:15:59 INFO     	 * (global step 11700: loss: 2.310851275920868, lr: 5e-05
2023-12-21 22:16:15 INFO     	 * (global step 11750: loss: 2.28811776638031, lr: 5e-05
2023-12-21 22:16:32 INFO     	 * (global step 11800: loss: 2.2577450275421143, lr: 5e-05
2023-12-21 22:16:33 INFO     [epoch 14/15] average loss: 2.295, lr: 5e-05
2023-12-21 22:16:33 INFO     saving model related files
2023-12-21 22:16:33 INFO     saving model
2023-12-21 22:16:34 INFO     saving tokenizer
2023-12-21 22:16:34 INFO     saving optimizer
2023-12-21 22:16:35 INFO     remove old optimizer files
2023-12-21 22:16:35 INFO     complete training: model ckpt was saved at small_combined_trained_ckpt/model_mntyya
2023-12-21 22:16:35 INFO     ## 2nd RUN (EVAL): Configuration 0/5 ##
2023-12-21 22:16:43 INFO     use spaCy answer extraction model: positionrank
2023-12-21 22:16:44 INFO     Model `small_combined_trained_ckpt/model_nxaqhy/epoch_1`
2023-12-21 22:16:44 INFO     	 * Num of GPU in use: 1
2023-12-21 22:16:44 INFO     	 * Prefix: True
2023-12-21 22:16:44 INFO     	 * Language: en (ignore at the training phase)
2023-12-21 22:16:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-21 22:45:22 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-21 23:10:34 WARNING  prediction not found at the evaluation
2023-12-21 23:10:34 WARNING  prediction not found at the evaluation
2023-12-21 23:10:34 INFO     	Bleu_1: 0.0
2023-12-21 23:10:34 INFO     	Bleu_2: 0.0
2023-12-21 23:10:34 INFO     	Bleu_3: 0.0
2023-12-21 23:10:34 INFO     	Bleu_4: 0.0
2023-12-21 23:10:35 WARNING  prediction not found at the evaluation
2023-12-21 23:10:35 WARNING  prediction not found at the evaluation
2023-12-21 23:10:35 INFO     	Bleu_1: 0.0
2023-12-21 23:10:35 INFO     	Bleu_2: 0.0
2023-12-21 23:10:35 INFO     	Bleu_3: 0.0
2023-12-21 23:10:35 INFO     	Bleu_4: 0.0
2023-12-21 23:11:07 INFO     use spaCy answer extraction model: positionrank
2023-12-21 23:11:07 INFO     Model `small_combined_trained_ckpt/model_nxaqhy/epoch_11`
2023-12-21 23:11:07 INFO     	 * Num of GPU in use: 1
2023-12-21 23:11:07 INFO     	 * Prefix: True
2023-12-21 23:11:07 INFO     	 * Language: en (ignore at the training phase)
2023-12-21 23:11:08 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-21 23:24:58 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-21 23:37:31 INFO     	Bleu_1: 0.26233053734216133
2023-12-21 23:37:31 INFO     	Bleu_2: 0.15026457897857692
2023-12-21 23:37:31 INFO     	Bleu_3: 0.08822282867312273
2023-12-21 23:37:31 INFO     	Bleu_4: 0.05841520392840926
2023-12-21 23:37:32 INFO     	Bleu_1: 0.23930288306633424
2023-12-21 23:37:32 INFO     	Bleu_2: 0.13480891632038644
2023-12-21 23:37:32 INFO     	Bleu_3: 0.07744145144289917
2023-12-21 23:37:32 INFO     	Bleu_4: 0.05066342458076253
2023-12-21 23:37:39 INFO     use spaCy answer extraction model: positionrank
2023-12-21 23:37:39 INFO     Model `small_combined_trained_ckpt/model_nxaqhy/epoch_12`
2023-12-21 23:37:39 INFO     	 * Num of GPU in use: 1
2023-12-21 23:37:39 INFO     	 * Prefix: True
2023-12-21 23:37:39 INFO     	 * Language: en (ignore at the training phase)
2023-12-21 23:37:41 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-21 23:51:30 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 00:03:50 INFO     	Bleu_1: 0.2467762259271133
2023-12-22 00:03:50 INFO     	Bleu_2: 0.14158429281733886
2023-12-22 00:03:50 INFO     	Bleu_3: 0.0827057139109147
2023-12-22 00:03:50 INFO     	Bleu_4: 0.05447110745799321
2023-12-22 00:03:52 INFO     	Bleu_1: 0.2276929692539232
2023-12-22 00:03:52 INFO     	Bleu_2: 0.12835021214021203
2023-12-22 00:03:52 INFO     	Bleu_3: 0.07347026678457413
2023-12-22 00:03:52 INFO     	Bleu_4: 0.04796200539883951
2023-12-22 00:03:58 INFO     use spaCy answer extraction model: positionrank
2023-12-22 00:03:58 INFO     Model `small_combined_trained_ckpt/model_nxaqhy/epoch_13`
2023-12-22 00:03:58 INFO     	 * Num of GPU in use: 1
2023-12-22 00:03:58 INFO     	 * Prefix: True
2023-12-22 00:03:58 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 00:03:59 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 00:17:08 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 00:29:17 INFO     	Bleu_1: 0.2423534347819063
2023-12-22 00:29:17 INFO     	Bleu_2: 0.13882098198178178
2023-12-22 00:29:17 INFO     	Bleu_3: 0.0808844542993979
2023-12-22 00:29:17 INFO     	Bleu_4: 0.05336419712936542
2023-12-22 00:29:19 INFO     	Bleu_1: 0.22791887088467344
2023-12-22 00:29:19 INFO     	Bleu_2: 0.12855766152276968
2023-12-22 00:29:19 INFO     	Bleu_3: 0.07372812963569643
2023-12-22 00:29:19 INFO     	Bleu_4: 0.048143679105180644
2023-12-22 00:29:24 INFO     use spaCy answer extraction model: positionrank
2023-12-22 00:29:24 INFO     Model `small_combined_trained_ckpt/model_nxaqhy/epoch_14`
2023-12-22 00:29:24 INFO     	 * Num of GPU in use: 1
2023-12-22 00:29:24 INFO     	 * Prefix: True
2023-12-22 00:29:24 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 00:29:25 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 00:42:27 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 00:54:13 INFO     	Bleu_1: 0.2506949212422584
2023-12-22 00:54:13 INFO     	Bleu_2: 0.14407143419134166
2023-12-22 00:54:13 INFO     	Bleu_3: 0.08421686852619735
2023-12-22 00:54:13 INFO     	Bleu_4: 0.055553600298322424
2023-12-22 00:54:14 INFO     	Bleu_1: 0.22672266872243768
2023-12-22 00:54:14 INFO     	Bleu_2: 0.12845372119254664
2023-12-22 00:54:14 INFO     	Bleu_3: 0.07391532987326231
2023-12-22 00:54:14 INFO     	Bleu_4: 0.04834815755174373
2023-12-22 00:54:21 INFO     use spaCy answer extraction model: positionrank
2023-12-22 00:54:21 INFO     Model `small_combined_trained_ckpt/model_nxaqhy/epoch_15`
2023-12-22 00:54:21 INFO     	 * Num of GPU in use: 1
2023-12-22 00:54:21 INFO     	 * Prefix: True
2023-12-22 00:54:21 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 00:54:22 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 01:07:27 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 01:19:19 INFO     	Bleu_1: 0.24523401668784803
2023-12-22 01:19:19 INFO     	Bleu_2: 0.1406778876749598
2023-12-22 01:19:19 INFO     	Bleu_3: 0.08190867590187775
2023-12-22 01:19:19 INFO     	Bleu_4: 0.05382221325614185
2023-12-22 01:19:20 INFO     	Bleu_1: 0.2256136799737907
2023-12-22 01:19:20 INFO     	Bleu_2: 0.12778768710215002
2023-12-22 01:19:20 INFO     	Bleu_3: 0.07337449338054272
2023-12-22 01:19:20 INFO     	Bleu_4: 0.047900532383766986
2023-12-22 01:19:26 INFO     use spaCy answer extraction model: positionrank
2023-12-22 01:19:26 INFO     Model `small_combined_trained_ckpt/model_nxaqhy/epoch_2`
2023-12-22 01:19:26 INFO     	 * Num of GPU in use: 1
2023-12-22 01:19:26 INFO     	 * Prefix: True
2023-12-22 01:19:26 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 01:19:28 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 01:32:51 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 01:44:29 INFO     	Bleu_1: 0.28636899530556564
2023-12-22 01:44:29 INFO     	Bleu_2: 0.1599853554231852
2023-12-22 01:44:29 INFO     	Bleu_3: 0.0915215822422444
2023-12-22 01:44:29 INFO     	Bleu_4: 0.05955642886549991
2023-12-22 01:44:30 INFO     	Bleu_1: 0.25918736472475645
2023-12-22 01:44:30 INFO     	Bleu_2: 0.1420689488272763
2023-12-22 01:44:30 INFO     	Bleu_3: 0.07930789179349422
2023-12-22 01:44:30 INFO     	Bleu_4: 0.05074772700388549
2023-12-22 01:44:35 INFO     use spaCy answer extraction model: positionrank
2023-12-22 01:44:36 INFO     Model `small_combined_trained_ckpt/model_nxaqhy/epoch_3`
2023-12-22 01:44:36 INFO     	 * Num of GPU in use: 1
2023-12-22 01:44:36 INFO     	 * Prefix: True
2023-12-22 01:44:36 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 01:44:37 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 01:55:40 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 02:05:34 INFO     	Bleu_1: 0.2958152528009731
2023-12-22 02:05:34 INFO     	Bleu_2: 0.16730365341788328
2023-12-22 02:05:34 INFO     	Bleu_3: 0.09724248674562573
2023-12-22 02:05:34 INFO     	Bleu_4: 0.06398849968007991
2023-12-22 02:05:35 INFO     	Bleu_1: 0.2730058647094641
2023-12-22 02:05:35 INFO     	Bleu_2: 0.15119286357124967
2023-12-22 02:05:35 INFO     	Bleu_3: 0.08575839390622574
2023-12-22 02:05:35 INFO     	Bleu_4: 0.05543194145473464
2023-12-22 02:05:40 INFO     use spaCy answer extraction model: positionrank
2023-12-22 02:05:41 INFO     Model `small_combined_trained_ckpt/model_nxaqhy/epoch_4`
2023-12-22 02:05:41 INFO     	 * Num of GPU in use: 1
2023-12-22 02:05:41 INFO     	 * Prefix: True
2023-12-22 02:05:41 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 02:05:42 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 02:15:58 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 02:25:27 INFO     	Bleu_1: 0.2976509739086813
2023-12-22 02:25:27 INFO     	Bleu_2: 0.16916774853117283
2023-12-22 02:25:27 INFO     	Bleu_3: 0.09922162928210902
2023-12-22 02:25:27 INFO     	Bleu_4: 0.06568944793598196
2023-12-22 02:25:28 INFO     	Bleu_1: 0.27581348566514535
2023-12-22 02:25:28 INFO     	Bleu_2: 0.15375402093817617
2023-12-22 02:25:28 INFO     	Bleu_3: 0.08874241864278005
2023-12-22 02:25:28 INFO     	Bleu_4: 0.05828963886653086
2023-12-22 02:25:34 INFO     use spaCy answer extraction model: positionrank
2023-12-22 02:25:35 INFO     Model `small_combined_trained_ckpt/model_nxaqhy/epoch_5`
2023-12-22 02:25:35 INFO     	 * Num of GPU in use: 1
2023-12-22 02:25:35 INFO     	 * Prefix: True
2023-12-22 02:25:35 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 02:25:36 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 02:36:00 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 02:45:40 INFO     	Bleu_1: 0.2938066184458909
2023-12-22 02:45:40 INFO     	Bleu_2: 0.1677281863366081
2023-12-22 02:45:40 INFO     	Bleu_3: 0.09854046811030649
2023-12-22 02:45:40 INFO     	Bleu_4: 0.06546240893905916
2023-12-22 02:45:41 INFO     	Bleu_1: 0.27101617967939956
2023-12-22 02:45:41 INFO     	Bleu_2: 0.15130134870280315
2023-12-22 02:45:41 INFO     	Bleu_3: 0.08681817813036612
2023-12-22 02:45:41 INFO     	Bleu_4: 0.05680835691817018
2023-12-22 02:45:48 INFO     use spaCy answer extraction model: positionrank
2023-12-22 02:45:49 INFO     Model `small_combined_trained_ckpt/model_nxaqhy/epoch_6`
2023-12-22 02:45:49 INFO     	 * Num of GPU in use: 1
2023-12-22 02:45:49 INFO     	 * Prefix: True
2023-12-22 02:45:49 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 02:45:50 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 02:57:08 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 03:07:36 INFO     	Bleu_1: 0.2860672733782651
2023-12-22 03:07:36 INFO     	Bleu_2: 0.1641022805583727
2023-12-22 03:07:36 INFO     	Bleu_3: 0.09675152208645246
2023-12-22 03:07:36 INFO     	Bleu_4: 0.06427407639301298
2023-12-22 03:07:37 INFO     	Bleu_1: 0.264921567850205
2023-12-22 03:07:37 INFO     	Bleu_2: 0.14832637746497598
2023-12-22 03:07:37 INFO     	Bleu_3: 0.08519219880852028
2023-12-22 03:07:37 INFO     	Bleu_4: 0.05566741400419778
2023-12-22 03:07:43 INFO     use spaCy answer extraction model: positionrank
2023-12-22 03:07:43 INFO     Model `small_combined_trained_ckpt/model_nxaqhy/epoch_7`
2023-12-22 03:07:43 INFO     	 * Num of GPU in use: 1
2023-12-22 03:07:43 INFO     	 * Prefix: True
2023-12-22 03:07:43 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 03:07:44 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 03:18:35 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 03:28:49 INFO     	Bleu_1: 0.28512952343286424
2023-12-22 03:28:49 INFO     	Bleu_2: 0.16388190183441498
2023-12-22 03:28:49 INFO     	Bleu_3: 0.09690405886723598
2023-12-22 03:28:49 INFO     	Bleu_4: 0.06475279129345127
2023-12-22 03:28:50 INFO     	Bleu_1: 0.2706688213905305
2023-12-22 03:28:50 INFO     	Bleu_2: 0.15291863418256516
2023-12-22 03:28:50 INFO     	Bleu_3: 0.08859474356063354
2023-12-22 03:28:50 INFO     	Bleu_4: 0.058221266583699444
2023-12-22 03:28:57 INFO     use spaCy answer extraction model: positionrank
2023-12-22 03:28:57 INFO     Model `small_combined_trained_ckpt/model_nxaqhy/epoch_8`
2023-12-22 03:28:57 INFO     	 * Num of GPU in use: 1
2023-12-22 03:28:57 INFO     	 * Prefix: True
2023-12-22 03:28:57 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 03:28:59 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 03:41:09 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 03:52:11 INFO     	Bleu_1: 0.26624460681221856
2023-12-22 03:52:11 INFO     	Bleu_2: 0.15269048839321706
2023-12-22 03:52:11 INFO     	Bleu_3: 0.08966669503225219
2023-12-22 03:52:11 INFO     	Bleu_4: 0.059578319677789225
2023-12-22 03:52:12 INFO     	Bleu_1: 0.2507846569264025
2023-12-22 03:52:12 INFO     	Bleu_2: 0.14147456578688206
2023-12-22 03:52:12 INFO     	Bleu_3: 0.08166306016414097
2023-12-22 03:52:12 INFO     	Bleu_4: 0.05369145641820939
2023-12-22 03:52:22 INFO     use spaCy answer extraction model: positionrank
2023-12-22 03:52:22 INFO     Model `small_combined_trained_ckpt/model_nxaqhy/epoch_9`
2023-12-22 03:52:22 INFO     	 * Num of GPU in use: 1
2023-12-22 03:52:22 INFO     	 * Prefix: True
2023-12-22 03:52:22 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 03:52:23 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 04:04:28 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 04:15:21 INFO     	Bleu_1: 0.2679336855371339
2023-12-22 04:15:21 INFO     	Bleu_2: 0.15463959797397314
2023-12-22 04:15:21 INFO     	Bleu_3: 0.09155267746018311
2023-12-22 04:15:21 INFO     	Bleu_4: 0.06115481977657837
2023-12-22 04:15:23 INFO     	Bleu_1: 0.250757753545823
2023-12-22 04:15:23 INFO     	Bleu_2: 0.1423580818444072
2023-12-22 04:15:23 INFO     	Bleu_3: 0.08275628052469804
2023-12-22 04:15:23 INFO     	Bleu_4: 0.054502528250770815
2023-12-22 04:15:23 INFO     ## 2nd RUN (EVAL): Configuration 1/5 ##
2023-12-22 04:15:33 INFO     use spaCy answer extraction model: positionrank
2023-12-22 04:15:33 INFO     Model `small_combined_trained_ckpt/model_vhyoja/epoch_1`
2023-12-22 04:15:33 INFO     	 * Num of GPU in use: 1
2023-12-22 04:15:33 INFO     	 * Prefix: True
2023-12-22 04:15:33 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 04:15:34 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 04:44:04 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 05:09:10 WARNING  prediction not found at the evaluation
2023-12-22 05:09:10 WARNING  prediction not found at the evaluation
2023-12-22 05:09:11 INFO     	Bleu_1: 0.0
2023-12-22 05:09:11 INFO     	Bleu_2: 0.0
2023-12-22 05:09:11 INFO     	Bleu_3: 0.0
2023-12-22 05:09:11 INFO     	Bleu_4: 0.0
2023-12-22 05:09:11 WARNING  prediction not found at the evaluation
2023-12-22 05:09:11 WARNING  prediction not found at the evaluation
2023-12-22 05:09:11 INFO     	Bleu_1: 0.0
2023-12-22 05:09:11 INFO     	Bleu_2: 0.0
2023-12-22 05:09:11 INFO     	Bleu_3: 0.0
2023-12-22 05:09:11 INFO     	Bleu_4: 0.0
2023-12-22 05:09:23 INFO     use spaCy answer extraction model: positionrank
2023-12-22 05:09:23 INFO     Model `small_combined_trained_ckpt/model_vhyoja/epoch_11`
2023-12-22 05:09:23 INFO     	 * Num of GPU in use: 1
2023-12-22 05:09:23 INFO     	 * Prefix: True
2023-12-22 05:09:23 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 05:09:24 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 05:21:04 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 05:31:37 INFO     	Bleu_1: 0.266921087377235
2023-12-22 05:31:37 INFO     	Bleu_2: 0.15450070698036591
2023-12-22 05:31:37 INFO     	Bleu_3: 0.09193159851192935
2023-12-22 05:31:37 INFO     	Bleu_4: 0.06148729359713629
2023-12-22 05:31:38 INFO     	Bleu_1: 0.25168019083483456
2023-12-22 05:31:38 INFO     	Bleu_2: 0.14276951886273218
2023-12-22 05:31:38 INFO     	Bleu_3: 0.08270756748645146
2023-12-22 05:31:38 INFO     	Bleu_4: 0.05421124997909319
2023-12-22 05:31:44 INFO     use spaCy answer extraction model: positionrank
2023-12-22 05:31:45 INFO     Model `small_combined_trained_ckpt/model_vhyoja/epoch_12`
2023-12-22 05:31:45 INFO     	 * Num of GPU in use: 1
2023-12-22 05:31:45 INFO     	 * Prefix: True
2023-12-22 05:31:45 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 05:31:46 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 05:43:16 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 05:53:53 INFO     	Bleu_1: 0.26912636539338436
2023-12-22 05:53:53 INFO     	Bleu_2: 0.15603364479729662
2023-12-22 05:53:53 INFO     	Bleu_3: 0.09318084461529995
2023-12-22 05:53:53 INFO     	Bleu_4: 0.0623871436785594
2023-12-22 05:53:54 INFO     	Bleu_1: 0.2522801508791295
2023-12-22 05:53:54 INFO     	Bleu_2: 0.1435256374446001
2023-12-22 05:53:54 INFO     	Bleu_3: 0.08374049917976861
2023-12-22 05:53:54 INFO     	Bleu_4: 0.05516451881324257
2023-12-22 05:54:01 INFO     use spaCy answer extraction model: positionrank
2023-12-22 05:54:01 INFO     Model `small_combined_trained_ckpt/model_vhyoja/epoch_13`
2023-12-22 05:54:01 INFO     	 * Num of GPU in use: 1
2023-12-22 05:54:01 INFO     	 * Prefix: True
2023-12-22 05:54:01 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 05:54:02 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 06:05:56 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 06:16:57 INFO     	Bleu_1: 0.2605870706722897
2023-12-22 06:16:57 INFO     	Bleu_2: 0.15116573810868242
2023-12-22 06:16:57 INFO     	Bleu_3: 0.08995152011283471
2023-12-22 06:16:57 INFO     	Bleu_4: 0.06009405826815223
2023-12-22 06:16:58 INFO     	Bleu_1: 0.2455968908298055
2023-12-22 06:16:58 INFO     	Bleu_2: 0.13970917832151375
2023-12-22 06:16:58 INFO     	Bleu_3: 0.0814658591582585
2023-12-22 06:16:58 INFO     	Bleu_4: 0.053723520915068455
2023-12-22 06:17:04 INFO     use spaCy answer extraction model: positionrank
2023-12-22 06:17:05 INFO     Model `small_combined_trained_ckpt/model_vhyoja/epoch_14`
2023-12-22 06:17:05 INFO     	 * Num of GPU in use: 1
2023-12-22 06:17:05 INFO     	 * Prefix: True
2023-12-22 06:17:05 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 06:17:06 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 06:29:00 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 06:40:08 INFO     	Bleu_1: 0.2596769382164046
2023-12-22 06:40:08 INFO     	Bleu_2: 0.15049859877711286
2023-12-22 06:40:08 INFO     	Bleu_3: 0.08929848182168248
2023-12-22 06:40:08 INFO     	Bleu_4: 0.059434938556747266
2023-12-22 06:40:09 INFO     	Bleu_1: 0.2457130793859964
2023-12-22 06:40:09 INFO     	Bleu_2: 0.14036559925179046
2023-12-22 06:40:09 INFO     	Bleu_3: 0.08222880979475805
2023-12-22 06:40:09 INFO     	Bleu_4: 0.05434413667120475
2023-12-22 06:40:15 INFO     use spaCy answer extraction model: positionrank
2023-12-22 06:40:15 INFO     Model `small_combined_trained_ckpt/model_vhyoja/epoch_15`
2023-12-22 06:40:15 INFO     	 * Num of GPU in use: 1
2023-12-22 06:40:15 INFO     	 * Prefix: True
2023-12-22 06:40:15 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 06:40:16 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 06:52:10 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 07:03:22 INFO     	Bleu_1: 0.25819110271257034
2023-12-22 07:03:22 INFO     	Bleu_2: 0.15008736243545776
2023-12-22 07:03:22 INFO     	Bleu_3: 0.08941544820347101
2023-12-22 07:03:22 INFO     	Bleu_4: 0.05979169169282299
2023-12-22 07:03:24 INFO     	Bleu_1: 0.2418005970000429
2023-12-22 07:03:24 INFO     	Bleu_2: 0.13845590425363002
2023-12-22 07:03:24 INFO     	Bleu_3: 0.08151717589161742
2023-12-22 07:03:24 INFO     	Bleu_4: 0.05417430206467296
2023-12-22 07:03:29 INFO     use spaCy answer extraction model: positionrank
2023-12-22 07:03:30 INFO     Model `small_combined_trained_ckpt/model_vhyoja/epoch_2`
2023-12-22 07:03:30 INFO     	 * Num of GPU in use: 1
2023-12-22 07:03:30 INFO     	 * Prefix: True
2023-12-22 07:03:30 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 07:03:31 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 07:16:58 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 07:28:37 INFO     	Bleu_1: 0.28636899530556564
2023-12-22 07:28:37 INFO     	Bleu_2: 0.1599853554231852
2023-12-22 07:28:37 INFO     	Bleu_3: 0.0915215822422444
2023-12-22 07:28:37 INFO     	Bleu_4: 0.05955642886549991
2023-12-22 07:28:38 INFO     	Bleu_1: 0.25918736472475645
2023-12-22 07:28:38 INFO     	Bleu_2: 0.1420689488272763
2023-12-22 07:28:38 INFO     	Bleu_3: 0.07930789179349422
2023-12-22 07:28:38 INFO     	Bleu_4: 0.05074772700388549
2023-12-22 07:28:44 INFO     use spaCy answer extraction model: positionrank
2023-12-22 07:28:45 INFO     Model `small_combined_trained_ckpt/model_vhyoja/epoch_3`
2023-12-22 07:28:45 INFO     	 * Num of GPU in use: 1
2023-12-22 07:28:45 INFO     	 * Prefix: True
2023-12-22 07:28:45 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 07:28:46 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 07:39:47 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 07:49:43 INFO     	Bleu_1: 0.2958152528009731
2023-12-22 07:49:43 INFO     	Bleu_2: 0.16730365341788328
2023-12-22 07:49:43 INFO     	Bleu_3: 0.09724248674562573
2023-12-22 07:49:43 INFO     	Bleu_4: 0.06398849968007991
2023-12-22 07:49:44 INFO     	Bleu_1: 0.2730058647094641
2023-12-22 07:49:44 INFO     	Bleu_2: 0.15119286357124967
2023-12-22 07:49:44 INFO     	Bleu_3: 0.08575839390622574
2023-12-22 07:49:44 INFO     	Bleu_4: 0.05543194145473464
2023-12-22 07:49:50 INFO     use spaCy answer extraction model: positionrank
2023-12-22 07:49:50 INFO     Model `small_combined_trained_ckpt/model_vhyoja/epoch_4`
2023-12-22 07:49:50 INFO     	 * Num of GPU in use: 1
2023-12-22 07:49:50 INFO     	 * Prefix: True
2023-12-22 07:49:50 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 07:49:51 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 08:00:09 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 08:09:38 INFO     	Bleu_1: 0.2976509739086813
2023-12-22 08:09:38 INFO     	Bleu_2: 0.16916774853117283
2023-12-22 08:09:38 INFO     	Bleu_3: 0.09922162928210902
2023-12-22 08:09:38 INFO     	Bleu_4: 0.06568944793598196
2023-12-22 08:09:39 INFO     	Bleu_1: 0.27581348566514535
2023-12-22 08:09:39 INFO     	Bleu_2: 0.15375402093817617
2023-12-22 08:09:39 INFO     	Bleu_3: 0.08874241864278005
2023-12-22 08:09:39 INFO     	Bleu_4: 0.05828963886653086
2023-12-22 08:09:47 INFO     use spaCy answer extraction model: positionrank
2023-12-22 08:09:47 INFO     Model `small_combined_trained_ckpt/model_vhyoja/epoch_5`
2023-12-22 08:09:47 INFO     	 * Num of GPU in use: 1
2023-12-22 08:09:47 INFO     	 * Prefix: True
2023-12-22 08:09:47 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 08:09:48 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 08:20:12 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 08:29:54 INFO     	Bleu_1: 0.2938066184458909
2023-12-22 08:29:54 INFO     	Bleu_2: 0.1677281863366081
2023-12-22 08:29:54 INFO     	Bleu_3: 0.09854046811030649
2023-12-22 08:29:54 INFO     	Bleu_4: 0.06546240893905916
2023-12-22 08:29:55 INFO     	Bleu_1: 0.27101617967939956
2023-12-22 08:29:55 INFO     	Bleu_2: 0.15130134870280315
2023-12-22 08:29:55 INFO     	Bleu_3: 0.08681817813036612
2023-12-22 08:29:55 INFO     	Bleu_4: 0.05680835691817018
2023-12-22 08:30:00 INFO     use spaCy answer extraction model: positionrank
2023-12-22 08:30:01 INFO     Model `small_combined_trained_ckpt/model_vhyoja/epoch_6`
2023-12-22 08:30:01 INFO     	 * Num of GPU in use: 1
2023-12-22 08:30:01 INFO     	 * Prefix: True
2023-12-22 08:30:01 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 08:30:02 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 08:41:22 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 08:51:51 INFO     	Bleu_1: 0.2860672733782651
2023-12-22 08:51:51 INFO     	Bleu_2: 0.1641022805583727
2023-12-22 08:51:51 INFO     	Bleu_3: 0.09675152208645246
2023-12-22 08:51:51 INFO     	Bleu_4: 0.06427407639301298
2023-12-22 08:51:53 INFO     	Bleu_1: 0.264921567850205
2023-12-22 08:51:53 INFO     	Bleu_2: 0.14832637746497598
2023-12-22 08:51:53 INFO     	Bleu_3: 0.08519219880852028
2023-12-22 08:51:53 INFO     	Bleu_4: 0.05566741400419778
2023-12-22 08:51:58 INFO     use spaCy answer extraction model: positionrank
2023-12-22 08:51:58 INFO     Model `small_combined_trained_ckpt/model_vhyoja/epoch_7`
2023-12-22 08:51:58 INFO     	 * Num of GPU in use: 1
2023-12-22 08:51:58 INFO     	 * Prefix: True
2023-12-22 08:51:58 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 08:52:00 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 09:02:53 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 09:13:06 INFO     	Bleu_1: 0.28512952343286424
2023-12-22 09:13:06 INFO     	Bleu_2: 0.16388190183441498
2023-12-22 09:13:06 INFO     	Bleu_3: 0.09690405886723598
2023-12-22 09:13:06 INFO     	Bleu_4: 0.06475279129345127
2023-12-22 09:13:07 INFO     	Bleu_1: 0.2706688213905305
2023-12-22 09:13:07 INFO     	Bleu_2: 0.15291863418256516
2023-12-22 09:13:07 INFO     	Bleu_3: 0.08859474356063354
2023-12-22 09:13:07 INFO     	Bleu_4: 0.058221266583699444
2023-12-22 09:13:13 INFO     use spaCy answer extraction model: positionrank
2023-12-22 09:13:13 INFO     Model `small_combined_trained_ckpt/model_vhyoja/epoch_8`
2023-12-22 09:13:13 INFO     	 * Num of GPU in use: 1
2023-12-22 09:13:13 INFO     	 * Prefix: True
2023-12-22 09:13:13 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 09:13:14 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 09:25:25 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 09:36:26 INFO     	Bleu_1: 0.26624460681221856
2023-12-22 09:36:26 INFO     	Bleu_2: 0.15269048839321706
2023-12-22 09:36:26 INFO     	Bleu_3: 0.08966669503225219
2023-12-22 09:36:26 INFO     	Bleu_4: 0.059578319677789225
2023-12-22 09:36:27 INFO     	Bleu_1: 0.2507846569264025
2023-12-22 09:36:27 INFO     	Bleu_2: 0.14147456578688206
2023-12-22 09:36:27 INFO     	Bleu_3: 0.08166306016414097
2023-12-22 09:36:27 INFO     	Bleu_4: 0.05369145641820939
2023-12-22 09:36:33 INFO     use spaCy answer extraction model: positionrank
2023-12-22 09:36:33 INFO     Model `small_combined_trained_ckpt/model_vhyoja/epoch_9`
2023-12-22 09:36:33 INFO     	 * Num of GPU in use: 1
2023-12-22 09:36:33 INFO     	 * Prefix: True
2023-12-22 09:36:33 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 09:36:35 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 09:48:39 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 09:59:32 INFO     	Bleu_1: 0.2679336855371339
2023-12-22 09:59:32 INFO     	Bleu_2: 0.15463959797397314
2023-12-22 09:59:32 INFO     	Bleu_3: 0.09155267746018311
2023-12-22 09:59:32 INFO     	Bleu_4: 0.06115481977657837
2023-12-22 09:59:33 INFO     	Bleu_1: 0.250757753545823
2023-12-22 09:59:33 INFO     	Bleu_2: 0.1423580818444072
2023-12-22 09:59:33 INFO     	Bleu_3: 0.08275628052469804
2023-12-22 09:59:33 INFO     	Bleu_4: 0.054502528250770815
2023-12-22 09:59:34 INFO     ## 2nd RUN (EVAL): Configuration 2/5 ##
2023-12-22 09:59:39 INFO     use spaCy answer extraction model: positionrank
2023-12-22 09:59:39 INFO     Model `small_combined_trained_ckpt/model_oprhlh/epoch_1`
2023-12-22 09:59:39 INFO     	 * Num of GPU in use: 1
2023-12-22 09:59:39 INFO     	 * Prefix: True
2023-12-22 09:59:39 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 09:59:40 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 10:14:59 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 10:28:35 INFO     	Bleu_1: 0.27336810940363626
2023-12-22 10:28:35 INFO     	Bleu_2: 0.1521211249076985
2023-12-22 10:28:35 INFO     	Bleu_3: 0.08640719638230847
2023-12-22 10:28:35 INFO     	Bleu_4: 0.05596640873937455
2023-12-22 10:28:36 INFO     	Bleu_1: 0.25163057802459604
2023-12-22 10:28:36 INFO     	Bleu_2: 0.13707530125503026
2023-12-22 10:28:36 INFO     	Bleu_3: 0.07569747332383535
2023-12-22 10:28:36 INFO     	Bleu_4: 0.04794895559430432
2023-12-22 10:28:46 INFO     use spaCy answer extraction model: positionrank
2023-12-22 10:28:46 INFO     Model `small_combined_trained_ckpt/model_oprhlh/epoch_11`
2023-12-22 10:28:46 INFO     	 * Num of GPU in use: 1
2023-12-22 10:28:46 INFO     	 * Prefix: True
2023-12-22 10:28:46 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 10:28:47 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 10:42:22 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 10:54:45 INFO     	Bleu_1: 0.23633567389548177
2023-12-22 10:54:45 INFO     	Bleu_2: 0.13656341173636324
2023-12-22 10:54:45 INFO     	Bleu_3: 0.08049628438004411
2023-12-22 10:54:45 INFO     	Bleu_4: 0.05350557414184639
2023-12-22 10:54:46 INFO     	Bleu_1: 0.22290411430538756
2023-12-22 10:54:46 INFO     	Bleu_2: 0.12681566470870703
2023-12-22 10:54:46 INFO     	Bleu_3: 0.07363281985144583
2023-12-22 10:54:46 INFO     	Bleu_4: 0.04833480828315996
2023-12-22 10:54:53 INFO     use spaCy answer extraction model: positionrank
2023-12-22 10:54:53 INFO     Model `small_combined_trained_ckpt/model_oprhlh/epoch_12`
2023-12-22 10:54:53 INFO     	 * Num of GPU in use: 1
2023-12-22 10:54:53 INFO     	 * Prefix: True
2023-12-22 10:54:53 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 10:54:54 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 11:07:38 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 11:19:18 INFO     	Bleu_1: 0.24522568751058307
2023-12-22 11:19:18 INFO     	Bleu_2: 0.14193382018784237
2023-12-22 11:19:18 INFO     	Bleu_3: 0.08385476816377105
2023-12-22 11:19:18 INFO     	Bleu_4: 0.05571166645554474
2023-12-22 11:19:20 INFO     	Bleu_1: 0.2257349344586355
2023-12-22 11:19:20 INFO     	Bleu_2: 0.12922684147530528
2023-12-22 11:19:20 INFO     	Bleu_3: 0.07552195961244369
2023-12-22 11:19:20 INFO     	Bleu_4: 0.0498417863005245
2023-12-22 11:19:25 INFO     use spaCy answer extraction model: positionrank
2023-12-22 11:19:25 INFO     Model `small_combined_trained_ckpt/model_oprhlh/epoch_13`
2023-12-22 11:19:25 INFO     	 * Num of GPU in use: 1
2023-12-22 11:19:25 INFO     	 * Prefix: True
2023-12-22 11:19:25 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 11:19:26 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 11:32:10 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 11:43:46 INFO     	Bleu_1: 0.23888455659877028
2023-12-22 11:43:46 INFO     	Bleu_2: 0.13788001222698634
2023-12-22 11:43:46 INFO     	Bleu_3: 0.08110923447443782
2023-12-22 11:43:46 INFO     	Bleu_4: 0.0536732981091381
2023-12-22 11:43:48 INFO     	Bleu_1: 0.22647026593689196
2023-12-22 11:43:48 INFO     	Bleu_2: 0.12956500972859158
2023-12-22 11:43:48 INFO     	Bleu_3: 0.0755799051402207
2023-12-22 11:43:48 INFO     	Bleu_4: 0.04982104758606759
2023-12-22 11:43:53 INFO     use spaCy answer extraction model: positionrank
2023-12-22 11:43:54 INFO     Model `small_combined_trained_ckpt/model_oprhlh/epoch_14`
2023-12-22 11:43:54 INFO     	 * Num of GPU in use: 1
2023-12-22 11:43:54 INFO     	 * Prefix: True
2023-12-22 11:43:54 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 11:43:55 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 11:56:22 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 12:08:00 INFO     	Bleu_1: 0.24197496810674682
2023-12-22 12:08:00 INFO     	Bleu_2: 0.14019164641434448
2023-12-22 12:08:00 INFO     	Bleu_3: 0.08279036635969939
2023-12-22 12:08:00 INFO     	Bleu_4: 0.054898906780224424
2023-12-22 12:08:01 INFO     	Bleu_1: 0.22729724817751917
2023-12-22 12:08:01 INFO     	Bleu_2: 0.13008566965540935
2023-12-22 12:08:01 INFO     	Bleu_3: 0.07591571679714618
2023-12-22 12:08:01 INFO     	Bleu_4: 0.049898216384108664
2023-12-22 12:08:07 INFO     use spaCy answer extraction model: positionrank
2023-12-22 12:08:07 INFO     Model `small_combined_trained_ckpt/model_oprhlh/epoch_15`
2023-12-22 12:08:07 INFO     	 * Num of GPU in use: 1
2023-12-22 12:08:07 INFO     	 * Prefix: True
2023-12-22 12:08:07 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 12:08:08 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 12:20:51 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 12:32:38 INFO     	Bleu_1: 0.24583110215138362
2023-12-22 12:32:38 INFO     	Bleu_2: 0.1431406432486248
2023-12-22 12:32:38 INFO     	Bleu_3: 0.08495690661045795
2023-12-22 12:32:38 INFO     	Bleu_4: 0.05658417090379591
2023-12-22 12:32:40 INFO     	Bleu_1: 0.22971344753522904
2023-12-22 12:32:40 INFO     	Bleu_2: 0.1316526598436018
2023-12-22 12:32:40 INFO     	Bleu_3: 0.07679968183694252
2023-12-22 12:32:40 INFO     	Bleu_4: 0.05043413894206764
2023-12-22 12:32:47 INFO     use spaCy answer extraction model: positionrank
2023-12-22 12:32:48 INFO     Model `small_combined_trained_ckpt/model_oprhlh/epoch_2`
2023-12-22 12:32:48 INFO     	 * Num of GPU in use: 1
2023-12-22 12:32:48 INFO     	 * Prefix: True
2023-12-22 12:32:48 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 12:32:49 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 12:43:09 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 12:52:23 INFO     	Bleu_1: 0.2958416897260783
2023-12-22 12:52:23 INFO     	Bleu_2: 0.16782656001798707
2023-12-22 12:52:23 INFO     	Bleu_3: 0.09826885710331516
2023-12-22 12:52:23 INFO     	Bleu_4: 0.06490599052984948
2023-12-22 12:52:24 INFO     	Bleu_1: 0.2743520247190915
2023-12-22 12:52:24 INFO     	Bleu_2: 0.1529694005034687
2023-12-22 12:52:24 INFO     	Bleu_3: 0.08788959963030449
2023-12-22 12:52:24 INFO     	Bleu_4: 0.057354225129501946
2023-12-22 12:52:31 INFO     use spaCy answer extraction model: positionrank
2023-12-22 12:52:31 INFO     Model `small_combined_trained_ckpt/model_oprhlh/epoch_3`
2023-12-22 12:52:31 INFO     	 * Num of GPU in use: 1
2023-12-22 12:52:31 INFO     	 * Prefix: True
2023-12-22 12:52:31 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 12:52:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 13:03:14 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 13:13:24 INFO     	Bleu_1: 0.2931779573819033
2023-12-22 13:13:24 INFO     	Bleu_2: 0.1679642439217249
2023-12-22 13:13:24 INFO     	Bleu_3: 0.09900018031960674
2023-12-22 13:13:24 INFO     	Bleu_4: 0.0658890416713825
2023-12-22 13:13:25 INFO     	Bleu_1: 0.27017063100588157
2023-12-22 13:13:25 INFO     	Bleu_2: 0.15127545074771495
2023-12-22 13:13:25 INFO     	Bleu_3: 0.08683136335169285
2023-12-22 13:13:25 INFO     	Bleu_4: 0.05672527099123468
2023-12-22 13:13:31 INFO     use spaCy answer extraction model: positionrank
2023-12-22 13:13:31 INFO     Model `small_combined_trained_ckpt/model_oprhlh/epoch_4`
2023-12-22 13:13:31 INFO     	 * Num of GPU in use: 1
2023-12-22 13:13:31 INFO     	 * Prefix: True
2023-12-22 13:13:31 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 13:13:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 13:24:22 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 13:34:29 INFO     	Bleu_1: 0.2876096836880044
2023-12-22 13:34:29 INFO     	Bleu_2: 0.16478775675661955
2023-12-22 13:34:29 INFO     	Bleu_3: 0.09753695218532865
2023-12-22 13:34:29 INFO     	Bleu_4: 0.06511939306073454
2023-12-22 13:34:30 INFO     	Bleu_1: 0.2701434606551202
2023-12-22 13:34:30 INFO     	Bleu_2: 0.1518157216780156
2023-12-22 13:34:30 INFO     	Bleu_3: 0.08818790604107916
2023-12-22 13:34:30 INFO     	Bleu_4: 0.05826275844352388
2023-12-22 13:34:36 INFO     use spaCy answer extraction model: positionrank
2023-12-22 13:34:36 INFO     Model `small_combined_trained_ckpt/model_oprhlh/epoch_5`
2023-12-22 13:34:36 INFO     	 * Num of GPU in use: 1
2023-12-22 13:34:36 INFO     	 * Prefix: True
2023-12-22 13:34:36 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 13:34:37 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 13:46:04 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 13:56:28 INFO     	Bleu_1: 0.2790263890162971
2023-12-22 13:56:28 INFO     	Bleu_2: 0.16039657059440773
2023-12-22 13:56:28 INFO     	Bleu_3: 0.09504889117627814
2023-12-22 13:56:28 INFO     	Bleu_4: 0.06352080824340683
2023-12-22 13:56:29 INFO     	Bleu_1: 0.2594838281290945
2023-12-22 13:56:29 INFO     	Bleu_2: 0.1456906408951213
2023-12-22 13:56:29 INFO     	Bleu_3: 0.08386898316626067
2023-12-22 13:56:29 INFO     	Bleu_4: 0.05509022180444603
2023-12-22 13:56:36 INFO     use spaCy answer extraction model: positionrank
2023-12-22 13:56:37 INFO     Model `small_combined_trained_ckpt/model_oprhlh/epoch_6`
2023-12-22 13:56:37 INFO     	 * Num of GPU in use: 1
2023-12-22 13:56:37 INFO     	 * Prefix: True
2023-12-22 13:56:37 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 13:56:38 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 14:08:25 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 14:19:21 INFO     	Bleu_1: 0.27098577383902184
2023-12-22 14:19:21 INFO     	Bleu_2: 0.15647991423482238
2023-12-22 14:19:21 INFO     	Bleu_3: 0.09291142086684812
2023-12-22 14:19:21 INFO     	Bleu_4: 0.06213612953189983
2023-12-22 14:19:22 INFO     	Bleu_1: 0.2525310972938071
2023-12-22 14:19:22 INFO     	Bleu_2: 0.14272478827776094
2023-12-22 14:19:22 INFO     	Bleu_3: 0.08268364223892913
2023-12-22 14:19:22 INFO     	Bleu_4: 0.05435286697856124
2023-12-22 14:19:29 INFO     use spaCy answer extraction model: positionrank
2023-12-22 14:19:29 INFO     Model `small_combined_trained_ckpt/model_oprhlh/epoch_7`
2023-12-22 14:19:29 INFO     	 * Num of GPU in use: 1
2023-12-22 14:19:29 INFO     	 * Prefix: True
2023-12-22 14:19:29 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 14:19:30 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 14:31:07 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 14:41:29 INFO     	Bleu_1: 0.2784124216507077
2023-12-22 14:41:29 INFO     	Bleu_2: 0.1613236303885431
2023-12-22 14:41:29 INFO     	Bleu_3: 0.09607894508945108
2023-12-22 14:41:29 INFO     	Bleu_4: 0.06430236758943239
2023-12-22 14:41:30 INFO     	Bleu_1: 0.2548822002763819
2023-12-22 14:41:30 INFO     	Bleu_2: 0.14483239669723025
2023-12-22 14:41:30 INFO     	Bleu_3: 0.08428712605281287
2023-12-22 14:41:30 INFO     	Bleu_4: 0.05551441359802253
2023-12-22 14:41:37 INFO     use spaCy answer extraction model: positionrank
2023-12-22 14:41:37 INFO     Model `small_combined_trained_ckpt/model_oprhlh/epoch_8`
2023-12-22 14:41:37 INFO     	 * Num of GPU in use: 1
2023-12-22 14:41:37 INFO     	 * Prefix: True
2023-12-22 14:41:37 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 14:41:38 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 14:53:59 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 15:05:22 INFO     	Bleu_1: 0.25327443799951804
2023-12-22 15:05:22 INFO     	Bleu_2: 0.14656816435794387
2023-12-22 15:05:22 INFO     	Bleu_3: 0.08689277996269268
2023-12-22 15:05:22 INFO     	Bleu_4: 0.057899567019315326
2023-12-22 15:05:23 INFO     	Bleu_1: 0.2373649141519084
2023-12-22 15:05:23 INFO     	Bleu_2: 0.1348015676046305
2023-12-22 15:05:23 INFO     	Bleu_3: 0.07816170446771337
2023-12-22 15:05:23 INFO     	Bleu_4: 0.05129551954736131
2023-12-22 15:05:30 INFO     use spaCy answer extraction model: positionrank
2023-12-22 15:05:31 INFO     Model `small_combined_trained_ckpt/model_oprhlh/epoch_9`
2023-12-22 15:05:31 INFO     	 * Num of GPU in use: 1
2023-12-22 15:05:31 INFO     	 * Prefix: True
2023-12-22 15:05:31 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 15:05:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 15:17:42 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 15:28:58 INFO     	Bleu_1: 0.2579306315291075
2023-12-22 15:28:58 INFO     	Bleu_2: 0.14989912053081716
2023-12-22 15:28:58 INFO     	Bleu_3: 0.08950933093299746
2023-12-22 15:28:58 INFO     	Bleu_4: 0.059874762955968754
2023-12-22 15:28:59 INFO     	Bleu_1: 0.23857094687796881
2023-12-22 15:28:59 INFO     	Bleu_2: 0.1363543649933899
2023-12-22 15:28:59 INFO     	Bleu_3: 0.07980198856951938
2023-12-22 15:28:59 INFO     	Bleu_4: 0.052657167026358996
2023-12-22 15:28:59 INFO     ## 2nd RUN (EVAL): Configuration 3/5 ##
2023-12-22 15:29:04 INFO     use spaCy answer extraction model: positionrank
2023-12-22 15:29:05 INFO     Model `small_combined_trained_ckpt/model_nrudfu/epoch_1`
2023-12-22 15:29:05 INFO     	 * Num of GPU in use: 1
2023-12-22 15:29:05 INFO     	 * Prefix: True
2023-12-22 15:29:05 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 15:29:06 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 15:44:24 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 15:58:01 INFO     	Bleu_1: 0.27336810940363626
2023-12-22 15:58:01 INFO     	Bleu_2: 0.1521211249076985
2023-12-22 15:58:01 INFO     	Bleu_3: 0.08640719638230847
2023-12-22 15:58:01 INFO     	Bleu_4: 0.05596640873937455
2023-12-22 15:58:02 INFO     	Bleu_1: 0.25163057802459604
2023-12-22 15:58:02 INFO     	Bleu_2: 0.13707530125503026
2023-12-22 15:58:02 INFO     	Bleu_3: 0.07569747332383535
2023-12-22 15:58:02 INFO     	Bleu_4: 0.04794895559430432
2023-12-22 15:58:11 INFO     use spaCy answer extraction model: positionrank
2023-12-22 15:58:12 INFO     Model `small_combined_trained_ckpt/model_nrudfu/epoch_11`
2023-12-22 15:58:12 INFO     	 * Num of GPU in use: 1
2023-12-22 15:58:12 INFO     	 * Prefix: True
2023-12-22 15:58:12 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 15:58:13 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 16:09:52 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 16:20:51 INFO     	Bleu_1: 0.2581866846007783
2023-12-22 16:20:51 INFO     	Bleu_2: 0.1501601994991749
2023-12-22 16:20:51 INFO     	Bleu_3: 0.08967532491492307
2023-12-22 16:20:51 INFO     	Bleu_4: 0.06000687324666407
2023-12-22 16:20:52 INFO     	Bleu_1: 0.2434386539014298
2023-12-22 16:20:52 INFO     	Bleu_2: 0.13989798925371907
2023-12-22 16:20:52 INFO     	Bleu_3: 0.08272222768412778
2023-12-22 16:20:52 INFO     	Bleu_4: 0.05508242663629612
2023-12-22 16:20:58 INFO     use spaCy answer extraction model: positionrank
2023-12-22 16:20:58 INFO     Model `small_combined_trained_ckpt/model_nrudfu/epoch_12`
2023-12-22 16:20:58 INFO     	 * Num of GPU in use: 1
2023-12-22 16:20:58 INFO     	 * Prefix: True
2023-12-22 16:20:58 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 16:20:59 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 16:32:43 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 16:43:37 INFO     	Bleu_1: 0.260845942510353
2023-12-22 16:43:37 INFO     	Bleu_2: 0.15166666177703383
2023-12-22 16:43:37 INFO     	Bleu_3: 0.09066371969955052
2023-12-22 16:43:37 INFO     	Bleu_4: 0.060639955676096584
2023-12-22 16:43:38 INFO     	Bleu_1: 0.24615472691881912
2023-12-22 16:43:38 INFO     	Bleu_2: 0.1414188669464544
2023-12-22 16:43:38 INFO     	Bleu_3: 0.0837018016852923
2023-12-22 16:43:38 INFO     	Bleu_4: 0.05562510561172689
2023-12-22 16:43:44 INFO     use spaCy answer extraction model: positionrank
2023-12-22 16:43:45 INFO     Model `small_combined_trained_ckpt/model_nrudfu/epoch_13`
2023-12-22 16:43:45 INFO     	 * Num of GPU in use: 1
2023-12-22 16:43:45 INFO     	 * Prefix: True
2023-12-22 16:43:45 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 16:43:46 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 16:56:07 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 17:07:28 INFO     	Bleu_1: 0.2529122679286486
2023-12-22 17:07:28 INFO     	Bleu_2: 0.14701303701158727
2023-12-22 17:07:28 INFO     	Bleu_3: 0.08771797240164562
2023-12-22 17:07:28 INFO     	Bleu_4: 0.05864510261656439
2023-12-22 17:07:29 INFO     	Bleu_1: 0.23655380124479894
2023-12-22 17:07:29 INFO     	Bleu_2: 0.13609733664106666
2023-12-22 17:07:29 INFO     	Bleu_3: 0.08043500134544093
2023-12-22 17:07:29 INFO     	Bleu_4: 0.05346662215898864
2023-12-22 17:07:36 INFO     use spaCy answer extraction model: positionrank
2023-12-22 17:07:36 INFO     Model `small_combined_trained_ckpt/model_nrudfu/epoch_14`
2023-12-22 17:07:36 INFO     	 * Num of GPU in use: 1
2023-12-22 17:07:36 INFO     	 * Prefix: True
2023-12-22 17:07:36 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 17:07:37 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 17:19:51 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 17:31:19 INFO     	Bleu_1: 0.24818469086656172
2023-12-22 17:31:19 INFO     	Bleu_2: 0.14492386941278068
2023-12-22 17:31:19 INFO     	Bleu_3: 0.08692664268628761
2023-12-22 17:31:19 INFO     	Bleu_4: 0.05831118881373479
2023-12-22 17:31:20 INFO     	Bleu_1: 0.23162792327845227
2023-12-22 17:31:20 INFO     	Bleu_2: 0.13315725345411322
2023-12-22 17:31:20 INFO     	Bleu_3: 0.07860521950977117
2023-12-22 17:31:20 INFO     	Bleu_4: 0.052178673499776555
2023-12-22 17:31:26 INFO     use spaCy answer extraction model: positionrank
2023-12-22 17:31:27 INFO     Model `small_combined_trained_ckpt/model_nrudfu/epoch_15`
2023-12-22 17:31:27 INFO     	 * Num of GPU in use: 1
2023-12-22 17:31:27 INFO     	 * Prefix: True
2023-12-22 17:31:27 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 17:31:28 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 17:43:44 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 17:55:05 INFO     	Bleu_1: 0.25292383514278816
2023-12-22 17:55:05 INFO     	Bleu_2: 0.14758092935965644
2023-12-22 17:55:05 INFO     	Bleu_3: 0.08835107609411921
2023-12-22 17:55:05 INFO     	Bleu_4: 0.0590411661723125
2023-12-22 17:55:06 INFO     	Bleu_1: 0.23566986396208148
2023-12-22 17:55:06 INFO     	Bleu_2: 0.1357437325910153
2023-12-22 17:55:06 INFO     	Bleu_3: 0.08040922988557597
2023-12-22 17:55:06 INFO     	Bleu_4: 0.0535461178850886
2023-12-22 17:55:13 INFO     use spaCy answer extraction model: positionrank
2023-12-22 17:55:13 INFO     Model `small_combined_trained_ckpt/model_nrudfu/epoch_2`
2023-12-22 17:55:13 INFO     	 * Num of GPU in use: 1
2023-12-22 17:55:13 INFO     	 * Prefix: True
2023-12-22 17:55:13 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 17:55:14 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 18:05:35 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 18:14:51 INFO     	Bleu_1: 0.2958416897260783
2023-12-22 18:14:51 INFO     	Bleu_2: 0.16782656001798707
2023-12-22 18:14:51 INFO     	Bleu_3: 0.09826885710331516
2023-12-22 18:14:51 INFO     	Bleu_4: 0.06490599052984948
2023-12-22 18:14:52 INFO     	Bleu_1: 0.2743520247190915
2023-12-22 18:14:52 INFO     	Bleu_2: 0.1529694005034687
2023-12-22 18:14:52 INFO     	Bleu_3: 0.08788959963030449
2023-12-22 18:14:52 INFO     	Bleu_4: 0.057354225129501946
2023-12-22 18:14:59 INFO     use spaCy answer extraction model: positionrank
2023-12-22 18:14:59 INFO     Model `small_combined_trained_ckpt/model_nrudfu/epoch_3`
2023-12-22 18:14:59 INFO     	 * Num of GPU in use: 1
2023-12-22 18:14:59 INFO     	 * Prefix: True
2023-12-22 18:14:59 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 18:15:00 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 18:25:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 18:35:58 INFO     	Bleu_1: 0.2931779573819033
2023-12-22 18:35:58 INFO     	Bleu_2: 0.1679642439217249
2023-12-22 18:35:58 INFO     	Bleu_3: 0.09900018031960674
2023-12-22 18:35:58 INFO     	Bleu_4: 0.0658890416713825
2023-12-22 18:35:59 INFO     	Bleu_1: 0.27017063100588157
2023-12-22 18:35:59 INFO     	Bleu_2: 0.15127545074771495
2023-12-22 18:35:59 INFO     	Bleu_3: 0.08683136335169285
2023-12-22 18:35:59 INFO     	Bleu_4: 0.05672527099123468
2023-12-22 18:36:05 INFO     use spaCy answer extraction model: positionrank
2023-12-22 18:36:05 INFO     Model `small_combined_trained_ckpt/model_nrudfu/epoch_4`
2023-12-22 18:36:05 INFO     	 * Num of GPU in use: 1
2023-12-22 18:36:05 INFO     	 * Prefix: True
2023-12-22 18:36:05 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 18:36:06 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 18:46:57 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 18:57:05 INFO     	Bleu_1: 0.2876096836880044
2023-12-22 18:57:05 INFO     	Bleu_2: 0.16478775675661955
2023-12-22 18:57:05 INFO     	Bleu_3: 0.09753695218532865
2023-12-22 18:57:05 INFO     	Bleu_4: 0.06511939306073454
2023-12-22 18:57:06 INFO     	Bleu_1: 0.2701434606551202
2023-12-22 18:57:06 INFO     	Bleu_2: 0.1518157216780156
2023-12-22 18:57:06 INFO     	Bleu_3: 0.08818790604107916
2023-12-22 18:57:06 INFO     	Bleu_4: 0.05826275844352388
2023-12-22 18:57:12 INFO     use spaCy answer extraction model: positionrank
2023-12-22 18:57:13 INFO     Model `small_combined_trained_ckpt/model_nrudfu/epoch_5`
2023-12-22 18:57:13 INFO     	 * Num of GPU in use: 1
2023-12-22 18:57:13 INFO     	 * Prefix: True
2023-12-22 18:57:13 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 18:57:14 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 19:08:43 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 19:19:09 INFO     	Bleu_1: 0.2790263890162971
2023-12-22 19:19:09 INFO     	Bleu_2: 0.16039657059440773
2023-12-22 19:19:09 INFO     	Bleu_3: 0.09504889117627814
2023-12-22 19:19:09 INFO     	Bleu_4: 0.06352080824340683
2023-12-22 19:19:10 INFO     	Bleu_1: 0.2594838281290945
2023-12-22 19:19:10 INFO     	Bleu_2: 0.1456906408951213
2023-12-22 19:19:10 INFO     	Bleu_3: 0.08386898316626067
2023-12-22 19:19:10 INFO     	Bleu_4: 0.05509022180444603
2023-12-22 19:19:16 INFO     use spaCy answer extraction model: positionrank
2023-12-22 19:19:16 INFO     Model `small_combined_trained_ckpt/model_nrudfu/epoch_6`
2023-12-22 19:19:16 INFO     	 * Num of GPU in use: 1
2023-12-22 19:19:16 INFO     	 * Prefix: True
2023-12-22 19:19:16 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 19:19:17 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 19:31:06 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 19:42:04 INFO     	Bleu_1: 0.27098577383902184
2023-12-22 19:42:04 INFO     	Bleu_2: 0.15647991423482238
2023-12-22 19:42:04 INFO     	Bleu_3: 0.09291142086684812
2023-12-22 19:42:04 INFO     	Bleu_4: 0.06213612953189983
2023-12-22 19:42:05 INFO     	Bleu_1: 0.2525310972938071
2023-12-22 19:42:05 INFO     	Bleu_2: 0.14272478827776094
2023-12-22 19:42:05 INFO     	Bleu_3: 0.08268364223892913
2023-12-22 19:42:05 INFO     	Bleu_4: 0.05435286697856124
2023-12-22 19:42:12 INFO     use spaCy answer extraction model: positionrank
2023-12-22 19:42:12 INFO     Model `small_combined_trained_ckpt/model_nrudfu/epoch_7`
2023-12-22 19:42:12 INFO     	 * Num of GPU in use: 1
2023-12-22 19:42:12 INFO     	 * Prefix: True
2023-12-22 19:42:12 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 19:42:13 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 19:53:51 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 20:04:16 INFO     	Bleu_1: 0.2784124216507077
2023-12-22 20:04:16 INFO     	Bleu_2: 0.1613236303885431
2023-12-22 20:04:16 INFO     	Bleu_3: 0.09607894508945108
2023-12-22 20:04:16 INFO     	Bleu_4: 0.06430236758943239
2023-12-22 20:04:17 INFO     	Bleu_1: 0.2548822002763819
2023-12-22 20:04:17 INFO     	Bleu_2: 0.14483239669723025
2023-12-22 20:04:17 INFO     	Bleu_3: 0.08428712605281287
2023-12-22 20:04:17 INFO     	Bleu_4: 0.05551441359802253
2023-12-22 20:04:23 INFO     use spaCy answer extraction model: positionrank
2023-12-22 20:04:23 INFO     Model `small_combined_trained_ckpt/model_nrudfu/epoch_8`
2023-12-22 20:04:23 INFO     	 * Num of GPU in use: 1
2023-12-22 20:04:23 INFO     	 * Prefix: True
2023-12-22 20:04:23 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 20:04:24 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 20:16:46 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 20:28:10 INFO     	Bleu_1: 0.25327443799951804
2023-12-22 20:28:10 INFO     	Bleu_2: 0.14656816435794387
2023-12-22 20:28:10 INFO     	Bleu_3: 0.08689277996269268
2023-12-22 20:28:10 INFO     	Bleu_4: 0.057899567019315326
2023-12-22 20:28:11 INFO     	Bleu_1: 0.2373649141519084
2023-12-22 20:28:11 INFO     	Bleu_2: 0.1348015676046305
2023-12-22 20:28:11 INFO     	Bleu_3: 0.07816170446771337
2023-12-22 20:28:11 INFO     	Bleu_4: 0.05129551954736131
2023-12-22 20:28:18 INFO     use spaCy answer extraction model: positionrank
2023-12-22 20:28:18 INFO     Model `small_combined_trained_ckpt/model_nrudfu/epoch_9`
2023-12-22 20:28:18 INFO     	 * Num of GPU in use: 1
2023-12-22 20:28:18 INFO     	 * Prefix: True
2023-12-22 20:28:18 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 20:28:20 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 20:40:31 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 20:51:47 INFO     	Bleu_1: 0.2579306315291075
2023-12-22 20:51:47 INFO     	Bleu_2: 0.14989912053081716
2023-12-22 20:51:47 INFO     	Bleu_3: 0.08950933093299746
2023-12-22 20:51:47 INFO     	Bleu_4: 0.059874762955968754
2023-12-22 20:51:48 INFO     	Bleu_1: 0.23857094687796881
2023-12-22 20:51:48 INFO     	Bleu_2: 0.1363543649933899
2023-12-22 20:51:48 INFO     	Bleu_3: 0.07980198856951938
2023-12-22 20:51:48 INFO     	Bleu_4: 0.052657167026358996
2023-12-22 20:51:48 INFO     ## 2nd RUN (EVAL): Configuration 4/5 ##
2023-12-22 20:51:54 INFO     use spaCy answer extraction model: positionrank
2023-12-22 20:51:54 INFO     Model `small_combined_trained_ckpt/model_mntyya/epoch_1`
2023-12-22 20:51:54 INFO     	 * Num of GPU in use: 1
2023-12-22 20:51:54 INFO     	 * Prefix: True
2023-12-22 20:51:54 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 20:51:56 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 21:03:05 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 21:12:57 INFO     	Bleu_1: 0.29154220968845307
2023-12-22 21:12:57 INFO     	Bleu_2: 0.16472788958867063
2023-12-22 21:12:57 INFO     	Bleu_3: 0.09607372980700629
2023-12-22 21:12:57 INFO     	Bleu_4: 0.06353068982886656
2023-12-22 21:12:58 INFO     	Bleu_1: 0.2681734512206152
2023-12-22 21:12:58 INFO     	Bleu_2: 0.14869429538265538
2023-12-22 21:12:58 INFO     	Bleu_3: 0.0848584196452923
2023-12-22 21:12:58 INFO     	Bleu_4: 0.0551388167611019
2023-12-22 21:13:07 INFO     use spaCy answer extraction model: positionrank
2023-12-22 21:13:07 INFO     Model `small_combined_trained_ckpt/model_mntyya/epoch_11`
2023-12-22 21:13:07 INFO     	 * Num of GPU in use: 1
2023-12-22 21:13:07 INFO     	 * Prefix: True
2023-12-22 21:13:07 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 21:13:08 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 21:25:31 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 21:36:59 INFO     	Bleu_1: 0.23658800372692695
2023-12-22 21:36:59 INFO     	Bleu_2: 0.13846158886596327
2023-12-22 21:36:59 INFO     	Bleu_3: 0.08276622504349929
2023-12-22 21:36:59 INFO     	Bleu_4: 0.05514505596594025
2023-12-22 21:37:00 INFO     	Bleu_1: 0.22455742004759194
2023-12-22 21:37:00 INFO     	Bleu_2: 0.1301360808373404
2023-12-22 21:37:00 INFO     	Bleu_3: 0.07701206308827338
2023-12-22 21:37:00 INFO     	Bleu_4: 0.05100863755259856
2023-12-22 21:37:05 INFO     use spaCy answer extraction model: positionrank
2023-12-22 21:37:05 INFO     Model `small_combined_trained_ckpt/model_mntyya/epoch_12`
2023-12-22 21:37:05 INFO     	 * Num of GPU in use: 1
2023-12-22 21:37:05 INFO     	 * Prefix: True
2023-12-22 21:37:05 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 21:37:07 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 21:49:14 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 22:00:48 INFO     	Bleu_1: 0.2466176551944572
2023-12-22 22:00:48 INFO     	Bleu_2: 0.14502526323996176
2023-12-22 22:00:48 INFO     	Bleu_3: 0.08770173930148624
2023-12-22 22:00:48 INFO     	Bleu_4: 0.059092158230739364
2023-12-22 22:00:49 INFO     	Bleu_1: 0.2311911914185603
2023-12-22 22:00:49 INFO     	Bleu_2: 0.1345784256315693
2023-12-22 22:00:49 INFO     	Bleu_3: 0.08030182771866488
2023-12-22 22:00:49 INFO     	Bleu_4: 0.05344889274554614
2023-12-22 22:00:54 INFO     use spaCy answer extraction model: positionrank
2023-12-22 22:00:55 INFO     Model `small_combined_trained_ckpt/model_mntyya/epoch_13`
2023-12-22 22:00:55 INFO     	 * Num of GPU in use: 1
2023-12-22 22:00:55 INFO     	 * Prefix: True
2023-12-22 22:00:55 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 22:00:56 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 22:13:22 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 22:24:58 INFO     	Bleu_1: 0.24991872687586034
2023-12-22 22:24:58 INFO     	Bleu_2: 0.14696215887770278
2023-12-22 22:24:58 INFO     	Bleu_3: 0.08884227156133696
2023-12-22 22:24:58 INFO     	Bleu_4: 0.05980602309020263
2023-12-22 22:24:59 INFO     	Bleu_1: 0.23123049537522045
2023-12-22 22:24:59 INFO     	Bleu_2: 0.13449066147438524
2023-12-22 22:24:59 INFO     	Bleu_3: 0.0803813536129627
2023-12-22 22:24:59 INFO     	Bleu_4: 0.05357470485950795
2023-12-22 22:25:04 INFO     use spaCy answer extraction model: positionrank
2023-12-22 22:25:04 INFO     Model `small_combined_trained_ckpt/model_mntyya/epoch_14`
2023-12-22 22:25:04 INFO     	 * Num of GPU in use: 1
2023-12-22 22:25:04 INFO     	 * Prefix: True
2023-12-22 22:25:04 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 22:25:05 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 22:37:37 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 22:49:17 INFO     	Bleu_1: 0.24278076454688496
2023-12-22 22:49:17 INFO     	Bleu_2: 0.14294997857423286
2023-12-22 22:49:17 INFO     	Bleu_3: 0.08625803588173646
2023-12-22 22:49:17 INFO     	Bleu_4: 0.05799093435998523
2023-12-22 22:49:18 INFO     	Bleu_1: 0.22768012590100586
2023-12-22 22:49:18 INFO     	Bleu_2: 0.1322941855838245
2023-12-22 22:49:18 INFO     	Bleu_3: 0.07905576761918365
2023-12-22 22:49:18 INFO     	Bleu_4: 0.05277500498210166
2023-12-22 22:49:23 INFO     use spaCy answer extraction model: positionrank
2023-12-22 22:49:24 INFO     Model `small_combined_trained_ckpt/model_mntyya/epoch_15`
2023-12-22 22:49:24 INFO     	 * Num of GPU in use: 1
2023-12-22 22:49:24 INFO     	 * Prefix: True
2023-12-22 22:49:24 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 22:49:25 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 23:01:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 23:13:13 INFO     	Bleu_1: 0.2532856184859511
2023-12-22 23:13:13 INFO     	Bleu_2: 0.14980125418190096
2023-12-22 23:13:13 INFO     	Bleu_3: 0.09124995315830826
2023-12-22 23:13:13 INFO     	Bleu_4: 0.06175774923574412
2023-12-22 23:13:14 INFO     	Bleu_1: 0.23753431451705853
2023-12-22 23:13:14 INFO     	Bleu_2: 0.13873695465283492
2023-12-22 23:13:14 INFO     	Bleu_3: 0.08344708589975092
2023-12-22 23:13:14 INFO     	Bleu_4: 0.0558405223865899
2023-12-22 23:13:23 INFO     use spaCy answer extraction model: positionrank
2023-12-22 23:13:23 INFO     Model `small_combined_trained_ckpt/model_mntyya/epoch_2`
2023-12-22 23:13:23 INFO     	 * Num of GPU in use: 1
2023-12-22 23:13:23 INFO     	 * Prefix: True
2023-12-22 23:13:23 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 23:13:24 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 23:24:03 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 23:34:12 INFO     	Bleu_1: 0.2852415931257784
2023-12-22 23:34:12 INFO     	Bleu_2: 0.16378241884906006
2023-12-22 23:34:12 INFO     	Bleu_3: 0.09730841199104874
2023-12-22 23:34:12 INFO     	Bleu_4: 0.06510688443729397
2023-12-22 23:34:13 INFO     	Bleu_1: 0.2671693004871895
2023-12-22 23:34:13 INFO     	Bleu_2: 0.14993089892629735
2023-12-22 23:34:13 INFO     	Bleu_3: 0.08659043613494109
2023-12-22 23:34:13 INFO     	Bleu_4: 0.05689976832950634
2023-12-22 23:34:18 INFO     use spaCy answer extraction model: positionrank
2023-12-22 23:34:19 INFO     Model `small_combined_trained_ckpt/model_mntyya/epoch_3`
2023-12-22 23:34:19 INFO     	 * Num of GPU in use: 1
2023-12-22 23:34:19 INFO     	 * Prefix: True
2023-12-22 23:34:19 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 23:34:20 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-22 23:46:01 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-22 23:56:37 INFO     	Bleu_1: 0.27047523660953837
2023-12-22 23:56:37 INFO     	Bleu_2: 0.1566562052110012
2023-12-22 23:56:37 INFO     	Bleu_3: 0.09340683642671496
2023-12-22 23:56:37 INFO     	Bleu_4: 0.06267675593152619
2023-12-22 23:56:38 INFO     	Bleu_1: 0.25380942756703484
2023-12-22 23:56:38 INFO     	Bleu_2: 0.14430100408086136
2023-12-22 23:56:38 INFO     	Bleu_3: 0.08404644100484578
2023-12-22 23:56:38 INFO     	Bleu_4: 0.05546001016624844
2023-12-22 23:56:43 INFO     use spaCy answer extraction model: positionrank
2023-12-22 23:56:44 INFO     Model `small_combined_trained_ckpt/model_mntyya/epoch_4`
2023-12-22 23:56:44 INFO     	 * Num of GPU in use: 1
2023-12-22 23:56:44 INFO     	 * Prefix: True
2023-12-22 23:56:44 INFO     	 * Language: en (ignore at the training phase)
2023-12-22 23:56:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-23 00:08:11 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-23 00:18:40 INFO     	Bleu_1: 0.27495483622578826
2023-12-23 00:18:40 INFO     	Bleu_2: 0.1595312952998006
2023-12-23 00:18:40 INFO     	Bleu_3: 0.09560584365638716
2023-12-23 00:18:40 INFO     	Bleu_4: 0.06421839963770096
2023-12-23 00:18:41 INFO     	Bleu_1: 0.2572649021676797
2023-12-23 00:18:41 INFO     	Bleu_2: 0.14719661966247877
2023-12-23 00:18:41 INFO     	Bleu_3: 0.08695294789219278
2023-12-23 00:18:41 INFO     	Bleu_4: 0.05786301939886399
2023-12-23 00:18:47 INFO     use spaCy answer extraction model: positionrank
2023-12-23 00:18:47 INFO     Model `small_combined_trained_ckpt/model_mntyya/epoch_5`
2023-12-23 00:18:47 INFO     	 * Num of GPU in use: 1
2023-12-23 00:18:47 INFO     	 * Prefix: True
2023-12-23 00:18:47 INFO     	 * Language: en (ignore at the training phase)
2023-12-23 00:18:48 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-23 00:30:40 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-23 00:41:33 INFO     	Bleu_1: 0.26318589477369286
2023-12-23 00:41:33 INFO     	Bleu_2: 0.15332017188817956
2023-12-23 00:41:33 INFO     	Bleu_3: 0.09205078506704734
2023-12-23 00:41:33 INFO     	Bleu_4: 0.061845116430448654
2023-12-23 00:41:34 INFO     	Bleu_1: 0.24441134969229106
2023-12-23 00:41:34 INFO     	Bleu_2: 0.14025769010061181
2023-12-23 00:41:34 INFO     	Bleu_3: 0.08286793686578578
2023-12-23 00:41:34 INFO     	Bleu_4: 0.05514614697106949
2023-12-23 00:41:42 INFO     use spaCy answer extraction model: positionrank
2023-12-23 00:41:42 INFO     Model `small_combined_trained_ckpt/model_mntyya/epoch_6`
2023-12-23 00:41:42 INFO     	 * Num of GPU in use: 1
2023-12-23 00:41:42 INFO     	 * Prefix: True
2023-12-23 00:41:42 INFO     	 * Language: en (ignore at the training phase)
2023-12-23 00:41:43 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-23 00:54:30 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-23 01:06:08 INFO     	Bleu_1: 0.23545299764965893
2023-12-23 01:06:08 INFO     	Bleu_2: 0.13752367950298577
2023-12-23 01:06:08 INFO     	Bleu_3: 0.0824743164440873
2023-12-23 01:06:08 INFO     	Bleu_4: 0.05521077046527048
2023-12-23 01:06:09 INFO     	Bleu_1: 0.218175757095507
2023-12-23 01:06:09 INFO     	Bleu_2: 0.1256103593532337
2023-12-23 01:06:09 INFO     	Bleu_3: 0.07397611531059071
2023-12-23 01:06:09 INFO     	Bleu_4: 0.04889328691030895
2023-12-23 01:06:15 INFO     use spaCy answer extraction model: positionrank
2023-12-23 01:06:15 INFO     Model `small_combined_trained_ckpt/model_mntyya/epoch_7`
2023-12-23 01:06:15 INFO     	 * Num of GPU in use: 1
2023-12-23 01:06:15 INFO     	 * Prefix: True
2023-12-23 01:06:15 INFO     	 * Language: en (ignore at the training phase)
2023-12-23 01:06:16 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-23 01:18:36 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-23 01:29:42 INFO     	Bleu_1: 0.2596468387975064
2023-12-23 01:29:43 INFO     	Bleu_2: 0.15245355506216274
2023-12-23 01:29:43 INFO     	Bleu_3: 0.09201407999103074
2023-12-23 01:29:43 INFO     	Bleu_4: 0.06186388552281749
2023-12-23 01:29:44 INFO     	Bleu_1: 0.23517466435711978
2023-12-23 01:29:44 INFO     	Bleu_2: 0.13602607720821944
2023-12-23 01:29:44 INFO     	Bleu_3: 0.08076385051530666
2023-12-23 01:29:44 INFO     	Bleu_4: 0.05374650325806385
2023-12-23 01:29:50 INFO     use spaCy answer extraction model: positionrank
2023-12-23 01:29:50 INFO     Model `small_combined_trained_ckpt/model_mntyya/epoch_8`
2023-12-23 01:29:50 INFO     	 * Num of GPU in use: 1
2023-12-23 01:29:50 INFO     	 * Prefix: True
2023-12-23 01:29:50 INFO     	 * Language: en (ignore at the training phase)
2023-12-23 01:29:51 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-23 01:43:17 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-23 01:55:32 INFO     	Bleu_1: 0.23200650715729956
2023-12-23 01:55:32 INFO     	Bleu_2: 0.13572729165718286
2023-12-23 01:55:32 INFO     	Bleu_3: 0.0812600881665195
2023-12-23 01:55:32 INFO     	Bleu_4: 0.054462286430271144
2023-12-23 01:55:33 INFO     	Bleu_1: 0.21713632496809812
2023-12-23 01:55:33 INFO     	Bleu_2: 0.12580240097701476
2023-12-23 01:55:33 INFO     	Bleu_3: 0.07494656764131068
2023-12-23 01:55:33 INFO     	Bleu_4: 0.05006427799136599
2023-12-23 01:55:39 INFO     use spaCy answer extraction model: positionrank
2023-12-23 01:55:39 INFO     Model `small_combined_trained_ckpt/model_mntyya/epoch_9`
2023-12-23 01:55:39 INFO     	 * Num of GPU in use: 1
2023-12-23 01:55:39 INFO     	 * Prefix: True
2023-12-23 01:55:39 INFO     	 * Language: en (ignore at the training phase)
2023-12-23 01:55:40 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.test.qag.pkl
2023-12-23 02:09:08 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/StellarMilk/squad_newsqat5-small.512.512.paragraph.questions_answers.validation.qag.pkl
2023-12-23 02:21:07 INFO     	Bleu_1: 0.23802210129712167
2023-12-23 02:21:07 INFO     	Bleu_2: 0.1396574360717807
2023-12-23 02:21:07 INFO     	Bleu_3: 0.08412491892630367
2023-12-23 02:21:07 INFO     	Bleu_4: 0.05638798879973754
2023-12-23 02:21:08 INFO     	Bleu_1: 0.21446676374738466
2023-12-23 02:21:08 INFO     	Bleu_2: 0.12426734406820268
2023-12-23 02:21:08 INFO     	Bleu_3: 0.07392872230297512
2023-12-23 02:21:08 INFO     	Bleu_4: 0.049151987300303644
2023-12-23 02:21:08 INFO     2nd RUN RESULTS: 
[('small_combined_trained_ckpt/model_oprhlh/epoch_3', 0.0658890416713825), ('small_combined_trained_ckpt/model_nrudfu/epoch_3', 0.0658890416713825), ('small_combined_trained_ckpt/model_nxaqhy/epoch_4', 0.06568944793598196), ('small_combined_trained_ckpt/model_vhyoja/epoch_4', 0.06568944793598196), ('small_combined_trained_ckpt/model_nxaqhy/epoch_5', 0.06546240893905916), ('small_combined_trained_ckpt/model_vhyoja/epoch_5', 0.06546240893905916), ('small_combined_trained_ckpt/model_oprhlh/epoch_4', 0.06511939306073454), ('small_combined_trained_ckpt/model_nrudfu/epoch_4', 0.06511939306073454), ('small_combined_trained_ckpt/model_mntyya/epoch_2', 0.06510688443729397), ('small_combined_trained_ckpt/model_oprhlh/epoch_2', 0.06490599052984948), ('small_combined_trained_ckpt/model_nrudfu/epoch_2', 0.06490599052984948), ('small_combined_trained_ckpt/model_nxaqhy/epoch_7', 0.06475279129345127), ('small_combined_trained_ckpt/model_vhyoja/epoch_7', 0.06475279129345127), ('small_combined_trained_ckpt/model_oprhlh/epoch_7', 0.06430236758943239), ('small_combined_trained_ckpt/model_nrudfu/epoch_7', 0.06430236758943239), ('small_combined_trained_ckpt/model_nxaqhy/epoch_6', 0.06427407639301298), ('small_combined_trained_ckpt/model_vhyoja/epoch_6', 0.06427407639301298), ('small_combined_trained_ckpt/model_mntyya/epoch_4', 0.06421839963770096), ('small_combined_trained_ckpt/model_nxaqhy/epoch_3', 0.06398849968007991), ('small_combined_trained_ckpt/model_vhyoja/epoch_3', 0.06398849968007991), ('small_combined_trained_ckpt/model_mntyya/epoch_1', 0.06353068982886656), ('small_combined_trained_ckpt/model_oprhlh/epoch_5', 0.06352080824340683), ('small_combined_trained_ckpt/model_nrudfu/epoch_5', 0.06352080824340683), ('small_combined_trained_ckpt/model_mntyya/epoch_3', 0.06267675593152619), ('small_combined_trained_ckpt/model_vhyoja/epoch_12', 0.0623871436785594), ('small_combined_trained_ckpt/model_oprhlh/epoch_6', 0.06213612953189983), ('small_combined_trained_ckpt/model_nrudfu/epoch_6', 0.06213612953189983), ('small_combined_trained_ckpt/model_mntyya/epoch_7', 0.06186388552281749), ('small_combined_trained_ckpt/model_mntyya/epoch_5', 0.061845116430448654), ('small_combined_trained_ckpt/model_mntyya/epoch_15', 0.06175774923574412), ('small_combined_trained_ckpt/model_vhyoja/epoch_11', 0.06148729359713629), ('small_combined_trained_ckpt/model_nxaqhy/epoch_9', 0.06115481977657837), ('small_combined_trained_ckpt/model_vhyoja/epoch_9', 0.06115481977657837), ('small_combined_trained_ckpt/model_nrudfu/epoch_12', 0.060639955676096584), ('small_combined_trained_ckpt/model_nxaqhy/epoch_10', 0.06025492648987384), ('small_combined_trained_ckpt/model_vhyoja/epoch_10', 0.06025492648987384), ('small_combined_trained_ckpt/model_vhyoja/epoch_13', 0.06009405826815223), ('small_combined_trained_ckpt/model_nrudfu/epoch_11', 0.06000687324666407), ('small_combined_trained_ckpt/model_oprhlh/epoch_9', 0.059874762955968754), ('small_combined_trained_ckpt/model_nrudfu/epoch_9', 0.059874762955968754), ('small_combined_trained_ckpt/model_mntyya/epoch_13', 0.05980602309020263), ('small_combined_trained_ckpt/model_vhyoja/epoch_15', 0.05979169169282299), ('small_combined_trained_ckpt/model_nxaqhy/epoch_8', 0.059578319677789225), ('small_combined_trained_ckpt/model_vhyoja/epoch_8', 0.059578319677789225), ('small_combined_trained_ckpt/model_nxaqhy/epoch_2', 0.05955642886549991), ('small_combined_trained_ckpt/model_vhyoja/epoch_2', 0.05955642886549991), ('small_combined_trained_ckpt/model_vhyoja/epoch_14', 0.059434938556747266), ('small_combined_trained_ckpt/model_oprhlh/epoch_10', 0.05941041226773688), ('small_combined_trained_ckpt/model_nrudfu/epoch_10', 0.05941041226773688), ('small_combined_trained_ckpt/model_mntyya/epoch_12', 0.059092158230739364), ('small_combined_trained_ckpt/model_nrudfu/epoch_15', 0.0590411661723125), ('small_combined_trained_ckpt/model_nrudfu/epoch_13', 0.05864510261656439), ('small_combined_trained_ckpt/model_nxaqhy/epoch_11', 0.05841520392840926), ('small_combined_trained_ckpt/model_nrudfu/epoch_14', 0.05831118881373479), ('small_combined_trained_ckpt/model_mntyya/epoch_14', 0.05799093435998523), ('small_combined_trained_ckpt/model_oprhlh/epoch_8', 0.057899567019315326), ('small_combined_trained_ckpt/model_nrudfu/epoch_8', 0.057899567019315326), ('small_combined_trained_ckpt/model_mntyya/epoch_10', 0.057894333626831364), ('small_combined_trained_ckpt/model_oprhlh/epoch_15', 0.05658417090379591), ('small_combined_trained_ckpt/model_mntyya/epoch_9', 0.05638798879973754), ('small_combined_trained_ckpt/model_oprhlh/epoch_1', 0.05596640873937455), ('small_combined_trained_ckpt/model_nrudfu/epoch_1', 0.05596640873937455), ('small_combined_trained_ckpt/model_oprhlh/epoch_12', 0.05571166645554474), ('small_combined_trained_ckpt/model_nxaqhy/epoch_14', 0.055553600298322424), ('small_combined_trained_ckpt/model_mntyya/epoch_6', 0.05521077046527048), ('small_combined_trained_ckpt/model_mntyya/epoch_11', 0.05514505596594025), ('small_combined_trained_ckpt/model_oprhlh/epoch_14', 0.054898906780224424), ('small_combined_trained_ckpt/model_nxaqhy/epoch_12', 0.05447110745799321), ('small_combined_trained_ckpt/model_mntyya/epoch_8', 0.054462286430271144), ('small_combined_trained_ckpt/model_nxaqhy/epoch_15', 0.05382221325614185), ('small_combined_trained_ckpt/model_oprhlh/epoch_13', 0.0536732981091381), ('small_combined_trained_ckpt/model_oprhlh/epoch_11', 0.05350557414184639), ('small_combined_trained_ckpt/model_nxaqhy/epoch_13', 0.05336419712936542), ('small_combined_trained_ckpt/model_nxaqhy/epoch_1', 0.0), ('small_combined_trained_ckpt/model_vhyoja/epoch_1', 0.0)]
2023-12-23 02:21:08 INFO     	 * rank: 0 | metric: 0.066 | model: small_combined_trained_ckpt/model_oprhlh/epoch_3 |
2023-12-23 02:21:08 INFO     	 * rank: 1 | metric: 0.066 | model: small_combined_trained_ckpt/model_nrudfu/epoch_3 |
2023-12-23 02:21:08 INFO     	 * rank: 2 | metric: 0.066 | model: small_combined_trained_ckpt/model_nxaqhy/epoch_4 |
2023-12-23 02:21:08 INFO     	 * rank: 3 | metric: 0.066 | model: small_combined_trained_ckpt/model_vhyoja/epoch_4 |
2023-12-23 02:21:08 INFO     	 * rank: 4 | metric: 0.065 | model: small_combined_trained_ckpt/model_nxaqhy/epoch_5 |
2023-12-23 02:21:08 INFO     	 * rank: 5 | metric: 0.065 | model: small_combined_trained_ckpt/model_vhyoja/epoch_5 |
2023-12-23 02:21:08 INFO     	 * rank: 6 | metric: 0.065 | model: small_combined_trained_ckpt/model_oprhlh/epoch_4 |
2023-12-23 02:21:08 INFO     	 * rank: 7 | metric: 0.065 | model: small_combined_trained_ckpt/model_nrudfu/epoch_4 |
2023-12-23 02:21:08 INFO     	 * rank: 8 | metric: 0.065 | model: small_combined_trained_ckpt/model_mntyya/epoch_2 |
2023-12-23 02:21:08 INFO     	 * rank: 9 | metric: 0.065 | model: small_combined_trained_ckpt/model_oprhlh/epoch_2 |
2023-12-23 02:21:08 INFO     	 * rank: 10 | metric: 0.065 | model: small_combined_trained_ckpt/model_nrudfu/epoch_2 |
2023-12-23 02:21:08 INFO     	 * rank: 11 | metric: 0.065 | model: small_combined_trained_ckpt/model_nxaqhy/epoch_7 |
2023-12-23 02:21:08 INFO     	 * rank: 12 | metric: 0.065 | model: small_combined_trained_ckpt/model_vhyoja/epoch_7 |
2023-12-23 02:21:08 INFO     	 * rank: 13 | metric: 0.064 | model: small_combined_trained_ckpt/model_oprhlh/epoch_7 |
2023-12-23 02:21:08 INFO     	 * rank: 14 | metric: 0.064 | model: small_combined_trained_ckpt/model_nrudfu/epoch_7 |
2023-12-23 02:21:08 INFO     	 * rank: 15 | metric: 0.064 | model: small_combined_trained_ckpt/model_nxaqhy/epoch_6 |
2023-12-23 02:21:08 INFO     	 * rank: 16 | metric: 0.064 | model: small_combined_trained_ckpt/model_vhyoja/epoch_6 |
2023-12-23 02:21:08 INFO     	 * rank: 17 | metric: 0.064 | model: small_combined_trained_ckpt/model_mntyya/epoch_4 |
2023-12-23 02:21:08 INFO     	 * rank: 18 | metric: 0.064 | model: small_combined_trained_ckpt/model_nxaqhy/epoch_3 |
2023-12-23 02:21:08 INFO     	 * rank: 19 | metric: 0.064 | model: small_combined_trained_ckpt/model_vhyoja/epoch_3 |
2023-12-23 02:21:08 INFO     	 * rank: 20 | metric: 0.064 | model: small_combined_trained_ckpt/model_mntyya/epoch_1 |
2023-12-23 02:21:08 INFO     	 * rank: 21 | metric: 0.064 | model: small_combined_trained_ckpt/model_oprhlh/epoch_5 |
2023-12-23 02:21:08 INFO     	 * rank: 22 | metric: 0.064 | model: small_combined_trained_ckpt/model_nrudfu/epoch_5 |
2023-12-23 02:21:08 INFO     	 * rank: 23 | metric: 0.063 | model: small_combined_trained_ckpt/model_mntyya/epoch_3 |
2023-12-23 02:21:08 INFO     	 * rank: 24 | metric: 0.062 | model: small_combined_trained_ckpt/model_vhyoja/epoch_12 |
2023-12-23 02:21:08 INFO     	 * rank: 25 | metric: 0.062 | model: small_combined_trained_ckpt/model_oprhlh/epoch_6 |
2023-12-23 02:21:08 INFO     	 * rank: 26 | metric: 0.062 | model: small_combined_trained_ckpt/model_nrudfu/epoch_6 |
2023-12-23 02:21:08 INFO     	 * rank: 27 | metric: 0.062 | model: small_combined_trained_ckpt/model_mntyya/epoch_7 |
2023-12-23 02:21:08 INFO     	 * rank: 28 | metric: 0.062 | model: small_combined_trained_ckpt/model_mntyya/epoch_5 |
2023-12-23 02:21:08 INFO     	 * rank: 29 | metric: 0.062 | model: small_combined_trained_ckpt/model_mntyya/epoch_15 |
2023-12-23 02:21:08 INFO     	 * rank: 30 | metric: 0.061 | model: small_combined_trained_ckpt/model_vhyoja/epoch_11 |
2023-12-23 02:21:08 INFO     	 * rank: 31 | metric: 0.061 | model: small_combined_trained_ckpt/model_nxaqhy/epoch_9 |
2023-12-23 02:21:08 INFO     	 * rank: 32 | metric: 0.061 | model: small_combined_trained_ckpt/model_vhyoja/epoch_9 |
2023-12-23 02:21:08 INFO     	 * rank: 33 | metric: 0.061 | model: small_combined_trained_ckpt/model_nrudfu/epoch_12 |
2023-12-23 02:21:08 INFO     	 * rank: 34 | metric: 0.06 | model: small_combined_trained_ckpt/model_nxaqhy/epoch_10 |
2023-12-23 02:21:08 INFO     	 * rank: 35 | metric: 0.06 | model: small_combined_trained_ckpt/model_vhyoja/epoch_10 |
2023-12-23 02:21:08 INFO     	 * rank: 36 | metric: 0.06 | model: small_combined_trained_ckpt/model_vhyoja/epoch_13 |
2023-12-23 02:21:08 INFO     	 * rank: 37 | metric: 0.06 | model: small_combined_trained_ckpt/model_nrudfu/epoch_11 |
2023-12-23 02:21:08 INFO     	 * rank: 38 | metric: 0.06 | model: small_combined_trained_ckpt/model_oprhlh/epoch_9 |
2023-12-23 02:21:08 INFO     	 * rank: 39 | metric: 0.06 | model: small_combined_trained_ckpt/model_nrudfu/epoch_9 |
2023-12-23 02:21:08 INFO     	 * rank: 40 | metric: 0.06 | model: small_combined_trained_ckpt/model_mntyya/epoch_13 |
2023-12-23 02:21:08 INFO     	 * rank: 41 | metric: 0.06 | model: small_combined_trained_ckpt/model_vhyoja/epoch_15 |
2023-12-23 02:21:08 INFO     	 * rank: 42 | metric: 0.06 | model: small_combined_trained_ckpt/model_nxaqhy/epoch_8 |
2023-12-23 02:21:08 INFO     	 * rank: 43 | metric: 0.06 | model: small_combined_trained_ckpt/model_vhyoja/epoch_8 |
2023-12-23 02:21:08 INFO     	 * rank: 44 | metric: 0.06 | model: small_combined_trained_ckpt/model_nxaqhy/epoch_2 |
2023-12-23 02:21:08 INFO     	 * rank: 45 | metric: 0.06 | model: small_combined_trained_ckpt/model_vhyoja/epoch_2 |
2023-12-23 02:21:08 INFO     	 * rank: 46 | metric: 0.059 | model: small_combined_trained_ckpt/model_vhyoja/epoch_14 |
2023-12-23 02:21:08 INFO     	 * rank: 47 | metric: 0.059 | model: small_combined_trained_ckpt/model_oprhlh/epoch_10 |
2023-12-23 02:21:08 INFO     	 * rank: 48 | metric: 0.059 | model: small_combined_trained_ckpt/model_nrudfu/epoch_10 |
2023-12-23 02:21:08 INFO     	 * rank: 49 | metric: 0.059 | model: small_combined_trained_ckpt/model_mntyya/epoch_12 |
2023-12-23 02:21:08 INFO     	 * rank: 50 | metric: 0.059 | model: small_combined_trained_ckpt/model_nrudfu/epoch_15 |
2023-12-23 02:21:08 INFO     	 * rank: 51 | metric: 0.059 | model: small_combined_trained_ckpt/model_nrudfu/epoch_13 |
2023-12-23 02:21:08 INFO     	 * rank: 52 | metric: 0.058 | model: small_combined_trained_ckpt/model_nxaqhy/epoch_11 |
2023-12-23 02:21:08 INFO     	 * rank: 53 | metric: 0.058 | model: small_combined_trained_ckpt/model_nrudfu/epoch_14 |
2023-12-23 02:21:08 INFO     	 * rank: 54 | metric: 0.058 | model: small_combined_trained_ckpt/model_mntyya/epoch_14 |
2023-12-23 02:21:08 INFO     	 * rank: 55 | metric: 0.058 | model: small_combined_trained_ckpt/model_oprhlh/epoch_8 |
2023-12-23 02:21:08 INFO     	 * rank: 56 | metric: 0.058 | model: small_combined_trained_ckpt/model_nrudfu/epoch_8 |
2023-12-23 02:21:08 INFO     	 * rank: 57 | metric: 0.058 | model: small_combined_trained_ckpt/model_mntyya/epoch_10 |
2023-12-23 02:21:08 INFO     	 * rank: 58 | metric: 0.057 | model: small_combined_trained_ckpt/model_oprhlh/epoch_15 |
2023-12-23 02:21:08 INFO     	 * rank: 59 | metric: 0.056 | model: small_combined_trained_ckpt/model_mntyya/epoch_9 |
2023-12-23 02:21:08 INFO     	 * rank: 60 | metric: 0.056 | model: small_combined_trained_ckpt/model_oprhlh/epoch_1 |
2023-12-23 02:21:08 INFO     	 * rank: 61 | metric: 0.056 | model: small_combined_trained_ckpt/model_nrudfu/epoch_1 |
2023-12-23 02:21:08 INFO     	 * rank: 62 | metric: 0.056 | model: small_combined_trained_ckpt/model_oprhlh/epoch_12 |
2023-12-23 02:21:08 INFO     	 * rank: 63 | metric: 0.056 | model: small_combined_trained_ckpt/model_nxaqhy/epoch_14 |
2023-12-23 02:21:08 INFO     	 * rank: 64 | metric: 0.055 | model: small_combined_trained_ckpt/model_mntyya/epoch_6 |
2023-12-23 02:21:08 INFO     	 * rank: 65 | metric: 0.055 | model: small_combined_trained_ckpt/model_mntyya/epoch_11 |
2023-12-23 02:21:08 INFO     	 * rank: 66 | metric: 0.055 | model: small_combined_trained_ckpt/model_oprhlh/epoch_14 |
2023-12-23 02:21:08 INFO     	 * rank: 67 | metric: 0.054 | model: small_combined_trained_ckpt/model_nxaqhy/epoch_12 |
2023-12-23 02:21:08 INFO     	 * rank: 68 | metric: 0.054 | model: small_combined_trained_ckpt/model_mntyya/epoch_8 |
2023-12-23 02:21:08 INFO     	 * rank: 69 | metric: 0.054 | model: small_combined_trained_ckpt/model_nxaqhy/epoch_15 |
2023-12-23 02:21:08 INFO     	 * rank: 70 | metric: 0.054 | model: small_combined_trained_ckpt/model_oprhlh/epoch_13 |
2023-12-23 02:21:08 INFO     	 * rank: 71 | metric: 0.054 | model: small_combined_trained_ckpt/model_oprhlh/epoch_11 |
2023-12-23 02:21:08 INFO     	 * rank: 72 | metric: 0.053 | model: small_combined_trained_ckpt/model_nxaqhy/epoch_13 |
2023-12-23 02:21:08 INFO     	 * rank: 73 | metric: 0.0 | model: small_combined_trained_ckpt/model_nxaqhy/epoch_1 |
2023-12-23 02:21:08 INFO     	 * rank: 74 | metric: 0.0 | model: small_combined_trained_ckpt/model_vhyoja/epoch_1 |
2023-12-23 02:21:08 INFO     creating small_combined_trained_ckpt/best_model
2023-12-23 02:21:08 INFO     copying small_combined_trained_ckpt/model_oprhlh/epoch_3/config.json -> small_combined_trained_ckpt/best_model
2023-12-23 02:21:08 INFO     copying small_combined_trained_ckpt/model_oprhlh/epoch_3/generation_config.json -> small_combined_trained_ckpt/best_model
2023-12-23 02:21:08 INFO     copying small_combined_trained_ckpt/model_oprhlh/epoch_3/pytorch_model.bin -> small_combined_trained_ckpt/best_model
2023-12-23 02:21:09 INFO     copying small_combined_trained_ckpt/model_oprhlh/epoch_3/tokenizer_config.json -> small_combined_trained_ckpt/best_model
2023-12-23 02:21:09 INFO     copying small_combined_trained_ckpt/model_oprhlh/epoch_3/special_tokens_map.json -> small_combined_trained_ckpt/best_model
2023-12-23 02:21:09 INFO     copying small_combined_trained_ckpt/model_oprhlh/epoch_3/added_tokens.json -> small_combined_trained_ckpt/best_model
2023-12-23 02:21:09 INFO     copying small_combined_trained_ckpt/model_oprhlh/epoch_3/spiece.model -> small_combined_trained_ckpt/best_model
2023-12-23 02:21:09 INFO     copying small_combined_trained_ckpt/model_oprhlh/epoch_3/tokenizer.json -> small_combined_trained_ckpt/best_model
2023-12-23 02:21:09 INFO     copying small_combined_trained_ckpt/model_oprhlh/epoch_3/trainer_config.json -> small_combined_trained_ckpt/best_model
2023-12-23 02:21:09 INFO     creating small_combined_trained_ckpt/best_model/eval
2023-12-23 02:21:09 INFO     copying small_combined_trained_ckpt/model_oprhlh/epoch_3/eval/samples.test.hyp.paragraph.questions_answers.StellarMilk_squad_newsqa.default.txt -> small_combined_trained_ckpt/best_model/eval
2023-12-23 02:21:09 INFO     copying small_combined_trained_ckpt/model_oprhlh/epoch_3/eval/samples.validation.hyp.paragraph.questions_answers.StellarMilk_squad_newsqa.default.txt -> small_combined_trained_ckpt/best_model/eval
2023-12-23 02:21:09 INFO     copying small_combined_trained_ckpt/model_oprhlh/epoch_3/eval/metric.first.answer.paragraph.questions_answers.StellarMilk_squad_newsqa.default.json -> small_combined_trained_ckpt/best_model/eval
