ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
xformers 0.0.23 requires torch==2.1.1, but you have torch 1.13.0 which is incompatible.
torchvision 0.16.2 requires torch==2.1.2, but you have torch 1.13.0 which is incompatible.
torchtext 0.16.1 requires torch==2.1.1, but you have torch 1.13.0 which is incompatible.
torchdata 0.7.1 requires torch>=2, but you have torch 1.13.0 which is incompatible.
torchaudio 2.1.2 requires torch==2.1.2, but you have torch 1.13.0 which is incompatible.
auto-gptq 0.5.1 requires transformers>=4.31.0, but you have transformers 4.26.1 which is incompatible.
2024-01-05 10:03:21 INFO     INITIALIZE GRID SEARCHER: 12 configs to try
2024-01-05 10:03:21 INFO     ## 1st RUN: Configuration 0/12 ##
2024-01-05 10:03:21 INFO     initialize model trainer
2024-01-05 10:03:21 INFO     initialize checkpoint at small_recreated_ckpt/model_efnljo
2024-01-05 10:03:21 INFO     hyperparameters
2024-01-05 10:03:21 INFO     	 * dataset_path: lmqg/qag_squad
2024-01-05 10:03:21 INFO     	 * dataset_name: default
2024-01-05 10:03:21 INFO     	 * input_types: ['paragraph']
2024-01-05 10:03:21 INFO     	 * output_types: ['questions_answers']
2024-01-05 10:03:21 INFO     	 * prefix_types: ['qag']
2024-01-05 10:03:21 INFO     	 * model: t5-small
2024-01-05 10:03:21 INFO     	 * max_length: 512
2024-01-05 10:03:21 INFO     	 * max_length_output: 256
2024-01-05 10:03:21 INFO     	 * epoch: 15
2024-01-05 10:03:21 INFO     	 * batch: 2
2024-01-05 10:03:21 INFO     	 * lr: 0.0001
2024-01-05 10:03:21 INFO     	 * fp16: False
2024-01-05 10:03:21 INFO     	 * random_seed: 1
2024-01-05 10:03:21 INFO     	 * gradient_accumulation_steps: 4
2024-01-05 10:03:21 INFO     	 * label_smoothing: 0.15
2024-01-05 10:03:21 INFO     initialize checkpoint with t5-small
2024-01-05 10:03:31 INFO     use spaCy answer extraction model: positionrank
2024-01-05 10:03:52 INFO     Model `t5-small`
2024-01-05 10:03:52 INFO     	 * Num of GPU in use: 1
2024-01-05 10:03:52 INFO     	 * Prefix: True
2024-01-05 10:03:52 INFO     	 * Language: en (ignore at the training phase)
2024-01-05 10:03:52 INFO     dataset preprocessing
/home2/g.torresgamez/.local/lib/python3.10/site-packages/datasets/load.py:2088: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
2024-01-05 10:03:56 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featurelmqg/qag_squad/t5-small.512.256.paragraph.questions_answers.train.qag.pkl
2024-01-05 10:04:02 INFO     start model training
2024-01-05 10:04:33 INFO     	 * (global step 50: loss: 1.4955462515354156, lr: 0.0001
2024-01-05 10:04:51 INFO     	 * (global step 100: loss: 1.1167991906404495, lr: 0.0001
2024-01-05 10:05:11 INFO     	 * (global step 150: loss: 0.9872274398803711, lr: 0.0001
2024-01-05 10:05:29 INFO     	 * (global step 200: loss: 0.7949968427419662, lr: 0.0001
2024-01-05 10:05:47 INFO     	 * (global step 250: loss: 0.7567170113325119, lr: 0.0001
2024-01-05 10:06:08 INFO     	 * (global step 300: loss: 0.7695069760084152, lr: 0.0001
2024-01-05 10:06:26 INFO     	 * (global step 350: loss: 0.8666670024394989, lr: 0.0001
2024-01-05 10:06:43 INFO     	 * (global step 400: loss: 1.0058775693178177, lr: 0.0001
2024-01-05 10:06:57 INFO     	 * (global step 450: loss: 0.7224209010601044, lr: 0.0001
2024-01-05 10:07:09 INFO     	 * (global step 500: loss: 0.8913671523332596, lr: 0.0001
2024-01-05 10:07:28 INFO     	 * (global step 550: loss: 0.8170754462480545, lr: 0.0001
2024-01-05 10:07:47 INFO     	 * (global step 600: loss: 0.9849455952644348, lr: 0.0001
2024-01-05 10:08:05 INFO     	 * (global step 650: loss: 0.7450027316808701, lr: 0.0001
2024-01-05 10:08:25 INFO     	 * (global step 700: loss: 0.6529485806822777, lr: 0.0001
2024-01-05 10:08:43 INFO     	 * (global step 750: loss: 0.6669048368930817, lr: 0.0001
2024-01-05 10:09:01 INFO     	 * (global step 800: loss: 0.5557146966457367, lr: 0.0001
2024-01-05 10:09:22 INFO     	 * (global step 850: loss: 0.7133287489414215, lr: 0.0001
2024-01-05 10:09:40 INFO     	 * (global step 900: loss: 0.7699460685253143, lr: 0.0001
2024-01-05 10:09:59 INFO     	 * (global step 950: loss: 0.606502640992403, lr: 0.0001
2024-01-05 10:10:20 INFO     	 * (global step 1000: loss: 1.0782562792301178, lr: 0.0001
2024-01-05 10:10:37 INFO     	 * (global step 1050: loss: 0.5924564749002457, lr: 0.0001
2024-01-05 10:10:55 INFO     	 * (global step 1100: loss: 0.6975871920585632, lr: 0.0001
2024-01-05 10:11:14 INFO     	 * (global step 1150: loss: 1.0991872400045395, lr: 0.0001
2024-01-05 10:11:31 INFO     	 * (global step 1200: loss: 0.5771647170186043, lr: 0.0001
2024-01-05 10:11:52 INFO     	 * (global step 1250: loss: 0.5625322610139847, lr: 0.0001
2024-01-05 10:12:06 INFO     	 * (global step 1300: loss: 0.49254054576158524, lr: 0.0001
2024-01-05 10:12:18 INFO     	 * (global step 1350: loss: 0.6944154351949692, lr: 0.0001
2024-01-05 10:12:34 INFO     	 * (global step 1400: loss: 0.7001536637544632, lr: 0.0001
2024-01-05 10:12:50 INFO     	 * (global step 1450: loss: 0.7616716101765633, lr: 0.0001
2024-01-05 10:13:07 INFO     	 * (global step 1500: loss: 0.6485500335693359, lr: 0.0001
2024-01-05 10:13:25 INFO     	 * (global step 1550: loss: 0.7057115137577057, lr: 0.0001
2024-01-05 10:13:43 INFO     	 * (global step 1600: loss: 0.7205837368965149, lr: 0.0001
2024-01-05 10:14:01 INFO     	 * (global step 1650: loss: 0.6137843802571297, lr: 0.0001
2024-01-05 10:14:17 INFO     	 * (global step 1700: loss: 0.6724001541733742, lr: 0.0001
2024-01-05 10:14:35 INFO     	 * (global step 1750: loss: 0.5811005458235741, lr: 0.0001
2024-01-05 10:14:51 INFO     	 * (global step 1800: loss: 0.6039937138557434, lr: 0.0001
2024-01-05 10:15:08 INFO     	 * (global step 1850: loss: 0.42758382111787796, lr: 0.0001
2024-01-05 10:15:27 INFO     	 * (global step 1900: loss: 0.4973658099770546, lr: 0.0001
2024-01-05 10:15:44 INFO     	 * (global step 1950: loss: 0.7655559629201889, lr: 0.0001
2024-01-05 10:16:01 INFO     	 * (global step 2000: loss: 0.8832087516784668, lr: 0.0001
2024-01-05 10:16:09 INFO     [epoch 0/15] average loss: 0.756, lr: 0.0001
2024-01-05 10:16:09 INFO     saving model related files
2024-01-05 10:16:09 INFO     saving model
2024-01-05 10:16:11 INFO     saving tokenizer
2024-01-05 10:16:11 INFO     saving optimizer
2024-01-05 10:16:13 INFO     remove old optimizer files
2024-01-05 10:16:24 INFO     	 * (global step 2050: loss: 0.48383356258273125, lr: 0.0001
2024-01-05 10:16:40 INFO     	 * (global step 2100: loss: 0.5991823077201843, lr: 0.0001
2024-01-05 10:16:57 INFO     	 * (global step 2150: loss: 0.7605701759457588, lr: 0.0001
2024-01-05 10:17:14 INFO     	 * (global step 2200: loss: 0.5370596647262573, lr: 0.0001
2024-01-05 10:17:26 INFO     	 * (global step 2250: loss: 0.5413005948066711, lr: 0.0001
2024-01-05 10:17:41 INFO     	 * (global step 2300: loss: 0.6818302199244499, lr: 0.0001
2024-01-05 10:18:01 INFO     	 * (global step 2350: loss: 0.6597145274281502, lr: 0.0001
2024-01-05 10:18:17 INFO     	 * (global step 2400: loss: 0.6029577329754829, lr: 0.0001
2024-01-05 10:18:36 INFO     	 * (global step 2450: loss: 0.5625817403197289, lr: 0.0001
2024-01-05 10:18:55 INFO     	 * (global step 2500: loss: 0.7014346271753311, lr: 0.0001
2024-01-05 10:19:13 INFO     	 * (global step 2550: loss: 0.5347775518894196, lr: 0.0001
2024-01-05 10:19:31 INFO     	 * (global step 2600: loss: 0.5813846439123154, lr: 0.0001
2024-01-05 10:19:48 INFO     	 * (global step 2650: loss: 0.7186765372753143, lr: 0.0001
2024-01-05 10:20:07 INFO     	 * (global step 2700: loss: 0.5090348720550537, lr: 0.0001
2024-01-05 10:20:24 INFO     	 * (global step 2750: loss: 0.5747836381196976, lr: 0.0001
2024-01-05 10:20:43 INFO     	 * (global step 2800: loss: 0.5952723771333694, lr: 0.0001
2024-01-05 10:21:02 INFO     	 * (global step 2850: loss: 0.6800964847207069, lr: 0.0001
2024-01-05 10:21:19 INFO     	 * (global step 2900: loss: 0.7318496853113174, lr: 0.0001
2024-01-05 10:21:37 INFO     	 * (global step 2950: loss: 0.6865103915333748, lr: 0.0001
2024-01-05 10:21:56 INFO     	 * (global step 3000: loss: 0.601239487528801, lr: 0.0001
2024-01-05 10:22:12 INFO     	 * (global step 3050: loss: 0.5731271877884865, lr: 0.0001
2024-01-05 10:22:28 INFO     	 * (global step 3100: loss: 0.5117603838443756, lr: 0.0001
2024-01-05 10:22:42 INFO     	 * (global step 3150: loss: 0.6665432900190353, lr: 0.0001
2024-01-05 10:22:54 INFO     	 * (global step 3200: loss: 0.506366603076458, lr: 0.0001
2024-01-05 10:23:11 INFO     	 * (global step 3250: loss: 0.7026764824986458, lr: 0.0001
2024-01-05 10:23:31 INFO     	 * (global step 3300: loss: 0.502581275999546, lr: 0.0001
2024-01-05 10:23:47 INFO     	 * (global step 3350: loss: 0.599525973200798, lr: 0.0001
2024-01-05 10:24:04 INFO     	 * (global step 3400: loss: 0.6905371993780136, lr: 0.0001
2024-01-05 10:24:24 INFO     	 * (global step 3450: loss: 0.6483494937419891, lr: 0.0001
2024-01-05 10:24:42 INFO     	 * (global step 3500: loss: 0.5972741544246674, lr: 0.0001
2024-01-05 10:25:00 INFO     	 * (global step 3550: loss: 0.5717227160930634, lr: 0.0001
2024-01-05 10:25:18 INFO     	 * (global step 3600: loss: 0.6140751540660858, lr: 0.0001
2024-01-05 10:25:38 INFO     	 * (global step 3650: loss: 0.7285308763384819, lr: 0.0001
2024-01-05 10:25:56 INFO     	 * (global step 3700: loss: 0.903793141245842, lr: 0.0001
2024-01-05 10:26:13 INFO     	 * (global step 3750: loss: 0.5883577913045883, lr: 0.0001
2024-01-05 10:26:34 INFO     	 * (global step 3800: loss: 0.544867642223835, lr: 0.0001
2024-01-05 10:26:50 INFO     	 * (global step 3850: loss: 0.8280017673969269, lr: 0.0001
2024-01-05 10:27:09 INFO     	 * (global step 3900: loss: 0.5622287318110466, lr: 0.0001
2024-01-05 10:27:27 INFO     	 * (global step 3950: loss: 0.4537043198943138, lr: 0.0001
2024-01-05 10:27:45 INFO     	 * (global step 4000: loss: 0.5218100249767303, lr: 0.0001
2024-01-05 10:28:03 INFO     [epoch 1/15] average loss: 0.613, lr: 0.0001
2024-01-05 10:28:03 INFO     saving model related files
2024-01-05 10:28:03 INFO     saving model
2024-01-05 10:28:05 INFO     saving tokenizer
2024-01-05 10:28:05 INFO     saving optimizer
2024-01-05 10:28:07 INFO     remove old optimizer files
2024-01-05 10:28:08 INFO     	 * (global step 4050: loss: 0.4244029000401497, lr: 0.0001
2024-01-05 10:28:26 INFO     	 * (global step 4100: loss: 0.5515774190425873, lr: 0.0001
2024-01-05 10:28:41 INFO     	 * (global step 4150: loss: 0.5551953464746475, lr: 0.0001
2024-01-05 10:28:53 INFO     	 * (global step 4200: loss: 0.7788235619664192, lr: 0.0001
2024-01-05 10:29:09 INFO     	 * (global step 4250: loss: 0.5896280854940414, lr: 0.0001
2024-01-05 10:29:27 INFO     	 * (global step 4300: loss: 0.436215840280056, lr: 0.0001
2024-01-05 10:29:45 INFO     	 * (global step 4350: loss: 0.6102551370859146, lr: 0.0001
2024-01-05 10:30:02 INFO     	 * (global step 4400: loss: 0.4497310593724251, lr: 0.0001
2024-01-05 10:30:21 INFO     	 * (global step 4450: loss: 0.48040473461151123, lr: 0.0001
2024-01-05 10:30:38 INFO     	 * (global step 4500: loss: 0.5777470618486404, lr: 0.0001
2024-01-05 10:30:56 INFO     	 * (global step 4550: loss: 0.5441614463925362, lr: 0.0001
2024-01-05 10:31:16 INFO     	 * (global step 4600: loss: 0.5100801587104797, lr: 0.0001
2024-01-05 10:31:32 INFO     	 * (global step 4650: loss: 0.7740804776549339, lr: 0.0001
2024-01-05 10:31:51 INFO     	 * (global step 4700: loss: 0.6086262166500092, lr: 0.0001
2024-01-05 10:32:09 INFO     	 * (global step 4750: loss: 0.5281629487872124, lr: 0.0001
2024-01-05 10:32:27 INFO     	 * (global step 4800: loss: 0.5610864087939262, lr: 0.0001
2024-01-05 10:32:44 INFO     	 * (global step 4850: loss: 0.5884587615728378, lr: 0.0001
2024-01-05 10:33:01 INFO     	 * (global step 4900: loss: 0.48436594009399414, lr: 0.0001
2024-01-05 10:33:20 INFO     	 * (global step 4950: loss: 0.4653683081269264, lr: 0.0001
2024-01-05 10:33:36 INFO     	 * (global step 5000: loss: 0.5057509914040565, lr: 0.0001
2024-01-05 10:33:55 INFO     	 * (global step 5050: loss: 0.6946709752082825, lr: 0.0001
2024-01-05 10:34:11 INFO     	 * (global step 5100: loss: 0.5176359266042709, lr: 0.0001
2024-01-05 10:34:23 INFO     	 * (global step 5150: loss: 0.62281683832407, lr: 0.0001
2024-01-05 10:34:36 INFO     	 * (global step 5200: loss: 0.5537361800670624, lr: 0.0001
2024-01-05 10:34:55 INFO     	 * (global step 5250: loss: 0.5500413402915001, lr: 0.0001
2024-01-05 10:35:12 INFO     	 * (global step 5300: loss: 0.5168954953551292, lr: 0.0001
2024-01-05 10:35:30 INFO     	 * (global step 5350: loss: 0.6020682454109192, lr: 0.0001
2024-01-05 10:35:50 INFO     	 * (global step 5400: loss: 0.6002035662531853, lr: 0.0001
2024-01-05 10:36:06 INFO     	 * (global step 5450: loss: 0.5306218937039375, lr: 0.0001
2024-01-05 10:36:25 INFO     	 * (global step 5500: loss: 0.4829062819480896, lr: 0.0001
2024-01-05 10:36:44 INFO     	 * (global step 5550: loss: 0.6051541790366173, lr: 0.0001
2024-01-05 10:37:01 INFO     	 * (global step 5600: loss: 0.6394712850451469, lr: 0.0001
2024-01-05 10:37:19 INFO     	 * (global step 5650: loss: 0.5172268748283386, lr: 0.0001
2024-01-05 10:37:37 INFO     	 * (global step 5700: loss: 0.5948804244399071, lr: 0.0001
2024-01-05 10:37:55 INFO     	 * (global step 5750: loss: 0.5748758167028427, lr: 0.0001
2024-01-05 10:38:13 INFO     	 * (global step 5800: loss: 0.5725243538618088, lr: 0.0001
2024-01-05 10:38:30 INFO     	 * (global step 5850: loss: 0.6729178726673126, lr: 0.0001
2024-01-05 10:38:49 INFO     	 * (global step 5900: loss: 0.5643344447016716, lr: 0.0001
2024-01-05 10:39:05 INFO     	 * (global step 5950: loss: 0.5078850761055946, lr: 0.0001
2024-01-05 10:39:23 INFO     	 * (global step 6000: loss: 0.5091547034680843, lr: 0.0001
2024-01-05 10:39:38 INFO     	 * (global step 6050: loss: 0.7067169025540352, lr: 0.0001
2024-01-05 10:39:43 INFO     [epoch 2/15] average loss: 0.58, lr: 0.0001
2024-01-05 10:39:43 INFO     saving model related files
2024-01-05 10:39:43 INFO     saving model
2024-01-05 10:39:44 INFO     saving tokenizer
2024-01-05 10:39:44 INFO     saving optimizer
2024-01-05 10:39:45 INFO     remove old optimizer files
2024-01-05 10:39:53 INFO     	 * (global step 6100: loss: 0.58934336155653, lr: 0.0001
2024-01-05 10:40:08 INFO     	 * (global step 6150: loss: 0.5370837301015854, lr: 0.0001
2024-01-05 10:40:27 INFO     	 * (global step 6200: loss: 0.6186140924692154, lr: 0.0001
2024-01-05 10:40:43 INFO     	 * (global step 6250: loss: 0.6054520383477211, lr: 0.0001
2024-01-05 10:41:01 INFO     	 * (global step 6300: loss: 0.4603010416030884, lr: 0.0001
2024-01-05 10:41:21 INFO     	 * (global step 6350: loss: 0.7544330954551697, lr: 0.0001
2024-01-05 10:41:36 INFO     	 * (global step 6400: loss: 0.5834188312292099, lr: 0.0001
2024-01-05 10:41:53 INFO     	 * (global step 6450: loss: 0.671795204281807, lr: 0.0001
2024-01-05 10:42:13 INFO     	 * (global step 6500: loss: 0.5947812125086784, lr: 0.0001
2024-01-05 10:42:29 INFO     	 * (global step 6550: loss: 0.4457644894719124, lr: 0.0001
2024-01-05 10:42:47 INFO     	 * (global step 6600: loss: 0.46473222970962524, lr: 0.0001
2024-01-05 10:43:04 INFO     	 * (global step 6650: loss: 0.44008657336235046, lr: 0.0001
2024-01-05 10:43:23 INFO     	 * (global step 6700: loss: 0.5274462103843689, lr: 0.0001
2024-01-05 10:43:39 INFO     	 * (global step 6750: loss: 0.5474156886339188, lr: 0.0001
2024-01-05 10:43:58 INFO     	 * (global step 6800: loss: 0.6241913139820099, lr: 0.0001
2024-01-05 10:44:17 INFO     	 * (global step 6850: loss: 0.48057037591934204, lr: 0.0001
2024-01-05 10:44:33 INFO     	 * (global step 6900: loss: 0.4921932741999626, lr: 0.0001
2024-01-05 10:44:47 INFO     	 * (global step 6950: loss: 0.5016979202628136, lr: 0.0001
2024-01-05 10:45:02 INFO     	 * (global step 7000: loss: 0.40479981154203415, lr: 0.0001
2024-01-05 10:45:17 INFO     	 * (global step 7050: loss: 0.6345546320080757, lr: 0.0001
2024-01-05 10:45:34 INFO     	 * (global step 7100: loss: 0.5715430900454521, lr: 0.0001
2024-01-05 10:45:54 INFO     	 * (global step 7150: loss: 0.48361460492014885, lr: 0.0001
2024-01-05 10:46:12 INFO     	 * (global step 7200: loss: 0.5673552379012108, lr: 0.0001
2024-01-05 10:46:30 INFO     	 * (global step 7250: loss: 0.5720323696732521, lr: 0.0001
2024-01-05 10:46:50 INFO     	 * (global step 7300: loss: 0.48879706114530563, lr: 0.0001
2024-01-05 10:47:07 INFO     	 * (global step 7350: loss: 0.5657302737236023, lr: 0.0001
2024-01-05 10:47:26 INFO     	 * (global step 7400: loss: 0.48712490499019623, lr: 0.0001
2024-01-05 10:47:44 INFO     	 * (global step 7450: loss: 0.49383482336997986, lr: 0.0001
2024-01-05 10:48:04 INFO     	 * (global step 7500: loss: 0.5818496160209179, lr: 0.0001
2024-01-05 10:48:22 INFO     	 * (global step 7550: loss: 0.6605261564254761, lr: 0.0001
2024-01-05 10:48:41 INFO     	 * (global step 7600: loss: 0.5007145032286644, lr: 0.0001
2024-01-05 10:49:02 INFO     	 * (global step 7650: loss: 0.53778425604105, lr: 0.0001
2024-01-05 10:49:19 INFO     	 * (global step 7700: loss: 0.4589865505695343, lr: 0.0001
2024-01-05 10:49:37 INFO     	 * (global step 7750: loss: 0.45527036115527153, lr: 0.0001
2024-01-05 10:49:56 INFO     	 * (global step 7800: loss: 0.4968734085559845, lr: 0.0001
2024-01-05 10:50:14 INFO     	 * (global step 7850: loss: 0.601158507168293, lr: 0.0001
2024-01-05 10:50:31 INFO     	 * (global step 7900: loss: 0.47984492406249046, lr: 0.0001
2024-01-05 10:50:49 INFO     	 * (global step 7950: loss: 0.6047621071338654, lr: 0.0001
2024-01-05 10:51:04 INFO     	 * (global step 8000: loss: 0.5773551166057587, lr: 0.0001
2024-01-05 10:51:17 INFO     	 * (global step 8050: loss: 0.6313177347183228, lr: 0.0001
2024-01-05 10:51:29 INFO     [epoch 3/15] average loss: 0.557, lr: 0.0001
2024-01-05 10:51:29 INFO     saving model related files
2024-01-05 10:51:29 INFO     saving model
2024-01-05 10:51:30 INFO     saving tokenizer
2024-01-05 10:51:31 INFO     saving optimizer
2024-01-05 10:51:33 INFO     remove old optimizer files
2024-01-05 10:51:35 INFO     	 * (global step 8100: loss: 0.5966724306344986, lr: 0.0001
2024-01-05 10:51:53 INFO     	 * (global step 8150: loss: 0.5838007107377052, lr: 0.0001
2024-01-05 10:52:10 INFO     	 * (global step 8200: loss: 0.38839009031653404, lr: 0.0001
2024-01-05 10:52:28 INFO     	 * (global step 8250: loss: 0.501910462975502, lr: 0.0001
2024-01-05 10:52:48 INFO     	 * (global step 8300: loss: 0.49083012342453003, lr: 0.0001
2024-01-05 10:53:04 INFO     	 * (global step 8350: loss: 0.673408530652523, lr: 0.0001
2024-01-05 10:53:22 INFO     	 * (global step 8400: loss: 0.5774637833237648, lr: 0.0001
2024-01-05 10:53:42 INFO     	 * (global step 8450: loss: 0.549015685915947, lr: 0.0001
2024-01-05 10:53:58 INFO     	 * (global step 8500: loss: 0.6107925772666931, lr: 0.0001
2024-01-05 10:54:15 INFO     	 * (global step 8550: loss: 0.39146314561367035, lr: 0.0001
2024-01-05 10:54:34 INFO     	 * (global step 8600: loss: 0.39218585938215256, lr: 0.0001
2024-01-05 10:54:52 INFO     	 * (global step 8650: loss: 0.5274000763893127, lr: 0.0001
2024-01-05 10:55:09 INFO     	 * (global step 8700: loss: 0.6649462431669235, lr: 0.0001
2024-01-05 10:55:25 INFO     	 * (global step 8750: loss: 0.4662552997469902, lr: 0.0001
2024-01-05 10:55:44 INFO     	 * (global step 8800: loss: 0.6145815774798393, lr: 0.0001
2024-01-05 10:56:00 INFO     	 * (global step 8850: loss: 0.5993986278772354, lr: 0.0001
2024-01-05 10:56:15 INFO     	 * (global step 8900: loss: 0.5239043608307838, lr: 0.0001
2024-01-05 10:56:29 INFO     	 * (global step 8950: loss: 0.5464335232973099, lr: 0.0001
2024-01-05 10:56:43 INFO     	 * (global step 9000: loss: 0.5230527594685555, lr: 0.0001
2024-01-05 10:56:59 INFO     	 * (global step 9050: loss: 0.5115008279681206, lr: 0.0001
2024-01-05 10:57:18 INFO     	 * (global step 9100: loss: 0.4732683524489403, lr: 0.0001
2024-01-05 10:57:36 INFO     	 * (global step 9150: loss: 0.34026137739419937, lr: 0.0001
2024-01-05 10:57:54 INFO     	 * (global step 9200: loss: 0.6707894206047058, lr: 0.0001
2024-01-05 10:58:12 INFO     	 * (global step 9250: loss: 0.43180064857006073, lr: 0.0001
2024-01-05 10:58:28 INFO     	 * (global step 9300: loss: 0.5296154841780663, lr: 0.0001
2024-01-05 10:58:45 INFO     	 * (global step 9350: loss: 0.5494293868541718, lr: 0.0001
2024-01-05 10:59:03 INFO     	 * (global step 9400: loss: 0.672790601849556, lr: 0.0001
2024-01-05 10:59:21 INFO     	 * (global step 9450: loss: 0.5448258966207504, lr: 0.0001
2024-01-05 10:59:38 INFO     	 * (global step 9500: loss: 0.5142890587449074, lr: 0.0001
2024-01-05 10:59:55 INFO     	 * (global step 9550: loss: 0.5235175788402557, lr: 0.0001
2024-01-05 11:00:14 INFO     	 * (global step 9600: loss: 0.44264473766088486, lr: 0.0001
2024-01-05 11:00:31 INFO     	 * (global step 9650: loss: 0.457332506775856, lr: 0.0001
2024-01-05 11:00:48 INFO     	 * (global step 9700: loss: 0.562262587249279, lr: 0.0001
2024-01-05 11:01:07 INFO     	 * (global step 9750: loss: 0.4762308821082115, lr: 0.0001
2024-01-05 11:01:23 INFO     	 * (global step 9800: loss: 0.5548513531684875, lr: 0.0001
2024-01-05 11:01:38 INFO     	 * (global step 9850: loss: 0.4031623676419258, lr: 0.0001
2024-01-05 11:01:52 INFO     	 * (global step 9900: loss: 0.49038032442331314, lr: 0.0001
2024-01-05 11:02:07 INFO     	 * (global step 9950: loss: 0.4865825027227402, lr: 0.0001
2024-01-05 11:02:24 INFO     	 * (global step 10000: loss: 0.46627277135849, lr: 0.0001
2024-01-05 11:02:43 INFO     	 * (global step 10050: loss: 0.5438554137945175, lr: 0.0001
2024-01-05 11:02:59 INFO     	 * (global step 10100: loss: 0.46640339493751526, lr: 0.0001
2024-01-05 11:03:07 INFO     [epoch 4/15] average loss: 0.54, lr: 0.0001
2024-01-05 11:03:07 INFO     saving model related files
2024-01-05 11:03:07 INFO     saving model
2024-01-05 11:03:08 INFO     saving tokenizer
2024-01-05 11:03:08 INFO     saving optimizer
2024-01-05 11:03:11 INFO     remove old optimizer files
2024-01-05 11:03:21 INFO     	 * (global step 10150: loss: 0.4105495885014534, lr: 0.0001
2024-01-05 11:03:40 INFO     	 * (global step 10200: loss: 0.46779511123895645, lr: 0.0001
2024-01-05 11:03:57 INFO     	 * (global step 10250: loss: 0.39775293320417404, lr: 0.0001
2024-01-05 11:04:13 INFO     	 * (global step 10300: loss: 0.4633867219090462, lr: 0.0001
2024-01-05 11:04:31 INFO     	 * (global step 10350: loss: 0.6072700023651123, lr: 0.0001
2024-01-05 11:04:49 INFO     	 * (global step 10400: loss: 0.5499310418963432, lr: 0.0001
2024-01-05 11:05:07 INFO     	 * (global step 10450: loss: 0.43664440512657166, lr: 0.0001
2024-01-05 11:05:23 INFO     	 * (global step 10500: loss: 0.47967173904180527, lr: 0.0001
2024-01-05 11:05:43 INFO     	 * (global step 10550: loss: 0.4832189604640007, lr: 0.0001
2024-01-05 11:05:58 INFO     	 * (global step 10600: loss: 0.5463766828179359, lr: 0.0001
2024-01-05 11:06:15 INFO     	 * (global step 10650: loss: 0.5807871147990227, lr: 0.0001
2024-01-05 11:06:34 INFO     	 * (global step 10700: loss: 0.499735489487648, lr: 0.0001
2024-01-05 11:06:50 INFO     	 * (global step 10750: loss: 0.3845597803592682, lr: 0.0001
2024-01-05 11:07:03 INFO     	 * (global step 10800: loss: 0.5210908874869347, lr: 0.0001
2024-01-05 11:07:16 INFO     	 * (global step 10850: loss: 0.5116683393716812, lr: 0.0001
2024-01-05 11:07:31 INFO     	 * (global step 10900: loss: 0.5932618379592896, lr: 0.0001
2024-01-05 11:07:48 INFO     	 * (global step 10950: loss: 0.5455011576414108, lr: 0.0001
2024-01-05 11:08:07 INFO     	 * (global step 11000: loss: 0.4692825600504875, lr: 0.0001
2024-01-05 11:08:22 INFO     	 * (global step 11050: loss: 0.4917216897010803, lr: 0.0001
2024-01-05 11:08:40 INFO     	 * (global step 11100: loss: 0.4848748929798603, lr: 0.0001
2024-01-05 11:08:58 INFO     	 * (global step 11150: loss: 0.4690546542406082, lr: 0.0001
2024-01-05 11:09:15 INFO     	 * (global step 11200: loss: 0.48420120775699615, lr: 0.0001
2024-01-05 11:09:32 INFO     	 * (global step 11250: loss: 0.6483311951160431, lr: 0.0001
2024-01-05 11:09:49 INFO     	 * (global step 11300: loss: 0.49281519651412964, lr: 0.0001
2024-01-05 11:10:08 INFO     	 * (global step 11350: loss: 0.4764911010861397, lr: 0.0001
2024-01-05 11:10:24 INFO     	 * (global step 11400: loss: 0.5021575093269348, lr: 0.0001
2024-01-05 11:10:42 INFO     	 * (global step 11450: loss: 0.38396643847227097, lr: 0.0001
2024-01-05 11:11:02 INFO     	 * (global step 11500: loss: 0.5004323199391365, lr: 0.0001
2024-01-05 11:11:17 INFO     	 * (global step 11550: loss: 0.5527226328849792, lr: 0.0001
2024-01-05 11:11:34 INFO     	 * (global step 11600: loss: 0.4492727965116501, lr: 0.0001
2024-01-05 11:11:53 INFO     	 * (global step 11650: loss: 0.3167554810643196, lr: 0.0001
2024-01-05 11:12:10 INFO     	 * (global step 11700: loss: 0.47321099042892456, lr: 0.0001
2024-01-05 11:12:23 INFO     	 * (global step 11750: loss: 0.5670232027769089, lr: 0.0001
2024-01-05 11:12:37 INFO     	 * (global step 11800: loss: 0.565028190612793, lr: 0.0001
2024-01-05 11:12:53 INFO     	 * (global step 11850: loss: 0.5910851545631886, lr: 0.0001
2024-01-05 11:13:09 INFO     	 * (global step 11900: loss: 0.40740544348955154, lr: 0.0001
2024-01-05 11:13:29 INFO     	 * (global step 11950: loss: 0.4332578256726265, lr: 0.0001
2024-01-05 11:13:46 INFO     	 * (global step 12000: loss: 0.42880725115537643, lr: 0.0001
2024-01-05 11:14:05 INFO     	 * (global step 12050: loss: 0.41512126475572586, lr: 0.0001
2024-01-05 11:14:23 INFO     	 * (global step 12100: loss: 0.49462641030550003, lr: 0.0001
2024-01-05 11:14:37 INFO     [epoch 5/15] average loss: 0.526, lr: 0.0001
2024-01-05 11:14:37 INFO     saving model related files
2024-01-05 11:14:37 INFO     saving model
2024-01-05 11:14:38 INFO     saving tokenizer
2024-01-05 11:14:38 INFO     saving optimizer
2024-01-05 11:14:41 INFO     remove old optimizer files
2024-01-05 11:14:43 INFO     	 * (global step 12150: loss: 0.4589686542749405, lr: 0.0001
2024-01-05 11:15:01 INFO     	 * (global step 12200: loss: 0.5046711713075638, lr: 0.0001
2024-01-05 11:15:19 INFO     	 * (global step 12250: loss: 0.5998752787709236, lr: 0.0001
2024-01-05 11:15:34 INFO     	 * (global step 12300: loss: 0.619462139904499, lr: 0.0001
2024-01-05 11:15:52 INFO     	 * (global step 12350: loss: 0.4375237599015236, lr: 0.0001
2024-01-05 11:16:11 INFO     	 * (global step 12400: loss: 0.549061544239521, lr: 0.0001
2024-01-05 11:16:28 INFO     	 * (global step 12450: loss: 0.6560651063919067, lr: 0.0001
2024-01-05 11:16:46 INFO     	 * (global step 12500: loss: 0.40178706124424934, lr: 0.0001
2024-01-05 11:17:04 INFO     	 * (global step 12550: loss: 0.5004288479685783, lr: 0.0001
2024-01-05 11:17:22 INFO     	 * (global step 12600: loss: 0.6261713355779648, lr: 0.0001
2024-01-05 11:17:39 INFO     	 * (global step 12650: loss: 0.6092318519949913, lr: 0.0001
2024-01-05 11:17:55 INFO     	 * (global step 12700: loss: 0.5516335517168045, lr: 0.0001
2024-01-05 11:18:08 INFO     	 * (global step 12750: loss: 0.5312933400273323, lr: 0.0001
2024-01-05 11:18:22 INFO     	 * (global step 12800: loss: 0.7628758028149605, lr: 0.0001
2024-01-05 11:18:40 INFO     	 * (global step 12850: loss: 0.5284901484847069, lr: 0.0001
2024-01-05 11:18:58 INFO     	 * (global step 12900: loss: 0.5214658081531525, lr: 0.0001
2024-01-05 11:19:14 INFO     	 * (global step 12950: loss: 0.5270924717187881, lr: 0.0001
2024-01-05 11:19:32 INFO     	 * (global step 13000: loss: 0.43528544157743454, lr: 0.0001
2024-01-05 11:19:49 INFO     	 * (global step 13050: loss: 0.5146926492452621, lr: 0.0001
2024-01-05 11:20:06 INFO     	 * (global step 13100: loss: 0.510573536157608, lr: 0.0001
2024-01-05 11:20:24 INFO     	 * (global step 13150: loss: 0.5587791427969933, lr: 0.0001
2024-01-05 11:20:42 INFO     	 * (global step 13200: loss: 0.4647675231099129, lr: 0.0001
2024-01-05 11:20:57 INFO     	 * (global step 13250: loss: 0.557974137365818, lr: 0.0001
2024-01-05 11:21:14 INFO     	 * (global step 13300: loss: 0.5108836218714714, lr: 0.0001
2024-01-05 11:21:33 INFO     	 * (global step 13350: loss: 0.6556730419397354, lr: 0.0001
2024-01-05 11:21:49 INFO     	 * (global step 13400: loss: 0.5596410855650902, lr: 0.0001
2024-01-05 11:22:06 INFO     	 * (global step 13450: loss: 0.5427503138780594, lr: 0.0001
2024-01-05 11:22:26 INFO     	 * (global step 13500: loss: 0.5279445052146912, lr: 0.0001
2024-01-05 11:22:39 INFO     	 * (global step 13550: loss: 0.42112261056900024, lr: 0.0001
2024-01-05 11:22:52 INFO     	 * (global step 13600: loss: 0.3915918506681919, lr: 0.0001
2024-01-05 11:23:06 INFO     	 * (global step 13650: loss: 0.4954230263829231, lr: 0.0001
2024-01-05 11:23:23 INFO     	 * (global step 13700: loss: 0.6094599664211273, lr: 0.0001
2024-01-05 11:23:40 INFO     	 * (global step 13750: loss: 0.48597538471221924, lr: 0.0001
2024-01-05 11:23:59 INFO     	 * (global step 13800: loss: 0.4105406478047371, lr: 0.0001
2024-01-05 11:24:15 INFO     	 * (global step 13850: loss: 0.5552844107151031, lr: 0.0001
2024-01-05 11:24:32 INFO     	 * (global step 13900: loss: 0.47094614058732986, lr: 0.0001
2024-01-05 11:24:51 INFO     	 * (global step 13950: loss: 0.502746157348156, lr: 0.0001
2024-01-05 11:25:06 INFO     	 * (global step 14000: loss: 0.56135593354702, lr: 0.0001
2024-01-05 11:25:23 INFO     	 * (global step 14050: loss: 0.5845224857330322, lr: 0.0001
2024-01-05 11:25:42 INFO     	 * (global step 14100: loss: 0.5195747911930084, lr: 0.0001
2024-01-05 11:25:58 INFO     	 * (global step 14150: loss: 0.5595905929803848, lr: 0.0001
2024-01-05 11:26:04 INFO     [epoch 6/15] average loss: 0.513, lr: 0.0001
2024-01-05 11:26:04 INFO     saving model related files
2024-01-05 11:26:04 INFO     saving model
2024-01-05 11:26:05 INFO     saving tokenizer
2024-01-05 11:26:05 INFO     saving optimizer
2024-01-05 11:26:08 INFO     remove old optimizer files
2024-01-05 11:26:18 INFO     	 * (global step 14200: loss: 0.49086906760931015, lr: 0.0001
2024-01-05 11:26:37 INFO     	 * (global step 14250: loss: 0.5263861268758774, lr: 0.0001
2024-01-05 11:26:53 INFO     	 * (global step 14300: loss: 0.5120199024677277, lr: 0.0001
2024-01-05 11:27:10 INFO     	 * (global step 14350: loss: 0.5828988403081894, lr: 0.0001
2024-01-05 11:27:26 INFO     	 * (global step 14400: loss: 0.46183454990386963, lr: 0.0001
2024-01-05 11:27:38 INFO     	 * (global step 14450: loss: 0.540866419672966, lr: 0.0001
2024-01-05 11:27:55 INFO     	 * (global step 14500: loss: 0.5100715756416321, lr: 0.0001
2024-01-05 11:28:16 INFO     	 * (global step 14550: loss: 0.4694499373435974, lr: 0.0001
2024-01-05 11:28:35 INFO     	 * (global step 14600: loss: 0.4317501224577427, lr: 0.0001
2024-01-05 11:28:56 INFO     	 * (global step 14650: loss: 0.4587354362010956, lr: 0.0001
2024-01-05 11:29:16 INFO     	 * (global step 14700: loss: 0.4748501628637314, lr: 0.0001
2024-01-05 11:29:37 INFO     	 * (global step 14750: loss: 0.36915434151887894, lr: 0.0001
2024-01-05 11:29:58 INFO     	 * (global step 14800: loss: 0.46061474084854126, lr: 0.0001
2024-01-05 11:30:18 INFO     	 * (global step 14850: loss: 0.46084268391132355, lr: 0.0001
2024-01-05 11:30:38 INFO     	 * (global step 14900: loss: 0.5685327425599098, lr: 0.0001
2024-01-05 11:30:57 INFO     	 * (global step 14950: loss: 0.55806964635849, lr: 0.0001
2024-01-05 11:31:17 INFO     	 * (global step 15000: loss: 0.5841346010565758, lr: 0.0001
2024-01-05 11:31:37 INFO     	 * (global step 15050: loss: 0.4883330911397934, lr: 0.0001
2024-01-05 11:31:58 INFO     	 * (global step 15100: loss: 0.5283453613519669, lr: 0.0001
2024-01-05 11:32:17 INFO     	 * (global step 15150: loss: 0.7935894578695297, lr: 0.0001
2024-01-05 11:32:36 INFO     	 * (global step 15200: loss: 0.5182494595646858, lr: 0.0001
2024-01-05 11:32:56 INFO     	 * (global step 15250: loss: 0.4074845090508461, lr: 0.0001
2024-01-05 11:33:11 INFO     	 * (global step 15300: loss: 0.5335349068045616, lr: 0.0001
2024-01-05 11:33:29 INFO     	 * (global step 15350: loss: 0.5163246393203735, lr: 0.0001
2024-01-05 11:33:47 INFO     	 * (global step 15400: loss: 0.43224573135375977, lr: 0.0001
2024-01-05 11:34:01 INFO     	 * (global step 15450: loss: 0.5930051803588867, lr: 0.0001
2024-01-05 11:34:12 INFO     	 * (global step 15500: loss: 0.5289953723549843, lr: 0.0001
2024-01-05 11:34:30 INFO     	 * (global step 15550: loss: 0.45099780708551407, lr: 0.0001
2024-01-05 11:34:47 INFO     	 * (global step 15600: loss: 0.5078125, lr: 0.0001
2024-01-05 11:35:03 INFO     	 * (global step 15650: loss: 0.5724261552095413, lr: 0.0001
2024-01-05 11:35:21 INFO     	 * (global step 15700: loss: 0.5826005712151527, lr: 0.0001
2024-01-05 11:35:38 INFO     	 * (global step 15750: loss: 0.42310071364045143, lr: 0.0001
2024-01-05 11:35:55 INFO     	 * (global step 15800: loss: 0.4678405188024044, lr: 0.0001
2024-01-05 11:36:13 INFO     	 * (global step 15850: loss: 0.6370821744203568, lr: 0.0001
2024-01-05 11:36:31 INFO     	 * (global step 15900: loss: 0.42140059173107147, lr: 0.0001
2024-01-05 11:36:47 INFO     	 * (global step 15950: loss: 0.42949777096509933, lr: 0.0001
2024-01-05 11:37:05 INFO     	 * (global step 16000: loss: 0.5374988466501236, lr: 0.0001
2024-01-05 11:37:23 INFO     	 * (global step 16050: loss: 0.5086580365896225, lr: 0.0001
2024-01-05 11:37:39 INFO     	 * (global step 16100: loss: 0.47166192159056664, lr: 0.0001
2024-01-05 11:37:57 INFO     	 * (global step 16150: loss: 0.6001443192362785, lr: 0.0001
2024-01-05 11:38:13 INFO     [epoch 7/15] average loss: 0.502, lr: 0.0001
2024-01-05 11:38:13 INFO     saving model related files
2024-01-05 11:38:13 INFO     saving model
2024-01-05 11:38:14 INFO     saving tokenizer
2024-01-05 11:38:14 INFO     saving optimizer
2024-01-05 11:38:16 INFO     remove old optimizer files
2024-01-05 11:38:18 INFO     	 * (global step 16200: loss: 0.5022148638963699, lr: 0.0001
2024-01-05 11:38:31 INFO     	 * (global step 16250: loss: 0.457267127931118, lr: 0.0001
2024-01-05 11:38:44 INFO     	 * (global step 16300: loss: 0.4795990586280823, lr: 0.0001
2024-01-05 11:38:59 INFO     	 * (global step 16350: loss: 0.44742175936698914, lr: 0.0001
2024-01-05 11:39:15 INFO     	 * (global step 16400: loss: 0.5076960399746895, lr: 0.0001
2024-01-05 11:39:35 INFO     	 * (global step 16450: loss: 0.591132864356041, lr: 0.0001
2024-01-05 11:39:51 INFO     	 * (global step 16500: loss: 0.3960685357451439, lr: 0.0001
2024-01-05 11:40:08 INFO     	 * (global step 16550: loss: 0.5763152539730072, lr: 0.0001
2024-01-05 11:40:28 INFO     	 * (global step 16600: loss: 0.5118898451328278, lr: 0.0001
2024-01-05 11:40:44 INFO     	 * (global step 16650: loss: 0.38414373621344566, lr: 0.0001
2024-01-05 11:41:01 INFO     	 * (global step 16700: loss: 0.5257851779460907, lr: 0.0001
2024-01-05 11:41:21 INFO     	 * (global step 16750: loss: 0.5565710589289665, lr: 0.0001
2024-01-05 11:41:38 INFO     	 * (global step 16800: loss: 0.7101994082331657, lr: 0.0001
2024-01-05 11:41:55 INFO     	 * (global step 16850: loss: 0.4039333499968052, lr: 0.0001
2024-01-05 11:42:14 INFO     	 * (global step 16900: loss: 0.5164197087287903, lr: 0.0001
2024-01-05 11:42:30 INFO     	 * (global step 16950: loss: 0.4223092310130596, lr: 0.0001
2024-01-05 11:42:47 INFO     	 * (global step 17000: loss: 0.5581961423158646, lr: 0.0001
2024-01-05 11:43:02 INFO     	 * (global step 17050: loss: 0.45794619619846344, lr: 0.0001
2024-01-05 11:43:15 INFO     	 * (global step 17100: loss: 0.4419848471879959, lr: 0.0001
2024-01-05 11:43:30 INFO     	 * (global step 17150: loss: 0.3140944093465805, lr: 0.0001
2024-01-05 11:43:50 INFO     	 * (global step 17200: loss: 0.3101065494120121, lr: 0.0001
2024-01-05 11:44:07 INFO     	 * (global step 17250: loss: 0.4154040962457657, lr: 0.0001
2024-01-05 11:44:24 INFO     	 * (global step 17300: loss: 0.48075101524591446, lr: 0.0001
2024-01-05 11:44:41 INFO     	 * (global step 17350: loss: 0.5125697329640388, lr: 0.0001
2024-01-05 11:44:58 INFO     	 * (global step 17400: loss: 0.37434985488653183, lr: 0.0001
2024-01-05 11:45:11 INFO     	 * (global step 17450: loss: 0.4906386509537697, lr: 0.0001
2024-01-05 11:45:24 INFO     	 * (global step 17500: loss: 0.5031690299510956, lr: 0.0001
2024-01-05 11:45:37 INFO     	 * (global step 17550: loss: 0.48585106804966927, lr: 0.0001
2024-01-05 11:45:51 INFO     	 * (global step 17600: loss: 0.5252267494797707, lr: 0.0001
2024-01-05 11:46:04 INFO     	 * (global step 17650: loss: 0.4540872946381569, lr: 0.0001
2024-01-05 11:46:17 INFO     	 * (global step 17700: loss: 0.37095415592193604, lr: 0.0001
2024-01-05 11:46:31 INFO     	 * (global step 17750: loss: 0.4270860552787781, lr: 0.0001
2024-01-05 11:46:44 INFO     	 * (global step 17800: loss: 0.4240192770957947, lr: 0.0001
2024-01-05 11:46:57 INFO     	 * (global step 17850: loss: 0.5158713161945343, lr: 0.0001
2024-01-05 11:47:10 INFO     	 * (global step 17900: loss: 0.439183734357357, lr: 0.0001
2024-01-05 11:47:21 INFO     	 * (global step 17950: loss: 0.5227502509951591, lr: 0.0001
2024-01-05 11:47:32 INFO     	 * (global step 18000: loss: 0.41366907209157944, lr: 0.0001
2024-01-05 11:47:47 INFO     	 * (global step 18050: loss: 0.3753909505903721, lr: 0.0001
2024-01-05 11:48:01 INFO     	 * (global step 18100: loss: 0.5573665052652359, lr: 0.0001
2024-01-05 11:48:15 INFO     	 * (global step 18150: loss: 0.4185846373438835, lr: 0.0001
2024-01-05 11:48:29 INFO     	 * (global step 18200: loss: 0.47487764805555344, lr: 0.0001
2024-01-05 11:48:34 INFO     [epoch 8/15] average loss: 0.491, lr: 0.0001
2024-01-05 11:48:34 INFO     saving model related files
2024-01-05 11:48:34 INFO     saving model
2024-01-05 11:48:35 INFO     saving tokenizer
2024-01-05 11:48:35 INFO     saving optimizer
2024-01-05 11:48:36 INFO     remove old optimizer files
2024-01-05 11:48:45 INFO     	 * (global step 18250: loss: 0.5083728656172752, lr: 0.0001
2024-01-05 11:48:58 INFO     	 * (global step 18300: loss: 0.4326314106583595, lr: 0.0001
2024-01-05 11:49:14 INFO     	 * (global step 18350: loss: 0.5082734003663063, lr: 0.0001
2024-01-05 11:49:27 INFO     	 * (global step 18400: loss: 0.4503106474876404, lr: 0.0001
2024-01-05 11:49:42 INFO     	 * (global step 18450: loss: 0.5625951439142227, lr: 0.0001
2024-01-05 11:49:55 INFO     	 * (global step 18500: loss: 0.4769355058670044, lr: 0.0001
2024-01-05 11:50:08 INFO     	 * (global step 18550: loss: 0.45416159927845, lr: 0.0001
2024-01-05 11:50:21 INFO     	 * (global step 18600: loss: 0.508095033466816, lr: 0.0001
2024-01-05 11:50:32 INFO     	 * (global step 18650: loss: 0.44266290962696075, lr: 0.0001
2024-01-05 11:50:43 INFO     	 * (global step 18700: loss: 0.6542918309569359, lr: 0.0001
2024-01-05 11:50:55 INFO     	 * (global step 18750: loss: 0.6313309222459793, lr: 0.0001
2024-01-05 11:51:09 INFO     	 * (global step 18800: loss: 0.4673551097512245, lr: 0.0001
2024-01-05 11:51:23 INFO     	 * (global step 18850: loss: 0.47272835671901703, lr: 0.0001
2024-01-05 11:51:36 INFO     	 * (global step 18900: loss: 0.37911556661129, lr: 0.0001
2024-01-05 11:51:50 INFO     	 * (global step 18950: loss: 0.40212415903806686, lr: 0.0001
2024-01-05 11:52:03 INFO     	 * (global step 19000: loss: 0.45854760706424713, lr: 0.0001
2024-01-05 11:52:16 INFO     	 * (global step 19050: loss: 0.48147744685411453, lr: 0.0001
2024-01-05 11:52:31 INFO     	 * (global step 19100: loss: 0.5157676078379154, lr: 0.0001
2024-01-05 11:52:46 INFO     	 * (global step 19150: loss: 0.4437032490968704, lr: 0.0001
2024-01-05 11:52:59 INFO     	 * (global step 19200: loss: 0.5836548879742622, lr: 0.0001
2024-01-05 11:53:12 INFO     	 * (global step 19250: loss: 0.42202015966176987, lr: 0.0001
2024-01-05 11:53:25 INFO     	 * (global step 19300: loss: 0.3479374088346958, lr: 0.0001
2024-01-05 11:53:36 INFO     	 * (global step 19350: loss: 0.49864669144153595, lr: 0.0001
2024-01-05 11:53:47 INFO     	 * (global step 19400: loss: 0.4247259795665741, lr: 0.0001
2024-01-05 11:53:59 INFO     	 * (global step 19450: loss: 0.40509653836488724, lr: 0.0001
2024-01-05 11:54:13 INFO     	 * (global step 19500: loss: 0.4452081546187401, lr: 0.0001
2024-01-05 11:54:26 INFO     	 * (global step 19550: loss: 0.4420243948698044, lr: 0.0001
2024-01-05 11:54:41 INFO     	 * (global step 19600: loss: 0.48745013028383255, lr: 0.0001
2024-01-05 11:54:56 INFO     	 * (global step 19650: loss: 0.43126221001148224, lr: 0.0001
2024-01-05 11:55:11 INFO     	 * (global step 19700: loss: 0.37988152354955673, lr: 0.0001
2024-01-05 11:55:26 INFO     	 * (global step 19750: loss: 0.6182368770241737, lr: 0.0001
2024-01-05 11:55:39 INFO     	 * (global step 19800: loss: 0.6305873021483421, lr: 0.0001
2024-01-05 11:55:53 INFO     	 * (global step 19850: loss: 0.4764581769704819, lr: 0.0001
2024-01-05 11:56:06 INFO     	 * (global step 19900: loss: 0.6541268303990364, lr: 0.0001
2024-01-05 11:56:21 INFO     	 * (global step 19950: loss: 0.4037722647190094, lr: 0.0001
2024-01-05 11:56:35 INFO     	 * (global step 20000: loss: 0.5667109936475754, lr: 0.0001
2024-01-05 11:56:46 INFO     	 * (global step 20050: loss: 0.4647979512810707, lr: 0.0001
2024-01-05 11:56:57 INFO     	 * (global step 20100: loss: 0.48997417092323303, lr: 0.0001
2024-01-05 11:57:09 INFO     	 * (global step 20150: loss: 0.37754178792238235, lr: 0.0001
2024-01-05 11:57:22 INFO     	 * (global step 20200: loss: 0.4290109910070896, lr: 0.0001
2024-01-05 11:57:34 INFO     [epoch 9/15] average loss: 0.482, lr: 0.0001
2024-01-05 11:57:34 INFO     saving model related files
2024-01-05 11:57:34 INFO     saving model
2024-01-05 11:57:34 INFO     saving tokenizer
2024-01-05 11:57:34 INFO     saving optimizer
2024-01-05 11:57:36 INFO     remove old optimizer files
2024-01-05 11:57:36 INFO     complete training: model ckpt was saved at small_recreated_ckpt/model_efnljo
2024-01-05 11:57:36 INFO     ## 1st RUN: Configuration 1/12 ##
2024-01-05 11:57:36 INFO     initialize model trainer
2024-01-05 11:57:36 INFO     initialize checkpoint at small_recreated_ckpt/model_eszyci
2024-01-05 11:57:36 INFO     hyperparameters
2024-01-05 11:57:36 INFO     	 * dataset_path: lmqg/qag_squad
2024-01-05 11:57:36 INFO     	 * dataset_name: default
2024-01-05 11:57:36 INFO     	 * input_types: ['paragraph']
2024-01-05 11:57:36 INFO     	 * output_types: ['questions_answers']
2024-01-05 11:57:36 INFO     	 * prefix_types: ['qag']
2024-01-05 11:57:36 INFO     	 * model: t5-small
2024-01-05 11:57:36 INFO     	 * max_length: 512
2024-01-05 11:57:36 INFO     	 * max_length_output: 256
2024-01-05 11:57:36 INFO     	 * epoch: 15
2024-01-05 11:57:36 INFO     	 * batch: 2
2024-01-05 11:57:36 INFO     	 * lr: 0.0001
2024-01-05 11:57:36 INFO     	 * fp16: False
2024-01-05 11:57:36 INFO     	 * random_seed: 1
2024-01-05 11:57:36 INFO     	 * gradient_accumulation_steps: 2
2024-01-05 11:57:36 INFO     	 * label_smoothing: 0.15
2024-01-05 11:57:36 INFO     initialize checkpoint with t5-small
2024-01-05 11:57:42 INFO     use spaCy answer extraction model: positionrank
2024-01-05 11:57:43 INFO     Model `t5-small`
2024-01-05 11:57:43 INFO     	 * Num of GPU in use: 1
2024-01-05 11:57:43 INFO     	 * Prefix: True
2024-01-05 11:57:43 INFO     	 * Language: en (ignore at the training phase)
2024-01-05 11:57:43 INFO     dataset preprocessing
2024-01-05 11:57:46 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featurelmqg/qag_squad/t5-small.512.256.paragraph.questions_answers.train.qag.pkl
2024-01-05 11:57:49 INFO     start model training
2024-01-05 11:57:56 INFO     	 * (global step 50: loss: 1.303089290857315, lr: 0.0001
2024-01-05 11:58:03 INFO     	 * (global step 100: loss: 1.1686377823352814, lr: 0.0001
2024-01-05 11:58:10 INFO     	 * (global step 150: loss: 0.6262632608413696, lr: 0.0001
2024-01-05 11:58:17 INFO     	 * (global step 200: loss: 1.103747010231018, lr: 0.0001
2024-01-05 11:58:25 INFO     	 * (global step 250: loss: 0.7705890238285065, lr: 0.0001
2024-01-05 11:58:32 INFO     	 * (global step 300: loss: 0.5720459073781967, lr: 0.0001
2024-01-05 11:58:39 INFO     	 * (global step 350: loss: 0.8380811214447021, lr: 0.0001
2024-01-05 11:58:46 INFO     	 * (global step 400: loss: 0.7204990386962891, lr: 0.0001
2024-01-05 11:58:54 INFO     	 * (global step 450: loss: 0.6205849945545197, lr: 0.0001
2024-01-05 11:59:01 INFO     	 * (global step 500: loss: 0.6517484486103058, lr: 0.0001
2024-01-05 11:59:08 INFO     	 * (global step 550: loss: 0.7729726433753967, lr: 0.0001
2024-01-05 11:59:15 INFO     	 * (global step 600: loss: 0.8552120327949524, lr: 0.0001
2024-01-05 11:59:22 INFO     	 * (global step 650: loss: 0.6944482028484344, lr: 0.0001
2024-01-05 11:59:29 INFO     	 * (global step 700: loss: 0.848553329706192, lr: 0.0001
2024-01-05 11:59:36 INFO     	 * (global step 750: loss: 0.8828501999378204, lr: 0.0001
2024-01-05 11:59:44 INFO     	 * (global step 800: loss: 1.0164539515972137, lr: 0.0001
2024-01-05 11:59:51 INFO     	 * (global step 850: loss: 0.8513102829456329, lr: 0.0001
2024-01-05 11:59:58 INFO     	 * (global step 900: loss: 0.6346902847290039, lr: 0.0001
2024-01-05 12:00:05 INFO     	 * (global step 950: loss: 0.54396191239357, lr: 0.0001
2024-01-05 12:00:12 INFO     	 * (global step 1000: loss: 0.7343804240226746, lr: 0.0001
2024-01-05 12:00:18 INFO     	 * (global step 1050: loss: 0.6326505243778229, lr: 0.0001
2024-01-05 12:00:23 INFO     	 * (global step 1100: loss: 0.5333124697208405, lr: 0.0001
2024-01-05 12:00:29 INFO     	 * (global step 1150: loss: 0.6590790450572968, lr: 0.0001
2024-01-05 12:00:35 INFO     	 * (global step 1200: loss: 1.1636273264884949, lr: 0.0001
2024-01-05 12:00:42 INFO     	 * (global step 1250: loss: 0.7640368044376373, lr: 0.0001
2024-01-05 12:00:49 INFO     	 * (global step 1300: loss: 0.6479207575321198, lr: 0.0001
2024-01-05 12:00:56 INFO     	 * (global step 1350: loss: 0.8581481873989105, lr: 0.0001
2024-01-05 12:01:03 INFO     	 * (global step 1400: loss: 0.4654606282711029, lr: 0.0001
2024-01-05 12:01:09 INFO     	 * (global step 1450: loss: 0.5559161007404327, lr: 0.0001
2024-01-05 12:01:17 INFO     	 * (global step 1500: loss: 0.651173323392868, lr: 0.0001
2024-01-05 12:01:24 INFO     	 * (global step 1550: loss: 0.8425848484039307, lr: 0.0001
2024-01-05 12:01:31 INFO     	 * (global step 1600: loss: 0.5258052349090576, lr: 0.0001
2024-01-05 12:01:38 INFO     	 * (global step 1650: loss: 0.5678486824035645, lr: 0.0001
2024-01-05 12:01:45 INFO     	 * (global step 1700: loss: 0.7164398729801178, lr: 0.0001
2024-01-05 12:01:52 INFO     	 * (global step 1750: loss: 0.7522214353084564, lr: 0.0001
2024-01-05 12:01:59 INFO     	 * (global step 1800: loss: 0.7311190962791443, lr: 0.0001
2024-01-05 12:02:06 INFO     	 * (global step 1850: loss: 0.6899243593215942, lr: 0.0001
2024-01-05 12:02:12 INFO     	 * (global step 1900: loss: 0.43768903613090515, lr: 0.0001
2024-01-05 12:02:19 INFO     	 * (global step 1950: loss: 0.6439747959375381, lr: 0.0001
2024-01-05 12:02:26 INFO     	 * (global step 2000: loss: 0.9798212349414825, lr: 0.0001
2024-01-05 12:02:33 INFO     	 * (global step 2050: loss: 0.6747467517852783, lr: 0.0001
2024-01-05 12:02:40 INFO     	 * (global step 2100: loss: 0.5115706473588943, lr: 0.0001
2024-01-05 12:02:47 INFO     	 * (global step 2150: loss: 0.5842686891555786, lr: 0.0001
2024-01-05 12:02:54 INFO     	 * (global step 2200: loss: 0.8736720979213715, lr: 0.0001
2024-01-05 12:03:01 INFO     	 * (global step 2250: loss: 0.5243139714002609, lr: 0.0001
2024-01-05 12:03:09 INFO     	 * (global step 2300: loss: 1.357063889503479, lr: 0.0001
2024-01-05 12:03:16 INFO     	 * (global step 2350: loss: 0.37667079269886017, lr: 0.0001
2024-01-05 12:03:23 INFO     	 * (global step 2400: loss: 0.5584305226802826, lr: 0.0001
2024-01-05 12:03:29 INFO     	 * (global step 2450: loss: 0.7029532194137573, lr: 0.0001
2024-01-05 12:03:34 INFO     	 * (global step 2500: loss: 0.49575401842594147, lr: 0.0001
2024-01-05 12:03:40 INFO     	 * (global step 2550: loss: 0.78110072016716, lr: 0.0001
2024-01-05 12:03:46 INFO     	 * (global step 2600: loss: 0.4305836260318756, lr: 0.0001
2024-01-05 12:03:53 INFO     	 * (global step 2650: loss: 0.5239709168672562, lr: 0.0001
2024-01-05 12:04:01 INFO     	 * (global step 2700: loss: 0.606340080499649, lr: 0.0001
2024-01-05 12:04:08 INFO     	 * (global step 2750: loss: 0.4780258983373642, lr: 0.0001
2024-01-05 12:04:15 INFO     	 * (global step 2800: loss: 0.48270784318447113, lr: 0.0001
2024-01-05 12:04:21 INFO     	 * (global step 2850: loss: 0.6027970016002655, lr: 0.0001
2024-01-05 12:04:28 INFO     	 * (global step 2900: loss: 0.7184377610683441, lr: 0.0001
2024-01-05 12:04:34 INFO     	 * (global step 2950: loss: 0.30307573080062866, lr: 0.0001
2024-01-05 12:04:41 INFO     	 * (global step 3000: loss: 0.8287463188171387, lr: 0.0001
2024-01-05 12:04:47 INFO     	 * (global step 3050: loss: 0.6846225559711456, lr: 0.0001
2024-01-05 12:04:54 INFO     	 * (global step 3100: loss: 0.7265467345714569, lr: 0.0001
2024-01-05 12:05:02 INFO     	 * (global step 3150: loss: 0.45433947443962097, lr: 0.0001
2024-01-05 12:05:09 INFO     	 * (global step 3200: loss: 0.7279062867164612, lr: 0.0001
2024-01-05 12:05:16 INFO     	 * (global step 3250: loss: 0.4584405869245529, lr: 0.0001
2024-01-05 12:05:23 INFO     	 * (global step 3300: loss: 0.575976625084877, lr: 0.0001
2024-01-05 12:05:30 INFO     	 * (global step 3350: loss: 0.5562085062265396, lr: 0.0001
2024-01-05 12:05:38 INFO     	 * (global step 3400: loss: 0.570853054523468, lr: 0.0001
2024-01-05 12:05:45 INFO     	 * (global step 3450: loss: 0.6552610695362091, lr: 0.0001
2024-01-05 12:05:52 INFO     	 * (global step 3500: loss: 0.6278488636016846, lr: 0.0001
2024-01-05 12:06:00 INFO     	 * (global step 3550: loss: 0.8739488124847412, lr: 0.0001
2024-01-05 12:06:07 INFO     	 * (global step 3600: loss: 0.633891150355339, lr: 0.0001
2024-01-05 12:06:15 INFO     	 * (global step 3650: loss: 0.6541568636894226, lr: 0.0001
2024-01-05 12:06:22 INFO     	 * (global step 3700: loss: 0.36814047396183014, lr: 0.0001
2024-01-05 12:06:29 INFO     	 * (global step 3750: loss: 0.5491434335708618, lr: 0.0001
2024-01-05 12:06:35 INFO     	 * (global step 3800: loss: 0.387891948223114, lr: 0.0001
2024-01-05 12:06:41 INFO     	 * (global step 3850: loss: 0.5865384042263031, lr: 0.0001
2024-01-05 12:06:47 INFO     	 * (global step 3900: loss: 0.7654967904090881, lr: 0.0001
2024-01-05 12:06:53 INFO     	 * (global step 3950: loss: 0.5856539309024811, lr: 0.0001
2024-01-05 12:07:00 INFO     	 * (global step 4000: loss: 0.5166183114051819, lr: 0.0001
2024-01-05 12:07:07 INFO     [epoch 0/15] average loss: 0.707, lr: 0.0001
2024-01-05 12:07:07 INFO     saving model related files
2024-01-05 12:07:07 INFO     saving model
2024-01-05 12:07:08 INFO     saving tokenizer
2024-01-05 12:07:08 INFO     saving optimizer
2024-01-05 12:07:09 INFO     remove old optimizer files
2024-01-05 12:07:10 INFO     	 * (global step 4050: loss: 0.580750048160553, lr: 0.0001
2024-01-05 12:07:17 INFO     	 * (global step 4100: loss: 0.5132003426551819, lr: 0.0001
2024-01-05 12:07:24 INFO     	 * (global step 4150: loss: 0.5943585634231567, lr: 0.0001
2024-01-05 12:07:32 INFO     	 * (global step 4200: loss: 0.483420193195343, lr: 0.0001
2024-01-05 12:07:38 INFO     	 * (global step 4250: loss: 0.6791492700576782, lr: 0.0001
2024-01-05 12:07:45 INFO     	 * (global step 4300: loss: 0.9040086716413498, lr: 0.0001
2024-01-05 12:07:53 INFO     	 * (global step 4350: loss: 0.4107389748096466, lr: 0.0001
2024-01-05 12:08:00 INFO     	 * (global step 4400: loss: 0.37714311480522156, lr: 0.0001
2024-01-05 12:08:06 INFO     	 * (global step 4450: loss: 0.859787106513977, lr: 0.0001
2024-01-05 12:08:13 INFO     	 * (global step 4500: loss: 0.4443403631448746, lr: 0.0001
2024-01-05 12:08:20 INFO     	 * (global step 4550: loss: 0.4862969368696213, lr: 0.0001
2024-01-05 12:08:27 INFO     	 * (global step 4600: loss: 0.5624281466007233, lr: 0.0001
2024-01-05 12:08:35 INFO     	 * (global step 4650: loss: 0.7686358094215393, lr: 0.0001
2024-01-05 12:08:42 INFO     	 * (global step 4700: loss: 0.789683997631073, lr: 0.0001
2024-01-05 12:08:49 INFO     	 * (global step 4750: loss: 0.7424970269203186, lr: 0.0001
2024-01-05 12:08:57 INFO     	 * (global step 4800: loss: 0.592161551117897, lr: 0.0001
2024-01-05 12:09:04 INFO     	 * (global step 4850: loss: 0.6516285836696625, lr: 0.0001
2024-01-05 12:09:11 INFO     	 * (global step 4900: loss: 0.5669206380844116, lr: 0.0001
2024-01-05 12:09:18 INFO     	 * (global step 4950: loss: 0.760570302605629, lr: 0.0001
2024-01-05 12:09:25 INFO     	 * (global step 5000: loss: 0.7339262962341309, lr: 0.0001
2024-01-05 12:09:32 INFO     	 * (global step 5050: loss: 0.67012158036232, lr: 0.0001
2024-01-05 12:09:39 INFO     	 * (global step 5100: loss: 0.5391008704900742, lr: 0.0001
2024-01-05 12:09:46 INFO     	 * (global step 5150: loss: 0.6186724007129669, lr: 0.0001
2024-01-05 12:09:53 INFO     	 * (global step 5200: loss: 0.49353812634944916, lr: 0.0001
2024-01-05 12:09:59 INFO     	 * (global step 5250: loss: 0.5707914084196091, lr: 0.0001
2024-01-05 12:10:04 INFO     	 * (global step 5300: loss: 0.7967041432857513, lr: 0.0001
2024-01-05 12:10:10 INFO     	 * (global step 5350: loss: 0.6045280396938324, lr: 0.0001
2024-01-05 12:10:16 INFO     	 * (global step 5400: loss: 0.5216416716575623, lr: 0.0001
2024-01-05 12:10:22 INFO     	 * (global step 5450: loss: 0.5396811962127686, lr: 0.0001
2024-01-05 12:10:29 INFO     	 * (global step 5500: loss: 0.47521549463272095, lr: 0.0001
2024-01-05 12:10:36 INFO     	 * (global step 5550: loss: 0.6547836661338806, lr: 0.0001
2024-01-05 12:10:43 INFO     	 * (global step 5600: loss: 0.6846855878829956, lr: 0.0001
2024-01-05 12:10:50 INFO     	 * (global step 5650: loss: 0.673979640007019, lr: 0.0001
2024-01-05 12:10:58 INFO     	 * (global step 5700: loss: 0.9261104464530945, lr: 0.0001
2024-01-05 12:11:05 INFO     	 * (global step 5750: loss: 0.4759729653596878, lr: 0.0001
2024-01-05 12:11:12 INFO     	 * (global step 5800: loss: 0.6506530940532684, lr: 0.0001
2024-01-05 12:11:19 INFO     	 * (global step 5850: loss: 0.5027799308300018, lr: 0.0001
2024-01-05 12:11:26 INFO     	 * (global step 5900: loss: 0.4353911280632019, lr: 0.0001
2024-01-05 12:11:33 INFO     	 * (global step 5950: loss: 0.7667158842086792, lr: 0.0001
2024-01-05 12:11:41 INFO     	 * (global step 6000: loss: 0.5290035009384155, lr: 0.0001
2024-01-05 12:11:48 INFO     	 * (global step 6050: loss: 0.48871299624443054, lr: 0.0001
2024-01-05 12:11:54 INFO     	 * (global step 6100: loss: 0.47409380972385406, lr: 0.0001
2024-01-05 12:12:01 INFO     	 * (global step 6150: loss: 0.5407615453004837, lr: 0.0001
2024-01-05 12:12:08 INFO     	 * (global step 6200: loss: 0.6097773313522339, lr: 0.0001
2024-01-05 12:12:15 INFO     	 * (global step 6250: loss: 0.43864698708057404, lr: 0.0001
2024-01-05 12:12:23 INFO     	 * (global step 6300: loss: 0.6103003919124603, lr: 0.0001
2024-01-05 12:12:30 INFO     	 * (global step 6350: loss: 0.6377963125705719, lr: 0.0001
2024-01-05 12:12:37 INFO     	 * (global step 6400: loss: 0.5089749693870544, lr: 0.0001
2024-01-05 12:12:44 INFO     	 * (global step 6450: loss: 0.4479086846113205, lr: 0.0001
2024-01-05 12:12:51 INFO     	 * (global step 6500: loss: 0.6814450025558472, lr: 0.0001
2024-01-05 12:12:58 INFO     	 * (global step 6550: loss: 0.7017224431037903, lr: 0.0001
2024-01-05 12:13:05 INFO     	 * (global step 6600: loss: 0.6502885520458221, lr: 0.0001
2024-01-05 12:13:12 INFO     	 * (global step 6650: loss: 0.5297877192497253, lr: 0.0001
2024-01-05 12:13:18 INFO     	 * (global step 6700: loss: 0.63301020860672, lr: 0.0001
2024-01-05 12:13:24 INFO     	 * (global step 6750: loss: 0.7719812989234924, lr: 0.0001
2024-01-05 12:13:30 INFO     	 * (global step 6800: loss: 0.740558922290802, lr: 0.0001
2024-01-05 12:13:36 INFO     	 * (global step 6850: loss: 0.5720982253551483, lr: 0.0001
2024-01-05 12:13:42 INFO     	 * (global step 6900: loss: 0.613235354423523, lr: 0.0001
2024-01-05 12:13:50 INFO     	 * (global step 6950: loss: 0.5400651395320892, lr: 0.0001
2024-01-05 12:13:56 INFO     	 * (global step 7000: loss: 0.5485456585884094, lr: 0.0001
2024-01-05 12:14:03 INFO     	 * (global step 7050: loss: 0.6330349445343018, lr: 0.0001
2024-01-05 12:14:11 INFO     	 * (global step 7100: loss: 0.45003414154052734, lr: 0.0001
2024-01-05 12:14:18 INFO     	 * (global step 7150: loss: 0.5939643234014511, lr: 0.0001
2024-01-05 12:14:25 INFO     	 * (global step 7200: loss: 0.5801130682229996, lr: 0.0001
2024-01-05 12:14:32 INFO     	 * (global step 7250: loss: 0.7444062232971191, lr: 0.0001
2024-01-05 12:14:39 INFO     	 * (global step 7300: loss: 0.7127464115619659, lr: 0.0001
2024-01-05 12:14:46 INFO     	 * (global step 7350: loss: 0.6208588480949402, lr: 0.0001
2024-01-05 12:14:53 INFO     	 * (global step 7400: loss: 0.689810037612915, lr: 0.0001
2024-01-05 12:15:01 INFO     	 * (global step 7450: loss: 0.5523024797439575, lr: 0.0001
2024-01-05 12:15:08 INFO     	 * (global step 7500: loss: 0.5159163773059845, lr: 0.0001
2024-01-05 12:15:15 INFO     	 * (global step 7550: loss: 0.7789847552776337, lr: 0.0001
2024-01-05 12:15:22 INFO     	 * (global step 7600: loss: 0.5765472650527954, lr: 0.0001
2024-01-05 12:15:29 INFO     	 * (global step 7650: loss: 0.8170532584190369, lr: 0.0001
2024-01-05 12:15:36 INFO     	 * (global step 7700: loss: 0.9035478234291077, lr: 0.0001
2024-01-05 12:15:43 INFO     	 * (global step 7750: loss: 0.5861708521842957, lr: 0.0001
2024-01-05 12:15:50 INFO     	 * (global step 7800: loss: 0.6112632155418396, lr: 0.0001
2024-01-05 12:15:57 INFO     	 * (global step 7850: loss: 0.5178573429584503, lr: 0.0001
2024-01-05 12:16:04 INFO     	 * (global step 7900: loss: 0.5059447586536407, lr: 0.0001
2024-01-05 12:16:11 INFO     	 * (global step 7950: loss: 0.5063498914241791, lr: 0.0001
2024-01-05 12:16:18 INFO     	 * (global step 8000: loss: 0.4779120087623596, lr: 0.0001
2024-01-05 12:16:26 INFO     	 * (global step 8050: loss: 0.7783236801624298, lr: 0.0001
2024-01-05 12:16:32 INFO     [epoch 1/15] average loss: 0.592, lr: 0.0001
2024-01-05 12:16:32 INFO     saving model related files
2024-01-05 12:16:32 INFO     saving model
2024-01-05 12:16:32 INFO     saving tokenizer
2024-01-05 12:16:32 INFO     saving optimizer
2024-01-05 12:16:33 INFO     remove old optimizer files
2024-01-05 12:16:34 INFO     	 * (global step 8100: loss: 0.4597747325897217, lr: 0.0001
2024-01-05 12:16:40 INFO     	 * (global step 8150: loss: 0.41460926830768585, lr: 0.0001
2024-01-05 12:16:45 INFO     	 * (global step 8200: loss: 0.4359128922224045, lr: 0.0001
2024-01-05 12:16:51 INFO     	 * (global step 8250: loss: 0.3338041305541992, lr: 0.0001
2024-01-05 12:16:57 INFO     	 * (global step 8300: loss: 0.5704264044761658, lr: 0.0001
2024-01-05 12:17:04 INFO     	 * (global step 8350: loss: 0.7514205276966095, lr: 0.0001
2024-01-05 12:17:11 INFO     	 * (global step 8400: loss: 0.4221508502960205, lr: 0.0001
2024-01-05 12:17:18 INFO     	 * (global step 8450: loss: 0.47742408514022827, lr: 0.0001
2024-01-05 12:17:25 INFO     	 * (global step 8500: loss: 0.6888743937015533, lr: 0.0001
2024-01-05 12:17:33 INFO     	 * (global step 8550: loss: 0.6003608256578445, lr: 0.0001
2024-01-05 12:17:40 INFO     	 * (global step 8600: loss: 0.3153257220983505, lr: 0.0001
2024-01-05 12:17:47 INFO     	 * (global step 8650: loss: 0.40005818009376526, lr: 0.0001
2024-01-05 12:17:54 INFO     	 * (global step 8700: loss: 0.6271826326847076, lr: 0.0001
2024-01-05 12:18:01 INFO     	 * (global step 8750: loss: 0.5214807540178299, lr: 0.0001
2024-01-05 12:18:08 INFO     	 * (global step 8800: loss: 0.29343780875205994, lr: 0.0001
2024-01-05 12:18:15 INFO     	 * (global step 8850: loss: 0.54488505423069, lr: 0.0001
2024-01-05 12:18:23 INFO     	 * (global step 8900: loss: 0.4499031901359558, lr: 0.0001
2024-01-05 12:18:30 INFO     	 * (global step 8950: loss: 0.35821904242038727, lr: 0.0001
2024-01-05 12:18:37 INFO     	 * (global step 9000: loss: 0.5179769694805145, lr: 0.0001
2024-01-05 12:18:44 INFO     	 * (global step 9050: loss: 0.6531154215335846, lr: 0.0001
2024-01-05 12:18:51 INFO     	 * (global step 9100: loss: 0.6196466386318207, lr: 0.0001
2024-01-05 12:18:58 INFO     	 * (global step 9150: loss: 0.5580106973648071, lr: 0.0001
2024-01-05 12:19:06 INFO     	 * (global step 9200: loss: 0.5816653668880463, lr: 0.0001
2024-01-05 12:19:13 INFO     	 * (global step 9250: loss: 0.4383899122476578, lr: 0.0001
2024-01-05 12:19:20 INFO     	 * (global step 9300: loss: 0.6712451279163361, lr: 0.0001
2024-01-05 12:19:27 INFO     	 * (global step 9350: loss: 0.39688660204410553, lr: 0.0001
2024-01-05 12:19:35 INFO     	 * (global step 9400: loss: 0.554320216178894, lr: 0.0001
2024-01-05 12:19:42 INFO     	 * (global step 9450: loss: 0.42636964470148087, lr: 0.0001
2024-01-05 12:19:49 INFO     	 * (global step 9500: loss: 0.49296535551548004, lr: 0.0001
2024-01-05 12:19:55 INFO     	 * (global step 9550: loss: 0.5934900939464569, lr: 0.0001
2024-01-05 12:20:01 INFO     	 * (global step 9600: loss: 0.5455190539360046, lr: 0.0001
2024-01-05 12:20:07 INFO     	 * (global step 9650: loss: 0.6333020478487015, lr: 0.0001
2024-01-05 12:20:13 INFO     	 * (global step 9700: loss: 0.693036824464798, lr: 0.0001
2024-01-05 12:20:19 INFO     	 * (global step 9750: loss: 0.7017604112625122, lr: 0.0001
2024-01-05 12:20:26 INFO     	 * (global step 9800: loss: 0.4502708464860916, lr: 0.0001
2024-01-05 12:20:33 INFO     	 * (global step 9850: loss: 0.6683889627456665, lr: 0.0001
2024-01-05 12:20:40 INFO     	 * (global step 9900: loss: 0.44688524305820465, lr: 0.0001
2024-01-05 12:20:48 INFO     	 * (global step 9950: loss: 0.40224313735961914, lr: 0.0001
2024-01-05 12:20:55 INFO     	 * (global step 10000: loss: 0.46267862617969513, lr: 0.0001
2024-01-05 12:21:02 INFO     	 * (global step 10050: loss: 0.5418111979961395, lr: 0.0001
2024-01-05 12:21:09 INFO     	 * (global step 10100: loss: 0.6184397339820862, lr: 0.0001
2024-01-05 12:21:17 INFO     	 * (global step 10150: loss: 0.3950997143983841, lr: 0.0001
2024-01-05 12:21:24 INFO     	 * (global step 10200: loss: 0.4721497893333435, lr: 0.0001
2024-01-05 12:21:31 INFO     	 * (global step 10250: loss: 0.5881737470626831, lr: 0.0001
2024-01-05 12:21:39 INFO     	 * (global step 10300: loss: 0.6357538104057312, lr: 0.0001
2024-01-05 12:21:46 INFO     	 * (global step 10350: loss: 0.32394055277109146, lr: 0.0001
2024-01-05 12:21:53 INFO     	 * (global step 10400: loss: 0.49366359412670135, lr: 0.0001
2024-01-05 12:22:00 INFO     	 * (global step 10450: loss: 0.6303909122943878, lr: 0.0001
2024-01-05 12:22:07 INFO     	 * (global step 10500: loss: 0.5642078220844269, lr: 0.0001
2024-01-05 12:22:14 INFO     	 * (global step 10550: loss: 0.5537782609462738, lr: 0.0001
2024-01-05 12:22:21 INFO     	 * (global step 10600: loss: 0.28715117275714874, lr: 0.0001
2024-01-05 12:22:29 INFO     	 * (global step 10650: loss: 0.5059494525194168, lr: 0.0001
2024-01-05 12:22:35 INFO     	 * (global step 10700: loss: 0.5412344336509705, lr: 0.0001
2024-01-05 12:22:43 INFO     	 * (global step 10750: loss: 0.5722174048423767, lr: 0.0001
2024-01-05 12:22:50 INFO     	 * (global step 10800: loss: 0.549854040145874, lr: 0.0001
2024-01-05 12:22:58 INFO     	 * (global step 10850: loss: 0.4894184321165085, lr: 0.0001
2024-01-05 12:23:05 INFO     	 * (global step 10900: loss: 0.5246449410915375, lr: 0.0001
2024-01-05 12:23:12 INFO     	 * (global step 10950: loss: 0.4444083273410797, lr: 0.0001
2024-01-05 12:23:18 INFO     	 * (global step 11000: loss: 0.39365537464618683, lr: 0.0001
2024-01-05 12:23:24 INFO     	 * (global step 11050: loss: 0.6372694373130798, lr: 0.0001
2024-01-05 12:23:30 INFO     	 * (global step 11100: loss: 0.5105943828821182, lr: 0.0001
2024-01-05 12:23:36 INFO     	 * (global step 11150: loss: 0.5420243442058563, lr: 0.0001
2024-01-05 12:23:43 INFO     	 * (global step 11200: loss: 0.4712010324001312, lr: 0.0001
2024-01-05 12:23:50 INFO     	 * (global step 11250: loss: 0.6036106646060944, lr: 0.0001
2024-01-05 12:23:57 INFO     	 * (global step 11300: loss: 0.4106040745973587, lr: 0.0001
2024-01-05 12:24:04 INFO     	 * (global step 11350: loss: 0.6319730877876282, lr: 0.0001
2024-01-05 12:24:11 INFO     	 * (global step 11400: loss: 0.6669328808784485, lr: 0.0001
2024-01-05 12:24:18 INFO     	 * (global step 11450: loss: 0.8075454831123352, lr: 0.0001
2024-01-05 12:24:26 INFO     	 * (global step 11500: loss: 0.5445196330547333, lr: 0.0001
2024-01-05 12:24:33 INFO     	 * (global step 11550: loss: 0.653472512960434, lr: 0.0001
2024-01-05 12:24:40 INFO     	 * (global step 11600: loss: 0.5529011338949203, lr: 0.0001
2024-01-05 12:24:47 INFO     	 * (global step 11650: loss: 0.8182001113891602, lr: 0.0001
2024-01-05 12:24:54 INFO     	 * (global step 11700: loss: 0.6043505072593689, lr: 0.0001
2024-01-05 12:25:01 INFO     	 * (global step 11750: loss: 0.570109948515892, lr: 0.0001
2024-01-05 12:25:09 INFO     	 * (global step 11800: loss: 0.3794823884963989, lr: 0.0001
2024-01-05 12:25:17 INFO     	 * (global step 11850: loss: 0.5395790785551071, lr: 0.0001
2024-01-05 12:25:24 INFO     	 * (global step 11900: loss: 0.4289090186357498, lr: 0.0001
2024-01-05 12:25:31 INFO     	 * (global step 11950: loss: 0.3630063533782959, lr: 0.0001
2024-01-05 12:25:39 INFO     	 * (global step 12000: loss: 0.645350456237793, lr: 0.0001
2024-01-05 12:25:46 INFO     	 * (global step 12050: loss: 0.4733153432607651, lr: 0.0001
2024-01-05 12:25:53 INFO     	 * (global step 12100: loss: 0.5917376577854156, lr: 0.0001
2024-01-05 12:26:00 INFO     [epoch 2/15] average loss: 0.56, lr: 0.0001
2024-01-05 12:26:00 INFO     saving model related files
2024-01-05 12:26:00 INFO     saving model
2024-01-05 12:26:00 INFO     saving tokenizer
2024-01-05 12:26:01 INFO     saving optimizer
2024-01-05 12:26:02 INFO     remove old optimizer files
2024-01-05 12:26:03 INFO     	 * (global step 12150: loss: 0.436451718211174, lr: 0.0001
2024-01-05 12:26:10 INFO     	 * (global step 12200: loss: 0.5520318001508713, lr: 0.0001
2024-01-05 12:26:18 INFO     	 * (global step 12250: loss: 0.4400871843099594, lr: 0.0001
2024-01-05 12:26:25 INFO     	 * (global step 12300: loss: 0.5151040554046631, lr: 0.0001
2024-01-05 12:26:32 INFO     	 * (global step 12350: loss: 0.6221558451652527, lr: 0.0001
2024-01-05 12:26:40 INFO     	 * (global step 12400: loss: 0.6886240839958191, lr: 0.0001
2024-01-05 12:26:46 INFO     	 * (global step 12450: loss: 0.3921911418437958, lr: 0.0001
2024-01-05 12:26:53 INFO     	 * (global step 12500: loss: 0.6049399822950363, lr: 0.0001
2024-01-05 12:26:59 INFO     	 * (global step 12550: loss: 0.4748748391866684, lr: 0.0001
2024-01-05 12:27:05 INFO     	 * (global step 12600: loss: 0.43497106432914734, lr: 0.0001
2024-01-05 12:27:11 INFO     	 * (global step 12650: loss: 0.5300930738449097, lr: 0.0001
2024-01-05 12:27:17 INFO     	 * (global step 12700: loss: 0.7365894317626953, lr: 0.0001
2024-01-05 12:27:23 INFO     	 * (global step 12750: loss: 0.6995287090539932, lr: 0.0001
2024-01-05 12:27:29 INFO     	 * (global step 12800: loss: 0.6481648087501526, lr: 0.0001
2024-01-05 12:27:35 INFO     	 * (global step 12850: loss: 0.754970133304596, lr: 0.0001
2024-01-05 12:27:41 INFO     	 * (global step 12900: loss: 0.5763629376888275, lr: 0.0001
2024-01-05 12:27:47 INFO     	 * (global step 12950: loss: 0.590217113494873, lr: 0.0001
2024-01-05 12:27:53 INFO     	 * (global step 13000: loss: 0.720920205116272, lr: 0.0001
2024-01-05 12:27:59 INFO     	 * (global step 13050: loss: 0.5118633806705475, lr: 0.0001
2024-01-05 12:28:05 INFO     	 * (global step 13100: loss: 0.4229267090559006, lr: 0.0001
2024-01-05 12:28:11 INFO     	 * (global step 13150: loss: 0.5382653623819351, lr: 0.0001
2024-01-05 12:28:17 INFO     	 * (global step 13200: loss: 0.4909801632165909, lr: 0.0001
2024-01-05 12:28:23 INFO     	 * (global step 13250: loss: 1.0334640443325043, lr: 0.0001
2024-01-05 12:28:29 INFO     	 * (global step 13300: loss: 0.3420742303133011, lr: 0.0001
2024-01-05 12:28:35 INFO     	 * (global step 13350: loss: 0.5298954546451569, lr: 0.0001
2024-01-05 12:28:41 INFO     	 * (global step 13400: loss: 0.6413123607635498, lr: 0.0001
2024-01-05 12:28:47 INFO     	 * (global step 13450: loss: 0.4896930754184723, lr: 0.0001
2024-01-05 12:28:53 INFO     	 * (global step 13500: loss: 0.6592121571302414, lr: 0.0001
2024-01-05 12:28:59 INFO     	 * (global step 13550: loss: 0.5397068560123444, lr: 0.0001
2024-01-05 12:29:05 INFO     	 * (global step 13600: loss: 0.6780078858137131, lr: 0.0001
2024-01-05 12:29:11 INFO     	 * (global step 13650: loss: 0.5643447935581207, lr: 0.0001
2024-01-05 12:29:17 INFO     	 * (global step 13700: loss: 0.5266675055027008, lr: 0.0001
2024-01-05 12:29:23 INFO     	 * (global step 13750: loss: 0.5073417127132416, lr: 0.0001
2024-01-05 12:29:29 INFO     	 * (global step 13800: loss: 0.5337444990873337, lr: 0.0001
2024-01-05 12:29:35 INFO     	 * (global step 13850: loss: 0.8549163341522217, lr: 0.0001
2024-01-05 12:29:41 INFO     	 * (global step 13900: loss: 0.3424244821071625, lr: 0.0001
2024-01-05 12:29:47 INFO     	 * (global step 13950: loss: 0.5725373029708862, lr: 0.0001
2024-01-05 12:29:53 INFO     	 * (global step 14000: loss: 0.43150225281715393, lr: 0.0001
2024-01-05 12:29:59 INFO     	 * (global step 14050: loss: 0.44750553369522095, lr: 0.0001
2024-01-05 12:30:05 INFO     	 * (global step 14100: loss: 0.5643258094787598, lr: 0.0001
2024-01-05 12:30:11 INFO     	 * (global step 14150: loss: 0.36969418823719025, lr: 0.0001
2024-01-05 12:30:17 INFO     	 * (global step 14200: loss: 0.5283865183591843, lr: 0.0001
2024-01-05 12:30:23 INFO     	 * (global step 14250: loss: 0.6571889221668243, lr: 0.0001
2024-01-05 12:30:29 INFO     	 * (global step 14300: loss: 0.41031111776828766, lr: 0.0001
2024-01-05 12:30:35 INFO     	 * (global step 14350: loss: 0.9569348394870758, lr: 0.0001
2024-01-05 12:30:41 INFO     	 * (global step 14400: loss: 0.6031577289104462, lr: 0.0001
2024-01-05 12:30:47 INFO     	 * (global step 14450: loss: 0.35190053284168243, lr: 0.0001
2024-01-05 12:30:52 INFO     	 * (global step 14500: loss: 0.49544769525527954, lr: 0.0001
2024-01-05 12:30:58 INFO     	 * (global step 14550: loss: 0.47673942148685455, lr: 0.0001
2024-01-05 12:31:04 INFO     	 * (global step 14600: loss: 0.49865029752254486, lr: 0.0001
2024-01-05 12:31:10 INFO     	 * (global step 14650: loss: 0.47605277597904205, lr: 0.0001
2024-01-05 12:31:16 INFO     	 * (global step 14700: loss: 0.38563072681427, lr: 0.0001
2024-01-05 12:31:22 INFO     	 * (global step 14750: loss: 0.5231826305389404, lr: 0.0001
2024-01-05 12:31:28 INFO     	 * (global step 14800: loss: 0.5014177411794662, lr: 0.0001
2024-01-05 12:31:34 INFO     	 * (global step 14850: loss: 0.4905789643526077, lr: 0.0001
2024-01-05 12:31:40 INFO     	 * (global step 14900: loss: 0.42318928241729736, lr: 0.0001
2024-01-05 12:31:46 INFO     	 * (global step 14950: loss: 0.41857030987739563, lr: 0.0001
2024-01-05 12:31:52 INFO     	 * (global step 15000: loss: 0.40816492587327957, lr: 0.0001
2024-01-05 12:31:58 INFO     	 * (global step 15050: loss: 0.6072034686803818, lr: 0.0001
2024-01-05 12:32:04 INFO     	 * (global step 15100: loss: 0.50387904047966, lr: 0.0001
2024-01-05 12:32:10 INFO     	 * (global step 15150: loss: 0.524216890335083, lr: 0.0001
2024-01-05 12:32:16 INFO     	 * (global step 15200: loss: 0.4755098819732666, lr: 0.0001
2024-01-05 12:32:22 INFO     	 * (global step 15250: loss: 0.39545436203479767, lr: 0.0001
2024-01-05 12:32:28 INFO     	 * (global step 15300: loss: 0.5102375447750092, lr: 0.0001
2024-01-05 12:32:34 INFO     	 * (global step 15350: loss: 0.6896848380565643, lr: 0.0001
2024-01-05 12:32:41 INFO     	 * (global step 15400: loss: 0.37876923382282257, lr: 0.0001
2024-01-05 12:32:47 INFO     	 * (global step 15450: loss: 0.4031914174556732, lr: 0.0001
2024-01-05 12:32:53 INFO     	 * (global step 15500: loss: 0.44616197794675827, lr: 0.0001
2024-01-05 12:32:59 INFO     	 * (global step 15550: loss: 0.6161586344242096, lr: 0.0001
2024-01-05 12:33:05 INFO     	 * (global step 15600: loss: 0.5268908143043518, lr: 0.0001
2024-01-05 12:33:11 INFO     	 * (global step 15650: loss: 0.6636406779289246, lr: 0.0001
2024-01-05 12:33:17 INFO     	 * (global step 15700: loss: 0.6327475607395172, lr: 0.0001
2024-01-05 12:33:23 INFO     	 * (global step 15750: loss: 0.5280536115169525, lr: 0.0001
2024-01-05 12:33:29 INFO     	 * (global step 15800: loss: 0.29500485956668854, lr: 0.0001
2024-01-05 12:33:35 INFO     	 * (global step 15850: loss: 0.5147566497325897, lr: 0.0001
2024-01-05 12:33:41 INFO     	 * (global step 15900: loss: 0.5984121263027191, lr: 0.0001
2024-01-05 12:33:47 INFO     	 * (global step 15950: loss: 0.6826953887939453, lr: 0.0001
2024-01-05 12:33:53 INFO     	 * (global step 16000: loss: 0.39940907061100006, lr: 0.0001
2024-01-05 12:34:00 INFO     	 * (global step 16050: loss: 0.3798461854457855, lr: 0.0001
2024-01-05 12:34:05 INFO     	 * (global step 16100: loss: 0.7111968100070953, lr: 0.0001
2024-01-05 12:34:11 INFO     	 * (global step 16150: loss: 0.4088759124279022, lr: 0.0001
2024-01-05 12:34:16 INFO     [epoch 3/15] average loss: 0.537, lr: 0.0001
2024-01-05 12:34:16 INFO     saving model related files
2024-01-05 12:34:16 INFO     saving model
2024-01-05 12:34:17 INFO     saving tokenizer
2024-01-05 12:34:17 INFO     saving optimizer
2024-01-05 12:34:18 INFO     remove old optimizer files
2024-01-05 12:34:19 INFO     	 * (global step 16200: loss: 0.6379839479923248, lr: 0.0001
2024-01-05 12:34:25 INFO     	 * (global step 16250: loss: 0.5413374900817871, lr: 0.0001
2024-01-05 12:34:31 INFO     	 * (global step 16300: loss: 0.4144895225763321, lr: 0.0001
2024-01-05 12:34:37 INFO     	 * (global step 16350: loss: 0.4553605169057846, lr: 0.0001
2024-01-05 12:34:43 INFO     	 * (global step 16400: loss: 0.26396916806697845, lr: 0.0001
2024-01-05 12:34:49 INFO     	 * (global step 16450: loss: 0.40889841318130493, lr: 0.0001
2024-01-05 12:34:55 INFO     	 * (global step 16500: loss: 0.434462770819664, lr: 0.0001
2024-01-05 12:35:01 INFO     	 * (global step 16550: loss: 0.5725524872541428, lr: 0.0001
2024-01-05 12:35:07 INFO     	 * (global step 16600: loss: 0.4476780593395233, lr: 0.0001
2024-01-05 12:35:13 INFO     	 * (global step 16650: loss: 0.5073367357254028, lr: 0.0001
2024-01-05 12:35:19 INFO     	 * (global step 16700: loss: 0.5049458742141724, lr: 0.0001
2024-01-05 12:35:25 INFO     	 * (global step 16750: loss: 0.4346219152212143, lr: 0.0001
2024-01-05 12:35:31 INFO     	 * (global step 16800: loss: 0.6327802091836929, lr: 0.0001
2024-01-05 12:35:37 INFO     	 * (global step 16850: loss: 0.32604216039180756, lr: 0.0001
2024-01-05 12:35:43 INFO     	 * (global step 16900: loss: 0.5792136341333389, lr: 0.0001
2024-01-05 12:35:49 INFO     	 * (global step 16950: loss: 0.5276850908994675, lr: 0.0001
2024-01-05 12:35:55 INFO     	 * (global step 17000: loss: 0.5849538147449493, lr: 0.0001
2024-01-05 12:36:01 INFO     	 * (global step 17050: loss: 0.5224072486162186, lr: 0.0001
2024-01-05 12:36:07 INFO     	 * (global step 17100: loss: 0.2498030886054039, lr: 0.0001
2024-01-05 12:36:13 INFO     	 * (global step 17150: loss: 0.5015943050384521, lr: 0.0001
2024-01-05 12:36:19 INFO     	 * (global step 17200: loss: 0.4395594447851181, lr: 0.0001
2024-01-05 12:36:25 INFO     	 * (global step 17250: loss: 0.3249077647924423, lr: 0.0001
2024-01-05 12:36:31 INFO     	 * (global step 17300: loss: 0.6610140204429626, lr: 0.0001
2024-01-05 12:36:37 INFO     	 * (global step 17350: loss: 0.6924057602882385, lr: 0.0001
2024-01-05 12:36:43 INFO     	 * (global step 17400: loss: 0.6968205273151398, lr: 0.0001
2024-01-05 12:36:49 INFO     	 * (global step 17450: loss: 0.44535817205905914, lr: 0.0001
2024-01-05 12:36:55 INFO     	 * (global step 17500: loss: 0.4413026422262192, lr: 0.0001
2024-01-05 12:37:00 INFO     	 * (global step 17550: loss: 0.44586050510406494, lr: 0.0001
2024-01-05 12:37:06 INFO     	 * (global step 17600: loss: 0.6163168102502823, lr: 0.0001
2024-01-05 12:37:12 INFO     	 * (global step 17650: loss: 0.4461231380701065, lr: 0.0001
2024-01-05 12:37:18 INFO     	 * (global step 17700: loss: 0.38944584876298904, lr: 0.0001
2024-01-05 12:37:24 INFO     	 * (global step 17750: loss: 0.6110750138759613, lr: 0.0001
2024-01-05 12:37:30 INFO     	 * (global step 17800: loss: 0.6765384823083878, lr: 0.0001
2024-01-05 12:37:36 INFO     	 * (global step 17850: loss: 0.6492207050323486, lr: 0.0001
2024-01-05 12:37:42 INFO     	 * (global step 17900: loss: 0.435633048415184, lr: 0.0001
2024-01-05 12:37:48 INFO     	 * (global step 17950: loss: 0.39772847294807434, lr: 0.0001
2024-01-05 12:37:53 INFO     	 * (global step 18000: loss: 0.521775633096695, lr: 0.0001
2024-01-05 12:37:59 INFO     	 * (global step 18050: loss: 0.4642358422279358, lr: 0.0001
2024-01-05 12:38:05 INFO     	 * (global step 18100: loss: 0.5461641997098923, lr: 0.0001
2024-01-05 12:38:11 INFO     	 * (global step 18150: loss: 0.5694211423397064, lr: 0.0001
2024-01-05 12:38:17 INFO     	 * (global step 18200: loss: 0.464722216129303, lr: 0.0001
2024-01-05 12:38:23 INFO     	 * (global step 18250: loss: 0.4413486570119858, lr: 0.0001
2024-01-05 12:38:28 INFO     	 * (global step 18300: loss: 0.31581274420022964, lr: 0.0001
2024-01-05 12:38:34 INFO     	 * (global step 18350: loss: 0.7728263437747955, lr: 0.0001
2024-01-05 12:38:40 INFO     	 * (global step 18400: loss: 0.6137024760246277, lr: 0.0001
2024-01-05 12:38:46 INFO     	 * (global step 18450: loss: 0.47898223996162415, lr: 0.0001
2024-01-05 12:38:51 INFO     	 * (global step 18500: loss: 0.46919582784175873, lr: 0.0001
2024-01-05 12:38:57 INFO     	 * (global step 18550: loss: 0.7758952379226685, lr: 0.0001
2024-01-05 12:39:03 INFO     	 * (global step 18600: loss: 0.4323651194572449, lr: 0.0001
2024-01-05 12:39:09 INFO     	 * (global step 18650: loss: 0.7017939686775208, lr: 0.0001
2024-01-05 12:39:15 INFO     	 * (global step 18700: loss: 0.5955058336257935, lr: 0.0001
2024-01-05 12:39:21 INFO     	 * (global step 18750: loss: 0.4632929414510727, lr: 0.0001
2024-01-05 12:39:26 INFO     	 * (global step 18800: loss: 0.43993014097213745, lr: 0.0001
2024-01-05 12:39:32 INFO     	 * (global step 18850: loss: 0.5899739861488342, lr: 0.0001
2024-01-05 12:39:38 INFO     	 * (global step 18900: loss: 0.48139289021492004, lr: 0.0001
2024-01-05 12:39:44 INFO     	 * (global step 18950: loss: 0.524220734834671, lr: 0.0001
2024-01-05 12:39:50 INFO     	 * (global step 19000: loss: 0.5210664868354797, lr: 0.0001
2024-01-05 12:39:56 INFO     	 * (global step 19050: loss: 0.7068502604961395, lr: 0.0001
2024-01-05 12:40:02 INFO     	 * (global step 19100: loss: 0.6024272441864014, lr: 0.0001
2024-01-05 12:40:08 INFO     	 * (global step 19150: loss: 0.5660291016101837, lr: 0.0001
2024-01-05 12:40:14 INFO     	 * (global step 19200: loss: 0.4637399762868881, lr: 0.0001
2024-01-05 12:40:20 INFO     	 * (global step 19250: loss: 0.5707328915596008, lr: 0.0001
2024-01-05 12:40:26 INFO     	 * (global step 19300: loss: 0.41135165095329285, lr: 0.0001
2024-01-05 12:40:32 INFO     	 * (global step 19350: loss: 0.6825726926326752, lr: 0.0001
2024-01-05 12:40:38 INFO     	 * (global step 19400: loss: 0.6560998857021332, lr: 0.0001
2024-01-05 12:40:44 INFO     	 * (global step 19450: loss: 0.4414575695991516, lr: 0.0001
2024-01-05 12:40:50 INFO     	 * (global step 19500: loss: 0.4600685387849808, lr: 0.0001
2024-01-05 12:40:56 INFO     	 * (global step 19550: loss: 0.30096665024757385, lr: 0.0001
2024-01-05 12:41:02 INFO     	 * (global step 19600: loss: 0.5201172083616257, lr: 0.0001
2024-01-05 12:41:08 INFO     	 * (global step 19650: loss: 0.38274557888507843, lr: 0.0001
2024-01-05 12:41:14 INFO     	 * (global step 19700: loss: 0.4486532062292099, lr: 0.0001
2024-01-05 12:41:20 INFO     	 * (global step 19750: loss: 0.3917398601770401, lr: 0.0001
2024-01-05 12:41:26 INFO     	 * (global step 19800: loss: 0.4501994848251343, lr: 0.0001
2024-01-05 12:41:32 INFO     	 * (global step 19850: loss: 0.31279341876506805, lr: 0.0001
2024-01-05 12:41:38 INFO     	 * (global step 19900: loss: 0.4488406330347061, lr: 0.0001
2024-01-05 12:41:44 INFO     	 * (global step 19950: loss: 0.46372802555561066, lr: 0.0001
2024-01-05 12:41:50 INFO     	 * (global step 20000: loss: 0.40065887570381165, lr: 0.0001
2024-01-05 12:41:56 INFO     	 * (global step 20050: loss: 0.5860095322132111, lr: 0.0001
2024-01-05 12:42:02 INFO     	 * (global step 20100: loss: 0.5760022401809692, lr: 0.0001
2024-01-05 12:42:08 INFO     	 * (global step 20150: loss: 0.4540977329015732, lr: 0.0001
2024-01-05 12:42:14 INFO     	 * (global step 20200: loss: 0.3845980763435364, lr: 0.0001
2024-01-05 12:42:19 INFO     [epoch 4/15] average loss: 0.519, lr: 0.0001
2024-01-05 12:42:19 INFO     saving model related files
2024-01-05 12:42:19 INFO     saving model
2024-01-05 12:42:20 INFO     saving tokenizer
2024-01-05 12:42:20 INFO     saving optimizer
2024-01-05 12:42:21 INFO     remove old optimizer files
2024-01-05 12:42:22 INFO     	 * (global step 20250: loss: 0.45808251202106476, lr: 0.0001
2024-01-05 12:42:28 INFO     	 * (global step 20300: loss: 0.4601583778858185, lr: 0.0001
2024-01-05 12:42:34 INFO     	 * (global step 20350: loss: 0.5965703129768372, lr: 0.0001
2024-01-05 12:42:40 INFO     	 * (global step 20400: loss: 0.4354320466518402, lr: 0.0001
2024-01-05 12:42:46 INFO     	 * (global step 20450: loss: 0.3492203801870346, lr: 0.0001
2024-01-05 12:42:52 INFO     	 * (global step 20500: loss: 0.3928762525320053, lr: 0.0001
2024-01-05 12:42:58 INFO     	 * (global step 20550: loss: 0.4884735196828842, lr: 0.0001
2024-01-05 12:43:04 INFO     	 * (global step 20600: loss: 0.5477443337440491, lr: 0.0001
2024-01-05 12:43:10 INFO     	 * (global step 20650: loss: 0.668808251619339, lr: 0.0001
2024-01-05 12:43:16 INFO     	 * (global step 20700: loss: 0.718908280134201, lr: 0.0001
2024-01-05 12:43:22 INFO     	 * (global step 20750: loss: 0.34169337153434753, lr: 0.0001
2024-01-05 12:43:28 INFO     	 * (global step 20800: loss: 0.6336717158555984, lr: 0.0001
2024-01-05 12:43:34 INFO     	 * (global step 20850: loss: 0.5102630257606506, lr: 0.0001
2024-01-05 12:43:40 INFO     	 * (global step 20900: loss: 0.36424994468688965, lr: 0.0001
2024-01-05 12:43:46 INFO     	 * (global step 20950: loss: 0.5893563628196716, lr: 0.0001
2024-01-05 12:43:52 INFO     	 * (global step 21000: loss: 0.3085700124502182, lr: 0.0001
2024-01-05 12:43:58 INFO     	 * (global step 21050: loss: 0.6239485442638397, lr: 0.0001
2024-01-05 12:44:04 INFO     	 * (global step 21100: loss: 0.49009452760219574, lr: 0.0001
2024-01-05 12:44:10 INFO     	 * (global step 21150: loss: 0.43429918587207794, lr: 0.0001
2024-01-05 12:44:16 INFO     	 * (global step 21200: loss: 0.3554064631462097, lr: 0.0001
2024-01-05 12:44:22 INFO     	 * (global step 21250: loss: 0.2965346872806549, lr: 0.0001
2024-01-05 12:44:28 INFO     	 * (global step 21300: loss: 0.5732930898666382, lr: 0.0001
2024-01-05 12:44:34 INFO     	 * (global step 21350: loss: 0.5645126402378082, lr: 0.0001
2024-01-05 12:44:40 INFO     	 * (global step 21400: loss: 0.4966695159673691, lr: 0.0001
2024-01-05 12:44:46 INFO     	 * (global step 21450: loss: 0.42400819063186646, lr: 0.0001
2024-01-05 12:44:52 INFO     	 * (global step 21500: loss: 0.36771219968795776, lr: 0.0001
2024-01-05 12:44:58 INFO     	 * (global step 21550: loss: 0.36376431584358215, lr: 0.0001
2024-01-05 12:45:04 INFO     	 * (global step 21600: loss: 0.4525562524795532, lr: 0.0001
2024-01-05 12:45:10 INFO     	 * (global step 21650: loss: 0.3883075565099716, lr: 0.0001
2024-01-05 12:45:16 INFO     	 * (global step 21700: loss: 0.5286919921636581, lr: 0.0001
2024-01-05 12:45:22 INFO     	 * (global step 21750: loss: 0.5459770262241364, lr: 0.0001
2024-01-05 12:45:28 INFO     	 * (global step 21800: loss: 0.592673659324646, lr: 0.0001
2024-01-05 12:45:34 INFO     	 * (global step 21850: loss: 0.6799644827842712, lr: 0.0001
2024-01-05 12:45:40 INFO     	 * (global step 21900: loss: 0.6014729887247086, lr: 0.0001
2024-01-05 12:45:46 INFO     	 * (global step 21950: loss: 0.6178592592477798, lr: 0.0001
2024-01-05 12:45:52 INFO     	 * (global step 22000: loss: 0.5721980929374695, lr: 0.0001
2024-01-05 12:45:58 INFO     	 * (global step 22050: loss: 0.5077605098485947, lr: 0.0001
2024-01-05 12:46:04 INFO     	 * (global step 22100: loss: 0.5089310705661774, lr: 0.0001
2024-01-05 12:46:10 INFO     	 * (global step 22150: loss: 0.39943769574165344, lr: 0.0001
2024-01-05 12:46:16 INFO     	 * (global step 22200: loss: 0.5328943133354187, lr: 0.0001
2024-01-05 12:46:22 INFO     	 * (global step 22250: loss: 0.657545268535614, lr: 0.0001
2024-01-05 12:46:28 INFO     	 * (global step 22300: loss: 0.5515155047178268, lr: 0.0001
2024-01-05 12:46:34 INFO     	 * (global step 22350: loss: 0.4768032729625702, lr: 0.0001
2024-01-05 12:46:40 INFO     	 * (global step 22400: loss: 0.48904165625572205, lr: 0.0001
2024-01-05 12:46:46 INFO     	 * (global step 22450: loss: 0.47931867837905884, lr: 0.0001
2024-01-05 12:46:52 INFO     	 * (global step 22500: loss: 0.5931890904903412, lr: 0.0001
2024-01-05 12:46:58 INFO     	 * (global step 22550: loss: 0.514013946056366, lr: 0.0001
2024-01-05 12:47:04 INFO     	 * (global step 22600: loss: 0.5517776608467102, lr: 0.0001
2024-01-05 12:47:10 INFO     	 * (global step 22650: loss: 0.4192158281803131, lr: 0.0001
2024-01-05 12:47:16 INFO     	 * (global step 22700: loss: 0.39434055984020233, lr: 0.0001
2024-01-05 12:47:22 INFO     	 * (global step 22750: loss: 0.3763171136379242, lr: 0.0001
2024-01-05 12:47:28 INFO     	 * (global step 22800: loss: 0.4299643039703369, lr: 0.0001
2024-01-05 12:47:35 INFO     	 * (global step 22850: loss: 0.512260690331459, lr: 0.0001
2024-01-05 12:47:40 INFO     	 * (global step 22900: loss: 0.37905123829841614, lr: 0.0001
2024-01-05 12:47:46 INFO     	 * (global step 22950: loss: 0.5155036300420761, lr: 0.0001
2024-01-05 12:47:52 INFO     	 * (global step 23000: loss: 0.5978139787912369, lr: 0.0001
2024-01-05 12:47:58 INFO     	 * (global step 23050: loss: 0.3567189872264862, lr: 0.0001
2024-01-05 12:48:04 INFO     	 * (global step 23100: loss: 0.548513799905777, lr: 0.0001
2024-01-05 12:48:10 INFO     	 * (global step 23150: loss: 0.5700260549783707, lr: 0.0001
2024-01-05 12:48:16 INFO     	 * (global step 23200: loss: 0.4020034521818161, lr: 0.0001
2024-01-05 12:48:22 INFO     	 * (global step 23250: loss: 0.5284169614315033, lr: 0.0001
2024-01-05 12:48:28 INFO     	 * (global step 23300: loss: 0.305403009057045, lr: 0.0001
2024-01-05 12:48:34 INFO     	 * (global step 23350: loss: 0.4451560229063034, lr: 0.0001
2024-01-05 12:48:40 INFO     	 * (global step 23400: loss: 0.39562278985977173, lr: 0.0001
2024-01-05 12:48:46 INFO     	 * (global step 23450: loss: 0.4776315689086914, lr: 0.0001
2024-01-05 12:48:52 INFO     	 * (global step 23500: loss: 0.4844683110713959, lr: 0.0001
2024-01-05 12:48:58 INFO     	 * (global step 23550: loss: 0.41730840504169464, lr: 0.0001
2024-01-05 12:49:04 INFO     	 * (global step 23600: loss: 0.5937103629112244, lr: 0.0001
2024-01-05 12:49:10 INFO     	 * (global step 23650: loss: 0.30011117458343506, lr: 0.0001
2024-01-05 12:49:16 INFO     	 * (global step 23700: loss: 0.47414083033800125, lr: 0.0001
2024-01-05 12:49:22 INFO     	 * (global step 23750: loss: 0.560117319226265, lr: 0.0001
2024-01-05 12:49:28 INFO     	 * (global step 23800: loss: 0.43459567427635193, lr: 0.0001
2024-01-05 12:49:34 INFO     	 * (global step 23850: loss: 0.44760623574256897, lr: 0.0001
2024-01-05 12:49:41 INFO     	 * (global step 23900: loss: 0.3959643095731735, lr: 0.0001
2024-01-05 12:49:48 INFO     	 * (global step 23950: loss: 0.8371979296207428, lr: 0.0001
2024-01-05 12:49:56 INFO     	 * (global step 24000: loss: 0.4247237592935562, lr: 0.0001
2024-01-05 12:50:04 INFO     	 * (global step 24050: loss: 0.6924324631690979, lr: 0.0001
2024-01-05 12:50:11 INFO     	 * (global step 24100: loss: 0.4571731388568878, lr: 0.0001
2024-01-05 12:50:19 INFO     	 * (global step 24150: loss: 0.526967853307724, lr: 0.0001
2024-01-05 12:50:26 INFO     	 * (global step 24200: loss: 0.5522763431072235, lr: 0.0001
2024-01-05 12:50:34 INFO     	 * (global step 24250: loss: 0.6074645519256592, lr: 0.0001
2024-01-05 12:50:40 INFO     [epoch 5/15] average loss: 0.504, lr: 0.0001
2024-01-05 12:50:40 INFO     saving model related files
2024-01-05 12:50:40 INFO     saving model
2024-01-05 12:50:41 INFO     saving tokenizer
2024-01-05 12:50:41 INFO     saving optimizer
2024-01-05 12:50:42 INFO     remove old optimizer files
2024-01-05 12:50:44 INFO     	 * (global step 24300: loss: 0.42851564288139343, lr: 0.0001
2024-01-05 12:50:52 INFO     	 * (global step 24350: loss: 0.44225722551345825, lr: 0.0001
2024-01-05 12:51:00 INFO     	 * (global step 24400: loss: 0.4120887964963913, lr: 0.0001
2024-01-05 12:51:07 INFO     	 * (global step 24450: loss: 0.39411963522434235, lr: 0.0001
2024-01-05 12:51:15 INFO     	 * (global step 24500: loss: 0.5510006994009018, lr: 0.0001
2024-01-05 12:51:22 INFO     	 * (global step 24550: loss: 0.43909852206707, lr: 0.0001
2024-01-05 12:51:29 INFO     	 * (global step 24600: loss: 0.5229098349809647, lr: 0.0001
2024-01-05 12:51:36 INFO     	 * (global step 24650: loss: 0.4601585865020752, lr: 0.0001
2024-01-05 12:51:44 INFO     	 * (global step 24700: loss: 0.35137684643268585, lr: 0.0001
2024-01-05 12:51:51 INFO     	 * (global step 24750: loss: 0.5463912189006805, lr: 0.0001
2024-01-05 12:51:58 INFO     	 * (global step 24800: loss: 0.6470979452133179, lr: 0.0001
2024-01-05 12:52:04 INFO     	 * (global step 24850: loss: 0.4844145178794861, lr: 0.0001
2024-01-05 12:52:10 INFO     	 * (global step 24900: loss: 0.5814547836780548, lr: 0.0001
2024-01-05 12:52:17 INFO     	 * (global step 24950: loss: 0.6683170795440674, lr: 0.0001
2024-01-05 12:52:23 INFO     	 * (global step 25000: loss: 0.40401240438222885, lr: 0.0001
2024-01-05 12:52:29 INFO     	 * (global step 25050: loss: 0.6197991669178009, lr: 0.0001
2024-01-05 12:52:35 INFO     	 * (global step 25100: loss: 0.4764016270637512, lr: 0.0001
2024-01-05 12:52:41 INFO     	 * (global step 25150: loss: 0.5451948344707489, lr: 0.0001
2024-01-05 12:52:47 INFO     	 * (global step 25200: loss: 0.6809330135583878, lr: 0.0001
2024-01-05 12:52:53 INFO     	 * (global step 25250: loss: 0.4336152970790863, lr: 0.0001
2024-01-05 12:53:00 INFO     	 * (global step 25300: loss: 0.7950825393199921, lr: 0.0001
2024-01-05 12:53:06 INFO     	 * (global step 25350: loss: 0.4075641334056854, lr: 0.0001
2024-01-05 12:53:12 INFO     	 * (global step 25400: loss: 0.5271472632884979, lr: 0.0001
2024-01-05 12:53:18 INFO     	 * (global step 25450: loss: 0.6889433562755585, lr: 0.0001
2024-01-05 12:53:24 INFO     	 * (global step 25500: loss: 0.5671034008264542, lr: 0.0001
2024-01-05 12:53:30 INFO     	 * (global step 25550: loss: 0.3709575980901718, lr: 0.0001
2024-01-05 12:53:37 INFO     	 * (global step 25600: loss: 0.7947028279304504, lr: 0.0001
2024-01-05 12:53:43 INFO     	 * (global step 25650: loss: 0.590126633644104, lr: 0.0001
2024-01-05 12:53:49 INFO     	 * (global step 25700: loss: 0.5778130292892456, lr: 0.0001
2024-01-05 12:53:55 INFO     	 * (global step 25750: loss: 0.2693088501691818, lr: 0.0001
2024-01-05 12:54:02 INFO     	 * (global step 25800: loss: 0.4375632554292679, lr: 0.0001
2024-01-05 12:54:08 INFO     	 * (global step 25850: loss: 0.46155864000320435, lr: 0.0001
2024-01-05 12:54:14 INFO     	 * (global step 25900: loss: 0.5274218618869781, lr: 0.0001
2024-01-05 12:54:20 INFO     	 * (global step 25950: loss: 0.4799944758415222, lr: 0.0001
2024-01-05 12:54:27 INFO     	 * (global step 26000: loss: 0.3403620570898056, lr: 0.0001
2024-01-05 12:54:33 INFO     	 * (global step 26050: loss: 0.40519602596759796, lr: 0.0001
2024-01-05 12:54:39 INFO     	 * (global step 26100: loss: 0.41389408707618713, lr: 0.0001
2024-01-05 12:54:45 INFO     	 * (global step 26150: loss: 0.4716099351644516, lr: 0.0001
2024-01-05 12:54:52 INFO     	 * (global step 26200: loss: 0.5664642006158829, lr: 0.0001
2024-01-05 12:54:58 INFO     	 * (global step 26250: loss: 0.3628374710679054, lr: 0.0001
2024-01-05 12:55:05 INFO     	 * (global step 26300: loss: 0.4774569571018219, lr: 0.0001
2024-01-05 12:55:11 INFO     	 * (global step 26350: loss: 0.4780191630125046, lr: 0.0001
2024-01-05 12:55:17 INFO     	 * (global step 26400: loss: 0.478329136967659, lr: 0.0001
2024-01-05 12:55:24 INFO     	 * (global step 26450: loss: 0.3921036869287491, lr: 0.0001
2024-01-05 12:55:30 INFO     	 * (global step 26500: loss: 0.7881836295127869, lr: 0.0001
2024-01-05 12:55:37 INFO     	 * (global step 26550: loss: 0.534700945019722, lr: 0.0001
2024-01-05 12:55:43 INFO     	 * (global step 26600: loss: 0.5254736840724945, lr: 0.0001
2024-01-05 12:55:50 INFO     	 * (global step 26650: loss: 0.4157745838165283, lr: 0.0001
2024-01-05 12:55:56 INFO     	 * (global step 26700: loss: 0.6366636008024216, lr: 0.0001
2024-01-05 12:56:02 INFO     	 * (global step 26750: loss: 0.3911912739276886, lr: 0.0001
2024-01-05 12:56:09 INFO     	 * (global step 26800: loss: 0.579288512468338, lr: 0.0001
2024-01-05 12:56:15 INFO     	 * (global step 26850: loss: 0.691485583782196, lr: 0.0001
2024-01-05 12:56:21 INFO     	 * (global step 26900: loss: 0.36064016819000244, lr: 0.0001
2024-01-05 12:56:27 INFO     	 * (global step 26950: loss: 0.4612424224615097, lr: 0.0001
2024-01-05 12:56:33 INFO     	 * (global step 27000: loss: 0.4771008789539337, lr: 0.0001
2024-01-05 12:56:39 INFO     	 * (global step 27050: loss: 0.39076604694128036, lr: 0.0001
2024-01-05 12:56:45 INFO     	 * (global step 27100: loss: 0.38615140318870544, lr: 0.0001
2024-01-05 12:56:51 INFO     	 * (global step 27150: loss: 0.5067654997110367, lr: 0.0001
2024-01-05 12:56:58 INFO     	 * (global step 27200: loss: 0.41079404950141907, lr: 0.0001
2024-01-05 12:57:04 INFO     	 * (global step 27250: loss: 0.3348078280687332, lr: 0.0001
2024-01-05 12:57:10 INFO     	 * (global step 27300: loss: 0.5337879508733749, lr: 0.0001
2024-01-05 12:57:16 INFO     	 * (global step 27350: loss: 0.41045619547367096, lr: 0.0001
2024-01-05 12:57:22 INFO     	 * (global step 27400: loss: 0.541522353887558, lr: 0.0001
2024-01-05 12:57:28 INFO     	 * (global step 27450: loss: 0.3921433240175247, lr: 0.0001
2024-01-05 12:57:34 INFO     	 * (global step 27500: loss: 0.4137090742588043, lr: 0.0001
2024-01-05 12:57:40 INFO     	 * (global step 27550: loss: 0.37721291184425354, lr: 0.0001
2024-01-05 12:57:46 INFO     	 * (global step 27600: loss: 0.3907296359539032, lr: 0.0001
2024-01-05 12:57:52 INFO     	 * (global step 27650: loss: 0.515687882900238, lr: 0.0001
2024-01-05 12:57:58 INFO     	 * (global step 27700: loss: 0.5053861141204834, lr: 0.0001
2024-01-05 12:58:05 INFO     	 * (global step 27750: loss: 0.680560439825058, lr: 0.0001
2024-01-05 12:58:11 INFO     	 * (global step 27800: loss: 0.5224780887365341, lr: 0.0001
2024-01-05 12:58:17 INFO     	 * (global step 27850: loss: 0.5645674765110016, lr: 0.0001
2024-01-05 12:58:23 INFO     	 * (global step 27900: loss: 0.5425562560558319, lr: 0.0001
2024-01-05 12:58:29 INFO     	 * (global step 27950: loss: 0.4038088023662567, lr: 0.0001
2024-01-05 12:58:35 INFO     	 * (global step 28000: loss: 0.42086534202098846, lr: 0.0001
2024-01-05 12:58:41 INFO     	 * (global step 28050: loss: 0.6947725415229797, lr: 0.0001
2024-01-05 12:58:47 INFO     	 * (global step 28100: loss: 0.5742668807506561, lr: 0.0001
2024-01-05 12:58:53 INFO     	 * (global step 28150: loss: 0.5041076689958572, lr: 0.0001
2024-01-05 12:58:59 INFO     	 * (global step 28200: loss: 0.5473205149173737, lr: 0.0001
2024-01-05 12:59:05 INFO     	 * (global step 28250: loss: 0.5304148197174072, lr: 0.0001
2024-01-05 12:59:11 INFO     	 * (global step 28300: loss: 0.6655971109867096, lr: 0.0001
2024-01-05 12:59:16 INFO     [epoch 6/15] average loss: 0.49, lr: 0.0001
2024-01-05 12:59:16 INFO     saving model related files
2024-01-05 12:59:16 INFO     saving model
2024-01-05 12:59:16 INFO     saving tokenizer
2024-01-05 12:59:16 INFO     saving optimizer
2024-01-05 12:59:17 INFO     remove old optimizer files
2024-01-05 12:59:19 INFO     	 * (global step 28350: loss: 0.5694217681884766, lr: 0.0001
2024-01-05 12:59:25 INFO     	 * (global step 28400: loss: 0.39418013393878937, lr: 0.0001
2024-01-05 12:59:31 INFO     	 * (global step 28450: loss: 0.48428842425346375, lr: 0.0001
2024-01-05 12:59:37 INFO     	 * (global step 28500: loss: 0.5175601691007614, lr: 0.0001
2024-01-05 12:59:44 INFO     	 * (global step 28550: loss: 0.5083819776773453, lr: 0.0001
2024-01-05 12:59:50 INFO     	 * (global step 28600: loss: 0.5481341630220413, lr: 0.0001
2024-01-05 12:59:56 INFO     	 * (global step 28650: loss: 0.5772793292999268, lr: 0.0001
2024-01-05 13:00:02 INFO     	 * (global step 28700: loss: 0.45721258223056793, lr: 0.0001
2024-01-05 13:00:08 INFO     	 * (global step 28750: loss: 0.5077037215232849, lr: 0.0001
2024-01-05 13:00:14 INFO     	 * (global step 28800: loss: 0.41719578206539154, lr: 0.0001
2024-01-05 13:00:20 INFO     	 * (global step 28850: loss: 0.5687490701675415, lr: 0.0001
2024-01-05 13:00:26 INFO     	 * (global step 28900: loss: 0.5390947908163071, lr: 0.0001
2024-01-05 13:00:32 INFO     	 * (global step 28950: loss: 0.4499734789133072, lr: 0.0001
2024-01-05 13:00:38 INFO     	 * (global step 29000: loss: 0.4503898322582245, lr: 0.0001
2024-01-05 13:00:44 INFO     	 * (global step 29050: loss: 0.3769010305404663, lr: 0.0001
2024-01-05 13:00:50 INFO     	 * (global step 29100: loss: 0.5390649735927582, lr: 0.0001
2024-01-05 13:00:56 INFO     	 * (global step 29150: loss: 0.6717571020126343, lr: 0.0001
2024-01-05 13:01:02 INFO     	 * (global step 29200: loss: 0.5868287980556488, lr: 0.0001
2024-01-05 13:01:08 INFO     	 * (global step 29250: loss: 0.4358878880739212, lr: 0.0001
2024-01-05 13:01:14 INFO     	 * (global step 29300: loss: 0.3554497957229614, lr: 0.0001
2024-01-05 13:01:20 INFO     	 * (global step 29350: loss: 0.5118902772665024, lr: 0.0001
2024-01-05 13:01:26 INFO     	 * (global step 29400: loss: 0.344851478934288, lr: 0.0001
2024-01-05 13:01:32 INFO     	 * (global step 29450: loss: 0.5074627697467804, lr: 0.0001
2024-01-05 13:01:38 INFO     	 * (global step 29500: loss: 0.3049662560224533, lr: 0.0001
2024-01-05 13:01:44 INFO     	 * (global step 29550: loss: 0.37914495170116425, lr: 0.0001
2024-01-05 13:01:50 INFO     	 * (global step 29600: loss: 0.3087754547595978, lr: 0.0001
2024-01-05 13:01:56 INFO     	 * (global step 29650: loss: 0.38569624722003937, lr: 0.0001
2024-01-05 13:02:02 INFO     	 * (global step 29700: loss: 0.4676399379968643, lr: 0.0001
2024-01-05 13:02:08 INFO     	 * (global step 29750: loss: 0.7504094541072845, lr: 0.0001
2024-01-05 13:02:14 INFO     	 * (global step 29800: loss: 0.4971041828393936, lr: 0.0001
2024-01-05 13:02:20 INFO     	 * (global step 29850: loss: 0.464510902762413, lr: 0.0001
2024-01-05 13:02:26 INFO     	 * (global step 29900: loss: 0.43432365357875824, lr: 0.0001
2024-01-05 13:02:32 INFO     	 * (global step 29950: loss: 0.4975969195365906, lr: 0.0001
2024-01-05 13:02:38 INFO     	 * (global step 30000: loss: 0.48776136338710785, lr: 0.0001
2024-01-05 13:02:45 INFO     	 * (global step 30050: loss: 0.47749489545822144, lr: 0.0001
2024-01-05 13:02:51 INFO     	 * (global step 30100: loss: 0.4659503102302551, lr: 0.0001
2024-01-05 13:02:57 INFO     	 * (global step 30150: loss: 0.44374406337738037, lr: 0.0001
2024-01-05 13:03:03 INFO     	 * (global step 30200: loss: 0.5316495001316071, lr: 0.0001
2024-01-05 13:03:09 INFO     	 * (global step 30250: loss: 0.5196805447340012, lr: 0.0001
2024-01-05 13:03:15 INFO     	 * (global step 30300: loss: 0.6057985424995422, lr: 0.0001
2024-01-05 13:03:21 INFO     	 * (global step 30350: loss: 0.30592695623636246, lr: 0.0001
2024-01-05 13:03:27 INFO     	 * (global step 30400: loss: 0.5296320617198944, lr: 0.0001
2024-01-05 13:03:33 INFO     	 * (global step 30450: loss: 0.9107649624347687, lr: 0.0001
2024-01-05 13:03:40 INFO     	 * (global step 30500: loss: 0.49788741767406464, lr: 0.0001
2024-01-05 13:03:46 INFO     	 * (global step 30550: loss: 0.2728448361158371, lr: 0.0001
2024-01-05 13:03:52 INFO     	 * (global step 30600: loss: 0.6038942337036133, lr: 0.0001
2024-01-05 13:03:58 INFO     	 * (global step 30650: loss: 0.6806901693344116, lr: 0.0001
2024-01-05 13:04:05 INFO     	 * (global step 30700: loss: 0.3476969450712204, lr: 0.0001
2024-01-05 13:04:11 INFO     	 * (global step 30750: loss: 0.46803246438503265, lr: 0.0001
2024-01-05 13:04:17 INFO     	 * (global step 30800: loss: 0.36671802401542664, lr: 0.0001
2024-01-05 13:04:23 INFO     	 * (global step 30850: loss: 0.31957343220710754, lr: 0.0001
2024-01-05 13:04:29 INFO     	 * (global step 30900: loss: 0.6312707364559174, lr: 0.0001
2024-01-05 13:04:35 INFO     	 * (global step 30950: loss: 0.3998827636241913, lr: 0.0001
2024-01-05 13:04:41 INFO     	 * (global step 31000: loss: 0.6065342724323273, lr: 0.0001
2024-01-05 13:04:47 INFO     	 * (global step 31050: loss: 0.35877323150634766, lr: 0.0001
2024-01-05 13:04:53 INFO     	 * (global step 31100: loss: 0.3589796647429466, lr: 0.0001
2024-01-05 13:05:00 INFO     	 * (global step 31150: loss: 0.42961160838603973, lr: 0.0001
2024-01-05 13:05:06 INFO     	 * (global step 31200: loss: 0.4559441804885864, lr: 0.0001
2024-01-05 13:05:12 INFO     	 * (global step 31250: loss: 0.44691938161849976, lr: 0.0001
2024-01-05 13:05:18 INFO     	 * (global step 31300: loss: 0.6535468101501465, lr: 0.0001
2024-01-05 13:05:24 INFO     	 * (global step 31350: loss: 0.49069851636886597, lr: 0.0001
2024-01-05 13:05:30 INFO     	 * (global step 31400: loss: 0.6918783485889435, lr: 0.0001
2024-01-05 13:05:37 INFO     	 * (global step 31450: loss: 0.5393168181180954, lr: 0.0001
2024-01-05 13:05:43 INFO     	 * (global step 31500: loss: 0.34420792758464813, lr: 0.0001
2024-01-05 13:05:49 INFO     	 * (global step 31550: loss: 0.38438600301742554, lr: 0.0001
2024-01-05 13:05:55 INFO     	 * (global step 31600: loss: 0.46597253531217575, lr: 0.0001
2024-01-05 13:06:01 INFO     	 * (global step 31650: loss: 0.5428638458251953, lr: 0.0001
2024-01-05 13:06:08 INFO     	 * (global step 31700: loss: 0.5647946000099182, lr: 0.0001
2024-01-05 13:06:14 INFO     	 * (global step 31750: loss: 0.49119675904512405, lr: 0.0001
2024-01-05 13:06:20 INFO     	 * (global step 31800: loss: 0.3883437514305115, lr: 0.0001
2024-01-05 13:06:26 INFO     	 * (global step 31850: loss: 0.5085549503564835, lr: 0.0001
2024-01-05 13:06:32 INFO     	 * (global step 31900: loss: 0.43098384141921997, lr: 0.0001
2024-01-05 13:06:38 INFO     	 * (global step 31950: loss: 0.4119308441877365, lr: 0.0001
2024-01-05 13:06:44 INFO     	 * (global step 32000: loss: 0.524066686630249, lr: 0.0001
2024-01-05 13:06:50 INFO     	 * (global step 32050: loss: 0.46031126379966736, lr: 0.0001
2024-01-05 13:06:56 INFO     	 * (global step 32100: loss: 0.48883675038814545, lr: 0.0001
2024-01-05 13:07:02 INFO     	 * (global step 32150: loss: 0.31358473002910614, lr: 0.0001
2024-01-05 13:07:08 INFO     	 * (global step 32200: loss: 0.4285232722759247, lr: 0.0001
2024-01-05 13:07:14 INFO     	 * (global step 32250: loss: 0.3378201872110367, lr: 0.0001
2024-01-05 13:07:20 INFO     	 * (global step 32300: loss: 0.49651989340782166, lr: 0.0001
2024-01-05 13:07:26 INFO     	 * (global step 32350: loss: 0.39492787420749664, lr: 0.0001
2024-01-05 13:07:31 INFO     [epoch 7/15] average loss: 0.478, lr: 0.0001
2024-01-05 13:07:31 INFO     saving model related files
2024-01-05 13:07:31 INFO     saving model
2024-01-05 13:07:31 INFO     saving tokenizer
2024-01-05 13:07:32 INFO     saving optimizer
2024-01-05 13:07:33 INFO     remove old optimizer files
2024-01-05 13:07:35 INFO     	 * (global step 32400: loss: 0.5864425301551819, lr: 0.0001
2024-01-05 13:07:41 INFO     	 * (global step 32450: loss: 0.302280455827713, lr: 0.0001
2024-01-05 13:07:47 INFO     	 * (global step 32500: loss: 0.4405011236667633, lr: 0.0001
2024-01-05 13:07:53 INFO     	 * (global step 32550: loss: 0.3724813610315323, lr: 0.0001
2024-01-05 13:07:59 INFO     	 * (global step 32600: loss: 0.4633125960826874, lr: 0.0001
2024-01-05 13:08:05 INFO     	 * (global step 32650: loss: 0.41586098074913025, lr: 0.0001
2024-01-05 13:08:11 INFO     	 * (global step 32700: loss: 0.3218833804130554, lr: 0.0001
2024-01-05 13:08:17 INFO     	 * (global step 32750: loss: 0.40174688398838043, lr: 0.0001
2024-01-05 13:08:23 INFO     	 * (global step 32800: loss: 0.5361566692590714, lr: 0.0001
2024-01-05 13:08:30 INFO     	 * (global step 32850: loss: 0.381805881857872, lr: 0.0001
2024-01-05 13:08:36 INFO     	 * (global step 32900: loss: 0.44471319019794464, lr: 0.0001
2024-01-05 13:08:42 INFO     	 * (global step 32950: loss: 0.5201423168182373, lr: 0.0001
2024-01-05 13:08:48 INFO     	 * (global step 33000: loss: 0.27856744825839996, lr: 0.0001
2024-01-05 13:08:55 INFO     	 * (global step 33050: loss: 0.45446373522281647, lr: 0.0001
2024-01-05 13:09:01 INFO     	 * (global step 33100: loss: 0.5308912098407745, lr: 0.0001
2024-01-05 13:09:07 INFO     	 * (global step 33150: loss: 0.37147051841020584, lr: 0.0001
2024-01-05 13:09:13 INFO     	 * (global step 33200: loss: 0.42676128447055817, lr: 0.0001
2024-01-05 13:09:19 INFO     	 * (global step 33250: loss: 0.45681189000606537, lr: 0.0001
2024-01-05 13:09:25 INFO     	 * (global step 33300: loss: 0.17089517414569855, lr: 0.0001
2024-01-05 13:09:31 INFO     	 * (global step 33350: loss: 0.555746003985405, lr: 0.0001
2024-01-05 13:09:38 INFO     	 * (global step 33400: loss: 0.4034304916858673, lr: 0.0001
2024-01-05 13:09:44 INFO     	 * (global step 33450: loss: 0.43578939139842987, lr: 0.0001
2024-01-05 13:09:50 INFO     	 * (global step 33500: loss: 0.5123643279075623, lr: 0.0001
2024-01-05 13:09:56 INFO     	 * (global step 33550: loss: 0.39511825889348984, lr: 0.0001
2024-01-05 13:10:02 INFO     	 * (global step 33600: loss: 0.5060104131698608, lr: 0.0001
2024-01-05 13:10:09 INFO     	 * (global step 33650: loss: 0.5412736982107162, lr: 0.0001
2024-01-05 13:10:15 INFO     	 * (global step 33700: loss: 0.309249185025692, lr: 0.0001
2024-01-05 13:10:21 INFO     	 * (global step 33750: loss: 0.3482581526041031, lr: 0.0001
2024-01-05 13:10:27 INFO     	 * (global step 33800: loss: 0.5056959986686707, lr: 0.0001
2024-01-05 13:10:33 INFO     	 * (global step 33850: loss: 0.41781097650527954, lr: 0.0001
2024-01-05 13:10:39 INFO     	 * (global step 33900: loss: 0.42952910810709, lr: 0.0001
2024-01-05 13:10:45 INFO     	 * (global step 33950: loss: 0.3844798058271408, lr: 0.0001
2024-01-05 13:10:51 INFO     	 * (global step 34000: loss: 0.5911538302898407, lr: 0.0001
2024-01-05 13:10:57 INFO     	 * (global step 34050: loss: 0.8085413575172424, lr: 0.0001
2024-01-05 13:11:03 INFO     	 * (global step 34100: loss: 0.5404388904571533, lr: 0.0001
2024-01-05 13:11:09 INFO     	 * (global step 34150: loss: 0.3758483827114105, lr: 0.0001
2024-01-05 13:11:15 INFO     	 * (global step 34200: loss: 0.492676705121994, lr: 0.0001
2024-01-05 13:11:21 INFO     	 * (global step 34250: loss: 0.4749104827642441, lr: 0.0001
2024-01-05 13:11:27 INFO     	 * (global step 34300: loss: 0.2990754544734955, lr: 0.0001
2024-01-05 13:11:33 INFO     	 * (global step 34350: loss: 0.45107024908065796, lr: 0.0001
2024-01-05 13:11:40 INFO     	 * (global step 34400: loss: 0.2909497693181038, lr: 0.0001
2024-01-05 13:11:46 INFO     	 * (global step 34450: loss: 0.47881321609020233, lr: 0.0001
2024-01-05 13:11:52 INFO     	 * (global step 34500: loss: 0.3033883720636368, lr: 0.0001
2024-01-05 13:11:58 INFO     	 * (global step 34550: loss: 0.3749382048845291, lr: 0.0001
2024-01-05 13:12:04 INFO     	 * (global step 34600: loss: 0.5316700637340546, lr: 0.0001
2024-01-05 13:12:11 INFO     	 * (global step 34650: loss: 0.4202738255262375, lr: 0.0001
2024-01-05 13:12:17 INFO     	 * (global step 34700: loss: 0.5908287167549133, lr: 0.0001
2024-01-05 13:12:23 INFO     	 * (global step 34750: loss: 0.5484847724437714, lr: 0.0001
2024-01-05 13:12:29 INFO     	 * (global step 34800: loss: 0.41096074879169464, lr: 0.0001
2024-01-05 13:12:35 INFO     	 * (global step 34850: loss: 0.6084157824516296, lr: 0.0001
2024-01-05 13:12:41 INFO     	 * (global step 34900: loss: 0.4376639276742935, lr: 0.0001
2024-01-05 13:12:47 INFO     	 * (global step 34950: loss: 0.4060351923108101, lr: 0.0001
2024-01-05 13:12:53 INFO     	 * (global step 35000: loss: 0.43190106749534607, lr: 0.0001
2024-01-05 13:12:59 INFO     	 * (global step 35050: loss: 0.7625885307788849, lr: 0.0001
2024-01-05 13:13:05 INFO     	 * (global step 35100: loss: 0.5177378356456757, lr: 0.0001
2024-01-05 13:13:11 INFO     	 * (global step 35150: loss: 0.48541271686553955, lr: 0.0001
2024-01-05 13:13:17 INFO     	 * (global step 35200: loss: 0.5215977132320404, lr: 0.0001
2024-01-05 13:13:23 INFO     	 * (global step 35250: loss: 0.43115437030792236, lr: 0.0001
2024-01-05 13:13:29 INFO     	 * (global step 35300: loss: 0.4193257838487625, lr: 0.0001
2024-01-05 13:13:35 INFO     	 * (global step 35350: loss: 0.3881823569536209, lr: 0.0001
2024-01-05 13:13:41 INFO     	 * (global step 35400: loss: 0.41705740988254547, lr: 0.0001
2024-01-05 13:13:47 INFO     	 * (global step 35450: loss: 0.44962364435195923, lr: 0.0001
2024-01-05 13:13:53 INFO     	 * (global step 35500: loss: 0.4213322699069977, lr: 0.0001
2024-01-05 13:13:59 INFO     	 * (global step 35550: loss: 0.42278194427490234, lr: 0.0001
2024-01-05 13:14:05 INFO     	 * (global step 35600: loss: 0.44386567175388336, lr: 0.0001
2024-01-05 13:14:11 INFO     	 * (global step 35650: loss: 0.26952914148569107, lr: 0.0001
2024-01-05 13:14:18 INFO     	 * (global step 35700: loss: 0.5556913614273071, lr: 0.0001
2024-01-05 13:14:23 INFO     	 * (global step 35750: loss: 0.47459040582180023, lr: 0.0001
2024-01-05 13:14:29 INFO     	 * (global step 35800: loss: 0.43054327368736267, lr: 0.0001
2024-01-05 13:14:35 INFO     	 * (global step 35850: loss: 0.4672546833753586, lr: 0.0001
2024-01-05 13:14:41 INFO     	 * (global step 35900: loss: 0.6428038477897644, lr: 0.0001
2024-01-05 13:14:47 INFO     	 * (global step 35950: loss: 0.7843621224164963, lr: 0.0001
2024-01-05 13:14:53 INFO     	 * (global step 36000: loss: 0.5098230987787247, lr: 0.0001
2024-01-05 13:15:00 INFO     	 * (global step 36050: loss: 0.6119358241558075, lr: 0.0001
2024-01-05 13:15:06 INFO     	 * (global step 36100: loss: 0.39684774726629257, lr: 0.0001
2024-01-05 13:15:12 INFO     	 * (global step 36150: loss: 0.4915962815284729, lr: 0.0001
2024-01-05 13:15:18 INFO     	 * (global step 36200: loss: 0.46011605858802795, lr: 0.0001
2024-01-05 13:15:24 INFO     	 * (global step 36250: loss: 0.40382903814315796, lr: 0.0001
2024-01-05 13:15:30 INFO     	 * (global step 36300: loss: 0.3814139664173126, lr: 0.0001
2024-01-05 13:15:36 INFO     	 * (global step 36350: loss: 0.35609154403209686, lr: 0.0001
2024-01-05 13:15:42 INFO     	 * (global step 36400: loss: 0.4034135192632675, lr: 0.0001
2024-01-05 13:15:46 INFO     [epoch 8/15] average loss: 0.467, lr: 0.0001
2024-01-05 13:15:46 INFO     saving model related files
2024-01-05 13:15:46 INFO     saving model
2024-01-05 13:15:46 INFO     saving tokenizer
2024-01-05 13:15:47 INFO     saving optimizer
2024-01-05 13:15:48 INFO     remove old optimizer files
2024-01-05 13:15:50 INFO     	 * (global step 36450: loss: 0.2866782173514366, lr: 0.0001
2024-01-05 13:15:56 INFO     	 * (global step 36500: loss: 0.5065562427043915, lr: 0.0001
2024-01-05 13:16:02 INFO     	 * (global step 36550: loss: 0.5128566175699234, lr: 0.0001
2024-01-05 13:16:08 INFO     	 * (global step 36600: loss: 0.31860796362161636, lr: 0.0001
2024-01-05 13:16:14 INFO     	 * (global step 36650: loss: 0.3780510872602463, lr: 0.0001
2024-01-05 13:16:20 INFO     	 * (global step 36700: loss: 0.4870031625032425, lr: 0.0001
2024-01-05 13:16:26 INFO     	 * (global step 36750: loss: 0.4396294504404068, lr: 0.0001
2024-01-05 13:16:32 INFO     	 * (global step 36800: loss: 0.606876477599144, lr: 0.0001
2024-01-05 13:16:38 INFO     	 * (global step 36850: loss: 0.44397637248039246, lr: 0.0001
2024-01-05 13:16:44 INFO     	 * (global step 36900: loss: 0.4651449918746948, lr: 0.0001
2024-01-05 13:16:50 INFO     	 * (global step 36950: loss: 0.3473818376660347, lr: 0.0001
2024-01-05 13:16:56 INFO     	 * (global step 37000: loss: 0.5890345275402069, lr: 0.0001
2024-01-05 13:17:02 INFO     	 * (global step 37050: loss: 0.4223220944404602, lr: 0.0001
2024-01-05 13:17:08 INFO     	 * (global step 37100: loss: 0.4675662964582443, lr: 0.0001
2024-01-05 13:17:15 INFO     	 * (global step 37150: loss: 0.7321919500827789, lr: 0.0001
2024-01-05 13:17:21 INFO     	 * (global step 37200: loss: 0.5818321257829666, lr: 0.0001
2024-01-05 13:17:26 INFO     	 * (global step 37250: loss: 0.49627260863780975, lr: 0.0001
2024-01-05 13:17:32 INFO     	 * (global step 37300: loss: 0.38408811390399933, lr: 0.0001
2024-01-05 13:17:38 INFO     	 * (global step 37350: loss: 0.33235087990760803, lr: 0.0001
2024-01-05 13:17:45 INFO     	 * (global step 37400: loss: 0.6568596661090851, lr: 0.0001
2024-01-05 13:17:51 INFO     	 * (global step 37450: loss: 0.2757169008255005, lr: 0.0001
2024-01-05 13:17:57 INFO     	 * (global step 37500: loss: 0.6900299489498138, lr: 0.0001
2024-01-05 13:18:03 INFO     	 * (global step 37550: loss: 0.32489413022994995, lr: 0.0001
2024-01-05 13:18:09 INFO     	 * (global step 37600: loss: 0.48228520154953003, lr: 0.0001
2024-01-05 13:18:15 INFO     	 * (global step 37650: loss: 0.40939249098300934, lr: 0.0001
2024-01-05 13:18:21 INFO     	 * (global step 37700: loss: 0.5030553936958313, lr: 0.0001
2024-01-05 13:18:27 INFO     	 * (global step 37750: loss: 0.5046706050634384, lr: 0.0001
2024-01-05 13:18:33 INFO     	 * (global step 37800: loss: 0.39213497936725616, lr: 0.0001
2024-01-05 13:18:39 INFO     	 * (global step 37850: loss: 0.36033400893211365, lr: 0.0001
2024-01-05 13:18:45 INFO     	 * (global step 37900: loss: 0.3836768567562103, lr: 0.0001
2024-01-05 13:18:52 INFO     	 * (global step 37950: loss: 0.26776833087205887, lr: 0.0001
2024-01-05 13:18:58 INFO     	 * (global step 38000: loss: 0.5117280185222626, lr: 0.0001
2024-01-05 13:19:04 INFO     	 * (global step 38050: loss: 0.49388487637043, lr: 0.0001
2024-01-05 13:19:10 INFO     	 * (global step 38100: loss: 0.4068217799067497, lr: 0.0001
2024-01-05 13:19:16 INFO     	 * (global step 38150: loss: 0.4880608022212982, lr: 0.0001
2024-01-05 13:19:22 INFO     	 * (global step 38200: loss: 0.3954120948910713, lr: 0.0001
2024-01-05 13:19:28 INFO     	 * (global step 38250: loss: 0.5207815319299698, lr: 0.0001
2024-01-05 13:19:34 INFO     	 * (global step 38300: loss: 0.4560431092977524, lr: 0.0001
2024-01-05 13:19:40 INFO     	 * (global step 38350: loss: 0.5518252849578857, lr: 0.0001
2024-01-05 13:19:46 INFO     	 * (global step 38400: loss: 0.6725575923919678, lr: 0.0001
2024-01-05 13:19:52 INFO     	 * (global step 38450: loss: 0.4371628910303116, lr: 0.0001
2024-01-05 13:19:58 INFO     	 * (global step 38500: loss: 0.41237664222717285, lr: 0.0001
2024-01-05 13:20:04 INFO     	 * (global step 38550: loss: 0.2371111549437046, lr: 0.0001
2024-01-05 13:20:10 INFO     	 * (global step 38600: loss: 0.2324826493859291, lr: 0.0001
2024-01-05 13:20:16 INFO     	 * (global step 38650: loss: 0.6882094293832779, lr: 0.0001
2024-01-05 13:20:22 INFO     	 * (global step 38700: loss: 0.39841943979263306, lr: 0.0001
2024-01-05 13:20:28 INFO     	 * (global step 38750: loss: 0.43608273565769196, lr: 0.0001
2024-01-05 13:20:34 INFO     	 * (global step 38800: loss: 0.24113105982542038, lr: 0.0001
2024-01-05 13:20:40 INFO     	 * (global step 38850: loss: 0.4628874957561493, lr: 0.0001
2024-01-05 13:20:46 INFO     	 * (global step 38900: loss: 0.3632468581199646, lr: 0.0001
2024-01-05 13:20:53 INFO     	 * (global step 38950: loss: 0.5510718077421188, lr: 0.0001
2024-01-05 13:20:59 INFO     	 * (global step 39000: loss: 0.3957275152206421, lr: 0.0001
2024-01-05 13:21:05 INFO     	 * (global step 39050: loss: 0.4538677781820297, lr: 0.0001
2024-01-05 13:21:11 INFO     	 * (global step 39100: loss: 0.4344892352819443, lr: 0.0001
2024-01-05 13:21:17 INFO     	 * (global step 39150: loss: 0.6147352010011673, lr: 0.0001
2024-01-05 13:21:23 INFO     	 * (global step 39200: loss: 0.3987829387187958, lr: 0.0001
2024-01-05 13:21:29 INFO     	 * (global step 39250: loss: 0.4566952586174011, lr: 0.0001
2024-01-05 13:21:35 INFO     	 * (global step 39300: loss: 0.4152769148349762, lr: 0.0001
2024-01-05 13:21:41 INFO     	 * (global step 39350: loss: 0.40277358889579773, lr: 0.0001
2024-01-05 13:21:47 INFO     	 * (global step 39400: loss: 0.3954289108514786, lr: 0.0001
2024-01-05 13:21:53 INFO     	 * (global step 39450: loss: 0.4735180288553238, lr: 0.0001
2024-01-05 13:21:59 INFO     	 * (global step 39500: loss: 0.5721611082553864, lr: 0.0001
2024-01-05 13:22:05 INFO     	 * (global step 39550: loss: 0.5450293272733688, lr: 0.0001
2024-01-05 13:22:11 INFO     	 * (global step 39600: loss: 0.6757841110229492, lr: 0.0001
2024-01-05 13:22:17 INFO     	 * (global step 39650: loss: 0.45644091069698334, lr: 0.0001
2024-01-05 13:22:23 INFO     	 * (global step 39700: loss: 0.40531477332115173, lr: 0.0001
2024-01-05 13:22:29 INFO     	 * (global step 39750: loss: 0.5514891445636749, lr: 0.0001
2024-01-05 13:22:35 INFO     	 * (global step 39800: loss: 0.8411406576633453, lr: 0.0001
2024-01-05 13:22:41 INFO     	 * (global step 39850: loss: 0.6966972053050995, lr: 0.0001
2024-01-05 13:22:47 INFO     	 * (global step 39900: loss: 0.3818083852529526, lr: 0.0001
2024-01-05 13:22:53 INFO     	 * (global step 39950: loss: 0.33816224336624146, lr: 0.0001
2024-01-05 13:22:59 INFO     	 * (global step 40000: loss: 0.7153388857841492, lr: 0.0001
2024-01-05 13:23:05 INFO     	 * (global step 40050: loss: 0.4391396790742874, lr: 0.0001
2024-01-05 13:23:11 INFO     	 * (global step 40100: loss: 0.5375024378299713, lr: 0.0001
2024-01-05 13:23:17 INFO     	 * (global step 40150: loss: 0.6013735234737396, lr: 0.0001
2024-01-05 13:23:23 INFO     	 * (global step 40200: loss: 0.561115175485611, lr: 0.0001
2024-01-05 13:23:29 INFO     	 * (global step 40250: loss: 0.5998769104480743, lr: 0.0001
2024-01-05 13:23:35 INFO     	 * (global step 40300: loss: 0.35308344662189484, lr: 0.0001
2024-01-05 13:23:41 INFO     	 * (global step 40350: loss: 0.5678191483020782, lr: 0.0001
2024-01-05 13:23:47 INFO     	 * (global step 40400: loss: 0.3396032452583313, lr: 0.0001
2024-01-05 13:23:53 INFO     	 * (global step 40450: loss: 0.3364346921443939, lr: 0.0001
2024-01-05 13:23:57 INFO     [epoch 9/15] average loss: 0.457, lr: 0.0001
2024-01-05 13:23:57 INFO     saving model related files
2024-01-05 13:23:57 INFO     saving model
2024-01-05 13:23:57 INFO     saving tokenizer
2024-01-05 13:23:57 INFO     saving optimizer
2024-01-05 13:23:58 INFO     remove old optimizer files
2024-01-05 13:23:59 INFO     complete training: model ckpt was saved at small_recreated_ckpt/model_eszyci
2024-01-05 13:24:04 INFO     ## 1st RUN: Configuration 2/12 ##
2024-01-05 13:24:04 INFO     initialize model trainer
2024-01-05 13:24:04 INFO     initialize checkpoint at small_recreated_ckpt/model_dpyopu
2024-01-05 13:24:05 INFO     hyperparameters
2024-01-05 13:24:05 INFO     	 * dataset_path: lmqg/qag_squad
2024-01-05 13:24:05 INFO     	 * dataset_name: default
2024-01-05 13:24:05 INFO     	 * input_types: ['paragraph']
2024-01-05 13:24:05 INFO     	 * output_types: ['questions_answers']
2024-01-05 13:24:05 INFO     	 * prefix_types: ['qag']
2024-01-05 13:24:05 INFO     	 * model: t5-small
2024-01-05 13:24:05 INFO     	 * max_length: 512
2024-01-05 13:24:05 INFO     	 * max_length_output: 256
2024-01-05 13:24:05 INFO     	 * epoch: 15
2024-01-05 13:24:05 INFO     	 * batch: 2
2024-01-05 13:24:05 INFO     	 * lr: 0.0001
2024-01-05 13:24:05 INFO     	 * fp16: False
2024-01-05 13:24:05 INFO     	 * random_seed: 1
2024-01-05 13:24:05 INFO     	 * gradient_accumulation_steps: 4
2024-01-05 13:24:05 INFO     	 * label_smoothing: 0.0
2024-01-05 13:24:05 INFO     initialize checkpoint with t5-small
2024-01-05 13:24:12 INFO     use spaCy answer extraction model: positionrank
2024-01-05 13:24:15 INFO     Model `t5-small`
2024-01-05 13:24:15 INFO     	 * Num of GPU in use: 1
2024-01-05 13:24:15 INFO     	 * Prefix: True
2024-01-05 13:24:15 INFO     	 * Language: en (ignore at the training phase)
2024-01-05 13:24:15 INFO     dataset preprocessing
2024-01-05 13:24:18 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featurelmqg/qag_squad/t5-small.512.256.paragraph.questions_answers.train.qag.pkl
2024-01-05 13:24:25 INFO     start model training
2024-01-05 13:24:36 INFO     	 * (global step 50: loss: 1.4955462515354156, lr: 0.0001
2024-01-05 13:24:48 INFO     	 * (global step 100: loss: 1.1167991906404495, lr: 0.0001
2024-01-05 13:24:59 INFO     	 * (global step 150: loss: 0.9872274398803711, lr: 0.0001
2024-01-05 13:25:10 INFO     	 * (global step 200: loss: 0.7949968427419662, lr: 0.0001
2024-01-05 13:25:22 INFO     	 * (global step 250: loss: 0.7567170113325119, lr: 0.0001
2024-01-05 13:25:33 INFO     	 * (global step 300: loss: 0.7695069760084152, lr: 0.0001
2024-01-05 13:25:45 INFO     	 * (global step 350: loss: 0.8666670024394989, lr: 0.0001
2024-01-05 13:25:56 INFO     	 * (global step 400: loss: 1.0058775693178177, lr: 0.0001
2024-01-05 13:26:08 INFO     	 * (global step 450: loss: 0.7224209010601044, lr: 0.0001
2024-01-05 13:26:19 INFO     	 * (global step 500: loss: 0.8913671523332596, lr: 0.0001
2024-01-05 13:26:31 INFO     	 * (global step 550: loss: 0.8170754462480545, lr: 0.0001
2024-01-05 13:26:42 INFO     	 * (global step 600: loss: 0.9849455952644348, lr: 0.0001
2024-01-05 13:26:54 INFO     	 * (global step 650: loss: 0.7450027316808701, lr: 0.0001
2024-01-05 13:27:05 INFO     	 * (global step 700: loss: 0.6529485806822777, lr: 0.0001
2024-01-05 13:27:17 INFO     	 * (global step 750: loss: 0.6669048368930817, lr: 0.0001
2024-01-05 13:27:28 INFO     	 * (global step 800: loss: 0.5557146966457367, lr: 0.0001
2024-01-05 13:27:40 INFO     	 * (global step 850: loss: 0.7133287489414215, lr: 0.0001
2024-01-05 13:27:51 INFO     	 * (global step 900: loss: 0.7699460685253143, lr: 0.0001
2024-01-05 13:28:03 INFO     	 * (global step 950: loss: 0.606502640992403, lr: 0.0001
2024-01-05 13:28:15 INFO     	 * (global step 1000: loss: 1.0782562792301178, lr: 0.0001
2024-01-05 13:28:27 INFO     	 * (global step 1050: loss: 0.5924564749002457, lr: 0.0001
2024-01-05 13:28:38 INFO     	 * (global step 1100: loss: 0.6975871920585632, lr: 0.0001
2024-01-05 13:28:50 INFO     	 * (global step 1150: loss: 1.0991872400045395, lr: 0.0001
2024-01-05 13:29:01 INFO     	 * (global step 1200: loss: 0.5771647170186043, lr: 0.0001
2024-01-05 13:29:13 INFO     	 * (global step 1250: loss: 0.5625322610139847, lr: 0.0001
2024-01-05 13:29:24 INFO     	 * (global step 1300: loss: 0.49254054576158524, lr: 0.0001
2024-01-05 13:29:36 INFO     	 * (global step 1350: loss: 0.6944154351949692, lr: 0.0001
2024-01-05 13:29:47 INFO     	 * (global step 1400: loss: 0.7001536637544632, lr: 0.0001
2024-01-05 13:29:59 INFO     	 * (global step 1450: loss: 0.7616716101765633, lr: 0.0001
2024-01-05 13:30:10 INFO     	 * (global step 1500: loss: 0.6485500335693359, lr: 0.0001
2024-01-05 13:30:22 INFO     	 * (global step 1550: loss: 0.7057115137577057, lr: 0.0001
2024-01-05 13:30:34 INFO     	 * (global step 1600: loss: 0.7205837368965149, lr: 0.0001
2024-01-05 13:30:45 INFO     	 * (global step 1650: loss: 0.6137843802571297, lr: 0.0001
2024-01-05 13:30:57 INFO     	 * (global step 1700: loss: 0.6724001541733742, lr: 0.0001
2024-01-05 13:31:08 INFO     	 * (global step 1750: loss: 0.5811005458235741, lr: 0.0001
2024-01-05 13:31:19 INFO     	 * (global step 1800: loss: 0.6039937138557434, lr: 0.0001
2024-01-05 13:31:31 INFO     	 * (global step 1850: loss: 0.42758382111787796, lr: 0.0001
2024-01-05 13:31:43 INFO     	 * (global step 1900: loss: 0.4973658099770546, lr: 0.0001
2024-01-05 13:31:54 INFO     	 * (global step 1950: loss: 0.7655559629201889, lr: 0.0001
2024-01-05 13:32:05 INFO     	 * (global step 2000: loss: 0.8832087516784668, lr: 0.0001
2024-01-05 13:32:11 INFO     [epoch 0/15] average loss: 0.756, lr: 0.0001
2024-01-05 13:32:11 INFO     saving model related files
2024-01-05 13:32:11 INFO     saving model
2024-01-05 13:32:12 INFO     saving tokenizer
2024-01-05 13:32:12 INFO     saving optimizer
2024-01-05 13:32:13 INFO     remove old optimizer files
2024-01-05 13:32:19 INFO     	 * (global step 2050: loss: 0.48383356258273125, lr: 0.0001
2024-01-05 13:32:30 INFO     	 * (global step 2100: loss: 0.5991823077201843, lr: 0.0001
2024-01-05 13:32:42 INFO     	 * (global step 2150: loss: 0.7605701759457588, lr: 0.0001
2024-01-05 13:32:53 INFO     	 * (global step 2200: loss: 0.5370596647262573, lr: 0.0001
2024-01-05 13:33:05 INFO     	 * (global step 2250: loss: 0.5413005948066711, lr: 0.0001
2024-01-05 13:33:16 INFO     	 * (global step 2300: loss: 0.6818302199244499, lr: 0.0001
2024-01-05 13:33:28 INFO     	 * (global step 2350: loss: 0.6597145274281502, lr: 0.0001
2024-01-05 13:33:39 INFO     	 * (global step 2400: loss: 0.6029577329754829, lr: 0.0001
2024-01-05 13:33:51 INFO     	 * (global step 2450: loss: 0.5625817403197289, lr: 0.0001
2024-01-05 13:34:02 INFO     	 * (global step 2500: loss: 0.7014346271753311, lr: 0.0001
2024-01-05 13:34:13 INFO     	 * (global step 2550: loss: 0.5347775518894196, lr: 0.0001
2024-01-05 13:34:25 INFO     	 * (global step 2600: loss: 0.5813846439123154, lr: 0.0001
2024-01-05 13:34:36 INFO     	 * (global step 2650: loss: 0.7186765372753143, lr: 0.0001
2024-01-05 13:34:48 INFO     	 * (global step 2700: loss: 0.5090348720550537, lr: 0.0001
2024-01-05 13:34:59 INFO     	 * (global step 2750: loss: 0.5747836381196976, lr: 0.0001
2024-01-05 13:35:11 INFO     	 * (global step 2800: loss: 0.5952723771333694, lr: 0.0001
2024-01-05 13:35:22 INFO     	 * (global step 2850: loss: 0.6800964847207069, lr: 0.0001
2024-01-05 13:35:33 INFO     	 * (global step 2900: loss: 0.7318496853113174, lr: 0.0001
2024-01-05 13:35:45 INFO     	 * (global step 2950: loss: 0.6865103915333748, lr: 0.0001
2024-01-05 13:35:56 INFO     	 * (global step 3000: loss: 0.601239487528801, lr: 0.0001
2024-01-05 13:36:08 INFO     	 * (global step 3050: loss: 0.5731271877884865, lr: 0.0001
2024-01-05 13:36:19 INFO     	 * (global step 3100: loss: 0.5117603838443756, lr: 0.0001
2024-01-05 13:36:30 INFO     	 * (global step 3150: loss: 0.6665432900190353, lr: 0.0001
2024-01-05 13:36:42 INFO     	 * (global step 3200: loss: 0.506366603076458, lr: 0.0001
2024-01-05 13:36:53 INFO     	 * (global step 3250: loss: 0.7026764824986458, lr: 0.0001
2024-01-05 13:37:05 INFO     	 * (global step 3300: loss: 0.502581275999546, lr: 0.0001
2024-01-05 13:37:16 INFO     	 * (global step 3350: loss: 0.599525973200798, lr: 0.0001
2024-01-05 13:37:28 INFO     	 * (global step 3400: loss: 0.6905371993780136, lr: 0.0001
2024-01-05 13:37:39 INFO     	 * (global step 3450: loss: 0.6483494937419891, lr: 0.0001
2024-01-05 13:37:51 INFO     	 * (global step 3500: loss: 0.5972741544246674, lr: 0.0001
2024-01-05 13:38:02 INFO     	 * (global step 3550: loss: 0.5717227160930634, lr: 0.0001
2024-01-05 13:38:14 INFO     	 * (global step 3600: loss: 0.6140751540660858, lr: 0.0001
2024-01-05 13:38:25 INFO     	 * (global step 3650: loss: 0.7285308763384819, lr: 0.0001
2024-01-05 13:38:36 INFO     	 * (global step 3700: loss: 0.903793141245842, lr: 0.0001
2024-01-05 13:38:48 INFO     	 * (global step 3750: loss: 0.5883577913045883, lr: 0.0001
2024-01-05 13:39:00 INFO     	 * (global step 3800: loss: 0.544867642223835, lr: 0.0001
2024-01-05 13:39:11 INFO     	 * (global step 3850: loss: 0.8280017673969269, lr: 0.0001
2024-01-05 13:39:23 INFO     	 * (global step 3900: loss: 0.5622287318110466, lr: 0.0001
2024-01-05 13:39:34 INFO     	 * (global step 3950: loss: 0.4537043198943138, lr: 0.0001
2024-01-05 13:39:46 INFO     	 * (global step 4000: loss: 0.5218100249767303, lr: 0.0001
2024-01-05 13:39:57 INFO     [epoch 1/15] average loss: 0.613, lr: 0.0001
2024-01-05 13:39:57 INFO     saving model related files
2024-01-05 13:39:57 INFO     saving model
2024-01-05 13:39:57 INFO     saving tokenizer
2024-01-05 13:39:57 INFO     saving optimizer
2024-01-05 13:39:58 INFO     remove old optimizer files
2024-01-05 13:39:59 INFO     	 * (global step 4050: loss: 0.4244029000401497, lr: 0.0001
2024-01-05 13:40:11 INFO     	 * (global step 4100: loss: 0.5515774190425873, lr: 0.0001
2024-01-05 13:40:22 INFO     	 * (global step 4150: loss: 0.5551953464746475, lr: 0.0001
2024-01-05 13:40:34 INFO     	 * (global step 4200: loss: 0.7788235619664192, lr: 0.0001
2024-01-05 13:40:45 INFO     	 * (global step 4250: loss: 0.5896280854940414, lr: 0.0001
2024-01-05 13:40:57 INFO     	 * (global step 4300: loss: 0.436215840280056, lr: 0.0001
2024-01-05 13:41:08 INFO     	 * (global step 4350: loss: 0.6102551370859146, lr: 0.0001
2024-01-05 13:41:20 INFO     	 * (global step 4400: loss: 0.4497310593724251, lr: 0.0001
2024-01-05 13:41:31 INFO     	 * (global step 4450: loss: 0.48040473461151123, lr: 0.0001
2024-01-05 13:41:42 INFO     	 * (global step 4500: loss: 0.5777470618486404, lr: 0.0001
2024-01-05 13:41:54 INFO     	 * (global step 4550: loss: 0.5441614463925362, lr: 0.0001
2024-01-05 13:42:06 INFO     	 * (global step 4600: loss: 0.5100801587104797, lr: 0.0001
2024-01-05 13:42:17 INFO     	 * (global step 4650: loss: 0.7740804776549339, lr: 0.0001
2024-01-05 13:42:29 INFO     	 * (global step 4700: loss: 0.6086262166500092, lr: 0.0001
2024-01-05 13:42:40 INFO     	 * (global step 4750: loss: 0.5281629487872124, lr: 0.0001
2024-01-05 13:42:52 INFO     	 * (global step 4800: loss: 0.5610864087939262, lr: 0.0001
2024-01-05 13:43:04 INFO     	 * (global step 4850: loss: 0.5884587615728378, lr: 0.0001
2024-01-05 13:43:15 INFO     	 * (global step 4900: loss: 0.48436594009399414, lr: 0.0001
2024-01-05 13:43:26 INFO     	 * (global step 4950: loss: 0.4653683081269264, lr: 0.0001
2024-01-05 13:43:38 INFO     	 * (global step 5000: loss: 0.5057509914040565, lr: 0.0001
2024-01-05 13:43:50 INFO     	 * (global step 5050: loss: 0.6946709752082825, lr: 0.0001
2024-01-05 13:44:06 INFO     	 * (global step 5100: loss: 0.5176359266042709, lr: 0.0001
2024-01-05 13:44:21 INFO     	 * (global step 5150: loss: 0.62281683832407, lr: 0.0001
2024-01-05 13:44:35 INFO     	 * (global step 5200: loss: 0.5537361800670624, lr: 0.0001
2024-01-05 13:44:51 INFO     	 * (global step 5250: loss: 0.5500413402915001, lr: 0.0001
2024-01-05 13:45:07 INFO     	 * (global step 5300: loss: 0.5168954953551292, lr: 0.0001
2024-01-05 13:45:23 INFO     	 * (global step 5350: loss: 0.6020682454109192, lr: 0.0001
2024-01-05 13:45:38 INFO     	 * (global step 5400: loss: 0.6002035662531853, lr: 0.0001
2024-01-05 13:45:53 INFO     	 * (global step 5450: loss: 0.5306218937039375, lr: 0.0001
2024-01-05 13:46:09 INFO     	 * (global step 5500: loss: 0.4829062819480896, lr: 0.0001
2024-01-05 13:46:24 INFO     	 * (global step 5550: loss: 0.6051541790366173, lr: 0.0001
2024-01-05 13:46:39 INFO     	 * (global step 5600: loss: 0.6394712850451469, lr: 0.0001
2024-01-05 13:46:53 INFO     	 * (global step 5650: loss: 0.5172268748283386, lr: 0.0001
2024-01-05 13:47:07 INFO     	 * (global step 5700: loss: 0.5948804244399071, lr: 0.0001
2024-01-05 13:47:18 INFO     	 * (global step 5750: loss: 0.5748758167028427, lr: 0.0001
2024-01-05 13:47:30 INFO     	 * (global step 5800: loss: 0.5725243538618088, lr: 0.0001
2024-01-05 13:47:45 INFO     	 * (global step 5850: loss: 0.6729178726673126, lr: 0.0001
2024-01-05 13:48:00 INFO     	 * (global step 5900: loss: 0.5643344447016716, lr: 0.0001
2024-01-05 13:48:15 INFO     	 * (global step 5950: loss: 0.5078850761055946, lr: 0.0001
2024-01-05 13:48:31 INFO     	 * (global step 6000: loss: 0.5091547034680843, lr: 0.0001
2024-01-05 13:48:46 INFO     	 * (global step 6050: loss: 0.7067169025540352, lr: 0.0001
2024-01-05 13:48:53 INFO     [epoch 2/15] average loss: 0.58, lr: 0.0001
2024-01-05 13:48:53 INFO     saving model related files
2024-01-05 13:48:53 INFO     saving model
2024-01-05 13:48:54 INFO     saving tokenizer
2024-01-05 13:48:54 INFO     saving optimizer
2024-01-05 13:48:57 INFO     remove old optimizer files
2024-01-05 13:49:05 INFO     	 * (global step 6100: loss: 0.58934336155653, lr: 0.0001
2024-01-05 13:49:21 INFO     	 * (global step 6150: loss: 0.5370837301015854, lr: 0.0001
2024-01-05 13:49:37 INFO     	 * (global step 6200: loss: 0.6186140924692154, lr: 0.0001
2024-01-05 13:49:53 INFO     	 * (global step 6250: loss: 0.6054520383477211, lr: 0.0001
2024-01-05 13:50:08 INFO     	 * (global step 6300: loss: 0.4603010416030884, lr: 0.0001
2024-01-05 13:50:23 INFO     	 * (global step 6350: loss: 0.7544330954551697, lr: 0.0001
2024-01-05 13:50:38 INFO     	 * (global step 6400: loss: 0.5834188312292099, lr: 0.0001
2024-01-05 13:50:54 INFO     	 * (global step 6450: loss: 0.671795204281807, lr: 0.0001
2024-01-05 13:51:09 INFO     	 * (global step 6500: loss: 0.5947812125086784, lr: 0.0001
2024-01-05 13:51:21 INFO     	 * (global step 6550: loss: 0.4457644894719124, lr: 0.0001
2024-01-05 13:51:32 INFO     	 * (global step 6600: loss: 0.46473222970962524, lr: 0.0001
2024-01-05 13:51:46 INFO     	 * (global step 6650: loss: 0.44008657336235046, lr: 0.0001
2024-01-05 13:52:00 INFO     	 * (global step 6700: loss: 0.5274462103843689, lr: 0.0001
2024-01-05 13:52:15 INFO     	 * (global step 6750: loss: 0.5474156886339188, lr: 0.0001
2024-01-05 13:52:30 INFO     	 * (global step 6800: loss: 0.6241913139820099, lr: 0.0001
2024-01-05 13:52:44 INFO     	 * (global step 6850: loss: 0.48057037591934204, lr: 0.0001
2024-01-05 13:52:59 INFO     	 * (global step 6900: loss: 0.4921932741999626, lr: 0.0001
2024-01-05 13:53:13 INFO     	 * (global step 6950: loss: 0.5016979202628136, lr: 0.0001
2024-01-05 13:53:27 INFO     	 * (global step 7000: loss: 0.40479981154203415, lr: 0.0001
2024-01-05 13:53:42 INFO     	 * (global step 7050: loss: 0.6345546320080757, lr: 0.0001
2024-01-05 13:53:57 INFO     	 * (global step 7100: loss: 0.5715430900454521, lr: 0.0001
2024-01-05 13:54:12 INFO     	 * (global step 7150: loss: 0.48361460492014885, lr: 0.0001
2024-01-05 13:54:26 INFO     	 * (global step 7200: loss: 0.5673552379012108, lr: 0.0001
2024-01-05 13:54:41 INFO     	 * (global step 7250: loss: 0.5720323696732521, lr: 0.0001
2024-01-05 13:54:54 INFO     	 * (global step 7300: loss: 0.48879706114530563, lr: 0.0001
2024-01-05 13:55:05 INFO     	 * (global step 7350: loss: 0.5657302737236023, lr: 0.0001
2024-01-05 13:55:18 INFO     	 * (global step 7400: loss: 0.48712490499019623, lr: 0.0001
2024-01-05 13:55:33 INFO     	 * (global step 7450: loss: 0.49383482336997986, lr: 0.0001
2024-01-05 13:55:47 INFO     	 * (global step 7500: loss: 0.5818496160209179, lr: 0.0001
2024-01-05 13:56:02 INFO     	 * (global step 7550: loss: 0.6605261564254761, lr: 0.0001
2024-01-05 13:56:16 INFO     	 * (global step 7600: loss: 0.5007145032286644, lr: 0.0001
2024-01-05 13:56:31 INFO     	 * (global step 7650: loss: 0.53778425604105, lr: 0.0001
2024-01-05 13:56:46 INFO     	 * (global step 7700: loss: 0.4589865505695343, lr: 0.0001
2024-01-05 13:57:01 INFO     	 * (global step 7750: loss: 0.45527036115527153, lr: 0.0001
2024-01-05 13:57:16 INFO     	 * (global step 7800: loss: 0.4968734085559845, lr: 0.0001
2024-01-05 13:57:30 INFO     	 * (global step 7850: loss: 0.601158507168293, lr: 0.0001
2024-01-05 13:57:45 INFO     	 * (global step 7900: loss: 0.47984492406249046, lr: 0.0001
2024-01-05 13:58:00 INFO     	 * (global step 7950: loss: 0.6047621071338654, lr: 0.0001
2024-01-05 13:58:13 INFO     	 * (global step 8000: loss: 0.5773551166057587, lr: 0.0001
2024-01-05 13:58:24 INFO     	 * (global step 8050: loss: 0.6313177347183228, lr: 0.0001
2024-01-05 13:58:35 INFO     [epoch 3/15] average loss: 0.557, lr: 0.0001
2024-01-05 13:58:35 INFO     saving model related files
2024-01-05 13:58:35 INFO     saving model
2024-01-05 13:58:36 INFO     saving tokenizer
2024-01-05 13:58:36 INFO     saving optimizer
2024-01-05 13:58:38 INFO     remove old optimizer files
2024-01-05 13:58:39 INFO     	 * (global step 8100: loss: 0.5966724306344986, lr: 0.0001
2024-01-05 13:58:54 INFO     	 * (global step 8150: loss: 0.5838007107377052, lr: 0.0001
2024-01-05 13:59:11 INFO     	 * (global step 8200: loss: 0.38839009031653404, lr: 0.0001
2024-01-05 13:59:26 INFO     	 * (global step 8250: loss: 0.501910462975502, lr: 0.0001
2024-01-05 13:59:41 INFO     	 * (global step 8300: loss: 0.49083012342453003, lr: 0.0001
2024-01-05 13:59:57 INFO     	 * (global step 8350: loss: 0.673408530652523, lr: 0.0001
2024-01-05 14:00:13 INFO     	 * (global step 8400: loss: 0.5774637833237648, lr: 0.0001
2024-01-05 14:00:28 INFO     	 * (global step 8450: loss: 0.549015685915947, lr: 0.0001
2024-01-05 14:00:43 INFO     	 * (global step 8500: loss: 0.6107925772666931, lr: 0.0001
2024-01-05 14:00:59 INFO     	 * (global step 8550: loss: 0.39146314561367035, lr: 0.0001
2024-01-05 14:01:15 INFO     	 * (global step 8600: loss: 0.39218585938215256, lr: 0.0001
2024-01-05 14:01:31 INFO     	 * (global step 8650: loss: 0.5274000763893127, lr: 0.0001
2024-01-05 14:01:47 INFO     	 * (global step 8700: loss: 0.6649462431669235, lr: 0.0001
2024-01-05 14:02:03 INFO     	 * (global step 8750: loss: 0.4662552997469902, lr: 0.0001
2024-01-05 14:02:17 INFO     	 * (global step 8800: loss: 0.6145815774798393, lr: 0.0001
2024-01-05 14:02:32 INFO     	 * (global step 8850: loss: 0.5993986278772354, lr: 0.0001
2024-01-05 14:02:43 INFO     	 * (global step 8900: loss: 0.5239043608307838, lr: 0.0001
2024-01-05 14:02:55 INFO     	 * (global step 8950: loss: 0.5464335232973099, lr: 0.0001
2024-01-05 14:03:09 INFO     	 * (global step 9000: loss: 0.5230527594685555, lr: 0.0001
2024-01-05 14:03:24 INFO     	 * (global step 9050: loss: 0.5115008279681206, lr: 0.0001
2024-01-05 14:03:38 INFO     	 * (global step 9100: loss: 0.4732683524489403, lr: 0.0001
2024-01-05 14:03:53 INFO     	 * (global step 9150: loss: 0.34026137739419937, lr: 0.0001
2024-01-05 14:04:08 INFO     	 * (global step 9200: loss: 0.6707894206047058, lr: 0.0001
2024-01-05 14:04:22 INFO     	 * (global step 9250: loss: 0.43180064857006073, lr: 0.0001
2024-01-05 14:04:36 INFO     	 * (global step 9300: loss: 0.5296154841780663, lr: 0.0001
2024-01-05 14:04:51 INFO     	 * (global step 9350: loss: 0.5494293868541718, lr: 0.0001
2024-01-05 14:05:05 INFO     	 * (global step 9400: loss: 0.672790601849556, lr: 0.0001
2024-01-05 14:05:20 INFO     	 * (global step 9450: loss: 0.5448258966207504, lr: 0.0001
2024-01-05 14:05:34 INFO     	 * (global step 9500: loss: 0.5142890587449074, lr: 0.0001
2024-01-05 14:05:48 INFO     	 * (global step 9550: loss: 0.5235175788402557, lr: 0.0001
2024-01-05 14:06:00 INFO     	 * (global step 9600: loss: 0.44264473766088486, lr: 0.0001
2024-01-05 14:06:12 INFO     	 * (global step 9650: loss: 0.457332506775856, lr: 0.0001
2024-01-05 14:06:26 INFO     	 * (global step 9700: loss: 0.562262587249279, lr: 0.0001
2024-01-05 14:06:41 INFO     	 * (global step 9750: loss: 0.4762308821082115, lr: 0.0001
2024-01-05 14:06:57 INFO     	 * (global step 9800: loss: 0.5548513531684875, lr: 0.0001
2024-01-05 14:07:13 INFO     	 * (global step 9850: loss: 0.4031623676419258, lr: 0.0001
2024-01-05 14:07:28 INFO     	 * (global step 9900: loss: 0.49038032442331314, lr: 0.0001
2024-01-05 14:07:44 INFO     	 * (global step 9950: loss: 0.4865825027227402, lr: 0.0001
2024-01-05 14:08:00 INFO     	 * (global step 10000: loss: 0.46627277135849, lr: 0.0001
2024-01-05 14:08:15 INFO     	 * (global step 10050: loss: 0.5438554137945175, lr: 0.0001
2024-01-05 14:08:31 INFO     	 * (global step 10100: loss: 0.46640339493751526, lr: 0.0001
2024-01-05 14:08:38 INFO     [epoch 4/15] average loss: 0.54, lr: 0.0001
2024-01-05 14:08:38 INFO     saving model related files
2024-01-05 14:08:38 INFO     saving model
2024-01-05 14:08:39 INFO     saving tokenizer
2024-01-05 14:08:39 INFO     saving optimizer
2024-01-05 14:08:41 INFO     remove old optimizer files
2024-01-05 14:08:50 INFO     	 * (global step 10150: loss: 0.4105495885014534, lr: 0.0001
2024-01-05 14:09:05 INFO     	 * (global step 10200: loss: 0.46779511123895645, lr: 0.0001
2024-01-05 14:09:20 INFO     	 * (global step 10250: loss: 0.39775293320417404, lr: 0.0001
2024-01-05 14:09:36 INFO     	 * (global step 10300: loss: 0.4633867219090462, lr: 0.0001
2024-01-05 14:09:51 INFO     	 * (global step 10350: loss: 0.6072700023651123, lr: 0.0001
2024-01-05 14:10:04 INFO     	 * (global step 10400: loss: 0.5499310418963432, lr: 0.0001
2024-01-05 14:10:15 INFO     	 * (global step 10450: loss: 0.43664440512657166, lr: 0.0001
2024-01-05 14:10:28 INFO     	 * (global step 10500: loss: 0.47967173904180527, lr: 0.0001
2024-01-05 14:10:42 INFO     	 * (global step 10550: loss: 0.4832189604640007, lr: 0.0001
2024-01-05 14:10:57 INFO     	 * (global step 10600: loss: 0.5463766828179359, lr: 0.0001
2024-01-05 14:11:12 INFO     	 * (global step 10650: loss: 0.5807871147990227, lr: 0.0001
2024-01-05 14:11:26 INFO     	 * (global step 10700: loss: 0.499735489487648, lr: 0.0001
2024-01-05 14:11:41 INFO     	 * (global step 10750: loss: 0.3845597803592682, lr: 0.0001
2024-01-05 14:11:56 INFO     	 * (global step 10800: loss: 0.5210908874869347, lr: 0.0001
2024-01-05 14:12:10 INFO     	 * (global step 10850: loss: 0.5116683393716812, lr: 0.0001
2024-01-05 14:12:24 INFO     	 * (global step 10900: loss: 0.5932618379592896, lr: 0.0001
2024-01-05 14:12:38 INFO     	 * (global step 10950: loss: 0.5455011576414108, lr: 0.0001
2024-01-05 14:12:53 INFO     	 * (global step 11000: loss: 0.4692825600504875, lr: 0.0001
2024-01-05 14:13:06 INFO     	 * (global step 11050: loss: 0.4917216897010803, lr: 0.0001
2024-01-05 14:13:17 INFO     	 * (global step 11100: loss: 0.4848748929798603, lr: 0.0001
2024-01-05 14:13:30 INFO     	 * (global step 11150: loss: 0.4690546542406082, lr: 0.0001
2024-01-05 14:13:44 INFO     	 * (global step 11200: loss: 0.48420120775699615, lr: 0.0001
2024-01-05 14:13:59 INFO     	 * (global step 11250: loss: 0.6483311951160431, lr: 0.0001
2024-01-05 14:14:15 INFO     	 * (global step 11300: loss: 0.49281519651412964, lr: 0.0001
2024-01-05 14:14:30 INFO     	 * (global step 11350: loss: 0.4764911010861397, lr: 0.0001
2024-01-05 14:14:44 INFO     	 * (global step 11400: loss: 0.5021575093269348, lr: 0.0001
2024-01-05 14:14:58 INFO     	 * (global step 11450: loss: 0.38396643847227097, lr: 0.0001
2024-01-05 14:15:13 INFO     	 * (global step 11500: loss: 0.5004323199391365, lr: 0.0001
2024-01-05 14:15:27 INFO     	 * (global step 11550: loss: 0.5527226328849792, lr: 0.0001
2024-01-05 14:15:42 INFO     	 * (global step 11600: loss: 0.4492727965116501, lr: 0.0001
2024-01-05 14:15:56 INFO     	 * (global step 11650: loss: 0.3167554810643196, lr: 0.0001
2024-01-05 14:16:11 INFO     	 * (global step 11700: loss: 0.47321099042892456, lr: 0.0001
2024-01-05 14:16:25 INFO     	 * (global step 11750: loss: 0.5670232027769089, lr: 0.0001
2024-01-05 14:16:38 INFO     	 * (global step 11800: loss: 0.565028190612793, lr: 0.0001
2024-01-05 14:16:49 INFO     	 * (global step 11850: loss: 0.5910851545631886, lr: 0.0001
2024-01-05 14:17:02 INFO     	 * (global step 11900: loss: 0.40740544348955154, lr: 0.0001
2024-01-05 14:17:17 INFO     	 * (global step 11950: loss: 0.4332578256726265, lr: 0.0001
2024-01-05 14:17:32 INFO     	 * (global step 12000: loss: 0.42880725115537643, lr: 0.0001
2024-01-05 14:17:47 INFO     	 * (global step 12050: loss: 0.41512126475572586, lr: 0.0001
2024-01-05 14:18:02 INFO     	 * (global step 12100: loss: 0.49462641030550003, lr: 0.0001
2024-01-05 14:18:15 INFO     [epoch 5/15] average loss: 0.526, lr: 0.0001
2024-01-05 14:18:15 INFO     saving model related files
2024-01-05 14:18:15 INFO     saving model
2024-01-05 14:18:16 INFO     saving tokenizer
2024-01-05 14:18:16 INFO     saving optimizer
2024-01-05 14:18:19 INFO     remove old optimizer files
2024-01-05 14:18:21 INFO     	 * (global step 12150: loss: 0.4589686542749405, lr: 0.0001
2024-01-05 14:18:36 INFO     	 * (global step 12200: loss: 0.5046711713075638, lr: 0.0001
2024-01-05 14:18:51 INFO     	 * (global step 12250: loss: 0.5998752787709236, lr: 0.0001
2024-01-05 14:19:06 INFO     	 * (global step 12300: loss: 0.619462139904499, lr: 0.0001
2024-01-05 14:19:21 INFO     	 * (global step 12350: loss: 0.4375237599015236, lr: 0.0001
2024-01-05 14:19:35 INFO     	 * (global step 12400: loss: 0.549061544239521, lr: 0.0001
2024-01-05 14:19:50 INFO     	 * (global step 12450: loss: 0.6560651063919067, lr: 0.0001
2024-01-05 14:20:04 INFO     	 * (global step 12500: loss: 0.40178706124424934, lr: 0.0001
2024-01-05 14:20:15 INFO     	 * (global step 12550: loss: 0.5004288479685783, lr: 0.0001
2024-01-05 14:20:27 INFO     	 * (global step 12600: loss: 0.6261713355779648, lr: 0.0001
2024-01-05 14:20:41 INFO     	 * (global step 12650: loss: 0.6092318519949913, lr: 0.0001
2024-01-05 14:20:55 INFO     	 * (global step 12700: loss: 0.5516335517168045, lr: 0.0001
2024-01-05 14:21:09 INFO     	 * (global step 12750: loss: 0.5312933400273323, lr: 0.0001
2024-01-05 14:21:23 INFO     	 * (global step 12800: loss: 0.7628758028149605, lr: 0.0001
2024-01-05 14:21:37 INFO     	 * (global step 12850: loss: 0.5284901484847069, lr: 0.0001
2024-01-05 14:21:51 INFO     	 * (global step 12900: loss: 0.5214658081531525, lr: 0.0001
2024-01-05 14:22:06 INFO     	 * (global step 12950: loss: 0.5270924717187881, lr: 0.0001
2024-01-05 14:22:19 INFO     	 * (global step 13000: loss: 0.43528544157743454, lr: 0.0001
2024-01-05 14:22:33 INFO     	 * (global step 13050: loss: 0.5146926492452621, lr: 0.0001
2024-01-05 14:22:48 INFO     	 * (global step 13100: loss: 0.510573536157608, lr: 0.0001
2024-01-05 14:23:03 INFO     	 * (global step 13150: loss: 0.5587791427969933, lr: 0.0001
2024-01-05 14:23:14 INFO     	 * (global step 13200: loss: 0.4647675231099129, lr: 0.0001
2024-01-05 14:23:26 INFO     	 * (global step 13250: loss: 0.557974137365818, lr: 0.0001
2024-01-05 14:23:40 INFO     	 * (global step 13300: loss: 0.5108836218714714, lr: 0.0001
2024-01-05 14:23:55 INFO     	 * (global step 13350: loss: 0.6556730419397354, lr: 0.0001
2024-01-05 14:24:10 INFO     	 * (global step 13400: loss: 0.5596410855650902, lr: 0.0001
2024-01-05 14:24:25 INFO     	 * (global step 13450: loss: 0.5427503138780594, lr: 0.0001
2024-01-05 14:24:40 INFO     	 * (global step 13500: loss: 0.5279445052146912, lr: 0.0001
2024-01-05 14:24:54 INFO     	 * (global step 13550: loss: 0.42112261056900024, lr: 0.0001
2024-01-05 14:25:09 INFO     	 * (global step 13600: loss: 0.3915918506681919, lr: 0.0001
2024-01-05 14:25:24 INFO     	 * (global step 13650: loss: 0.4954230263829231, lr: 0.0001
2024-01-05 14:25:38 INFO     	 * (global step 13700: loss: 0.6094599664211273, lr: 0.0001
2024-01-05 14:25:53 INFO     	 * (global step 13750: loss: 0.48597538471221924, lr: 0.0001
2024-01-05 14:26:08 INFO     	 * (global step 13800: loss: 0.4105406478047371, lr: 0.0001
2024-01-05 14:26:23 INFO     	 * (global step 13850: loss: 0.5552844107151031, lr: 0.0001
2024-01-05 14:26:37 INFO     	 * (global step 13900: loss: 0.47094614058732986, lr: 0.0001
2024-01-05 14:26:48 INFO     	 * (global step 13950: loss: 0.502746157348156, lr: 0.0001
2024-01-05 14:27:00 INFO     	 * (global step 14000: loss: 0.56135593354702, lr: 0.0001
2024-01-05 14:27:14 INFO     	 * (global step 14050: loss: 0.5845224857330322, lr: 0.0001
2024-01-05 14:27:29 INFO     	 * (global step 14100: loss: 0.5195747911930084, lr: 0.0001
2024-01-05 14:27:43 INFO     	 * (global step 14150: loss: 0.5595905929803848, lr: 0.0001
2024-01-05 14:27:49 INFO     [epoch 6/15] average loss: 0.513, lr: 0.0001
2024-01-05 14:27:49 INFO     saving model related files
2024-01-05 14:27:49 INFO     saving model
2024-01-05 14:27:50 INFO     saving tokenizer
2024-01-05 14:27:50 INFO     saving optimizer
2024-01-05 14:27:52 INFO     remove old optimizer files
2024-01-05 14:28:01 INFO     	 * (global step 14200: loss: 0.49086906760931015, lr: 0.0001
2024-01-05 14:28:16 INFO     	 * (global step 14250: loss: 0.5263861268758774, lr: 0.0001
2024-01-05 14:28:30 INFO     	 * (global step 14300: loss: 0.5120199024677277, lr: 0.0001
2024-01-05 14:28:45 INFO     	 * (global step 14350: loss: 0.5828988403081894, lr: 0.0001
2024-01-05 14:28:59 INFO     	 * (global step 14400: loss: 0.46183454990386963, lr: 0.0001
2024-01-05 14:29:14 INFO     	 * (global step 14450: loss: 0.540866419672966, lr: 0.0001
2024-01-05 14:29:28 INFO     	 * (global step 14500: loss: 0.5100715756416321, lr: 0.0001
2024-01-05 14:29:42 INFO     	 * (global step 14550: loss: 0.4694499373435974, lr: 0.0001
2024-01-05 14:29:53 INFO     	 * (global step 14600: loss: 0.4317501224577427, lr: 0.0001
2024-01-05 14:30:05 INFO     	 * (global step 14650: loss: 0.4587354362010956, lr: 0.0001
2024-01-05 14:30:19 INFO     	 * (global step 14700: loss: 0.4748501628637314, lr: 0.0001
2024-01-05 14:30:33 INFO     	 * (global step 14750: loss: 0.36915434151887894, lr: 0.0001
2024-01-05 14:30:48 INFO     	 * (global step 14800: loss: 0.46061474084854126, lr: 0.0001
2024-01-05 14:31:02 INFO     	 * (global step 14850: loss: 0.46084268391132355, lr: 0.0001
2024-01-05 14:31:17 INFO     	 * (global step 14900: loss: 0.5685327425599098, lr: 0.0001
2024-01-05 14:31:31 INFO     	 * (global step 14950: loss: 0.55806964635849, lr: 0.0001
2024-01-05 14:31:47 INFO     	 * (global step 15000: loss: 0.5841346010565758, lr: 0.0001
2024-01-05 14:32:01 INFO     	 * (global step 15050: loss: 0.4883330911397934, lr: 0.0001
2024-01-05 14:32:15 INFO     	 * (global step 15100: loss: 0.5283453613519669, lr: 0.0001
2024-01-05 14:32:29 INFO     	 * (global step 15150: loss: 0.7935894578695297, lr: 0.0001
2024-01-05 14:32:44 INFO     	 * (global step 15200: loss: 0.5182494595646858, lr: 0.0001
2024-01-05 14:32:57 INFO     	 * (global step 15250: loss: 0.4074845090508461, lr: 0.0001
2024-01-05 14:33:09 INFO     	 * (global step 15300: loss: 0.5335349068045616, lr: 0.0001
2024-01-05 14:33:21 INFO     	 * (global step 15350: loss: 0.5163246393203735, lr: 0.0001
2024-01-05 14:33:35 INFO     	 * (global step 15400: loss: 0.43224573135375977, lr: 0.0001
2024-01-05 14:33:49 INFO     	 * (global step 15450: loss: 0.5930051803588867, lr: 0.0001
2024-01-05 14:34:02 INFO     	 * (global step 15500: loss: 0.5289953723549843, lr: 0.0001
2024-01-05 14:34:16 INFO     	 * (global step 15550: loss: 0.45099780708551407, lr: 0.0001
2024-01-05 14:34:30 INFO     	 * (global step 15600: loss: 0.5078125, lr: 0.0001
2024-01-05 14:34:43 INFO     	 * (global step 15650: loss: 0.5724261552095413, lr: 0.0001
2024-01-05 14:34:58 INFO     	 * (global step 15700: loss: 0.5826005712151527, lr: 0.0001
2024-01-05 14:35:12 INFO     	 * (global step 15750: loss: 0.42310071364045143, lr: 0.0001
2024-01-05 14:35:26 INFO     	 * (global step 15800: loss: 0.4678405188024044, lr: 0.0001
2024-01-05 14:35:38 INFO     	 * (global step 15850: loss: 0.6370821744203568, lr: 0.0001
2024-01-05 14:35:50 INFO     	 * (global step 15900: loss: 0.42140059173107147, lr: 0.0001
2024-01-05 14:36:02 INFO     	 * (global step 15950: loss: 0.42949777096509933, lr: 0.0001
2024-01-05 14:36:17 INFO     	 * (global step 16000: loss: 0.5374988466501236, lr: 0.0001
2024-01-05 14:36:32 INFO     	 * (global step 16050: loss: 0.5086580365896225, lr: 0.0001
2024-01-05 14:36:47 INFO     	 * (global step 16100: loss: 0.47166192159056664, lr: 0.0001
2024-01-05 14:37:02 INFO     	 * (global step 16150: loss: 0.6001443192362785, lr: 0.0001
2024-01-05 14:37:14 INFO     [epoch 7/15] average loss: 0.502, lr: 0.0001
2024-01-05 14:37:14 INFO     saving model related files
2024-01-05 14:37:14 INFO     saving model
2024-01-05 14:37:15 INFO     saving tokenizer
2024-01-05 14:37:16 INFO     saving optimizer
2024-01-05 14:37:18 INFO     remove old optimizer files
2024-01-05 14:37:20 INFO     	 * (global step 16200: loss: 0.5022148638963699, lr: 0.0001
2024-01-05 14:37:34 INFO     	 * (global step 16250: loss: 0.457267127931118, lr: 0.0001
2024-01-05 14:37:49 INFO     	 * (global step 16300: loss: 0.4795990586280823, lr: 0.0001
2024-01-05 14:38:04 INFO     	 * (global step 16350: loss: 0.44742175936698914, lr: 0.0001
2024-01-05 14:38:18 INFO     	 * (global step 16400: loss: 0.5076960399746895, lr: 0.0001
2024-01-05 14:38:33 INFO     	 * (global step 16450: loss: 0.591132864356041, lr: 0.0001
2024-01-05 14:38:48 INFO     	 * (global step 16500: loss: 0.3960685357451439, lr: 0.0001
2024-01-05 14:39:00 INFO     	 * (global step 16550: loss: 0.5763152539730072, lr: 0.0001
2024-01-05 14:39:11 INFO     	 * (global step 16600: loss: 0.5118898451328278, lr: 0.0001
2024-01-05 14:39:24 INFO     	 * (global step 16650: loss: 0.38414373621344566, lr: 0.0001
2024-01-05 14:39:38 INFO     	 * (global step 16700: loss: 0.5257851779460907, lr: 0.0001
2024-01-05 14:39:52 INFO     	 * (global step 16750: loss: 0.5565710589289665, lr: 0.0001
2024-01-05 14:40:07 INFO     	 * (global step 16800: loss: 0.7101994082331657, lr: 0.0001
2024-01-05 14:40:22 INFO     	 * (global step 16850: loss: 0.4039333499968052, lr: 0.0001
2024-01-05 14:40:36 INFO     	 * (global step 16900: loss: 0.5164197087287903, lr: 0.0001
2024-01-05 14:40:51 INFO     	 * (global step 16950: loss: 0.4223092310130596, lr: 0.0001
2024-01-05 14:41:05 INFO     	 * (global step 17000: loss: 0.5581961423158646, lr: 0.0001
2024-01-05 14:41:20 INFO     	 * (global step 17050: loss: 0.45794619619846344, lr: 0.0001
2024-01-05 14:41:34 INFO     	 * (global step 17100: loss: 0.4419848471879959, lr: 0.0001
2024-01-05 14:41:48 INFO     	 * (global step 17150: loss: 0.3140944093465805, lr: 0.0001
2024-01-05 14:42:00 INFO     	 * (global step 17200: loss: 0.3101065494120121, lr: 0.0001
2024-01-05 14:42:12 INFO     	 * (global step 17250: loss: 0.4154040962457657, lr: 0.0001
2024-01-05 14:42:24 INFO     	 * (global step 17300: loss: 0.48075101524591446, lr: 0.0001
2024-01-05 14:42:37 INFO     	 * (global step 17350: loss: 0.5125697329640388, lr: 0.0001
2024-01-05 14:42:51 INFO     	 * (global step 17400: loss: 0.37434985488653183, lr: 0.0001
2024-01-05 14:43:05 INFO     	 * (global step 17450: loss: 0.4906386509537697, lr: 0.0001
2024-01-05 14:43:18 INFO     	 * (global step 17500: loss: 0.5031690299510956, lr: 0.0001
2024-01-05 14:43:32 INFO     	 * (global step 17550: loss: 0.48585106804966927, lr: 0.0001
2024-01-05 14:43:45 INFO     	 * (global step 17600: loss: 0.5252267494797707, lr: 0.0001
2024-01-05 14:43:59 INFO     	 * (global step 17650: loss: 0.4540872946381569, lr: 0.0001
2024-01-05 14:44:13 INFO     	 * (global step 17700: loss: 0.37095415592193604, lr: 0.0001
2024-01-05 14:44:26 INFO     	 * (global step 17750: loss: 0.4270860552787781, lr: 0.0001
2024-01-05 14:44:38 INFO     	 * (global step 17800: loss: 0.4240192770957947, lr: 0.0001
2024-01-05 14:44:49 INFO     	 * (global step 17850: loss: 0.5158713161945343, lr: 0.0001
2024-01-05 14:45:01 INFO     	 * (global step 17900: loss: 0.439183734357357, lr: 0.0001
2024-01-05 14:45:15 INFO     	 * (global step 17950: loss: 0.5227502509951591, lr: 0.0001
2024-01-05 14:45:30 INFO     	 * (global step 18000: loss: 0.41366907209157944, lr: 0.0001
2024-01-05 14:45:44 INFO     	 * (global step 18050: loss: 0.3753909505903721, lr: 0.0001
2024-01-05 14:45:57 INFO     	 * (global step 18100: loss: 0.5573665052652359, lr: 0.0001
2024-01-05 14:46:11 INFO     	 * (global step 18150: loss: 0.4185846373438835, lr: 0.0001
2024-01-05 14:46:26 INFO     	 * (global step 18200: loss: 0.47487764805555344, lr: 0.0001
2024-01-05 14:46:31 INFO     [epoch 8/15] average loss: 0.491, lr: 0.0001
2024-01-05 14:46:31 INFO     saving model related files
2024-01-05 14:46:31 INFO     saving model
2024-01-05 14:46:31 INFO     saving tokenizer
2024-01-05 14:46:31 INFO     saving optimizer
2024-01-05 14:46:33 INFO     remove old optimizer files
2024-01-05 14:46:43 INFO     	 * (global step 18250: loss: 0.5083728656172752, lr: 0.0001
2024-01-05 14:46:58 INFO     	 * (global step 18300: loss: 0.4326314106583595, lr: 0.0001
2024-01-05 14:47:12 INFO     	 * (global step 18350: loss: 0.5082734003663063, lr: 0.0001
2024-01-05 14:47:27 INFO     	 * (global step 18400: loss: 0.4503106474876404, lr: 0.0001
2024-01-05 14:47:41 INFO     	 * (global step 18450: loss: 0.5625951439142227, lr: 0.0001
2024-01-05 14:47:52 INFO     	 * (global step 18500: loss: 0.4769355058670044, lr: 0.0001
2024-01-05 14:48:04 INFO     	 * (global step 18550: loss: 0.45416159927845, lr: 0.0001
2024-01-05 14:48:17 INFO     	 * (global step 18600: loss: 0.508095033466816, lr: 0.0001
2024-01-05 14:48:31 INFO     	 * (global step 18650: loss: 0.44266290962696075, lr: 0.0001
2024-01-05 14:48:45 INFO     	 * (global step 18700: loss: 0.6542918309569359, lr: 0.0001
2024-01-05 14:48:59 INFO     	 * (global step 18750: loss: 0.6313309222459793, lr: 0.0001
2024-01-05 14:49:14 INFO     	 * (global step 18800: loss: 0.4673551097512245, lr: 0.0001
2024-01-05 14:49:28 INFO     	 * (global step 18850: loss: 0.47272835671901703, lr: 0.0001
2024-01-05 14:49:42 INFO     	 * (global step 18900: loss: 0.37911556661129, lr: 0.0001
2024-01-05 14:49:56 INFO     	 * (global step 18950: loss: 0.40212415903806686, lr: 0.0001
2024-01-05 14:50:11 INFO     	 * (global step 19000: loss: 0.45854760706424713, lr: 0.0001
2024-01-05 14:50:25 INFO     	 * (global step 19050: loss: 0.48147744685411453, lr: 0.0001
2024-01-05 14:50:36 INFO     	 * (global step 19100: loss: 0.5157676078379154, lr: 0.0001
2024-01-05 14:50:48 INFO     	 * (global step 19150: loss: 0.4437032490968704, lr: 0.0001
2024-01-05 14:51:01 INFO     	 * (global step 19200: loss: 0.5836548879742622, lr: 0.0001
2024-01-05 14:51:17 INFO     	 * (global step 19250: loss: 0.42202015966176987, lr: 0.0001
2024-01-05 14:51:31 INFO     	 * (global step 19300: loss: 0.3479374088346958, lr: 0.0001
2024-01-05 14:51:45 INFO     	 * (global step 19350: loss: 0.49864669144153595, lr: 0.0001
2024-01-05 14:52:00 INFO     	 * (global step 19400: loss: 0.4247259795665741, lr: 0.0001
2024-01-05 14:52:14 INFO     	 * (global step 19450: loss: 0.40509653836488724, lr: 0.0001
2024-01-05 14:52:28 INFO     	 * (global step 19500: loss: 0.4452081546187401, lr: 0.0001
2024-01-05 14:52:43 INFO     	 * (global step 19550: loss: 0.4420243948698044, lr: 0.0001
2024-01-05 14:52:57 INFO     	 * (global step 19600: loss: 0.48745013028383255, lr: 0.0001
2024-01-05 14:53:11 INFO     	 * (global step 19650: loss: 0.43126221001148224, lr: 0.0001
2024-01-05 14:53:26 INFO     	 * (global step 19700: loss: 0.37988152354955673, lr: 0.0001
2024-01-05 14:53:39 INFO     	 * (global step 19750: loss: 0.6182368770241737, lr: 0.0001
2024-01-05 14:53:50 INFO     	 * (global step 19800: loss: 0.6305873021483421, lr: 0.0001
2024-01-05 14:54:02 INFO     	 * (global step 19850: loss: 0.4764581769704819, lr: 0.0001
2024-01-05 14:54:15 INFO     	 * (global step 19900: loss: 0.6541268303990364, lr: 0.0001
2024-01-05 14:54:29 INFO     	 * (global step 19950: loss: 0.4037722647190094, lr: 0.0001
2024-01-05 14:54:43 INFO     	 * (global step 20000: loss: 0.5667109936475754, lr: 0.0001
2024-01-05 14:54:58 INFO     	 * (global step 20050: loss: 0.4647979512810707, lr: 0.0001
2024-01-05 14:55:11 INFO     	 * (global step 20100: loss: 0.48997417092323303, lr: 0.0001
2024-01-05 14:55:25 INFO     	 * (global step 20150: loss: 0.37754178792238235, lr: 0.0001
2024-01-05 14:55:39 INFO     	 * (global step 20200: loss: 0.4290109910070896, lr: 0.0001
2024-01-05 14:55:51 INFO     [epoch 9/15] average loss: 0.482, lr: 0.0001
2024-01-05 14:55:51 INFO     saving model related files
2024-01-05 14:55:51 INFO     saving model
2024-01-05 14:55:51 INFO     saving tokenizer
2024-01-05 14:55:51 INFO     saving optimizer
2024-01-05 14:55:53 INFO     remove old optimizer files
2024-01-05 14:55:53 INFO     complete training: model ckpt was saved at small_recreated_ckpt/model_dpyopu
2024-01-05 14:55:54 INFO     ## 1st RUN: Configuration 3/12 ##
2024-01-05 14:55:54 INFO     initialize model trainer
2024-01-05 14:55:54 INFO     initialize checkpoint at small_recreated_ckpt/model_mzgdpa
2024-01-05 14:55:54 INFO     hyperparameters
2024-01-05 14:55:54 INFO     	 * dataset_path: lmqg/qag_squad
2024-01-05 14:55:54 INFO     	 * dataset_name: default
2024-01-05 14:55:54 INFO     	 * input_types: ['paragraph']
2024-01-05 14:55:54 INFO     	 * output_types: ['questions_answers']
2024-01-05 14:55:54 INFO     	 * prefix_types: ['qag']
2024-01-05 14:55:54 INFO     	 * model: t5-small
2024-01-05 14:55:54 INFO     	 * max_length: 512
2024-01-05 14:55:54 INFO     	 * max_length_output: 256
2024-01-05 14:55:54 INFO     	 * epoch: 15
2024-01-05 14:55:54 INFO     	 * batch: 2
2024-01-05 14:55:54 INFO     	 * lr: 0.0001
2024-01-05 14:55:54 INFO     	 * fp16: False
2024-01-05 14:55:54 INFO     	 * random_seed: 1
2024-01-05 14:55:54 INFO     	 * gradient_accumulation_steps: 2
2024-01-05 14:55:54 INFO     	 * label_smoothing: 0.0
2024-01-05 14:55:54 INFO     initialize checkpoint with t5-small
2024-01-05 14:56:02 INFO     use spaCy answer extraction model: positionrank
2024-01-05 14:56:04 INFO     Model `t5-small`
2024-01-05 14:56:04 INFO     	 * Num of GPU in use: 1
2024-01-05 14:56:04 INFO     	 * Prefix: True
2024-01-05 14:56:04 INFO     	 * Language: en (ignore at the training phase)
2024-01-05 14:56:04 INFO     dataset preprocessing
2024-01-05 14:56:06 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featurelmqg/qag_squad/t5-small.512.256.paragraph.questions_answers.train.qag.pkl
2024-01-05 14:56:11 INFO     start model training
2024-01-05 14:56:16 INFO     	 * (global step 50: loss: 1.303089290857315, lr: 0.0001
2024-01-05 14:56:22 INFO     	 * (global step 100: loss: 1.1686377823352814, lr: 0.0001
2024-01-05 14:56:28 INFO     	 * (global step 150: loss: 0.6262632608413696, lr: 0.0001
2024-01-05 14:56:34 INFO     	 * (global step 200: loss: 1.103747010231018, lr: 0.0001
2024-01-05 14:56:41 INFO     	 * (global step 250: loss: 0.7705890238285065, lr: 0.0001
2024-01-05 14:56:49 INFO     	 * (global step 300: loss: 0.5720459073781967, lr: 0.0001
2024-01-05 14:56:56 INFO     	 * (global step 350: loss: 0.8380811214447021, lr: 0.0001
2024-01-05 14:57:03 INFO     	 * (global step 400: loss: 0.7204990386962891, lr: 0.0001
2024-01-05 14:57:11 INFO     	 * (global step 450: loss: 0.6205849945545197, lr: 0.0001
2024-01-05 14:57:18 INFO     	 * (global step 500: loss: 0.6517484486103058, lr: 0.0001
2024-01-05 14:57:26 INFO     	 * (global step 550: loss: 0.7729726433753967, lr: 0.0001
2024-01-05 14:57:33 INFO     	 * (global step 600: loss: 0.8552120327949524, lr: 0.0001
2024-01-05 14:57:41 INFO     	 * (global step 650: loss: 0.6944482028484344, lr: 0.0001
2024-01-05 14:57:49 INFO     	 * (global step 700: loss: 0.848553329706192, lr: 0.0001
2024-01-05 14:57:56 INFO     	 * (global step 750: loss: 0.8828501999378204, lr: 0.0001
2024-01-05 14:58:04 INFO     	 * (global step 800: loss: 1.0164539515972137, lr: 0.0001
2024-01-05 14:58:12 INFO     	 * (global step 850: loss: 0.8513102829456329, lr: 0.0001
2024-01-05 14:58:20 INFO     	 * (global step 900: loss: 0.6346902847290039, lr: 0.0001
2024-01-05 14:58:28 INFO     	 * (global step 950: loss: 0.54396191239357, lr: 0.0001
2024-01-05 14:58:36 INFO     	 * (global step 1000: loss: 0.7343804240226746, lr: 0.0001
2024-01-05 14:58:44 INFO     	 * (global step 1050: loss: 0.6326505243778229, lr: 0.0001
2024-01-05 14:58:51 INFO     	 * (global step 1100: loss: 0.5333124697208405, lr: 0.0001
2024-01-05 14:58:59 INFO     	 * (global step 1150: loss: 0.6590790450572968, lr: 0.0001
2024-01-05 14:59:06 INFO     	 * (global step 1200: loss: 1.1636273264884949, lr: 0.0001
2024-01-05 14:59:13 INFO     	 * (global step 1250: loss: 0.7640368044376373, lr: 0.0001
2024-01-05 14:59:19 INFO     	 * (global step 1300: loss: 0.6479207575321198, lr: 0.0001
2024-01-05 14:59:25 INFO     	 * (global step 1350: loss: 0.8581481873989105, lr: 0.0001
2024-01-05 14:59:31 INFO     	 * (global step 1400: loss: 0.4654606282711029, lr: 0.0001
2024-01-05 14:59:37 INFO     	 * (global step 1450: loss: 0.5559161007404327, lr: 0.0001
2024-01-05 14:59:45 INFO     	 * (global step 1500: loss: 0.651173323392868, lr: 0.0001
2024-01-05 14:59:53 INFO     	 * (global step 1550: loss: 0.8425848484039307, lr: 0.0001
2024-01-05 15:00:01 INFO     	 * (global step 1600: loss: 0.5258052349090576, lr: 0.0001
2024-01-05 15:00:08 INFO     	 * (global step 1650: loss: 0.5678486824035645, lr: 0.0001
2024-01-05 15:00:16 INFO     	 * (global step 1700: loss: 0.7164398729801178, lr: 0.0001
2024-01-05 15:00:24 INFO     	 * (global step 1750: loss: 0.7522214353084564, lr: 0.0001
2024-01-05 15:00:31 INFO     	 * (global step 1800: loss: 0.7311190962791443, lr: 0.0001
2024-01-05 15:00:40 INFO     	 * (global step 1850: loss: 0.6899243593215942, lr: 0.0001
2024-01-05 15:00:47 INFO     	 * (global step 1900: loss: 0.43768903613090515, lr: 0.0001
2024-01-05 15:00:55 INFO     	 * (global step 1950: loss: 0.6439747959375381, lr: 0.0001
2024-01-05 15:01:03 INFO     	 * (global step 2000: loss: 0.9798212349414825, lr: 0.0001
2024-01-05 15:01:10 INFO     	 * (global step 2050: loss: 0.6747467517852783, lr: 0.0001
2024-01-05 15:01:17 INFO     	 * (global step 2100: loss: 0.5115706473588943, lr: 0.0001
2024-01-05 15:01:25 INFO     	 * (global step 2150: loss: 0.5842686891555786, lr: 0.0001
2024-01-05 15:01:33 INFO     	 * (global step 2200: loss: 0.8736720979213715, lr: 0.0001
2024-01-05 15:01:40 INFO     	 * (global step 2250: loss: 0.5243139714002609, lr: 0.0001
2024-01-05 15:01:47 INFO     	 * (global step 2300: loss: 1.357063889503479, lr: 0.0001
2024-01-05 15:01:56 INFO     	 * (global step 2350: loss: 0.37667079269886017, lr: 0.0001
2024-01-05 15:02:03 INFO     	 * (global step 2400: loss: 0.5584305226802826, lr: 0.0001
2024-01-05 15:02:11 INFO     	 * (global step 2450: loss: 0.7029532194137573, lr: 0.0001
2024-01-05 15:02:16 INFO     	 * (global step 2500: loss: 0.49575401842594147, lr: 0.0001
2024-01-05 15:02:22 INFO     	 * (global step 2550: loss: 0.78110072016716, lr: 0.0001
2024-01-05 15:02:28 INFO     	 * (global step 2600: loss: 0.4305836260318756, lr: 0.0001
2024-01-05 15:02:34 INFO     	 * (global step 2650: loss: 0.5239709168672562, lr: 0.0001
2024-01-05 15:02:42 INFO     	 * (global step 2700: loss: 0.606340080499649, lr: 0.0001
2024-01-05 15:02:49 INFO     	 * (global step 2750: loss: 0.4780258983373642, lr: 0.0001
2024-01-05 15:02:57 INFO     	 * (global step 2800: loss: 0.48270784318447113, lr: 0.0001
2024-01-05 15:03:04 INFO     	 * (global step 2850: loss: 0.6027970016002655, lr: 0.0001
2024-01-05 15:03:11 INFO     	 * (global step 2900: loss: 0.7184377610683441, lr: 0.0001
2024-01-05 15:03:19 INFO     	 * (global step 2950: loss: 0.30307573080062866, lr: 0.0001
2024-01-05 15:03:26 INFO     	 * (global step 3000: loss: 0.8287463188171387, lr: 0.0001
2024-01-05 15:03:34 INFO     	 * (global step 3050: loss: 0.6846225559711456, lr: 0.0001
2024-01-05 15:03:41 INFO     	 * (global step 3100: loss: 0.7265467345714569, lr: 0.0001
2024-01-05 15:03:49 INFO     	 * (global step 3150: loss: 0.45433947443962097, lr: 0.0001
2024-01-05 15:03:56 INFO     	 * (global step 3200: loss: 0.7279062867164612, lr: 0.0001
2024-01-05 15:04:03 INFO     	 * (global step 3250: loss: 0.4584405869245529, lr: 0.0001
2024-01-05 15:04:11 INFO     	 * (global step 3300: loss: 0.575976625084877, lr: 0.0001
2024-01-05 15:04:18 INFO     	 * (global step 3350: loss: 0.5562085062265396, lr: 0.0001
2024-01-05 15:04:25 INFO     	 * (global step 3400: loss: 0.570853054523468, lr: 0.0001
2024-01-05 15:04:33 INFO     	 * (global step 3450: loss: 0.6552610695362091, lr: 0.0001
2024-01-05 15:04:39 INFO     	 * (global step 3500: loss: 0.6278488636016846, lr: 0.0001
2024-01-05 15:04:45 INFO     	 * (global step 3550: loss: 0.8739488124847412, lr: 0.0001
2024-01-05 15:04:51 INFO     	 * (global step 3600: loss: 0.633891150355339, lr: 0.0001
2024-01-05 15:04:58 INFO     	 * (global step 3650: loss: 0.6541568636894226, lr: 0.0001
2024-01-05 15:05:03 INFO     	 * (global step 3700: loss: 0.36814047396183014, lr: 0.0001
2024-01-05 15:05:10 INFO     	 * (global step 3750: loss: 0.5491434335708618, lr: 0.0001
2024-01-05 15:05:18 INFO     	 * (global step 3800: loss: 0.387891948223114, lr: 0.0001
2024-01-05 15:05:26 INFO     	 * (global step 3850: loss: 0.5865384042263031, lr: 0.0001
2024-01-05 15:05:34 INFO     	 * (global step 3900: loss: 0.7654967904090881, lr: 0.0001
2024-01-05 15:05:42 INFO     	 * (global step 3950: loss: 0.5856539309024811, lr: 0.0001
2024-01-05 15:05:50 INFO     	 * (global step 4000: loss: 0.5166183114051819, lr: 0.0001
2024-01-05 15:05:57 INFO     [epoch 0/15] average loss: 0.707, lr: 0.0001
2024-01-05 15:05:57 INFO     saving model related files
2024-01-05 15:05:57 INFO     saving model
2024-01-05 15:05:58 INFO     saving tokenizer
2024-01-05 15:05:58 INFO     saving optimizer
2024-01-05 15:06:00 INFO     remove old optimizer files
2024-01-05 15:06:00 INFO     	 * (global step 4050: loss: 0.580750048160553, lr: 0.0001
2024-01-05 15:06:08 INFO     	 * (global step 4100: loss: 0.5132003426551819, lr: 0.0001
2024-01-05 15:06:15 INFO     	 * (global step 4150: loss: 0.5943585634231567, lr: 0.0001
2024-01-05 15:06:23 INFO     	 * (global step 4200: loss: 0.483420193195343, lr: 0.0001
2024-01-05 15:06:31 INFO     	 * (global step 4250: loss: 0.6791492700576782, lr: 0.0001
2024-01-05 15:06:38 INFO     	 * (global step 4300: loss: 0.9040086716413498, lr: 0.0001
2024-01-05 15:06:45 INFO     	 * (global step 4350: loss: 0.4107389748096466, lr: 0.0001
2024-01-05 15:06:53 INFO     	 * (global step 4400: loss: 0.37714311480522156, lr: 0.0001
2024-01-05 15:07:00 INFO     	 * (global step 4450: loss: 0.859787106513977, lr: 0.0001
2024-01-05 15:07:08 INFO     	 * (global step 4500: loss: 0.4443403631448746, lr: 0.0001
2024-01-05 15:07:16 INFO     	 * (global step 4550: loss: 0.4862969368696213, lr: 0.0001
2024-01-05 15:07:23 INFO     	 * (global step 4600: loss: 0.5624281466007233, lr: 0.0001
2024-01-05 15:07:31 INFO     	 * (global step 4650: loss: 0.7686358094215393, lr: 0.0001
2024-01-05 15:07:37 INFO     	 * (global step 4700: loss: 0.789683997631073, lr: 0.0001
2024-01-05 15:07:43 INFO     	 * (global step 4750: loss: 0.7424970269203186, lr: 0.0001
2024-01-05 15:07:49 INFO     	 * (global step 4800: loss: 0.592161551117897, lr: 0.0001
2024-01-05 15:07:55 INFO     	 * (global step 4850: loss: 0.6516285836696625, lr: 0.0001
2024-01-05 15:08:01 INFO     	 * (global step 4900: loss: 0.5669206380844116, lr: 0.0001
2024-01-05 15:08:08 INFO     	 * (global step 4950: loss: 0.760570302605629, lr: 0.0001
2024-01-05 15:08:16 INFO     	 * (global step 5000: loss: 0.7339262962341309, lr: 0.0001
2024-01-05 15:08:23 INFO     	 * (global step 5050: loss: 0.67012158036232, lr: 0.0001
2024-01-05 15:08:30 INFO     	 * (global step 5100: loss: 0.5391008704900742, lr: 0.0001
2024-01-05 15:08:37 INFO     	 * (global step 5150: loss: 0.6186724007129669, lr: 0.0001
2024-01-05 15:08:45 INFO     	 * (global step 5200: loss: 0.49353812634944916, lr: 0.0001
2024-01-05 15:08:52 INFO     	 * (global step 5250: loss: 0.5707914084196091, lr: 0.0001
2024-01-05 15:08:59 INFO     	 * (global step 5300: loss: 0.7967041432857513, lr: 0.0001
2024-01-05 15:09:07 INFO     	 * (global step 5350: loss: 0.6045280396938324, lr: 0.0001
2024-01-05 15:09:14 INFO     	 * (global step 5400: loss: 0.5216416716575623, lr: 0.0001
2024-01-05 15:09:21 INFO     	 * (global step 5450: loss: 0.5396811962127686, lr: 0.0001
2024-01-05 15:09:28 INFO     	 * (global step 5500: loss: 0.47521549463272095, lr: 0.0001
2024-01-05 15:09:35 INFO     	 * (global step 5550: loss: 0.6547836661338806, lr: 0.0001
2024-01-05 15:09:43 INFO     	 * (global step 5600: loss: 0.6846855878829956, lr: 0.0001
2024-01-05 15:09:50 INFO     	 * (global step 5650: loss: 0.673979640007019, lr: 0.0001
2024-01-05 15:09:57 INFO     	 * (global step 5700: loss: 0.9261104464530945, lr: 0.0001
2024-01-05 15:10:04 INFO     	 * (global step 5750: loss: 0.4759729653596878, lr: 0.0001
2024-01-05 15:10:09 INFO     	 * (global step 5800: loss: 0.6506530940532684, lr: 0.0001
2024-01-05 15:10:15 INFO     	 * (global step 5850: loss: 0.5027799308300018, lr: 0.0001
2024-01-05 15:10:22 INFO     	 * (global step 5900: loss: 0.4353911280632019, lr: 0.0001
2024-01-05 15:10:28 INFO     	 * (global step 5950: loss: 0.7667158842086792, lr: 0.0001
2024-01-05 15:10:35 INFO     	 * (global step 6000: loss: 0.5290035009384155, lr: 0.0001
2024-01-05 15:10:43 INFO     	 * (global step 6050: loss: 0.48871299624443054, lr: 0.0001
2024-01-05 15:10:50 INFO     	 * (global step 6100: loss: 0.47409380972385406, lr: 0.0001
2024-01-05 15:10:58 INFO     	 * (global step 6150: loss: 0.5407615453004837, lr: 0.0001
2024-01-05 15:11:06 INFO     	 * (global step 6200: loss: 0.6097773313522339, lr: 0.0001
2024-01-05 15:11:13 INFO     	 * (global step 6250: loss: 0.43864698708057404, lr: 0.0001
2024-01-05 15:11:21 INFO     	 * (global step 6300: loss: 0.6103003919124603, lr: 0.0001
2024-01-05 15:11:29 INFO     	 * (global step 6350: loss: 0.6377963125705719, lr: 0.0001
2024-01-05 15:11:37 INFO     	 * (global step 6400: loss: 0.5089749693870544, lr: 0.0001
2024-01-05 15:11:45 INFO     	 * (global step 6450: loss: 0.4479086846113205, lr: 0.0001
2024-01-05 15:11:53 INFO     	 * (global step 6500: loss: 0.6814450025558472, lr: 0.0001
2024-01-05 15:12:00 INFO     	 * (global step 6550: loss: 0.7017224431037903, lr: 0.0001
2024-01-05 15:12:08 INFO     	 * (global step 6600: loss: 0.6502885520458221, lr: 0.0001
2024-01-05 15:12:16 INFO     	 * (global step 6650: loss: 0.5297877192497253, lr: 0.0001
2024-01-05 15:12:24 INFO     	 * (global step 6700: loss: 0.63301020860672, lr: 0.0001
2024-01-05 15:12:32 INFO     	 * (global step 6750: loss: 0.7719812989234924, lr: 0.0001
2024-01-05 15:12:40 INFO     	 * (global step 6800: loss: 0.740558922290802, lr: 0.0001
2024-01-05 15:12:48 INFO     	 * (global step 6850: loss: 0.5720982253551483, lr: 0.0001
2024-01-05 15:12:56 INFO     	 * (global step 6900: loss: 0.613235354423523, lr: 0.0001
2024-01-05 15:13:04 INFO     	 * (global step 6950: loss: 0.5400651395320892, lr: 0.0001
2024-01-05 15:13:12 INFO     	 * (global step 7000: loss: 0.5485456585884094, lr: 0.0001
2024-01-05 15:13:19 INFO     	 * (global step 7050: loss: 0.6330349445343018, lr: 0.0001
2024-01-05 15:13:27 INFO     	 * (global step 7100: loss: 0.45003414154052734, lr: 0.0001
2024-01-05 15:13:34 INFO     	 * (global step 7150: loss: 0.5939643234014511, lr: 0.0001
2024-01-05 15:13:40 INFO     	 * (global step 7200: loss: 0.5801130682229996, lr: 0.0001
2024-01-05 15:13:45 INFO     	 * (global step 7250: loss: 0.7444062232971191, lr: 0.0001
2024-01-05 15:13:52 INFO     	 * (global step 7300: loss: 0.7127464115619659, lr: 0.0001
2024-01-05 15:13:58 INFO     	 * (global step 7350: loss: 0.6208588480949402, lr: 0.0001
2024-01-05 15:14:05 INFO     	 * (global step 7400: loss: 0.689810037612915, lr: 0.0001
2024-01-05 15:14:13 INFO     	 * (global step 7450: loss: 0.5523024797439575, lr: 0.0001
2024-01-05 15:14:20 INFO     	 * (global step 7500: loss: 0.5159163773059845, lr: 0.0001
2024-01-05 15:14:27 INFO     	 * (global step 7550: loss: 0.7789847552776337, lr: 0.0001
2024-01-05 15:14:34 INFO     	 * (global step 7600: loss: 0.5765472650527954, lr: 0.0001
2024-01-05 15:14:42 INFO     	 * (global step 7650: loss: 0.8170532584190369, lr: 0.0001
2024-01-05 15:14:49 INFO     	 * (global step 7700: loss: 0.9035478234291077, lr: 0.0001
2024-01-05 15:14:56 INFO     	 * (global step 7750: loss: 0.5861708521842957, lr: 0.0001
2024-01-05 15:15:03 INFO     	 * (global step 7800: loss: 0.6112632155418396, lr: 0.0001
2024-01-05 15:15:11 INFO     	 * (global step 7850: loss: 0.5178573429584503, lr: 0.0001
2024-01-05 15:15:18 INFO     	 * (global step 7900: loss: 0.5059447586536407, lr: 0.0001
2024-01-05 15:15:25 INFO     	 * (global step 7950: loss: 0.5063498914241791, lr: 0.0001
2024-01-05 15:15:33 INFO     	 * (global step 8000: loss: 0.4779120087623596, lr: 0.0001
2024-01-05 15:15:40 INFO     	 * (global step 8050: loss: 0.7783236801624298, lr: 0.0001
2024-01-05 15:15:47 INFO     [epoch 1/15] average loss: 0.592, lr: 0.0001
2024-01-05 15:15:47 INFO     saving model related files
2024-01-05 15:15:47 INFO     saving model
2024-01-05 15:15:48 INFO     saving tokenizer
2024-01-05 15:15:48 INFO     saving optimizer
2024-01-05 15:15:50 INFO     remove old optimizer files
2024-01-05 15:15:50 INFO     	 * (global step 8100: loss: 0.4597747325897217, lr: 0.0001
2024-01-05 15:15:58 INFO     	 * (global step 8150: loss: 0.41460926830768585, lr: 0.0001
2024-01-05 15:16:05 INFO     	 * (global step 8200: loss: 0.4359128922224045, lr: 0.0001
2024-01-05 15:16:12 INFO     	 * (global step 8250: loss: 0.3338041305541992, lr: 0.0001
2024-01-05 15:16:19 INFO     	 * (global step 8300: loss: 0.5704264044761658, lr: 0.0001
2024-01-05 15:16:25 INFO     	 * (global step 8350: loss: 0.7514205276966095, lr: 0.0001
2024-01-05 15:16:31 INFO     	 * (global step 8400: loss: 0.4221508502960205, lr: 0.0001
2024-01-05 15:16:37 INFO     	 * (global step 8450: loss: 0.47742408514022827, lr: 0.0001
2024-01-05 15:16:43 INFO     	 * (global step 8500: loss: 0.6888743937015533, lr: 0.0001
2024-01-05 15:16:50 INFO     	 * (global step 8550: loss: 0.6003608256578445, lr: 0.0001
2024-01-05 15:16:58 INFO     	 * (global step 8600: loss: 0.3153257220983505, lr: 0.0001
2024-01-05 15:17:05 INFO     	 * (global step 8650: loss: 0.40005818009376526, lr: 0.0001
2024-01-05 15:17:13 INFO     	 * (global step 8700: loss: 0.6271826326847076, lr: 0.0001
2024-01-05 15:17:20 INFO     	 * (global step 8750: loss: 0.5214807540178299, lr: 0.0001
2024-01-05 15:17:28 INFO     	 * (global step 8800: loss: 0.29343780875205994, lr: 0.0001
2024-01-05 15:17:36 INFO     	 * (global step 8850: loss: 0.54488505423069, lr: 0.0001
2024-01-05 15:17:44 INFO     	 * (global step 8900: loss: 0.4499031901359558, lr: 0.0001
2024-01-05 15:17:51 INFO     	 * (global step 8950: loss: 0.35821904242038727, lr: 0.0001
2024-01-05 15:17:58 INFO     	 * (global step 9000: loss: 0.5179769694805145, lr: 0.0001
2024-01-05 15:18:06 INFO     	 * (global step 9050: loss: 0.6531154215335846, lr: 0.0001
2024-01-05 15:18:13 INFO     	 * (global step 9100: loss: 0.6196466386318207, lr: 0.0001
2024-01-05 15:18:21 INFO     	 * (global step 9150: loss: 0.5580106973648071, lr: 0.0001
2024-01-05 15:18:28 INFO     	 * (global step 9200: loss: 0.5816653668880463, lr: 0.0001
2024-01-05 15:18:36 INFO     	 * (global step 9250: loss: 0.4383899122476578, lr: 0.0001
2024-01-05 15:18:44 INFO     	 * (global step 9300: loss: 0.6712451279163361, lr: 0.0001
2024-01-05 15:18:51 INFO     	 * (global step 9350: loss: 0.39688660204410553, lr: 0.0001
2024-01-05 15:18:58 INFO     	 * (global step 9400: loss: 0.554320216178894, lr: 0.0001
2024-01-05 15:19:06 INFO     	 * (global step 9450: loss: 0.42636964470148087, lr: 0.0001
2024-01-05 15:19:14 INFO     	 * (global step 9500: loss: 0.49296535551548004, lr: 0.0001
2024-01-05 15:19:22 INFO     	 * (global step 9550: loss: 0.5934900939464569, lr: 0.0001
2024-01-05 15:19:29 INFO     	 * (global step 9600: loss: 0.5455190539360046, lr: 0.0001
2024-01-05 15:19:34 INFO     	 * (global step 9650: loss: 0.6333020478487015, lr: 0.0001
2024-01-05 15:19:40 INFO     	 * (global step 9700: loss: 0.693036824464798, lr: 0.0001
2024-01-05 15:19:46 INFO     	 * (global step 9750: loss: 0.7017604112625122, lr: 0.0001
2024-01-05 15:19:52 INFO     	 * (global step 9800: loss: 0.4502708464860916, lr: 0.0001
2024-01-05 15:20:00 INFO     	 * (global step 9850: loss: 0.6683889627456665, lr: 0.0001
2024-01-05 15:20:07 INFO     	 * (global step 9900: loss: 0.44688524305820465, lr: 0.0001
2024-01-05 15:20:15 INFO     	 * (global step 9950: loss: 0.40224313735961914, lr: 0.0001
2024-01-05 15:20:22 INFO     	 * (global step 10000: loss: 0.46267862617969513, lr: 0.0001
2024-01-05 15:20:30 INFO     	 * (global step 10050: loss: 0.5418111979961395, lr: 0.0001
2024-01-05 15:20:37 INFO     	 * (global step 10100: loss: 0.6184397339820862, lr: 0.0001
2024-01-05 15:20:45 INFO     	 * (global step 10150: loss: 0.3950997143983841, lr: 0.0001
2024-01-05 15:20:52 INFO     	 * (global step 10200: loss: 0.4721497893333435, lr: 0.0001
2024-01-05 15:20:59 INFO     	 * (global step 10250: loss: 0.5881737470626831, lr: 0.0001
2024-01-05 15:21:07 INFO     	 * (global step 10300: loss: 0.6357538104057312, lr: 0.0001
2024-01-05 15:21:15 INFO     	 * (global step 10350: loss: 0.32394055277109146, lr: 0.0001
2024-01-05 15:21:22 INFO     	 * (global step 10400: loss: 0.49366359412670135, lr: 0.0001
2024-01-05 15:21:30 INFO     	 * (global step 10450: loss: 0.6303909122943878, lr: 0.0001
2024-01-05 15:21:37 INFO     	 * (global step 10500: loss: 0.5642078220844269, lr: 0.0001
2024-01-05 15:21:44 INFO     	 * (global step 10550: loss: 0.5537782609462738, lr: 0.0001
2024-01-05 15:21:52 INFO     	 * (global step 10600: loss: 0.28715117275714874, lr: 0.0001
2024-01-05 15:21:59 INFO     	 * (global step 10650: loss: 0.5059494525194168, lr: 0.0001
2024-01-05 15:22:07 INFO     	 * (global step 10700: loss: 0.5412344336509705, lr: 0.0001
2024-01-05 15:22:14 INFO     	 * (global step 10750: loss: 0.5722174048423767, lr: 0.0001
2024-01-05 15:22:20 INFO     	 * (global step 10800: loss: 0.549854040145874, lr: 0.0001
2024-01-05 15:22:25 INFO     	 * (global step 10850: loss: 0.4894184321165085, lr: 0.0001
2024-01-05 15:22:31 INFO     	 * (global step 10900: loss: 0.5246449410915375, lr: 0.0001
2024-01-05 15:22:38 INFO     	 * (global step 10950: loss: 0.4444083273410797, lr: 0.0001
2024-01-05 15:22:45 INFO     	 * (global step 11000: loss: 0.39365537464618683, lr: 0.0001
2024-01-05 15:22:53 INFO     	 * (global step 11050: loss: 0.6372694373130798, lr: 0.0001
2024-01-05 15:23:01 INFO     	 * (global step 11100: loss: 0.5105943828821182, lr: 0.0001
2024-01-05 15:23:08 INFO     	 * (global step 11150: loss: 0.5420243442058563, lr: 0.0001
2024-01-05 15:23:16 INFO     	 * (global step 11200: loss: 0.4712010324001312, lr: 0.0001
2024-01-05 15:23:23 INFO     	 * (global step 11250: loss: 0.6036106646060944, lr: 0.0001
2024-01-05 15:23:31 INFO     	 * (global step 11300: loss: 0.4106040745973587, lr: 0.0001
2024-01-05 15:23:38 INFO     	 * (global step 11350: loss: 0.6319730877876282, lr: 0.0001
2024-01-05 15:23:46 INFO     	 * (global step 11400: loss: 0.6669328808784485, lr: 0.0001
2024-01-05 15:23:54 INFO     	 * (global step 11450: loss: 0.8075454831123352, lr: 0.0001
2024-01-05 15:24:01 INFO     	 * (global step 11500: loss: 0.5445196330547333, lr: 0.0001
2024-01-05 15:24:09 INFO     	 * (global step 11550: loss: 0.653472512960434, lr: 0.0001
2024-01-05 15:24:16 INFO     	 * (global step 11600: loss: 0.5529011338949203, lr: 0.0001
2024-01-05 15:24:23 INFO     	 * (global step 11650: loss: 0.8182001113891602, lr: 0.0001
2024-01-05 15:24:31 INFO     	 * (global step 11700: loss: 0.6043505072593689, lr: 0.0001
2024-01-05 15:24:39 INFO     	 * (global step 11750: loss: 0.570109948515892, lr: 0.0001
2024-01-05 15:24:46 INFO     	 * (global step 11800: loss: 0.3794823884963989, lr: 0.0001
2024-01-05 15:24:54 INFO     	 * (global step 11850: loss: 0.5395790785551071, lr: 0.0001
2024-01-05 15:25:01 INFO     	 * (global step 11900: loss: 0.4289090186357498, lr: 0.0001
2024-01-05 15:25:07 INFO     	 * (global step 11950: loss: 0.3630063533782959, lr: 0.0001
2024-01-05 15:25:13 INFO     	 * (global step 12000: loss: 0.645350456237793, lr: 0.0001
2024-01-05 15:25:19 INFO     	 * (global step 12050: loss: 0.4733153432607651, lr: 0.0001
2024-01-05 15:25:25 INFO     	 * (global step 12100: loss: 0.5917376577854156, lr: 0.0001
2024-01-05 15:25:31 INFO     [epoch 2/15] average loss: 0.56, lr: 0.0001
2024-01-05 15:25:31 INFO     saving model related files
2024-01-05 15:25:31 INFO     saving model
2024-01-05 15:25:33 INFO     saving tokenizer
2024-01-05 15:25:33 INFO     saving optimizer
2024-01-05 15:25:35 INFO     remove old optimizer files
2024-01-05 15:25:36 INFO     	 * (global step 12150: loss: 0.436451718211174, lr: 0.0001
2024-01-05 15:25:44 INFO     	 * (global step 12200: loss: 0.5520318001508713, lr: 0.0001
2024-01-05 15:25:52 INFO     	 * (global step 12250: loss: 0.4400871843099594, lr: 0.0001
2024-01-05 15:26:00 INFO     	 * (global step 12300: loss: 0.5151040554046631, lr: 0.0001
2024-01-05 15:26:08 INFO     	 * (global step 12350: loss: 0.6221558451652527, lr: 0.0001
2024-01-05 15:26:15 INFO     	 * (global step 12400: loss: 0.6886240839958191, lr: 0.0001
2024-01-05 15:26:23 INFO     	 * (global step 12450: loss: 0.3921911418437958, lr: 0.0001
2024-01-05 15:26:30 INFO     	 * (global step 12500: loss: 0.6049399822950363, lr: 0.0001
2024-01-05 15:26:38 INFO     	 * (global step 12550: loss: 0.4748748391866684, lr: 0.0001
2024-01-05 15:26:46 INFO     	 * (global step 12600: loss: 0.43497106432914734, lr: 0.0001
2024-01-05 15:26:54 INFO     	 * (global step 12650: loss: 0.5300930738449097, lr: 0.0001
2024-01-05 15:27:03 INFO     	 * (global step 12700: loss: 0.7365894317626953, lr: 0.0001
2024-01-05 15:27:11 INFO     	 * (global step 12750: loss: 0.6995287090539932, lr: 0.0001
2024-01-05 15:27:18 INFO     	 * (global step 12800: loss: 0.6481648087501526, lr: 0.0001
2024-01-05 15:27:26 INFO     	 * (global step 12850: loss: 0.754970133304596, lr: 0.0001
2024-01-05 15:27:34 INFO     	 * (global step 12900: loss: 0.5763629376888275, lr: 0.0001
2024-01-05 15:27:42 INFO     	 * (global step 12950: loss: 0.590217113494873, lr: 0.0001
2024-01-05 15:27:50 INFO     	 * (global step 13000: loss: 0.720920205116272, lr: 0.0001
2024-01-05 15:27:57 INFO     	 * (global step 13050: loss: 0.5118633806705475, lr: 0.0001
2024-01-05 15:28:06 INFO     	 * (global step 13100: loss: 0.4229267090559006, lr: 0.0001
2024-01-05 15:28:14 INFO     	 * (global step 13150: loss: 0.5382653623819351, lr: 0.0001
2024-01-05 15:28:22 INFO     	 * (global step 13200: loss: 0.4909801632165909, lr: 0.0001
2024-01-05 15:28:28 INFO     	 * (global step 13250: loss: 1.0334640443325043, lr: 0.0001
2024-01-05 15:28:34 INFO     	 * (global step 13300: loss: 0.3420742303133011, lr: 0.0001
2024-01-05 15:28:40 INFO     	 * (global step 13350: loss: 0.5298954546451569, lr: 0.0001
2024-01-05 15:28:46 INFO     	 * (global step 13400: loss: 0.6413123607635498, lr: 0.0001
2024-01-05 15:28:53 INFO     	 * (global step 13450: loss: 0.4896930754184723, lr: 0.0001
2024-01-05 15:29:01 INFO     	 * (global step 13500: loss: 0.6592121571302414, lr: 0.0001
2024-01-05 15:29:07 INFO     	 * (global step 13550: loss: 0.5397068560123444, lr: 0.0001
2024-01-05 15:29:15 INFO     	 * (global step 13600: loss: 0.6780078858137131, lr: 0.0001
2024-01-05 15:29:23 INFO     	 * (global step 13650: loss: 0.5643447935581207, lr: 0.0001
2024-01-05 15:29:30 INFO     	 * (global step 13700: loss: 0.5266675055027008, lr: 0.0001
2024-01-05 15:29:38 INFO     	 * (global step 13750: loss: 0.5073417127132416, lr: 0.0001
2024-01-05 15:29:46 INFO     	 * (global step 13800: loss: 0.5337444990873337, lr: 0.0001
2024-01-05 15:29:54 INFO     	 * (global step 13850: loss: 0.8549163341522217, lr: 0.0001
2024-01-05 15:30:01 INFO     	 * (global step 13900: loss: 0.3424244821071625, lr: 0.0001
2024-01-05 15:30:09 INFO     	 * (global step 13950: loss: 0.5725373029708862, lr: 0.0001
2024-01-05 15:30:16 INFO     	 * (global step 14000: loss: 0.43150225281715393, lr: 0.0001
2024-01-05 15:30:24 INFO     	 * (global step 14050: loss: 0.44750553369522095, lr: 0.0001
2024-01-05 15:30:32 INFO     	 * (global step 14100: loss: 0.5643258094787598, lr: 0.0001
2024-01-05 15:30:39 INFO     	 * (global step 14150: loss: 0.36969418823719025, lr: 0.0001
2024-01-05 15:30:47 INFO     	 * (global step 14200: loss: 0.5283865183591843, lr: 0.0001
2024-01-05 15:30:54 INFO     	 * (global step 14250: loss: 0.6571889221668243, lr: 0.0001
2024-01-05 15:31:01 INFO     	 * (global step 14300: loss: 0.41031111776828766, lr: 0.0001
2024-01-05 15:31:09 INFO     	 * (global step 14350: loss: 0.9569348394870758, lr: 0.0001
2024-01-05 15:31:16 INFO     	 * (global step 14400: loss: 0.6031577289104462, lr: 0.0001
2024-01-05 15:31:24 INFO     	 * (global step 14450: loss: 0.35190053284168243, lr: 0.0001
2024-01-05 15:31:31 INFO     	 * (global step 14500: loss: 0.49544769525527954, lr: 0.0001
2024-01-05 15:31:37 INFO     	 * (global step 14550: loss: 0.47673942148685455, lr: 0.0001
2024-01-05 15:31:43 INFO     	 * (global step 14600: loss: 0.49865029752254486, lr: 0.0001
2024-01-05 15:31:49 INFO     	 * (global step 14650: loss: 0.47605277597904205, lr: 0.0001
2024-01-05 15:31:55 INFO     	 * (global step 14700: loss: 0.38563072681427, lr: 0.0001
2024-01-05 15:32:03 INFO     	 * (global step 14750: loss: 0.5231826305389404, lr: 0.0001
2024-01-05 15:32:10 INFO     	 * (global step 14800: loss: 0.5014177411794662, lr: 0.0001
2024-01-05 15:32:17 INFO     	 * (global step 14850: loss: 0.4905789643526077, lr: 0.0001
2024-01-05 15:32:25 INFO     	 * (global step 14900: loss: 0.42318928241729736, lr: 0.0001
2024-01-05 15:32:33 INFO     	 * (global step 14950: loss: 0.41857030987739563, lr: 0.0001
2024-01-05 15:32:41 INFO     	 * (global step 15000: loss: 0.40816492587327957, lr: 0.0001
2024-01-05 15:32:48 INFO     	 * (global step 15050: loss: 0.6072034686803818, lr: 0.0001
2024-01-05 15:32:55 INFO     	 * (global step 15100: loss: 0.50387904047966, lr: 0.0001
2024-01-05 15:33:03 INFO     	 * (global step 15150: loss: 0.524216890335083, lr: 0.0001
2024-01-05 15:33:10 INFO     	 * (global step 15200: loss: 0.4755098819732666, lr: 0.0001
2024-01-05 15:33:18 INFO     	 * (global step 15250: loss: 0.39545436203479767, lr: 0.0001
2024-01-05 15:33:25 INFO     	 * (global step 15300: loss: 0.5102375447750092, lr: 0.0001
2024-01-05 15:33:33 INFO     	 * (global step 15350: loss: 0.6896848380565643, lr: 0.0001
2024-01-05 15:33:40 INFO     	 * (global step 15400: loss: 0.37876923382282257, lr: 0.0001
2024-01-05 15:33:47 INFO     	 * (global step 15450: loss: 0.4031914174556732, lr: 0.0001
2024-01-05 15:33:55 INFO     	 * (global step 15500: loss: 0.44616197794675827, lr: 0.0001
2024-01-05 15:34:02 INFO     	 * (global step 15550: loss: 0.6161586344242096, lr: 0.0001
2024-01-05 15:34:10 INFO     	 * (global step 15600: loss: 0.5268908143043518, lr: 0.0001
2024-01-05 15:34:17 INFO     	 * (global step 15650: loss: 0.6636406779289246, lr: 0.0001
2024-01-05 15:34:25 INFO     	 * (global step 15700: loss: 0.6327475607395172, lr: 0.0001
2024-01-05 15:34:32 INFO     	 * (global step 15750: loss: 0.5280536115169525, lr: 0.0001
2024-01-05 15:34:38 INFO     	 * (global step 15800: loss: 0.29500485956668854, lr: 0.0001
2024-01-05 15:34:44 INFO     	 * (global step 15850: loss: 0.5147566497325897, lr: 0.0001
2024-01-05 15:34:50 INFO     	 * (global step 15900: loss: 0.5984121263027191, lr: 0.0001
2024-01-05 15:34:56 INFO     	 * (global step 15950: loss: 0.6826953887939453, lr: 0.0001
2024-01-05 15:35:02 INFO     	 * (global step 16000: loss: 0.39940907061100006, lr: 0.0001
2024-01-05 15:35:10 INFO     	 * (global step 16050: loss: 0.3798461854457855, lr: 0.0001
2024-01-05 15:35:17 INFO     	 * (global step 16100: loss: 0.7111968100070953, lr: 0.0001
2024-01-05 15:35:25 INFO     	 * (global step 16150: loss: 0.4088759124279022, lr: 0.0001
2024-01-05 15:35:31 INFO     [epoch 3/15] average loss: 0.537, lr: 0.0001
2024-01-05 15:35:31 INFO     saving model related files
2024-01-05 15:35:31 INFO     saving model
2024-01-05 15:35:32 INFO     saving tokenizer
2024-01-05 15:35:32 INFO     saving optimizer
2024-01-05 15:35:34 INFO     remove old optimizer files
2024-01-05 15:35:36 INFO     	 * (global step 16200: loss: 0.6379839479923248, lr: 0.0001
2024-01-05 15:35:43 INFO     	 * (global step 16250: loss: 0.5413374900817871, lr: 0.0001
2024-01-05 15:35:51 INFO     	 * (global step 16300: loss: 0.4144895225763321, lr: 0.0001
2024-01-05 15:35:59 INFO     	 * (global step 16350: loss: 0.4553605169057846, lr: 0.0001
2024-01-05 15:36:07 INFO     	 * (global step 16400: loss: 0.26396916806697845, lr: 0.0001
2024-01-05 15:36:14 INFO     	 * (global step 16450: loss: 0.40889841318130493, lr: 0.0001
2024-01-05 15:36:22 INFO     	 * (global step 16500: loss: 0.434462770819664, lr: 0.0001
2024-01-05 15:36:30 INFO     	 * (global step 16550: loss: 0.5725524872541428, lr: 0.0001
2024-01-05 15:36:38 INFO     	 * (global step 16600: loss: 0.4476780593395233, lr: 0.0001
2024-01-05 15:36:45 INFO     	 * (global step 16650: loss: 0.5073367357254028, lr: 0.0001
2024-01-05 15:36:53 INFO     	 * (global step 16700: loss: 0.5049458742141724, lr: 0.0001
2024-01-05 15:37:00 INFO     	 * (global step 16750: loss: 0.4346219152212143, lr: 0.0001
2024-01-05 15:37:08 INFO     	 * (global step 16800: loss: 0.6327802091836929, lr: 0.0001
2024-01-05 15:37:16 INFO     	 * (global step 16850: loss: 0.32604216039180756, lr: 0.0001
2024-01-05 15:37:24 INFO     	 * (global step 16900: loss: 0.5792136341333389, lr: 0.0001
2024-01-05 15:37:32 INFO     	 * (global step 16950: loss: 0.5276850908994675, lr: 0.0001
2024-01-05 15:37:39 INFO     	 * (global step 17000: loss: 0.5849538147449493, lr: 0.0001
2024-01-05 15:37:45 INFO     	 * (global step 17050: loss: 0.5224072486162186, lr: 0.0001
2024-01-05 15:37:51 INFO     	 * (global step 17100: loss: 0.2498030886054039, lr: 0.0001
2024-01-05 15:37:57 INFO     	 * (global step 17150: loss: 0.5015943050384521, lr: 0.0001
2024-01-05 15:38:03 INFO     	 * (global step 17200: loss: 0.4395594447851181, lr: 0.0001
2024-01-05 15:38:10 INFO     	 * (global step 17250: loss: 0.3249077647924423, lr: 0.0001
2024-01-05 15:38:18 INFO     	 * (global step 17300: loss: 0.6610140204429626, lr: 0.0001
2024-01-05 15:38:26 INFO     	 * (global step 17350: loss: 0.6924057602882385, lr: 0.0001
2024-01-05 15:38:35 INFO     	 * (global step 17400: loss: 0.6968205273151398, lr: 0.0001
2024-01-05 15:38:42 INFO     	 * (global step 17450: loss: 0.44535817205905914, lr: 0.0001
2024-01-05 15:38:50 INFO     	 * (global step 17500: loss: 0.4413026422262192, lr: 0.0001
2024-01-05 15:38:58 INFO     	 * (global step 17550: loss: 0.44586050510406494, lr: 0.0001
2024-01-05 15:39:06 INFO     	 * (global step 17600: loss: 0.6163168102502823, lr: 0.0001
2024-01-05 15:39:14 INFO     	 * (global step 17650: loss: 0.4461231380701065, lr: 0.0001
2024-01-05 15:39:21 INFO     	 * (global step 17700: loss: 0.38944584876298904, lr: 0.0001
2024-01-05 15:39:29 INFO     	 * (global step 17750: loss: 0.6110750138759613, lr: 0.0001
2024-01-05 15:39:37 INFO     	 * (global step 17800: loss: 0.6765384823083878, lr: 0.0001
2024-01-05 15:39:45 INFO     	 * (global step 17850: loss: 0.6492207050323486, lr: 0.0001
2024-01-05 15:39:53 INFO     	 * (global step 17900: loss: 0.435633048415184, lr: 0.0001
2024-01-05 15:40:00 INFO     	 * (global step 17950: loss: 0.39772847294807434, lr: 0.0001
2024-01-05 15:40:08 INFO     	 * (global step 18000: loss: 0.521775633096695, lr: 0.0001
2024-01-05 15:40:16 INFO     	 * (global step 18050: loss: 0.4642358422279358, lr: 0.0001
2024-01-05 15:40:24 INFO     	 * (global step 18100: loss: 0.5461641997098923, lr: 0.0001
2024-01-05 15:40:31 INFO     	 * (global step 18150: loss: 0.5694211423397064, lr: 0.0001
2024-01-05 15:40:39 INFO     	 * (global step 18200: loss: 0.464722216129303, lr: 0.0001
2024-01-05 15:40:47 INFO     	 * (global step 18250: loss: 0.4413486570119858, lr: 0.0001
2024-01-05 15:40:55 INFO     	 * (global step 18300: loss: 0.31581274420022964, lr: 0.0001
2024-01-05 15:41:03 INFO     	 * (global step 18350: loss: 0.7728263437747955, lr: 0.0001
2024-01-05 15:41:10 INFO     	 * (global step 18400: loss: 0.6137024760246277, lr: 0.0001
2024-01-05 15:41:16 INFO     	 * (global step 18450: loss: 0.47898223996162415, lr: 0.0001
2024-01-05 15:41:22 INFO     	 * (global step 18500: loss: 0.46919582784175873, lr: 0.0001
2024-01-05 15:41:28 INFO     	 * (global step 18550: loss: 0.7758952379226685, lr: 0.0001
2024-01-05 15:41:35 INFO     	 * (global step 18600: loss: 0.4323651194572449, lr: 0.0001
2024-01-05 15:41:42 INFO     	 * (global step 18650: loss: 0.7017939686775208, lr: 0.0001
2024-01-05 15:41:49 INFO     	 * (global step 18700: loss: 0.5955058336257935, lr: 0.0001
2024-01-05 15:41:57 INFO     	 * (global step 18750: loss: 0.4632929414510727, lr: 0.0001
2024-01-05 15:42:04 INFO     	 * (global step 18800: loss: 0.43993014097213745, lr: 0.0001
2024-01-05 15:42:13 INFO     	 * (global step 18850: loss: 0.5899739861488342, lr: 0.0001
2024-01-05 15:42:20 INFO     	 * (global step 18900: loss: 0.48139289021492004, lr: 0.0001
2024-01-05 15:42:28 INFO     	 * (global step 18950: loss: 0.524220734834671, lr: 0.0001
2024-01-05 15:42:36 INFO     	 * (global step 19000: loss: 0.5210664868354797, lr: 0.0001
2024-01-05 15:42:44 INFO     	 * (global step 19050: loss: 0.7068502604961395, lr: 0.0001
2024-01-05 15:42:52 INFO     	 * (global step 19100: loss: 0.6024272441864014, lr: 0.0001
2024-01-05 15:43:00 INFO     	 * (global step 19150: loss: 0.5660291016101837, lr: 0.0001
2024-01-05 15:43:08 INFO     	 * (global step 19200: loss: 0.4637399762868881, lr: 0.0001
2024-01-05 15:43:15 INFO     	 * (global step 19250: loss: 0.5707328915596008, lr: 0.0001
2024-01-05 15:43:23 INFO     	 * (global step 19300: loss: 0.41135165095329285, lr: 0.0001
2024-01-05 15:43:30 INFO     	 * (global step 19350: loss: 0.6825726926326752, lr: 0.0001
2024-01-05 15:43:38 INFO     	 * (global step 19400: loss: 0.6560998857021332, lr: 0.0001
2024-01-05 15:43:46 INFO     	 * (global step 19450: loss: 0.4414575695991516, lr: 0.0001
2024-01-05 15:43:53 INFO     	 * (global step 19500: loss: 0.4600685387849808, lr: 0.0001
2024-01-05 15:44:00 INFO     	 * (global step 19550: loss: 0.30096665024757385, lr: 0.0001
2024-01-05 15:44:09 INFO     	 * (global step 19600: loss: 0.5201172083616257, lr: 0.0001
2024-01-05 15:44:17 INFO     	 * (global step 19650: loss: 0.38274557888507843, lr: 0.0001
2024-01-05 15:44:25 INFO     	 * (global step 19700: loss: 0.4486532062292099, lr: 0.0001
2024-01-05 15:44:32 INFO     	 * (global step 19750: loss: 0.3917398601770401, lr: 0.0001
2024-01-05 15:44:39 INFO     	 * (global step 19800: loss: 0.4501994848251343, lr: 0.0001
2024-01-05 15:44:45 INFO     	 * (global step 19850: loss: 0.31279341876506805, lr: 0.0001
2024-01-05 15:44:51 INFO     	 * (global step 19900: loss: 0.4488406330347061, lr: 0.0001
2024-01-05 15:44:57 INFO     	 * (global step 19950: loss: 0.46372802555561066, lr: 0.0001
2024-01-05 15:45:04 INFO     	 * (global step 20000: loss: 0.40065887570381165, lr: 0.0001
2024-01-05 15:45:11 INFO     	 * (global step 20050: loss: 0.5860095322132111, lr: 0.0001
2024-01-05 15:45:20 INFO     	 * (global step 20100: loss: 0.5760022401809692, lr: 0.0001
2024-01-05 15:45:28 INFO     	 * (global step 20150: loss: 0.4540977329015732, lr: 0.0001
2024-01-05 15:45:36 INFO     	 * (global step 20200: loss: 0.3845980763435364, lr: 0.0001
2024-01-05 15:45:42 INFO     [epoch 4/15] average loss: 0.519, lr: 0.0001
2024-01-05 15:45:42 INFO     saving model related files
2024-01-05 15:45:42 INFO     saving model
2024-01-05 15:45:43 INFO     saving tokenizer
2024-01-05 15:45:43 INFO     saving optimizer
2024-01-05 15:45:45 INFO     remove old optimizer files
2024-01-05 15:45:46 INFO     	 * (global step 20250: loss: 0.45808251202106476, lr: 0.0001
2024-01-05 15:45:54 INFO     	 * (global step 20300: loss: 0.4601583778858185, lr: 0.0001
2024-01-05 15:46:01 INFO     	 * (global step 20350: loss: 0.5965703129768372, lr: 0.0001
2024-01-05 15:46:09 INFO     	 * (global step 20400: loss: 0.4354320466518402, lr: 0.0001
2024-01-05 15:46:16 INFO     	 * (global step 20450: loss: 0.3492203801870346, lr: 0.0001
2024-01-05 15:46:24 INFO     	 * (global step 20500: loss: 0.3928762525320053, lr: 0.0001
2024-01-05 15:46:32 INFO     	 * (global step 20550: loss: 0.4884735196828842, lr: 0.0001
2024-01-05 15:46:39 INFO     	 * (global step 20600: loss: 0.5477443337440491, lr: 0.0001
2024-01-05 15:46:47 INFO     	 * (global step 20650: loss: 0.668808251619339, lr: 0.0001
2024-01-05 15:46:55 INFO     	 * (global step 20700: loss: 0.718908280134201, lr: 0.0001
2024-01-05 15:47:03 INFO     	 * (global step 20750: loss: 0.34169337153434753, lr: 0.0001
2024-01-05 15:47:10 INFO     	 * (global step 20800: loss: 0.6336717158555984, lr: 0.0001
2024-01-05 15:47:18 INFO     	 * (global step 20850: loss: 0.5102630257606506, lr: 0.0001
2024-01-05 15:47:25 INFO     	 * (global step 20900: loss: 0.36424994468688965, lr: 0.0001
2024-01-05 15:47:33 INFO     	 * (global step 20950: loss: 0.5893563628196716, lr: 0.0001
2024-01-05 15:47:39 INFO     	 * (global step 21000: loss: 0.3085700124502182, lr: 0.0001
2024-01-05 15:47:45 INFO     	 * (global step 21050: loss: 0.6239485442638397, lr: 0.0001
2024-01-05 15:47:51 INFO     	 * (global step 21100: loss: 0.49009452760219574, lr: 0.0001
2024-01-05 15:47:57 INFO     	 * (global step 21150: loss: 0.43429918587207794, lr: 0.0001
2024-01-05 15:48:03 INFO     	 * (global step 21200: loss: 0.3554064631462097, lr: 0.0001
2024-01-05 15:48:09 INFO     	 * (global step 21250: loss: 0.2965346872806549, lr: 0.0001
2024-01-05 15:48:15 INFO     	 * (global step 21300: loss: 0.5732930898666382, lr: 0.0001
2024-01-05 15:48:21 INFO     	 * (global step 21350: loss: 0.5645126402378082, lr: 0.0001
2024-01-05 15:48:27 INFO     	 * (global step 21400: loss: 0.4966695159673691, lr: 0.0001
2024-01-05 15:48:33 INFO     	 * (global step 21450: loss: 0.42400819063186646, lr: 0.0001
2024-01-05 15:48:39 INFO     	 * (global step 21500: loss: 0.36771219968795776, lr: 0.0001
2024-01-05 15:48:45 INFO     	 * (global step 21550: loss: 0.36376431584358215, lr: 0.0001
2024-01-05 15:48:51 INFO     	 * (global step 21600: loss: 0.4525562524795532, lr: 0.0001
2024-01-05 15:48:57 INFO     	 * (global step 21650: loss: 0.3883075565099716, lr: 0.0001
2024-01-05 15:49:03 INFO     	 * (global step 21700: loss: 0.5286919921636581, lr: 0.0001
2024-01-05 15:49:09 INFO     	 * (global step 21750: loss: 0.5459770262241364, lr: 0.0001
2024-01-05 15:49:15 INFO     	 * (global step 21800: loss: 0.592673659324646, lr: 0.0001
2024-01-05 15:49:21 INFO     	 * (global step 21850: loss: 0.6799644827842712, lr: 0.0001
2024-01-05 15:49:27 INFO     	 * (global step 21900: loss: 0.6014729887247086, lr: 0.0001
2024-01-05 15:49:33 INFO     	 * (global step 21950: loss: 0.6178592592477798, lr: 0.0001
2024-01-05 15:49:39 INFO     	 * (global step 22000: loss: 0.5721980929374695, lr: 0.0001
2024-01-05 15:49:45 INFO     	 * (global step 22050: loss: 0.5077605098485947, lr: 0.0001
2024-01-05 15:49:51 INFO     	 * (global step 22100: loss: 0.5089310705661774, lr: 0.0001
2024-01-05 15:49:57 INFO     	 * (global step 22150: loss: 0.39943769574165344, lr: 0.0001
2024-01-05 15:50:03 INFO     	 * (global step 22200: loss: 0.5328943133354187, lr: 0.0001
2024-01-05 15:50:09 INFO     	 * (global step 22250: loss: 0.657545268535614, lr: 0.0001
2024-01-05 15:50:15 INFO     	 * (global step 22300: loss: 0.5515155047178268, lr: 0.0001
2024-01-05 15:50:21 INFO     	 * (global step 22350: loss: 0.4768032729625702, lr: 0.0001
2024-01-05 15:50:27 INFO     	 * (global step 22400: loss: 0.48904165625572205, lr: 0.0001
2024-01-05 15:50:32 INFO     	 * (global step 22450: loss: 0.47931867837905884, lr: 0.0001
2024-01-05 15:50:38 INFO     	 * (global step 22500: loss: 0.5931890904903412, lr: 0.0001
2024-01-05 15:50:44 INFO     	 * (global step 22550: loss: 0.514013946056366, lr: 0.0001
2024-01-05 15:50:50 INFO     	 * (global step 22600: loss: 0.5517776608467102, lr: 0.0001
2024-01-05 15:50:56 INFO     	 * (global step 22650: loss: 0.4192158281803131, lr: 0.0001
2024-01-05 15:51:02 INFO     	 * (global step 22700: loss: 0.39434055984020233, lr: 0.0001
2024-01-05 15:51:08 INFO     	 * (global step 22750: loss: 0.3763171136379242, lr: 0.0001
2024-01-05 15:51:14 INFO     	 * (global step 22800: loss: 0.4299643039703369, lr: 0.0001
2024-01-05 15:51:20 INFO     	 * (global step 22850: loss: 0.512260690331459, lr: 0.0001
2024-01-05 15:51:26 INFO     	 * (global step 22900: loss: 0.37905123829841614, lr: 0.0001
2024-01-05 15:51:32 INFO     	 * (global step 22950: loss: 0.5155036300420761, lr: 0.0001
2024-01-05 15:51:39 INFO     	 * (global step 23000: loss: 0.5978139787912369, lr: 0.0001
2024-01-05 15:51:45 INFO     	 * (global step 23050: loss: 0.3567189872264862, lr: 0.0001
2024-01-05 15:51:51 INFO     	 * (global step 23100: loss: 0.548513799905777, lr: 0.0001
2024-01-05 15:51:57 INFO     	 * (global step 23150: loss: 0.5700260549783707, lr: 0.0001
2024-01-05 15:52:03 INFO     	 * (global step 23200: loss: 0.4020034521818161, lr: 0.0001
2024-01-05 15:52:09 INFO     	 * (global step 23250: loss: 0.5284169614315033, lr: 0.0001
2024-01-05 15:52:15 INFO     	 * (global step 23300: loss: 0.305403009057045, lr: 0.0001
2024-01-05 15:52:21 INFO     	 * (global step 23350: loss: 0.4451560229063034, lr: 0.0001
2024-01-05 15:52:27 INFO     	 * (global step 23400: loss: 0.39562278985977173, lr: 0.0001
2024-01-05 15:52:33 INFO     	 * (global step 23450: loss: 0.4776315689086914, lr: 0.0001
2024-01-05 15:52:39 INFO     	 * (global step 23500: loss: 0.4844683110713959, lr: 0.0001
2024-01-05 15:52:45 INFO     	 * (global step 23550: loss: 0.41730840504169464, lr: 0.0001
2024-01-05 15:52:51 INFO     	 * (global step 23600: loss: 0.5937103629112244, lr: 0.0001
2024-01-05 15:52:57 INFO     	 * (global step 23650: loss: 0.30011117458343506, lr: 0.0001
2024-01-05 15:53:03 INFO     	 * (global step 23700: loss: 0.47414083033800125, lr: 0.0001
2024-01-05 15:53:09 INFO     	 * (global step 23750: loss: 0.560117319226265, lr: 0.0001
2024-01-05 15:53:15 INFO     	 * (global step 23800: loss: 0.43459567427635193, lr: 0.0001
2024-01-05 15:53:21 INFO     	 * (global step 23850: loss: 0.44760623574256897, lr: 0.0001
2024-01-05 15:53:27 INFO     	 * (global step 23900: loss: 0.3959643095731735, lr: 0.0001
2024-01-05 15:53:33 INFO     	 * (global step 23950: loss: 0.8371979296207428, lr: 0.0001
2024-01-05 15:53:39 INFO     	 * (global step 24000: loss: 0.4247237592935562, lr: 0.0001
2024-01-05 15:53:45 INFO     	 * (global step 24050: loss: 0.6924324631690979, lr: 0.0001
2024-01-05 15:53:51 INFO     	 * (global step 24100: loss: 0.4571731388568878, lr: 0.0001
2024-01-05 15:53:57 INFO     	 * (global step 24150: loss: 0.526967853307724, lr: 0.0001
2024-01-05 15:54:03 INFO     	 * (global step 24200: loss: 0.5522763431072235, lr: 0.0001
2024-01-05 15:54:09 INFO     	 * (global step 24250: loss: 0.6074645519256592, lr: 0.0001
2024-01-05 15:54:13 INFO     [epoch 5/15] average loss: 0.504, lr: 0.0001
2024-01-05 15:54:13 INFO     saving model related files
2024-01-05 15:54:13 INFO     saving model
2024-01-05 15:54:14 INFO     saving tokenizer
2024-01-05 15:54:14 INFO     saving optimizer
2024-01-05 15:54:15 INFO     remove old optimizer files
2024-01-05 15:54:16 INFO     	 * (global step 24300: loss: 0.42851564288139343, lr: 0.0001
2024-01-05 15:54:22 INFO     	 * (global step 24350: loss: 0.44225722551345825, lr: 0.0001
2024-01-05 15:54:28 INFO     	 * (global step 24400: loss: 0.4120887964963913, lr: 0.0001
2024-01-05 15:54:34 INFO     	 * (global step 24450: loss: 0.39411963522434235, lr: 0.0001
2024-01-05 15:54:40 INFO     	 * (global step 24500: loss: 0.5510006994009018, lr: 0.0001
2024-01-05 15:54:46 INFO     	 * (global step 24550: loss: 0.43909852206707, lr: 0.0001
2024-01-05 15:54:52 INFO     	 * (global step 24600: loss: 0.5229098349809647, lr: 0.0001
2024-01-05 15:54:58 INFO     	 * (global step 24650: loss: 0.4601585865020752, lr: 0.0001
2024-01-05 15:55:04 INFO     	 * (global step 24700: loss: 0.35137684643268585, lr: 0.0001
2024-01-05 15:55:10 INFO     	 * (global step 24750: loss: 0.5463912189006805, lr: 0.0001
2024-01-05 15:55:16 INFO     	 * (global step 24800: loss: 0.6470979452133179, lr: 0.0001
2024-01-05 15:55:22 INFO     	 * (global step 24850: loss: 0.4844145178794861, lr: 0.0001
2024-01-05 15:55:28 INFO     	 * (global step 24900: loss: 0.5814547836780548, lr: 0.0001
2024-01-05 15:55:34 INFO     	 * (global step 24950: loss: 0.6683170795440674, lr: 0.0001
2024-01-05 15:55:40 INFO     	 * (global step 25000: loss: 0.40401240438222885, lr: 0.0001
2024-01-05 15:55:46 INFO     	 * (global step 25050: loss: 0.6197991669178009, lr: 0.0001
2024-01-05 15:55:52 INFO     	 * (global step 25100: loss: 0.4764016270637512, lr: 0.0001
2024-01-05 15:55:58 INFO     	 * (global step 25150: loss: 0.5451948344707489, lr: 0.0001
2024-01-05 15:56:04 INFO     	 * (global step 25200: loss: 0.6809330135583878, lr: 0.0001
2024-01-05 15:56:10 INFO     	 * (global step 25250: loss: 0.4336152970790863, lr: 0.0001
2024-01-05 15:56:16 INFO     	 * (global step 25300: loss: 0.7950825393199921, lr: 0.0001
2024-01-05 15:56:22 INFO     	 * (global step 25350: loss: 0.4075641334056854, lr: 0.0001
2024-01-05 15:56:27 INFO     	 * (global step 25400: loss: 0.5271472632884979, lr: 0.0001
2024-01-05 15:56:33 INFO     	 * (global step 25450: loss: 0.6889433562755585, lr: 0.0001
2024-01-05 15:56:39 INFO     	 * (global step 25500: loss: 0.5671034008264542, lr: 0.0001
2024-01-05 15:56:45 INFO     	 * (global step 25550: loss: 0.3709575980901718, lr: 0.0001
2024-01-05 15:56:51 INFO     	 * (global step 25600: loss: 0.7947028279304504, lr: 0.0001
2024-01-05 15:56:57 INFO     	 * (global step 25650: loss: 0.590126633644104, lr: 0.0001
2024-01-05 15:57:03 INFO     	 * (global step 25700: loss: 0.5778130292892456, lr: 0.0001
2024-01-05 15:57:09 INFO     	 * (global step 25750: loss: 0.2693088501691818, lr: 0.0001
2024-01-05 15:57:15 INFO     	 * (global step 25800: loss: 0.4375632554292679, lr: 0.0001
2024-01-05 15:57:21 INFO     	 * (global step 25850: loss: 0.46155864000320435, lr: 0.0001
2024-01-05 15:57:27 INFO     	 * (global step 25900: loss: 0.5274218618869781, lr: 0.0001
2024-01-05 15:57:33 INFO     	 * (global step 25950: loss: 0.4799944758415222, lr: 0.0001
2024-01-05 15:57:39 INFO     	 * (global step 26000: loss: 0.3403620570898056, lr: 0.0001
2024-01-05 15:57:45 INFO     	 * (global step 26050: loss: 0.40519602596759796, lr: 0.0001
2024-01-05 15:57:51 INFO     	 * (global step 26100: loss: 0.41389408707618713, lr: 0.0001
2024-01-05 15:57:57 INFO     	 * (global step 26150: loss: 0.4716099351644516, lr: 0.0001
2024-01-05 15:58:03 INFO     	 * (global step 26200: loss: 0.5664642006158829, lr: 0.0001
2024-01-05 15:58:09 INFO     	 * (global step 26250: loss: 0.3628374710679054, lr: 0.0001
2024-01-05 15:58:15 INFO     	 * (global step 26300: loss: 0.4774569571018219, lr: 0.0001
2024-01-05 15:58:21 INFO     	 * (global step 26350: loss: 0.4780191630125046, lr: 0.0001
2024-01-05 15:58:27 INFO     	 * (global step 26400: loss: 0.478329136967659, lr: 0.0001
2024-01-05 15:58:33 INFO     	 * (global step 26450: loss: 0.3921036869287491, lr: 0.0001
2024-01-05 15:58:39 INFO     	 * (global step 26500: loss: 0.7881836295127869, lr: 0.0001
2024-01-05 15:58:45 INFO     	 * (global step 26550: loss: 0.534700945019722, lr: 0.0001
2024-01-05 15:58:51 INFO     	 * (global step 26600: loss: 0.5254736840724945, lr: 0.0001
2024-01-05 15:58:57 INFO     	 * (global step 26650: loss: 0.4157745838165283, lr: 0.0001
2024-01-05 15:59:03 INFO     	 * (global step 26700: loss: 0.6366636008024216, lr: 0.0001
2024-01-05 15:59:09 INFO     	 * (global step 26750: loss: 0.3911912739276886, lr: 0.0001
2024-01-05 15:59:15 INFO     	 * (global step 26800: loss: 0.579288512468338, lr: 0.0001
2024-01-05 15:59:21 INFO     	 * (global step 26850: loss: 0.691485583782196, lr: 0.0001
2024-01-05 15:59:27 INFO     	 * (global step 26900: loss: 0.36064016819000244, lr: 0.0001
2024-01-05 15:59:33 INFO     	 * (global step 26950: loss: 0.4612424224615097, lr: 0.0001
2024-01-05 15:59:39 INFO     	 * (global step 27000: loss: 0.4771008789539337, lr: 0.0001
2024-01-05 15:59:45 INFO     	 * (global step 27050: loss: 0.39076604694128036, lr: 0.0001
2024-01-05 15:59:51 INFO     	 * (global step 27100: loss: 0.38615140318870544, lr: 0.0001
2024-01-05 15:59:57 INFO     	 * (global step 27150: loss: 0.5067654997110367, lr: 0.0001
2024-01-05 16:00:03 INFO     	 * (global step 27200: loss: 0.41079404950141907, lr: 0.0001
2024-01-05 16:00:09 INFO     	 * (global step 27250: loss: 0.3348078280687332, lr: 0.0001
2024-01-05 16:00:15 INFO     	 * (global step 27300: loss: 0.5337879508733749, lr: 0.0001
2024-01-05 16:00:21 INFO     	 * (global step 27350: loss: 0.41045619547367096, lr: 0.0001
2024-01-05 16:00:27 INFO     	 * (global step 27400: loss: 0.541522353887558, lr: 0.0001
2024-01-05 16:00:33 INFO     	 * (global step 27450: loss: 0.3921433240175247, lr: 0.0001
2024-01-05 16:00:39 INFO     	 * (global step 27500: loss: 0.4137090742588043, lr: 0.0001
2024-01-05 16:00:45 INFO     	 * (global step 27550: loss: 0.37721291184425354, lr: 0.0001
2024-01-05 16:00:51 INFO     	 * (global step 27600: loss: 0.3907296359539032, lr: 0.0001
2024-01-05 16:00:57 INFO     	 * (global step 27650: loss: 0.515687882900238, lr: 0.0001
2024-01-05 16:01:03 INFO     	 * (global step 27700: loss: 0.5053861141204834, lr: 0.0001
2024-01-05 16:01:09 INFO     	 * (global step 27750: loss: 0.680560439825058, lr: 0.0001
2024-01-05 16:01:15 INFO     	 * (global step 27800: loss: 0.5224780887365341, lr: 0.0001
2024-01-05 16:01:21 INFO     	 * (global step 27850: loss: 0.5645674765110016, lr: 0.0001
2024-01-05 16:01:27 INFO     	 * (global step 27900: loss: 0.5425562560558319, lr: 0.0001
2024-01-05 16:01:33 INFO     	 * (global step 27950: loss: 0.4038088023662567, lr: 0.0001
2024-01-05 16:01:39 INFO     	 * (global step 28000: loss: 0.42086534202098846, lr: 0.0001
2024-01-05 16:01:45 INFO     	 * (global step 28050: loss: 0.6947725415229797, lr: 0.0001
2024-01-05 16:01:51 INFO     	 * (global step 28100: loss: 0.5742668807506561, lr: 0.0001
2024-01-05 16:01:57 INFO     	 * (global step 28150: loss: 0.5041076689958572, lr: 0.0001
2024-01-05 16:02:03 INFO     	 * (global step 28200: loss: 0.5473205149173737, lr: 0.0001
2024-01-05 16:02:09 INFO     	 * (global step 28250: loss: 0.5304148197174072, lr: 0.0001
2024-01-05 16:02:15 INFO     	 * (global step 28300: loss: 0.6655971109867096, lr: 0.0001
2024-01-05 16:02:20 INFO     [epoch 6/15] average loss: 0.49, lr: 0.0001
2024-01-05 16:02:20 INFO     saving model related files
2024-01-05 16:02:20 INFO     saving model
2024-01-05 16:02:20 INFO     saving tokenizer
2024-01-05 16:02:20 INFO     saving optimizer
2024-01-05 16:02:22 INFO     remove old optimizer files
2024-01-05 16:02:23 INFO     	 * (global step 28350: loss: 0.5694217681884766, lr: 0.0001
2024-01-05 16:02:29 INFO     	 * (global step 28400: loss: 0.39418013393878937, lr: 0.0001
2024-01-05 16:02:35 INFO     	 * (global step 28450: loss: 0.48428842425346375, lr: 0.0001
2024-01-05 16:02:41 INFO     	 * (global step 28500: loss: 0.5175601691007614, lr: 0.0001
2024-01-05 16:02:47 INFO     	 * (global step 28550: loss: 0.5083819776773453, lr: 0.0001
2024-01-05 16:02:53 INFO     	 * (global step 28600: loss: 0.5481341630220413, lr: 0.0001
2024-01-05 16:02:59 INFO     	 * (global step 28650: loss: 0.5772793292999268, lr: 0.0001
2024-01-05 16:03:05 INFO     	 * (global step 28700: loss: 0.45721258223056793, lr: 0.0001
2024-01-05 16:03:11 INFO     	 * (global step 28750: loss: 0.5077037215232849, lr: 0.0001
2024-01-05 16:03:18 INFO     	 * (global step 28800: loss: 0.41719578206539154, lr: 0.0001
2024-01-05 16:03:24 INFO     	 * (global step 28850: loss: 0.5687490701675415, lr: 0.0001
2024-01-05 16:03:30 INFO     	 * (global step 28900: loss: 0.5390947908163071, lr: 0.0001
2024-01-05 16:03:36 INFO     	 * (global step 28950: loss: 0.4499734789133072, lr: 0.0001
2024-01-05 16:03:42 INFO     	 * (global step 29000: loss: 0.4503898322582245, lr: 0.0001
2024-01-05 16:03:48 INFO     	 * (global step 29050: loss: 0.3769010305404663, lr: 0.0001
2024-01-05 16:03:54 INFO     	 * (global step 29100: loss: 0.5390649735927582, lr: 0.0001
2024-01-05 16:04:00 INFO     	 * (global step 29150: loss: 0.6717571020126343, lr: 0.0001
2024-01-05 16:04:06 INFO     	 * (global step 29200: loss: 0.5868287980556488, lr: 0.0001
2024-01-05 16:04:12 INFO     	 * (global step 29250: loss: 0.4358878880739212, lr: 0.0001
2024-01-05 16:04:18 INFO     	 * (global step 29300: loss: 0.3554497957229614, lr: 0.0001
2024-01-05 16:04:24 INFO     	 * (global step 29350: loss: 0.5118902772665024, lr: 0.0001
2024-01-05 16:04:30 INFO     	 * (global step 29400: loss: 0.344851478934288, lr: 0.0001
2024-01-05 16:04:36 INFO     	 * (global step 29450: loss: 0.5074627697467804, lr: 0.0001
2024-01-05 16:04:42 INFO     	 * (global step 29500: loss: 0.3049662560224533, lr: 0.0001
2024-01-05 16:04:48 INFO     	 * (global step 29550: loss: 0.37914495170116425, lr: 0.0001
2024-01-05 16:04:54 INFO     	 * (global step 29600: loss: 0.3087754547595978, lr: 0.0001
2024-01-05 16:05:00 INFO     	 * (global step 29650: loss: 0.38569624722003937, lr: 0.0001
2024-01-05 16:05:06 INFO     	 * (global step 29700: loss: 0.4676399379968643, lr: 0.0001
2024-01-05 16:05:12 INFO     	 * (global step 29750: loss: 0.7504094541072845, lr: 0.0001
2024-01-05 16:05:18 INFO     	 * (global step 29800: loss: 0.4971041828393936, lr: 0.0001
2024-01-05 16:05:24 INFO     	 * (global step 29850: loss: 0.464510902762413, lr: 0.0001
2024-01-05 16:05:30 INFO     	 * (global step 29900: loss: 0.43432365357875824, lr: 0.0001
2024-01-05 16:05:36 INFO     	 * (global step 29950: loss: 0.4975969195365906, lr: 0.0001
2024-01-05 16:05:41 INFO     	 * (global step 30000: loss: 0.48776136338710785, lr: 0.0001
2024-01-05 16:05:48 INFO     	 * (global step 30050: loss: 0.47749489545822144, lr: 0.0001
2024-01-05 16:05:54 INFO     	 * (global step 30100: loss: 0.4659503102302551, lr: 0.0001
2024-01-05 16:06:00 INFO     	 * (global step 30150: loss: 0.44374406337738037, lr: 0.0001
2024-01-05 16:06:06 INFO     	 * (global step 30200: loss: 0.5316495001316071, lr: 0.0001
2024-01-05 16:06:12 INFO     	 * (global step 30250: loss: 0.5196805447340012, lr: 0.0001
2024-01-05 16:06:18 INFO     	 * (global step 30300: loss: 0.6057985424995422, lr: 0.0001
2024-01-05 16:06:24 INFO     	 * (global step 30350: loss: 0.30592695623636246, lr: 0.0001
2024-01-05 16:06:30 INFO     	 * (global step 30400: loss: 0.5296320617198944, lr: 0.0001
2024-01-05 16:06:36 INFO     	 * (global step 30450: loss: 0.9107649624347687, lr: 0.0001
2024-01-05 16:06:42 INFO     	 * (global step 30500: loss: 0.49788741767406464, lr: 0.0001
2024-01-05 16:06:48 INFO     	 * (global step 30550: loss: 0.2728448361158371, lr: 0.0001
2024-01-05 16:06:54 INFO     	 * (global step 30600: loss: 0.6038942337036133, lr: 0.0001
2024-01-05 16:07:00 INFO     	 * (global step 30650: loss: 0.6806901693344116, lr: 0.0001
2024-01-05 16:07:06 INFO     	 * (global step 30700: loss: 0.3476969450712204, lr: 0.0001
2024-01-05 16:07:12 INFO     	 * (global step 30750: loss: 0.46803246438503265, lr: 0.0001
2024-01-05 16:07:18 INFO     	 * (global step 30800: loss: 0.36671802401542664, lr: 0.0001
2024-01-05 16:07:24 INFO     	 * (global step 30850: loss: 0.31957343220710754, lr: 0.0001
2024-01-05 16:07:30 INFO     	 * (global step 30900: loss: 0.6312707364559174, lr: 0.0001
2024-01-05 16:07:36 INFO     	 * (global step 30950: loss: 0.3998827636241913, lr: 0.0001
2024-01-05 16:07:42 INFO     	 * (global step 31000: loss: 0.6065342724323273, lr: 0.0001
2024-01-05 16:07:48 INFO     	 * (global step 31050: loss: 0.35877323150634766, lr: 0.0001
2024-01-05 16:07:54 INFO     	 * (global step 31100: loss: 0.3589796647429466, lr: 0.0001
2024-01-05 16:08:00 INFO     	 * (global step 31150: loss: 0.42961160838603973, lr: 0.0001
2024-01-05 16:08:06 INFO     	 * (global step 31200: loss: 0.4559441804885864, lr: 0.0001
2024-01-05 16:08:12 INFO     	 * (global step 31250: loss: 0.44691938161849976, lr: 0.0001
2024-01-05 16:08:19 INFO     	 * (global step 31300: loss: 0.6535468101501465, lr: 0.0001
2024-01-05 16:08:25 INFO     	 * (global step 31350: loss: 0.49069851636886597, lr: 0.0001
2024-01-05 16:08:31 INFO     	 * (global step 31400: loss: 0.6918783485889435, lr: 0.0001
2024-01-05 16:08:37 INFO     	 * (global step 31450: loss: 0.5393168181180954, lr: 0.0001
2024-01-05 16:08:43 INFO     	 * (global step 31500: loss: 0.34420792758464813, lr: 0.0001
2024-01-05 16:08:49 INFO     	 * (global step 31550: loss: 0.38438600301742554, lr: 0.0001
2024-01-05 16:08:55 INFO     	 * (global step 31600: loss: 0.46597253531217575, lr: 0.0001
2024-01-05 16:09:01 INFO     	 * (global step 31650: loss: 0.5428638458251953, lr: 0.0001
2024-01-05 16:09:07 INFO     	 * (global step 31700: loss: 0.5647946000099182, lr: 0.0001
2024-01-05 16:09:13 INFO     	 * (global step 31750: loss: 0.49119675904512405, lr: 0.0001
2024-01-05 16:09:19 INFO     	 * (global step 31800: loss: 0.3883437514305115, lr: 0.0001
2024-01-05 16:09:25 INFO     	 * (global step 31850: loss: 0.5085549503564835, lr: 0.0001
2024-01-05 16:09:31 INFO     	 * (global step 31900: loss: 0.43098384141921997, lr: 0.0001
2024-01-05 16:09:37 INFO     	 * (global step 31950: loss: 0.4119308441877365, lr: 0.0001
2024-01-05 16:09:43 INFO     	 * (global step 32000: loss: 0.524066686630249, lr: 0.0001
2024-01-05 16:09:49 INFO     	 * (global step 32050: loss: 0.46031126379966736, lr: 0.0001
2024-01-05 16:09:55 INFO     	 * (global step 32100: loss: 0.48883675038814545, lr: 0.0001
2024-01-05 16:10:01 INFO     	 * (global step 32150: loss: 0.31358473002910614, lr: 0.0001
2024-01-05 16:10:07 INFO     	 * (global step 32200: loss: 0.4285232722759247, lr: 0.0001
2024-01-05 16:10:13 INFO     	 * (global step 32250: loss: 0.3378201872110367, lr: 0.0001
2024-01-05 16:10:19 INFO     	 * (global step 32300: loss: 0.49651989340782166, lr: 0.0001
2024-01-05 16:10:25 INFO     	 * (global step 32350: loss: 0.39492787420749664, lr: 0.0001
2024-01-05 16:10:30 INFO     [epoch 7/15] average loss: 0.478, lr: 0.0001
2024-01-05 16:10:30 INFO     saving model related files
2024-01-05 16:10:30 INFO     saving model
2024-01-05 16:10:30 INFO     saving tokenizer
2024-01-05 16:10:30 INFO     saving optimizer
2024-01-05 16:10:31 INFO     remove old optimizer files
2024-01-05 16:10:33 INFO     	 * (global step 32400: loss: 0.5864425301551819, lr: 0.0001
2024-01-05 16:10:39 INFO     	 * (global step 32450: loss: 0.302280455827713, lr: 0.0001
2024-01-05 16:10:45 INFO     	 * (global step 32500: loss: 0.4405011236667633, lr: 0.0001
2024-01-05 16:10:51 INFO     	 * (global step 32550: loss: 0.3724813610315323, lr: 0.0001
2024-01-05 16:10:57 INFO     	 * (global step 32600: loss: 0.4633125960826874, lr: 0.0001
2024-01-05 16:11:03 INFO     	 * (global step 32650: loss: 0.41586098074913025, lr: 0.0001
2024-01-05 16:11:09 INFO     	 * (global step 32700: loss: 0.3218833804130554, lr: 0.0001
2024-01-05 16:11:15 INFO     	 * (global step 32750: loss: 0.40174688398838043, lr: 0.0001
2024-01-05 16:11:21 INFO     	 * (global step 32800: loss: 0.5361566692590714, lr: 0.0001
2024-01-05 16:11:27 INFO     	 * (global step 32850: loss: 0.381805881857872, lr: 0.0001
2024-01-05 16:11:33 INFO     	 * (global step 32900: loss: 0.44471319019794464, lr: 0.0001
2024-01-05 16:11:39 INFO     	 * (global step 32950: loss: 0.5201423168182373, lr: 0.0001
2024-01-05 16:11:45 INFO     	 * (global step 33000: loss: 0.27856744825839996, lr: 0.0001
2024-01-05 16:11:51 INFO     	 * (global step 33050: loss: 0.45446373522281647, lr: 0.0001
2024-01-05 16:11:57 INFO     	 * (global step 33100: loss: 0.5308912098407745, lr: 0.0001
2024-01-05 16:12:03 INFO     	 * (global step 33150: loss: 0.37147051841020584, lr: 0.0001
2024-01-05 16:12:09 INFO     	 * (global step 33200: loss: 0.42676128447055817, lr: 0.0001
2024-01-05 16:12:15 INFO     	 * (global step 33250: loss: 0.45681189000606537, lr: 0.0001
2024-01-05 16:12:21 INFO     	 * (global step 33300: loss: 0.17089517414569855, lr: 0.0001
2024-01-05 16:12:27 INFO     	 * (global step 33350: loss: 0.555746003985405, lr: 0.0001
2024-01-05 16:12:33 INFO     	 * (global step 33400: loss: 0.4034304916858673, lr: 0.0001
2024-01-05 16:12:39 INFO     	 * (global step 33450: loss: 0.43578939139842987, lr: 0.0001
2024-01-05 16:12:45 INFO     	 * (global step 33500: loss: 0.5123643279075623, lr: 0.0001
2024-01-05 16:12:51 INFO     	 * (global step 33550: loss: 0.39511825889348984, lr: 0.0001
2024-01-05 16:12:57 INFO     	 * (global step 33600: loss: 0.5060104131698608, lr: 0.0001
2024-01-05 16:13:03 INFO     	 * (global step 33650: loss: 0.5412736982107162, lr: 0.0001
2024-01-05 16:13:09 INFO     	 * (global step 33700: loss: 0.309249185025692, lr: 0.0001
2024-01-05 16:13:15 INFO     	 * (global step 33750: loss: 0.3482581526041031, lr: 0.0001
2024-01-05 16:13:21 INFO     	 * (global step 33800: loss: 0.5056959986686707, lr: 0.0001
2024-01-05 16:13:27 INFO     	 * (global step 33850: loss: 0.41781097650527954, lr: 0.0001
2024-01-05 16:13:33 INFO     	 * (global step 33900: loss: 0.42952910810709, lr: 0.0001
2024-01-05 16:13:39 INFO     	 * (global step 33950: loss: 0.3844798058271408, lr: 0.0001
2024-01-05 16:13:45 INFO     	 * (global step 34000: loss: 0.5911538302898407, lr: 0.0001
2024-01-05 16:13:51 INFO     	 * (global step 34050: loss: 0.8085413575172424, lr: 0.0001
2024-01-05 16:13:57 INFO     	 * (global step 34100: loss: 0.5404388904571533, lr: 0.0001
2024-01-05 16:14:03 INFO     	 * (global step 34150: loss: 0.3758483827114105, lr: 0.0001
2024-01-05 16:14:09 INFO     	 * (global step 34200: loss: 0.492676705121994, lr: 0.0001
2024-01-05 16:14:15 INFO     	 * (global step 34250: loss: 0.4749104827642441, lr: 0.0001
2024-01-05 16:14:21 INFO     	 * (global step 34300: loss: 0.2990754544734955, lr: 0.0001
2024-01-05 16:14:27 INFO     	 * (global step 34350: loss: 0.45107024908065796, lr: 0.0001
2024-01-05 16:14:33 INFO     	 * (global step 34400: loss: 0.2909497693181038, lr: 0.0001
2024-01-05 16:14:39 INFO     	 * (global step 34450: loss: 0.47881321609020233, lr: 0.0001
2024-01-05 16:14:45 INFO     	 * (global step 34500: loss: 0.3033883720636368, lr: 0.0001
2024-01-05 16:14:51 INFO     	 * (global step 34550: loss: 0.3749382048845291, lr: 0.0001
2024-01-05 16:14:57 INFO     	 * (global step 34600: loss: 0.5316700637340546, lr: 0.0001
2024-01-05 16:15:03 INFO     	 * (global step 34650: loss: 0.4202738255262375, lr: 0.0001
2024-01-05 16:15:09 INFO     	 * (global step 34700: loss: 0.5908287167549133, lr: 0.0001
2024-01-05 16:15:15 INFO     	 * (global step 34750: loss: 0.5484847724437714, lr: 0.0001
2024-01-05 16:15:21 INFO     	 * (global step 34800: loss: 0.41096074879169464, lr: 0.0001
2024-01-05 16:15:27 INFO     	 * (global step 34850: loss: 0.6084157824516296, lr: 0.0001
2024-01-05 16:15:33 INFO     	 * (global step 34900: loss: 0.4376639276742935, lr: 0.0001
2024-01-05 16:15:39 INFO     	 * (global step 34950: loss: 0.4060351923108101, lr: 0.0001
2024-01-05 16:15:46 INFO     	 * (global step 35000: loss: 0.43190106749534607, lr: 0.0001
2024-01-05 16:15:52 INFO     	 * (global step 35050: loss: 0.7625885307788849, lr: 0.0001
2024-01-05 16:15:58 INFO     	 * (global step 35100: loss: 0.5177378356456757, lr: 0.0001
2024-01-05 16:16:04 INFO     	 * (global step 35150: loss: 0.48541271686553955, lr: 0.0001
2024-01-05 16:16:10 INFO     	 * (global step 35200: loss: 0.5215977132320404, lr: 0.0001
2024-01-05 16:16:16 INFO     	 * (global step 35250: loss: 0.43115437030792236, lr: 0.0001
2024-01-05 16:16:22 INFO     	 * (global step 35300: loss: 0.4193257838487625, lr: 0.0001
2024-01-05 16:16:27 INFO     	 * (global step 35350: loss: 0.3881823569536209, lr: 0.0001
2024-01-05 16:16:33 INFO     	 * (global step 35400: loss: 0.41705740988254547, lr: 0.0001
2024-01-05 16:16:39 INFO     	 * (global step 35450: loss: 0.44962364435195923, lr: 0.0001
2024-01-05 16:16:45 INFO     	 * (global step 35500: loss: 0.4213322699069977, lr: 0.0001
2024-01-05 16:16:51 INFO     	 * (global step 35550: loss: 0.42278194427490234, lr: 0.0001
2024-01-05 16:16:57 INFO     	 * (global step 35600: loss: 0.44386567175388336, lr: 0.0001
2024-01-05 16:17:03 INFO     	 * (global step 35650: loss: 0.26952914148569107, lr: 0.0001
2024-01-05 16:17:09 INFO     	 * (global step 35700: loss: 0.5556913614273071, lr: 0.0001
2024-01-05 16:17:15 INFO     	 * (global step 35750: loss: 0.47459040582180023, lr: 0.0001
2024-01-05 16:17:21 INFO     	 * (global step 35800: loss: 0.43054327368736267, lr: 0.0001
2024-01-05 16:17:27 INFO     	 * (global step 35850: loss: 0.4672546833753586, lr: 0.0001
2024-01-05 16:17:33 INFO     	 * (global step 35900: loss: 0.6428038477897644, lr: 0.0001
2024-01-05 16:17:39 INFO     	 * (global step 35950: loss: 0.7843621224164963, lr: 0.0001
2024-01-05 16:17:45 INFO     	 * (global step 36000: loss: 0.5098230987787247, lr: 0.0001
2024-01-05 16:17:51 INFO     	 * (global step 36050: loss: 0.6119358241558075, lr: 0.0001
2024-01-05 16:17:57 INFO     	 * (global step 36100: loss: 0.39684774726629257, lr: 0.0001
2024-01-05 16:18:03 INFO     	 * (global step 36150: loss: 0.4915962815284729, lr: 0.0001
2024-01-05 16:18:09 INFO     	 * (global step 36200: loss: 0.46011605858802795, lr: 0.0001
2024-01-05 16:18:15 INFO     	 * (global step 36250: loss: 0.40382903814315796, lr: 0.0001
2024-01-05 16:18:21 INFO     	 * (global step 36300: loss: 0.3814139664173126, lr: 0.0001
2024-01-05 16:18:27 INFO     	 * (global step 36350: loss: 0.35609154403209686, lr: 0.0001
2024-01-05 16:18:33 INFO     	 * (global step 36400: loss: 0.4034135192632675, lr: 0.0001
2024-01-05 16:18:37 INFO     [epoch 8/15] average loss: 0.467, lr: 0.0001
2024-01-05 16:18:37 INFO     saving model related files
2024-01-05 16:18:37 INFO     saving model
2024-01-05 16:18:37 INFO     saving tokenizer
2024-01-05 16:18:38 INFO     saving optimizer
2024-01-05 16:18:39 INFO     remove old optimizer files
2024-01-05 16:18:41 INFO     	 * (global step 36450: loss: 0.2866782173514366, lr: 0.0001
2024-01-05 16:18:47 INFO     	 * (global step 36500: loss: 0.5065562427043915, lr: 0.0001
2024-01-05 16:18:53 INFO     	 * (global step 36550: loss: 0.5128566175699234, lr: 0.0001
2024-01-05 16:18:59 INFO     	 * (global step 36600: loss: 0.31860796362161636, lr: 0.0001
2024-01-05 16:19:05 INFO     	 * (global step 36650: loss: 0.3780510872602463, lr: 0.0001
2024-01-05 16:19:11 INFO     	 * (global step 36700: loss: 0.4870031625032425, lr: 0.0001
2024-01-05 16:19:17 INFO     	 * (global step 36750: loss: 0.4396294504404068, lr: 0.0001
2024-01-05 16:19:23 INFO     	 * (global step 36800: loss: 0.606876477599144, lr: 0.0001
2024-01-05 16:19:29 INFO     	 * (global step 36850: loss: 0.44397637248039246, lr: 0.0001
2024-01-05 16:19:35 INFO     	 * (global step 36900: loss: 0.4651449918746948, lr: 0.0001
2024-01-05 16:19:41 INFO     	 * (global step 36950: loss: 0.3473818376660347, lr: 0.0001
2024-01-05 16:19:47 INFO     	 * (global step 37000: loss: 0.5890345275402069, lr: 0.0001
2024-01-05 16:19:53 INFO     	 * (global step 37050: loss: 0.4223220944404602, lr: 0.0001
2024-01-05 16:19:59 INFO     	 * (global step 37100: loss: 0.4675662964582443, lr: 0.0001
2024-01-05 16:20:05 INFO     	 * (global step 37150: loss: 0.7321919500827789, lr: 0.0001
2024-01-05 16:20:11 INFO     	 * (global step 37200: loss: 0.5818321257829666, lr: 0.0001
2024-01-05 16:20:17 INFO     	 * (global step 37250: loss: 0.49627260863780975, lr: 0.0001
2024-01-05 16:20:22 INFO     	 * (global step 37300: loss: 0.38408811390399933, lr: 0.0001
2024-01-05 16:20:28 INFO     	 * (global step 37350: loss: 0.33235087990760803, lr: 0.0001
2024-01-05 16:20:34 INFO     	 * (global step 37400: loss: 0.6568596661090851, lr: 0.0001
2024-01-05 16:20:40 INFO     	 * (global step 37450: loss: 0.2757169008255005, lr: 0.0001
2024-01-05 16:20:46 INFO     	 * (global step 37500: loss: 0.6900299489498138, lr: 0.0001
2024-01-05 16:20:52 INFO     	 * (global step 37550: loss: 0.32489413022994995, lr: 0.0001
2024-01-05 16:20:58 INFO     	 * (global step 37600: loss: 0.48228520154953003, lr: 0.0001
2024-01-05 16:21:04 INFO     	 * (global step 37650: loss: 0.40939249098300934, lr: 0.0001
2024-01-05 16:21:10 INFO     	 * (global step 37700: loss: 0.5030553936958313, lr: 0.0001
2024-01-05 16:21:16 INFO     	 * (global step 37750: loss: 0.5046706050634384, lr: 0.0001
2024-01-05 16:21:22 INFO     	 * (global step 37800: loss: 0.39213497936725616, lr: 0.0001
2024-01-05 16:21:28 INFO     	 * (global step 37850: loss: 0.36033400893211365, lr: 0.0001
2024-01-05 16:21:34 INFO     	 * (global step 37900: loss: 0.3836768567562103, lr: 0.0001
2024-01-05 16:21:40 INFO     	 * (global step 37950: loss: 0.26776833087205887, lr: 0.0001
2024-01-05 16:21:46 INFO     	 * (global step 38000: loss: 0.5117280185222626, lr: 0.0001
2024-01-05 16:21:52 INFO     	 * (global step 38050: loss: 0.49388487637043, lr: 0.0001
2024-01-05 16:21:58 INFO     	 * (global step 38100: loss: 0.4068217799067497, lr: 0.0001
2024-01-05 16:22:04 INFO     	 * (global step 38150: loss: 0.4880608022212982, lr: 0.0001
2024-01-05 16:22:10 INFO     	 * (global step 38200: loss: 0.3954120948910713, lr: 0.0001
2024-01-05 16:22:16 INFO     	 * (global step 38250: loss: 0.5207815319299698, lr: 0.0001
2024-01-05 16:22:22 INFO     	 * (global step 38300: loss: 0.4560431092977524, lr: 0.0001
2024-01-05 16:22:28 INFO     	 * (global step 38350: loss: 0.5518252849578857, lr: 0.0001
2024-01-05 16:22:34 INFO     	 * (global step 38400: loss: 0.6725575923919678, lr: 0.0001
2024-01-05 16:22:40 INFO     	 * (global step 38450: loss: 0.4371628910303116, lr: 0.0001
2024-01-05 16:22:46 INFO     	 * (global step 38500: loss: 0.41237664222717285, lr: 0.0001
2024-01-05 16:22:53 INFO     	 * (global step 38550: loss: 0.2371111549437046, lr: 0.0001
2024-01-05 16:22:58 INFO     	 * (global step 38600: loss: 0.2324826493859291, lr: 0.0001
2024-01-05 16:23:05 INFO     	 * (global step 38650: loss: 0.6882094293832779, lr: 0.0001
2024-01-05 16:23:11 INFO     	 * (global step 38700: loss: 0.39841943979263306, lr: 0.0001
2024-01-05 16:23:16 INFO     	 * (global step 38750: loss: 0.43608273565769196, lr: 0.0001
2024-01-05 16:23:23 INFO     	 * (global step 38800: loss: 0.24113105982542038, lr: 0.0001
2024-01-05 16:23:28 INFO     	 * (global step 38850: loss: 0.4628874957561493, lr: 0.0001
2024-01-05 16:23:34 INFO     	 * (global step 38900: loss: 0.3632468581199646, lr: 0.0001
2024-01-05 16:23:40 INFO     	 * (global step 38950: loss: 0.5510718077421188, lr: 0.0001
2024-01-05 16:23:47 INFO     	 * (global step 39000: loss: 0.3957275152206421, lr: 0.0001
2024-01-05 16:23:53 INFO     	 * (global step 39050: loss: 0.4538677781820297, lr: 0.0001
2024-01-05 16:23:59 INFO     	 * (global step 39100: loss: 0.4344892352819443, lr: 0.0001
2024-01-05 16:24:05 INFO     	 * (global step 39150: loss: 0.6147352010011673, lr: 0.0001
2024-01-05 16:24:11 INFO     	 * (global step 39200: loss: 0.3987829387187958, lr: 0.0001
2024-01-05 16:24:17 INFO     	 * (global step 39250: loss: 0.4566952586174011, lr: 0.0001
2024-01-05 16:24:23 INFO     	 * (global step 39300: loss: 0.4152769148349762, lr: 0.0001
2024-01-05 16:24:29 INFO     	 * (global step 39350: loss: 0.40277358889579773, lr: 0.0001
2024-01-05 16:24:35 INFO     	 * (global step 39400: loss: 0.3954289108514786, lr: 0.0001
2024-01-05 16:24:41 INFO     	 * (global step 39450: loss: 0.4735180288553238, lr: 0.0001
2024-01-05 16:24:47 INFO     	 * (global step 39500: loss: 0.5721611082553864, lr: 0.0001
2024-01-05 16:24:53 INFO     	 * (global step 39550: loss: 0.5450293272733688, lr: 0.0001
2024-01-05 16:24:59 INFO     	 * (global step 39600: loss: 0.6757841110229492, lr: 0.0001
2024-01-05 16:25:05 INFO     	 * (global step 39650: loss: 0.45644091069698334, lr: 0.0001
2024-01-05 16:25:11 INFO     	 * (global step 39700: loss: 0.40531477332115173, lr: 0.0001
2024-01-05 16:25:17 INFO     	 * (global step 39750: loss: 0.5514891445636749, lr: 0.0001
2024-01-05 16:25:23 INFO     	 * (global step 39800: loss: 0.8411406576633453, lr: 0.0001
2024-01-05 16:25:29 INFO     	 * (global step 39850: loss: 0.6966972053050995, lr: 0.0001
2024-01-05 16:25:35 INFO     	 * (global step 39900: loss: 0.3818083852529526, lr: 0.0001
2024-01-05 16:25:41 INFO     	 * (global step 39950: loss: 0.33816224336624146, lr: 0.0001
2024-01-05 16:25:47 INFO     	 * (global step 40000: loss: 0.7153388857841492, lr: 0.0001
2024-01-05 16:25:53 INFO     	 * (global step 40050: loss: 0.4391396790742874, lr: 0.0001
2024-01-05 16:25:59 INFO     	 * (global step 40100: loss: 0.5375024378299713, lr: 0.0001
2024-01-05 16:26:05 INFO     	 * (global step 40150: loss: 0.6013735234737396, lr: 0.0001
2024-01-05 16:26:11 INFO     	 * (global step 40200: loss: 0.561115175485611, lr: 0.0001
2024-01-05 16:26:17 INFO     	 * (global step 40250: loss: 0.5998769104480743, lr: 0.0001
2024-01-05 16:26:23 INFO     	 * (global step 40300: loss: 0.35308344662189484, lr: 0.0001
2024-01-05 16:26:29 INFO     	 * (global step 40350: loss: 0.5678191483020782, lr: 0.0001
2024-01-05 16:26:35 INFO     	 * (global step 40400: loss: 0.3396032452583313, lr: 0.0001
2024-01-05 16:26:40 INFO     	 * (global step 40450: loss: 0.3364346921443939, lr: 0.0001
2024-01-05 16:26:44 INFO     [epoch 9/15] average loss: 0.457, lr: 0.0001
2024-01-05 16:26:44 INFO     saving model related files
2024-01-05 16:26:44 INFO     saving model
2024-01-05 16:26:45 INFO     saving tokenizer
2024-01-05 16:26:45 INFO     saving optimizer
2024-01-05 16:26:46 INFO     remove old optimizer files
2024-01-05 16:26:46 INFO     complete training: model ckpt was saved at small_recreated_ckpt/model_mzgdpa
2024-01-05 16:26:46 INFO     ## 1st RUN: Configuration 4/12 ##
2024-01-05 16:26:46 INFO     initialize model trainer
2024-01-05 16:26:46 INFO     initialize checkpoint at small_recreated_ckpt/model_mntyya
2024-01-05 16:26:46 INFO     hyperparameters
2024-01-05 16:26:46 INFO     	 * dataset_path: lmqg/qag_squad
2024-01-05 16:26:46 INFO     	 * dataset_name: default
2024-01-05 16:26:46 INFO     	 * input_types: ['paragraph']
2024-01-05 16:26:46 INFO     	 * output_types: ['questions_answers']
2024-01-05 16:26:46 INFO     	 * prefix_types: ['qag']
2024-01-05 16:26:46 INFO     	 * model: t5-small
2024-01-05 16:26:46 INFO     	 * max_length: 512
2024-01-05 16:26:46 INFO     	 * max_length_output: 256
2024-01-05 16:26:46 INFO     	 * epoch: 15
2024-01-05 16:26:46 INFO     	 * batch: 2
2024-01-05 16:26:46 INFO     	 * lr: 5e-05
2024-01-05 16:26:46 INFO     	 * fp16: False
2024-01-05 16:26:46 INFO     	 * random_seed: 1
2024-01-05 16:26:46 INFO     	 * gradient_accumulation_steps: 4
2024-01-05 16:26:46 INFO     	 * label_smoothing: 0.15
2024-01-05 16:26:46 INFO     initialize checkpoint with t5-small
2024-01-05 16:26:53 INFO     use spaCy answer extraction model: positionrank
2024-01-05 16:26:55 INFO     Model `t5-small`
2024-01-05 16:26:55 INFO     	 * Num of GPU in use: 1
2024-01-05 16:26:55 INFO     	 * Prefix: True
2024-01-05 16:26:55 INFO     	 * Language: en (ignore at the training phase)
2024-01-05 16:26:56 INFO     dataset preprocessing
2024-01-05 16:26:59 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featurelmqg/qag_squad/t5-small.512.256.paragraph.questions_answers.train.qag.pkl
2024-01-05 16:27:04 INFO     start model training
2024-01-05 16:27:15 INFO     	 * (global step 50: loss: 1.9683461487293243, lr: 5e-05
2024-01-05 16:27:26 INFO     	 * (global step 100: loss: 1.356455773115158, lr: 5e-05
2024-01-05 16:27:38 INFO     	 * (global step 150: loss: 1.171513020992279, lr: 5e-05
2024-01-05 16:27:49 INFO     	 * (global step 200: loss: 0.9567255824804306, lr: 5e-05
2024-01-05 16:28:00 INFO     	 * (global step 250: loss: 0.9093507677316666, lr: 5e-05
2024-01-05 16:28:11 INFO     	 * (global step 300: loss: 0.9120561629533768, lr: 5e-05
2024-01-05 16:28:23 INFO     	 * (global step 350: loss: 0.9905566275119781, lr: 5e-05
2024-01-05 16:28:35 INFO     	 * (global step 400: loss: 1.1026918292045593, lr: 5e-05
2024-01-05 16:28:46 INFO     	 * (global step 450: loss: 0.7788396924734116, lr: 5e-05
2024-01-05 16:28:58 INFO     	 * (global step 500: loss: 0.9390723407268524, lr: 5e-05
2024-01-05 16:29:10 INFO     	 * (global step 550: loss: 0.8875989615917206, lr: 5e-05
2024-01-05 16:29:21 INFO     	 * (global step 600: loss: 1.0437647700309753, lr: 5e-05
2024-01-05 16:29:32 INFO     	 * (global step 650: loss: 0.8059968650341034, lr: 5e-05
2024-01-05 16:29:44 INFO     	 * (global step 700: loss: 0.7046518325805664, lr: 5e-05
2024-01-05 16:29:56 INFO     	 * (global step 750: loss: 0.7406651377677917, lr: 5e-05
2024-01-05 16:30:07 INFO     	 * (global step 800: loss: 0.5896588414907455, lr: 5e-05
2024-01-05 16:30:19 INFO     	 * (global step 850: loss: 0.7721086889505386, lr: 5e-05
2024-01-05 16:30:31 INFO     	 * (global step 900: loss: 0.8177237212657928, lr: 5e-05
2024-01-05 16:30:43 INFO     	 * (global step 950: loss: 0.6391644515097141, lr: 5e-05
2024-01-05 16:30:55 INFO     	 * (global step 1000: loss: 1.1417423635721207, lr: 5e-05
2024-01-05 16:31:06 INFO     	 * (global step 1050: loss: 0.63432177901268, lr: 5e-05
2024-01-05 16:31:18 INFO     	 * (global step 1100: loss: 0.749979630112648, lr: 5e-05
2024-01-05 16:31:30 INFO     	 * (global step 1150: loss: 1.1428949683904648, lr: 5e-05
2024-01-05 16:31:42 INFO     	 * (global step 1200: loss: 0.6139503717422485, lr: 5e-05
2024-01-05 16:31:54 INFO     	 * (global step 1250: loss: 0.5974080041050911, lr: 5e-05
2024-01-05 16:32:06 INFO     	 * (global step 1300: loss: 0.527667723596096, lr: 5e-05
2024-01-05 16:32:18 INFO     	 * (global step 1350: loss: 0.7422125041484833, lr: 5e-05
2024-01-05 16:32:30 INFO     	 * (global step 1400: loss: 0.7432255148887634, lr: 5e-05
2024-01-05 16:32:42 INFO     	 * (global step 1450: loss: 0.8126007616519928, lr: 5e-05
2024-01-05 16:32:54 INFO     	 * (global step 1500: loss: 0.6822608560323715, lr: 5e-05
2024-01-05 16:33:06 INFO     	 * (global step 1550: loss: 0.7468858361244202, lr: 5e-05
2024-01-05 16:33:17 INFO     	 * (global step 1600: loss: 0.7782741039991379, lr: 5e-05
2024-01-05 16:33:28 INFO     	 * (global step 1650: loss: 0.6408639475703239, lr: 5e-05
2024-01-05 16:33:41 INFO     	 * (global step 1700: loss: 0.7089401185512543, lr: 5e-05
2024-01-05 16:33:55 INFO     	 * (global step 1750: loss: 0.6033055335283279, lr: 5e-05
2024-01-05 16:34:09 INFO     	 * (global step 1800: loss: 0.6442025750875473, lr: 5e-05
2024-01-05 16:34:24 INFO     	 * (global step 1850: loss: 0.4581522196531296, lr: 5e-05
2024-01-05 16:34:38 INFO     	 * (global step 1900: loss: 0.5260118767619133, lr: 5e-05
2024-01-05 16:34:52 INFO     	 * (global step 1950: loss: 0.8001906424760818, lr: 5e-05
2024-01-05 16:35:07 INFO     	 * (global step 2000: loss: 0.9214302077889442, lr: 5e-05
2024-01-05 16:35:13 INFO     [epoch 0/15] average loss: 0.844, lr: 5e-05
2024-01-05 16:35:13 INFO     saving model related files
2024-01-05 16:35:13 INFO     saving model
2024-01-05 16:35:14 INFO     saving tokenizer
2024-01-05 16:35:14 INFO     saving optimizer
2024-01-05 16:35:16 INFO     remove old optimizer files
2024-01-05 16:35:23 INFO     	 * (global step 2050: loss: 0.5207161530852318, lr: 5e-05
2024-01-05 16:35:37 INFO     	 * (global step 2100: loss: 0.6235640570521355, lr: 5e-05
2024-01-05 16:35:51 INFO     	 * (global step 2150: loss: 0.8029514849185944, lr: 5e-05
2024-01-05 16:36:03 INFO     	 * (global step 2200: loss: 0.572873480618, lr: 5e-05
2024-01-05 16:36:15 INFO     	 * (global step 2250: loss: 0.5722057521343231, lr: 5e-05
2024-01-05 16:36:27 INFO     	 * (global step 2300: loss: 0.7193123772740364, lr: 5e-05
2024-01-05 16:36:42 INFO     	 * (global step 2350: loss: 0.6858168840408325, lr: 5e-05
2024-01-05 16:36:57 INFO     	 * (global step 2400: loss: 0.6333549693226814, lr: 5e-05
2024-01-05 16:37:11 INFO     	 * (global step 2450: loss: 0.6030596643686295, lr: 5e-05
2024-01-05 16:37:26 INFO     	 * (global step 2500: loss: 0.7331376671791077, lr: 5e-05
2024-01-05 16:37:41 INFO     	 * (global step 2550: loss: 0.5679949596524239, lr: 5e-05
2024-01-05 16:37:56 INFO     	 * (global step 2600: loss: 0.6219414174556732, lr: 5e-05
2024-01-05 16:38:11 INFO     	 * (global step 2650: loss: 0.7452861070632935, lr: 5e-05
2024-01-05 16:38:26 INFO     	 * (global step 2700: loss: 0.5476554483175278, lr: 5e-05
2024-01-05 16:38:40 INFO     	 * (global step 2750: loss: 0.6060774028301239, lr: 5e-05
2024-01-05 16:38:54 INFO     	 * (global step 2800: loss: 0.6290666311979294, lr: 5e-05
2024-01-05 16:39:09 INFO     	 * (global step 2850: loss: 0.7195910513401031, lr: 5e-05
2024-01-05 16:39:24 INFO     	 * (global step 2900: loss: 0.7823194116353989, lr: 5e-05
2024-01-05 16:39:36 INFO     	 * (global step 2950: loss: 0.7296600267291069, lr: 5e-05
2024-01-05 16:39:48 INFO     	 * (global step 3000: loss: 0.6369543671607971, lr: 5e-05
2024-01-05 16:40:00 INFO     	 * (global step 3050: loss: 0.594692274928093, lr: 5e-05
2024-01-05 16:40:16 INFO     	 * (global step 3100: loss: 0.5493701174855232, lr: 5e-05
2024-01-05 16:40:31 INFO     	 * (global step 3150: loss: 0.6956853121519089, lr: 5e-05
2024-01-05 16:40:46 INFO     	 * (global step 3200: loss: 0.546018086373806, lr: 5e-05
2024-01-05 16:41:01 INFO     	 * (global step 3250: loss: 0.7384836450219154, lr: 5e-05
2024-01-05 16:41:16 INFO     	 * (global step 3300: loss: 0.5285576358437538, lr: 5e-05
2024-01-05 16:41:31 INFO     	 * (global step 3350: loss: 0.6356758028268814, lr: 5e-05
2024-01-05 16:41:47 INFO     	 * (global step 3400: loss: 0.7273606508970261, lr: 5e-05
2024-01-05 16:42:02 INFO     	 * (global step 3450: loss: 0.6776612401008606, lr: 5e-05
2024-01-05 16:42:17 INFO     	 * (global step 3500: loss: 0.6288524344563484, lr: 5e-05
2024-01-05 16:42:33 INFO     	 * (global step 3550: loss: 0.5964941903948784, lr: 5e-05
2024-01-05 16:42:48 INFO     	 * (global step 3600: loss: 0.6519294083118439, lr: 5e-05
2024-01-05 16:43:03 INFO     	 * (global step 3650: loss: 0.7636253237724304, lr: 5e-05
2024-01-05 16:43:16 INFO     	 * (global step 3700: loss: 0.9449896961450577, lr: 5e-05
2024-01-05 16:43:27 INFO     	 * (global step 3750: loss: 0.6208747550845146, lr: 5e-05
2024-01-05 16:43:39 INFO     	 * (global step 3800: loss: 0.5737085565924644, lr: 5e-05
2024-01-05 16:43:53 INFO     	 * (global step 3850: loss: 0.8724498301744461, lr: 5e-05
2024-01-05 16:44:07 INFO     	 * (global step 3900: loss: 0.5949727147817612, lr: 5e-05
2024-01-05 16:44:21 INFO     	 * (global step 3950: loss: 0.46728190034627914, lr: 5e-05
2024-01-05 16:44:35 INFO     	 * (global step 4000: loss: 0.5433912053704262, lr: 5e-05
2024-01-05 16:44:49 INFO     [epoch 1/15] average loss: 0.649, lr: 5e-05
2024-01-05 16:44:49 INFO     saving model related files
2024-01-05 16:44:49 INFO     saving model
2024-01-05 16:44:49 INFO     saving tokenizer
2024-01-05 16:44:49 INFO     saving optimizer
2024-01-05 16:44:51 INFO     remove old optimizer files
2024-01-05 16:44:52 INFO     	 * (global step 4050: loss: 0.46088119223713875, lr: 5e-05
2024-01-05 16:45:06 INFO     	 * (global step 4100: loss: 0.5883832946419716, lr: 5e-05
2024-01-05 16:45:20 INFO     	 * (global step 4150: loss: 0.5952820628881454, lr: 5e-05
2024-01-05 16:45:34 INFO     	 * (global step 4200: loss: 0.8200926184654236, lr: 5e-05
2024-01-05 16:45:48 INFO     	 * (global step 4250: loss: 0.6265074834227562, lr: 5e-05
2024-01-05 16:46:01 INFO     	 * (global step 4300: loss: 0.45802123844623566, lr: 5e-05
2024-01-05 16:46:12 INFO     	 * (global step 4350: loss: 0.6519149094820023, lr: 5e-05
2024-01-05 16:46:24 INFO     	 * (global step 4400: loss: 0.4869554936885834, lr: 5e-05
2024-01-05 16:46:38 INFO     	 * (global step 4450: loss: 0.5146061778068542, lr: 5e-05
2024-01-05 16:46:52 INFO     	 * (global step 4500: loss: 0.6014266535639763, lr: 5e-05
2024-01-05 16:47:07 INFO     	 * (global step 4550: loss: 0.5808875784277916, lr: 5e-05
2024-01-05 16:47:22 INFO     	 * (global step 4600: loss: 0.5335203930735588, lr: 5e-05
2024-01-05 16:47:36 INFO     	 * (global step 4650: loss: 0.8149071931838989, lr: 5e-05
2024-01-05 16:47:51 INFO     	 * (global step 4700: loss: 0.6368646919727325, lr: 5e-05
2024-01-05 16:48:05 INFO     	 * (global step 4750: loss: 0.5615806430578232, lr: 5e-05
2024-01-05 16:48:19 INFO     	 * (global step 4800: loss: 0.5962616503238678, lr: 5e-05
2024-01-05 16:48:33 INFO     	 * (global step 4850: loss: 0.6198903918266296, lr: 5e-05
2024-01-05 16:48:48 INFO     	 * (global step 4900: loss: 0.515105664730072, lr: 5e-05
2024-01-05 16:49:02 INFO     	 * (global step 4950: loss: 0.48833467066287994, lr: 5e-05
2024-01-05 16:49:16 INFO     	 * (global step 5000: loss: 0.5380774512887001, lr: 5e-05
2024-01-05 16:49:30 INFO     	 * (global step 5050: loss: 0.7409514486789703, lr: 5e-05
2024-01-05 16:49:42 INFO     	 * (global step 5100: loss: 0.5544579476118088, lr: 5e-05
2024-01-05 16:49:53 INFO     	 * (global step 5150: loss: 0.656633049249649, lr: 5e-05
2024-01-05 16:50:07 INFO     	 * (global step 5200: loss: 0.5846219807863235, lr: 5e-05
2024-01-05 16:50:23 INFO     	 * (global step 5250: loss: 0.5705863386392593, lr: 5e-05
2024-01-05 16:50:38 INFO     	 * (global step 5300: loss: 0.5374017357826233, lr: 5e-05
2024-01-05 16:50:53 INFO     	 * (global step 5350: loss: 0.6392663568258286, lr: 5e-05
2024-01-05 16:51:09 INFO     	 * (global step 5400: loss: 0.6311511918902397, lr: 5e-05
2024-01-05 16:51:24 INFO     	 * (global step 5450: loss: 0.5647475272417068, lr: 5e-05
2024-01-05 16:51:38 INFO     	 * (global step 5500: loss: 0.5222877115011215, lr: 5e-05
2024-01-05 16:51:53 INFO     	 * (global step 5550: loss: 0.6356829479336739, lr: 5e-05
2024-01-05 16:52:08 INFO     	 * (global step 5600: loss: 0.6626552864909172, lr: 5e-05
2024-01-05 16:52:23 INFO     	 * (global step 5650: loss: 0.5433778166770935, lr: 5e-05
2024-01-05 16:52:38 INFO     	 * (global step 5700: loss: 0.6358453035354614, lr: 5e-05
2024-01-05 16:52:52 INFO     	 * (global step 5750: loss: 0.5996612757444382, lr: 5e-05
2024-01-05 16:53:06 INFO     	 * (global step 5800: loss: 0.6132113113999367, lr: 5e-05
2024-01-05 16:53:17 INFO     	 * (global step 5850: loss: 0.7014027386903763, lr: 5e-05
2024-01-05 16:53:28 INFO     	 * (global step 5900: loss: 0.5967185944318771, lr: 5e-05
2024-01-05 16:53:42 INFO     	 * (global step 5950: loss: 0.5329428091645241, lr: 5e-05
2024-01-05 16:53:56 INFO     	 * (global step 6000: loss: 0.5410545133054256, lr: 5e-05
2024-01-05 16:54:11 INFO     	 * (global step 6050: loss: 0.7441586256027222, lr: 5e-05
2024-01-05 16:54:17 INFO     [epoch 2/15] average loss: 0.613, lr: 5e-05
2024-01-05 16:54:17 INFO     saving model related files
2024-01-05 16:54:17 INFO     saving model
2024-01-05 16:54:18 INFO     saving tokenizer
2024-01-05 16:54:18 INFO     saving optimizer
2024-01-05 16:54:20 INFO     remove old optimizer files
2024-01-05 16:54:28 INFO     	 * (global step 6100: loss: 0.6218877583742142, lr: 5e-05
2024-01-05 16:54:42 INFO     	 * (global step 6150: loss: 0.5763398557901382, lr: 5e-05
2024-01-05 16:54:56 INFO     	 * (global step 6200: loss: 0.6379717662930489, lr: 5e-05
2024-01-05 16:55:11 INFO     	 * (global step 6250: loss: 0.6541084200143814, lr: 5e-05
2024-01-05 16:55:25 INFO     	 * (global step 6300: loss: 0.49488699436187744, lr: 5e-05
2024-01-05 16:55:39 INFO     	 * (global step 6350: loss: 0.7998945713043213, lr: 5e-05
2024-01-05 16:55:53 INFO     	 * (global step 6400: loss: 0.6304809153079987, lr: 5e-05
2024-01-05 16:56:04 INFO     	 * (global step 6450: loss: 0.7122280895709991, lr: 5e-05
2024-01-05 16:56:16 INFO     	 * (global step 6500: loss: 0.6272619813680649, lr: 5e-05
2024-01-05 16:56:29 INFO     	 * (global step 6550: loss: 0.4856308326125145, lr: 5e-05
2024-01-05 16:56:45 INFO     	 * (global step 6600: loss: 0.5010330080986023, lr: 5e-05
2024-01-05 16:57:00 INFO     	 * (global step 6650: loss: 0.46497008204460144, lr: 5e-05
2024-01-05 16:57:15 INFO     	 * (global step 6700: loss: 0.5598894581198692, lr: 5e-05
2024-01-05 16:57:29 INFO     	 * (global step 6750: loss: 0.5743438377976418, lr: 5e-05
2024-01-05 16:57:44 INFO     	 * (global step 6800: loss: 0.6616055890917778, lr: 5e-05
2024-01-05 16:57:58 INFO     	 * (global step 6850: loss: 0.5218518674373627, lr: 5e-05
2024-01-05 16:58:12 INFO     	 * (global step 6900: loss: 0.5212390869855881, lr: 5e-05
2024-01-05 16:58:27 INFO     	 * (global step 6950: loss: 0.5328209474682808, lr: 5e-05
2024-01-05 16:58:41 INFO     	 * (global step 7000: loss: 0.435153491795063, lr: 5e-05
2024-01-05 16:58:57 INFO     	 * (global step 7050: loss: 0.6832427158951759, lr: 5e-05
2024-01-05 16:59:12 INFO     	 * (global step 7100: loss: 0.6048837974667549, lr: 5e-05
2024-01-05 16:59:26 INFO     	 * (global step 7150: loss: 0.5163929164409637, lr: 5e-05
2024-01-05 16:59:39 INFO     	 * (global step 7200: loss: 0.6065421998500824, lr: 5e-05
2024-01-05 16:59:50 INFO     	 * (global step 7250: loss: 0.6047801673412323, lr: 5e-05
2024-01-05 17:00:02 INFO     	 * (global step 7300: loss: 0.5213684663176537, lr: 5e-05
2024-01-05 17:00:16 INFO     	 * (global step 7350: loss: 0.5975173935294151, lr: 5e-05
2024-01-05 17:00:30 INFO     	 * (global step 7400: loss: 0.516302801668644, lr: 5e-05
2024-01-05 17:00:44 INFO     	 * (global step 7450: loss: 0.5236732587218285, lr: 5e-05
2024-01-05 17:00:58 INFO     	 * (global step 7500: loss: 0.6193896606564522, lr: 5e-05
2024-01-05 17:01:12 INFO     	 * (global step 7550: loss: 0.706953763961792, lr: 5e-05
2024-01-05 17:01:26 INFO     	 * (global step 7600: loss: 0.5242600440979004, lr: 5e-05
2024-01-05 17:01:40 INFO     	 * (global step 7650: loss: 0.5692737624049187, lr: 5e-05
2024-01-05 17:01:54 INFO     	 * (global step 7700: loss: 0.48906778544187546, lr: 5e-05
2024-01-05 17:02:08 INFO     	 * (global step 7750: loss: 0.47431453317403793, lr: 5e-05
2024-01-05 17:02:23 INFO     	 * (global step 7800: loss: 0.5315188467502594, lr: 5e-05
2024-01-05 17:02:36 INFO     	 * (global step 7850: loss: 0.6419678330421448, lr: 5e-05
2024-01-05 17:02:48 INFO     	 * (global step 7900: loss: 0.510166697204113, lr: 5e-05
2024-01-05 17:02:59 INFO     	 * (global step 7950: loss: 0.6362283602356911, lr: 5e-05
2024-01-05 17:03:13 INFO     	 * (global step 8000: loss: 0.6043230593204498, lr: 5e-05
2024-01-05 17:03:28 INFO     	 * (global step 8050: loss: 0.6638045907020569, lr: 5e-05
2024-01-05 17:03:43 INFO     [epoch 3/15] average loss: 0.59, lr: 5e-05
2024-01-05 17:03:43 INFO     saving model related files
2024-01-05 17:03:43 INFO     saving model
2024-01-05 17:03:43 INFO     saving tokenizer
2024-01-05 17:03:44 INFO     saving optimizer
2024-01-05 17:03:45 INFO     remove old optimizer files
2024-01-05 17:03:47 INFO     	 * (global step 8100: loss: 0.6232620179653168, lr: 5e-05
2024-01-05 17:04:02 INFO     	 * (global step 8150: loss: 0.6177085340023041, lr: 5e-05
2024-01-05 17:04:17 INFO     	 * (global step 8200: loss: 0.4246470481157303, lr: 5e-05
2024-01-05 17:04:32 INFO     	 * (global step 8250: loss: 0.524510957300663, lr: 5e-05
2024-01-05 17:04:47 INFO     	 * (global step 8300: loss: 0.51724524050951, lr: 5e-05
2024-01-05 17:05:02 INFO     	 * (global step 8350: loss: 0.7287378311157227, lr: 5e-05
2024-01-05 17:05:17 INFO     	 * (global step 8400: loss: 0.6151045486330986, lr: 5e-05
2024-01-05 17:05:32 INFO     	 * (global step 8450: loss: 0.5654485523700714, lr: 5e-05
2024-01-05 17:05:48 INFO     	 * (global step 8500: loss: 0.6520281881093979, lr: 5e-05
2024-01-05 17:06:04 INFO     	 * (global step 8550: loss: 0.4243975132703781, lr: 5e-05
2024-01-05 17:06:19 INFO     	 * (global step 8600: loss: 0.41564276069402695, lr: 5e-05
2024-01-05 17:06:30 INFO     	 * (global step 8650: loss: 0.5498397052288055, lr: 5e-05
2024-01-05 17:06:42 INFO     	 * (global step 8700: loss: 0.7100074589252472, lr: 5e-05
2024-01-05 17:06:55 INFO     	 * (global step 8750: loss: 0.5010459050536156, lr: 5e-05
2024-01-05 17:07:09 INFO     	 * (global step 8800: loss: 0.6550595536828041, lr: 5e-05
2024-01-05 17:07:24 INFO     	 * (global step 8850: loss: 0.6281488686800003, lr: 5e-05
2024-01-05 17:07:39 INFO     	 * (global step 8900: loss: 0.5527530536055565, lr: 5e-05
2024-01-05 17:07:53 INFO     	 * (global step 8950: loss: 0.5842651724815369, lr: 5e-05
2024-01-05 17:08:08 INFO     	 * (global step 9000: loss: 0.5590618699789047, lr: 5e-05
2024-01-05 17:08:23 INFO     	 * (global step 9050: loss: 0.5390918105840683, lr: 5e-05
2024-01-05 17:08:38 INFO     	 * (global step 9100: loss: 0.5032876878976822, lr: 5e-05
2024-01-05 17:08:53 INFO     	 * (global step 9150: loss: 0.35514408349990845, lr: 5e-05
2024-01-05 17:09:07 INFO     	 * (global step 9200: loss: 0.7019302397966385, lr: 5e-05
2024-01-05 17:09:21 INFO     	 * (global step 9250: loss: 0.4671875834465027, lr: 5e-05
2024-01-05 17:09:36 INFO     	 * (global step 9300: loss: 0.5589302405714989, lr: 5e-05
2024-01-05 17:09:51 INFO     	 * (global step 9350: loss: 0.5889337733387947, lr: 5e-05
2024-01-05 17:10:03 INFO     	 * (global step 9400: loss: 0.7247853353619576, lr: 5e-05
2024-01-05 17:10:15 INFO     	 * (global step 9450: loss: 0.5807426869869232, lr: 5e-05
2024-01-05 17:10:27 INFO     	 * (global step 9500: loss: 0.5442078039050102, lr: 5e-05
2024-01-05 17:10:42 INFO     	 * (global step 9550: loss: 0.5547611266374588, lr: 5e-05
2024-01-05 17:10:56 INFO     	 * (global step 9600: loss: 0.4682481214404106, lr: 5e-05
2024-01-05 17:11:10 INFO     	 * (global step 9650: loss: 0.48092224448919296, lr: 5e-05
2024-01-05 17:11:23 INFO     	 * (global step 9700: loss: 0.5899596810340881, lr: 5e-05
2024-01-05 17:11:37 INFO     	 * (global step 9750: loss: 0.5030924901366234, lr: 5e-05
2024-01-05 17:11:51 INFO     	 * (global step 9800: loss: 0.5874130502343178, lr: 5e-05
2024-01-05 17:12:05 INFO     	 * (global step 9850: loss: 0.4291035383939743, lr: 5e-05
2024-01-05 17:12:19 INFO     	 * (global step 9900: loss: 0.5209133699536324, lr: 5e-05
2024-01-05 17:12:32 INFO     	 * (global step 9950: loss: 0.5146957412362099, lr: 5e-05
2024-01-05 17:12:46 INFO     	 * (global step 10000: loss: 0.4942067041993141, lr: 5e-05
2024-01-05 17:12:57 INFO     	 * (global step 10050: loss: 0.5716823264956474, lr: 5e-05
2024-01-05 17:13:09 INFO     	 * (global step 10100: loss: 0.49755043536424637, lr: 5e-05
2024-01-05 17:13:15 INFO     [epoch 4/15] average loss: 0.574, lr: 5e-05
2024-01-05 17:13:15 INFO     saving model related files
2024-01-05 17:13:15 INFO     saving model
2024-01-05 17:13:16 INFO     saving tokenizer
2024-01-05 17:13:16 INFO     saving optimizer
2024-01-05 17:13:17 INFO     remove old optimizer files
2024-01-05 17:13:26 INFO     	 * (global step 10150: loss: 0.44564832001924515, lr: 5e-05
2024-01-05 17:13:41 INFO     	 * (global step 10200: loss: 0.503976546227932, lr: 5e-05
2024-01-05 17:13:57 INFO     	 * (global step 10250: loss: 0.43054844439029694, lr: 5e-05
2024-01-05 17:14:12 INFO     	 * (global step 10300: loss: 0.49593163281679153, lr: 5e-05
2024-01-05 17:14:26 INFO     	 * (global step 10350: loss: 0.653833232820034, lr: 5e-05
2024-01-05 17:14:42 INFO     	 * (global step 10400: loss: 0.5750895440578461, lr: 5e-05
2024-01-05 17:14:57 INFO     	 * (global step 10450: loss: 0.4785066992044449, lr: 5e-05
2024-01-05 17:15:12 INFO     	 * (global step 10500: loss: 0.5268029272556305, lr: 5e-05
2024-01-05 17:15:26 INFO     	 * (global step 10550: loss: 0.5192427709698677, lr: 5e-05
2024-01-05 17:15:42 INFO     	 * (global step 10600: loss: 0.5693782269954681, lr: 5e-05
2024-01-05 17:15:56 INFO     	 * (global step 10650: loss: 0.6229522675275803, lr: 5e-05
2024-01-05 17:16:12 INFO     	 * (global step 10700: loss: 0.5297592207789421, lr: 5e-05
2024-01-05 17:16:24 INFO     	 * (global step 10750: loss: 0.4118359386920929, lr: 5e-05
2024-01-05 17:16:36 INFO     	 * (global step 10800: loss: 0.5673370212316513, lr: 5e-05
2024-01-05 17:16:48 INFO     	 * (global step 10850: loss: 0.542991153895855, lr: 5e-05
2024-01-05 17:17:03 INFO     	 * (global step 10900: loss: 0.6412922739982605, lr: 5e-05
2024-01-05 17:17:18 INFO     	 * (global step 10950: loss: 0.5749920159578323, lr: 5e-05
2024-01-05 17:17:33 INFO     	 * (global step 11000: loss: 0.49513351917266846, lr: 5e-05
2024-01-05 17:17:48 INFO     	 * (global step 11050: loss: 0.5275279432535172, lr: 5e-05
2024-01-05 17:18:03 INFO     	 * (global step 11100: loss: 0.5223701596260071, lr: 5e-05
2024-01-05 17:18:18 INFO     	 * (global step 11150: loss: 0.501884363591671, lr: 5e-05
2024-01-05 17:18:33 INFO     	 * (global step 11200: loss: 0.5176438987255096, lr: 5e-05
2024-01-05 17:18:48 INFO     	 * (global step 11250: loss: 0.687283992767334, lr: 5e-05
2024-01-05 17:19:04 INFO     	 * (global step 11300: loss: 0.5218319520354271, lr: 5e-05
2024-01-05 17:19:19 INFO     	 * (global step 11350: loss: 0.5056614577770233, lr: 5e-05
2024-01-05 17:19:34 INFO     	 * (global step 11400: loss: 0.5320426225662231, lr: 5e-05
2024-01-05 17:19:47 INFO     	 * (global step 11450: loss: 0.4070732370018959, lr: 5e-05
2024-01-05 17:20:01 INFO     	 * (global step 11500: loss: 0.5191408917307854, lr: 5e-05
2024-01-05 17:20:12 INFO     	 * (global step 11550: loss: 0.5881900787353516, lr: 5e-05
2024-01-05 17:20:24 INFO     	 * (global step 11600: loss: 0.4889931157231331, lr: 5e-05
2024-01-05 17:20:38 INFO     	 * (global step 11650: loss: 0.335394985973835, lr: 5e-05
2024-01-05 17:20:52 INFO     	 * (global step 11700: loss: 0.5078701227903366, lr: 5e-05
2024-01-05 17:21:07 INFO     	 * (global step 11750: loss: 0.6092711389064789, lr: 5e-05
2024-01-05 17:21:21 INFO     	 * (global step 11800: loss: 0.5973800718784332, lr: 5e-05
2024-01-05 17:21:36 INFO     	 * (global step 11850: loss: 0.617163609713316, lr: 5e-05
2024-01-05 17:21:51 INFO     	 * (global step 11900: loss: 0.44471127539873123, lr: 5e-05
2024-01-05 17:22:07 INFO     	 * (global step 11950: loss: 0.45790495723485947, lr: 5e-05
2024-01-05 17:22:22 INFO     	 * (global step 12000: loss: 0.4564303681254387, lr: 5e-05
2024-01-05 17:22:38 INFO     	 * (global step 12050: loss: 0.4400981366634369, lr: 5e-05
2024-01-05 17:22:52 INFO     	 * (global step 12100: loss: 0.5254119858145714, lr: 5e-05
2024-01-05 17:23:05 INFO     [epoch 5/15] average loss: 0.561, lr: 5e-05
2024-01-05 17:23:05 INFO     saving model related files
2024-01-05 17:23:05 INFO     saving model
2024-01-05 17:23:06 INFO     saving tokenizer
2024-01-05 17:23:06 INFO     saving optimizer
2024-01-05 17:23:08 INFO     remove old optimizer files
2024-01-05 17:23:10 INFO     	 * (global step 12150: loss: 0.5068404898047447, lr: 5e-05
2024-01-05 17:23:24 INFO     	 * (global step 12200: loss: 0.5357495695352554, lr: 5e-05
2024-01-05 17:23:38 INFO     	 * (global step 12250: loss: 0.6559005677700043, lr: 5e-05
2024-01-05 17:23:49 INFO     	 * (global step 12300: loss: 0.649157777428627, lr: 5e-05
2024-01-05 17:24:01 INFO     	 * (global step 12350: loss: 0.47106895223259926, lr: 5e-05
2024-01-05 17:24:15 INFO     	 * (global step 12400: loss: 0.5785598680377007, lr: 5e-05
2024-01-05 17:24:30 INFO     	 * (global step 12450: loss: 0.7053924798965454, lr: 5e-05
2024-01-05 17:24:45 INFO     	 * (global step 12500: loss: 0.42895985022187233, lr: 5e-05
2024-01-05 17:24:59 INFO     	 * (global step 12550: loss: 0.5375899225473404, lr: 5e-05
2024-01-05 17:25:13 INFO     	 * (global step 12600: loss: 0.6656161397695541, lr: 5e-05
2024-01-05 17:25:28 INFO     	 * (global step 12650: loss: 0.6518242806196213, lr: 5e-05
2024-01-05 17:25:43 INFO     	 * (global step 12700: loss: 0.5939182788133621, lr: 5e-05
2024-01-05 17:25:56 INFO     	 * (global step 12750: loss: 0.5751826018095016, lr: 5e-05
2024-01-05 17:26:11 INFO     	 * (global step 12800: loss: 0.8242700397968292, lr: 5e-05
2024-01-05 17:26:25 INFO     	 * (global step 12850: loss: 0.5723149254918098, lr: 5e-05
2024-01-05 17:26:40 INFO     	 * (global step 12900: loss: 0.5638207793235779, lr: 5e-05
2024-01-05 17:26:52 INFO     	 * (global step 12950: loss: 0.560040608048439, lr: 5e-05
2024-01-05 17:27:04 INFO     	 * (global step 13000: loss: 0.4722724184393883, lr: 5e-05
2024-01-05 17:27:16 INFO     	 * (global step 13050: loss: 0.5525238662958145, lr: 5e-05
2024-01-05 17:27:30 INFO     	 * (global step 13100: loss: 0.5438164919614792, lr: 5e-05
2024-01-05 17:27:44 INFO     	 * (global step 13150: loss: 0.6028380766510963, lr: 5e-05
2024-01-05 17:27:58 INFO     	 * (global step 13200: loss: 0.49883243441581726, lr: 5e-05
2024-01-05 17:28:11 INFO     	 * (global step 13250: loss: 0.6048965081572533, lr: 5e-05
2024-01-05 17:28:25 INFO     	 * (global step 13300: loss: 0.5580820590257645, lr: 5e-05
2024-01-05 17:28:39 INFO     	 * (global step 13350: loss: 0.6987747922539711, lr: 5e-05
2024-01-05 17:28:53 INFO     	 * (global step 13400: loss: 0.5896581709384918, lr: 5e-05
2024-01-05 17:29:07 INFO     	 * (global step 13450: loss: 0.5946943610906601, lr: 5e-05
2024-01-05 17:29:20 INFO     	 * (global step 13500: loss: 0.5631847307085991, lr: 5e-05
2024-01-05 17:29:31 INFO     	 * (global step 13550: loss: 0.4597732722759247, lr: 5e-05
2024-01-05 17:29:43 INFO     	 * (global step 13600: loss: 0.4189523681998253, lr: 5e-05
2024-01-05 17:29:58 INFO     	 * (global step 13650: loss: 0.5318679139018059, lr: 5e-05
2024-01-05 17:30:13 INFO     	 * (global step 13700: loss: 0.6411689594388008, lr: 5e-05
2024-01-05 17:30:28 INFO     	 * (global step 13750: loss: 0.5305882692337036, lr: 5e-05
2024-01-05 17:30:43 INFO     	 * (global step 13800: loss: 0.4361007437109947, lr: 5e-05
2024-01-05 17:30:58 INFO     	 * (global step 13850: loss: 0.5908961296081543, lr: 5e-05
2024-01-05 17:31:13 INFO     	 * (global step 13900: loss: 0.5012756437063217, lr: 5e-05
2024-01-05 17:31:27 INFO     	 * (global step 13950: loss: 0.533865325152874, lr: 5e-05
2024-01-05 17:31:42 INFO     	 * (global step 14000: loss: 0.5828364193439484, lr: 5e-05
2024-01-05 17:31:56 INFO     	 * (global step 14050: loss: 0.6213707253336906, lr: 5e-05
2024-01-05 17:32:10 INFO     	 * (global step 14100: loss: 0.5626535639166832, lr: 5e-05
2024-01-05 17:32:23 INFO     	 * (global step 14150: loss: 0.5792265012860298, lr: 5e-05
2024-01-05 17:32:27 INFO     [epoch 6/15] average loss: 0.55, lr: 5e-05
2024-01-05 17:32:27 INFO     saving model related files
2024-01-05 17:32:27 INFO     saving model
2024-01-05 17:32:28 INFO     saving tokenizer
2024-01-05 17:32:28 INFO     saving optimizer
2024-01-05 17:32:29 INFO     remove old optimizer files
2024-01-05 17:32:36 INFO     	 * (global step 14200: loss: 0.5344368442893028, lr: 5e-05
2024-01-05 17:32:48 INFO     	 * (global step 14250: loss: 0.5642394572496414, lr: 5e-05
2024-01-05 17:33:03 INFO     	 * (global step 14300: loss: 0.5555459782481194, lr: 5e-05
2024-01-05 17:33:17 INFO     	 * (global step 14350: loss: 0.632681094110012, lr: 5e-05
2024-01-05 17:33:31 INFO     	 * (global step 14400: loss: 0.49595674127340317, lr: 5e-05
2024-01-05 17:33:46 INFO     	 * (global step 14450: loss: 0.5849487483501434, lr: 5e-05
2024-01-05 17:34:01 INFO     	 * (global step 14500: loss: 0.5514007359743118, lr: 5e-05
2024-01-05 17:34:15 INFO     	 * (global step 14550: loss: 0.4994504302740097, lr: 5e-05
2024-01-05 17:34:29 INFO     	 * (global step 14600: loss: 0.465929064899683, lr: 5e-05
2024-01-05 17:34:44 INFO     	 * (global step 14650: loss: 0.4974125176668167, lr: 5e-05
2024-01-05 17:34:58 INFO     	 * (global step 14700: loss: 0.5133092626929283, lr: 5e-05
2024-01-05 17:35:13 INFO     	 * (global step 14750: loss: 0.39549680054187775, lr: 5e-05
2024-01-05 17:35:27 INFO     	 * (global step 14800: loss: 0.49810267984867096, lr: 5e-05
2024-01-05 17:35:40 INFO     	 * (global step 14850: loss: 0.4990215599536896, lr: 5e-05
2024-01-05 17:35:51 INFO     	 * (global step 14900: loss: 0.6242067441344261, lr: 5e-05
2024-01-05 17:36:02 INFO     	 * (global step 14950: loss: 0.5966302901506424, lr: 5e-05
2024-01-05 17:36:16 INFO     	 * (global step 15000: loss: 0.616851694881916, lr: 5e-05
2024-01-05 17:36:29 INFO     	 * (global step 15050: loss: 0.5198159143328667, lr: 5e-05
2024-01-05 17:36:43 INFO     	 * (global step 15100: loss: 0.5804509446024895, lr: 5e-05
2024-01-05 17:36:57 INFO     	 * (global step 15150: loss: 0.8575648814439774, lr: 5e-05
2024-01-05 17:37:11 INFO     	 * (global step 15200: loss: 0.5521436110138893, lr: 5e-05
2024-01-05 17:37:25 INFO     	 * (global step 15250: loss: 0.4405636563897133, lr: 5e-05
2024-01-05 17:37:39 INFO     	 * (global step 15300: loss: 0.579118549823761, lr: 5e-05
2024-01-05 17:37:53 INFO     	 * (global step 15350: loss: 0.5518776699900627, lr: 5e-05
2024-01-05 17:38:07 INFO     	 * (global step 15400: loss: 0.4687693640589714, lr: 5e-05
2024-01-05 17:38:19 INFO     	 * (global step 15450: loss: 0.6268031224608421, lr: 5e-05
2024-01-05 17:38:30 INFO     	 * (global step 15500: loss: 0.5686560124158859, lr: 5e-05
2024-01-05 17:38:43 INFO     	 * (global step 15550: loss: 0.48154035210609436, lr: 5e-05
2024-01-05 17:38:57 INFO     	 * (global step 15600: loss: 0.5493120886385441, lr: 5e-05
2024-01-05 17:39:12 INFO     	 * (global step 15650: loss: 0.6336802244186401, lr: 5e-05
2024-01-05 17:39:26 INFO     	 * (global step 15700: loss: 0.6124756410717964, lr: 5e-05
2024-01-05 17:39:41 INFO     	 * (global step 15750: loss: 0.4567703753709793, lr: 5e-05
2024-01-05 17:39:56 INFO     	 * (global step 15800: loss: 0.49711647629737854, lr: 5e-05
2024-01-05 17:40:10 INFO     	 * (global step 15850: loss: 0.6824605017900467, lr: 5e-05
2024-01-05 17:40:25 INFO     	 * (global step 15900: loss: 0.44706766307353973, lr: 5e-05
2024-01-05 17:40:39 INFO     	 * (global step 15950: loss: 0.47531720995903015, lr: 5e-05
2024-01-05 17:40:54 INFO     	 * (global step 16000: loss: 0.5765651911497116, lr: 5e-05
2024-01-05 17:41:08 INFO     	 * (global step 16050: loss: 0.5534645915031433, lr: 5e-05
2024-01-05 17:41:22 INFO     	 * (global step 16100: loss: 0.5119264498353004, lr: 5e-05
2024-01-05 17:41:33 INFO     	 * (global step 16150: loss: 0.6435621157288551, lr: 5e-05
2024-01-05 17:41:43 INFO     [epoch 7/15] average loss: 0.541, lr: 5e-05
2024-01-05 17:41:43 INFO     saving model related files
2024-01-05 17:41:43 INFO     saving model
2024-01-05 17:41:44 INFO     saving tokenizer
2024-01-05 17:41:44 INFO     saving optimizer
2024-01-05 17:41:45 INFO     remove old optimizer files
2024-01-05 17:41:47 INFO     	 * (global step 16200: loss: 0.5465739220380783, lr: 5e-05
2024-01-05 17:42:01 INFO     	 * (global step 16250: loss: 0.49447036534547806, lr: 5e-05
2024-01-05 17:42:15 INFO     	 * (global step 16300: loss: 0.5240076035261154, lr: 5e-05
2024-01-05 17:42:29 INFO     	 * (global step 16350: loss: 0.48291415721178055, lr: 5e-05
2024-01-05 17:42:43 INFO     	 * (global step 16400: loss: 0.5534755140542984, lr: 5e-05
2024-01-05 17:42:57 INFO     	 * (global step 16450: loss: 0.6505303084850311, lr: 5e-05
2024-01-05 17:43:11 INFO     	 * (global step 16500: loss: 0.42265684902668, lr: 5e-05
2024-01-05 17:43:25 INFO     	 * (global step 16550: loss: 0.6246493011713028, lr: 5e-05
2024-01-05 17:43:39 INFO     	 * (global step 16600: loss: 0.5493596196174622, lr: 5e-05
2024-01-05 17:43:52 INFO     	 * (global step 16650: loss: 0.4142032749950886, lr: 5e-05
2024-01-05 17:44:04 INFO     	 * (global step 16700: loss: 0.5697811394929886, lr: 5e-05
2024-01-05 17:44:15 INFO     	 * (global step 16750: loss: 0.6078511029481888, lr: 5e-05
2024-01-05 17:44:27 INFO     	 * (global step 16800: loss: 0.7577462419867516, lr: 5e-05
2024-01-05 17:44:42 INFO     	 * (global step 16850: loss: 0.4399362727999687, lr: 5e-05
2024-01-05 17:44:57 INFO     	 * (global step 16900: loss: 0.5620894059538841, lr: 5e-05
2024-01-05 17:45:11 INFO     	 * (global step 16950: loss: 0.45349733904004097, lr: 5e-05
2024-01-05 17:45:26 INFO     	 * (global step 17000: loss: 0.6014223471283913, lr: 5e-05
2024-01-05 17:45:41 INFO     	 * (global step 17050: loss: 0.5005240924656391, lr: 5e-05
2024-01-05 17:45:55 INFO     	 * (global step 17100: loss: 0.47662822902202606, lr: 5e-05
2024-01-05 17:46:09 INFO     	 * (global step 17150: loss: 0.3417856767773628, lr: 5e-05
2024-01-05 17:46:24 INFO     	 * (global step 17200: loss: 0.33910978212952614, lr: 5e-05
2024-01-05 17:46:38 INFO     	 * (global step 17250: loss: 0.4500102251768112, lr: 5e-05
2024-01-05 17:46:53 INFO     	 * (global step 17300: loss: 0.5251582339406013, lr: 5e-05
2024-01-05 17:47:05 INFO     	 * (global step 17350: loss: 0.5502899810671806, lr: 5e-05
2024-01-05 17:47:16 INFO     	 * (global step 17400: loss: 0.4036504626274109, lr: 5e-05
2024-01-05 17:47:28 INFO     	 * (global step 17450: loss: 0.5359626784920692, lr: 5e-05
2024-01-05 17:47:42 INFO     	 * (global step 17500: loss: 0.5474370568990707, lr: 5e-05
2024-01-05 17:47:56 INFO     	 * (global step 17550: loss: 0.518701832741499, lr: 5e-05
2024-01-05 17:48:10 INFO     	 * (global step 17600: loss: 0.5615285709500313, lr: 5e-05
2024-01-05 17:48:24 INFO     	 * (global step 17650: loss: 0.48077288269996643, lr: 5e-05
2024-01-05 17:48:39 INFO     	 * (global step 17700: loss: 0.40827593952417374, lr: 5e-05
2024-01-05 17:48:53 INFO     	 * (global step 17750: loss: 0.46244344115257263, lr: 5e-05
2024-01-05 17:49:07 INFO     	 * (global step 17800: loss: 0.45966511964797974, lr: 5e-05
2024-01-05 17:49:21 INFO     	 * (global step 17850: loss: 0.5712070167064667, lr: 5e-05
2024-01-05 17:49:35 INFO     	 * (global step 17900: loss: 0.47534503042697906, lr: 5e-05
2024-01-05 17:49:48 INFO     	 * (global step 17950: loss: 0.5748903378844261, lr: 5e-05
2024-01-05 17:49:59 INFO     	 * (global step 18000: loss: 0.45468973368406296, lr: 5e-05
2024-01-05 17:50:11 INFO     	 * (global step 18050: loss: 0.40883680060505867, lr: 5e-05
2024-01-05 17:50:27 INFO     	 * (global step 18100: loss: 0.6017995998263359, lr: 5e-05
2024-01-05 17:50:42 INFO     	 * (global step 18150: loss: 0.4542306289076805, lr: 5e-05
2024-01-05 17:50:56 INFO     	 * (global step 18200: loss: 0.5188814103603363, lr: 5e-05
2024-01-05 17:51:01 INFO     [epoch 8/15] average loss: 0.532, lr: 5e-05
2024-01-05 17:51:01 INFO     saving model related files
2024-01-05 17:51:01 INFO     saving model
2024-01-05 17:51:02 INFO     saving tokenizer
2024-01-05 17:51:02 INFO     saving optimizer
2024-01-05 17:51:04 INFO     remove old optimizer files
2024-01-05 17:51:14 INFO     	 * (global step 18250: loss: 0.5599981620907784, lr: 5e-05
2024-01-05 17:51:29 INFO     	 * (global step 18300: loss: 0.4851861000061035, lr: 5e-05
2024-01-05 17:51:43 INFO     	 * (global step 18350: loss: 0.5620520859956741, lr: 5e-05
2024-01-05 17:51:57 INFO     	 * (global step 18400: loss: 0.5030723661184311, lr: 5e-05
2024-01-05 17:52:12 INFO     	 * (global step 18450: loss: 0.6144862473011017, lr: 5e-05
2024-01-05 17:52:27 INFO     	 * (global step 18500: loss: 0.5153355598449707, lr: 5e-05
2024-01-05 17:52:41 INFO     	 * (global step 18550: loss: 0.5005007907748222, lr: 5e-05
2024-01-05 17:52:55 INFO     	 * (global step 18600: loss: 0.5539601072669029, lr: 5e-05
2024-01-05 17:53:06 INFO     	 * (global step 18650: loss: 0.49128683656454086, lr: 5e-05
2024-01-05 17:53:18 INFO     	 * (global step 18700: loss: 0.7187148630619049, lr: 5e-05
2024-01-05 17:53:32 INFO     	 * (global step 18750: loss: 0.6831805109977722, lr: 5e-05
2024-01-05 17:53:47 INFO     	 * (global step 18800: loss: 0.5157729014754295, lr: 5e-05
2024-01-05 17:54:02 INFO     	 * (global step 18850: loss: 0.504826731979847, lr: 5e-05
2024-01-05 17:54:16 INFO     	 * (global step 18900: loss: 0.4098557233810425, lr: 5e-05
2024-01-05 17:54:31 INFO     	 * (global step 18950: loss: 0.4368209168314934, lr: 5e-05
2024-01-05 17:54:45 INFO     	 * (global step 19000: loss: 0.4945560023188591, lr: 5e-05
2024-01-05 17:54:59 INFO     	 * (global step 19050: loss: 0.5157957077026367, lr: 5e-05
2024-01-05 17:55:14 INFO     	 * (global step 19100: loss: 0.5627866461873055, lr: 5e-05
2024-01-05 17:55:27 INFO     	 * (global step 19150: loss: 0.47884541749954224, lr: 5e-05
2024-01-05 17:55:41 INFO     	 * (global step 19200: loss: 0.6287148147821426, lr: 5e-05
2024-01-05 17:55:56 INFO     	 * (global step 19250: loss: 0.4514346867799759, lr: 5e-05
2024-01-05 17:56:07 INFO     	 * (global step 19300: loss: 0.38860030844807625, lr: 5e-05
2024-01-05 17:56:18 INFO     	 * (global step 19350: loss: 0.5419337004423141, lr: 5e-05
2024-01-05 17:56:31 INFO     	 * (global step 19400: loss: 0.45823315158486366, lr: 5e-05
2024-01-05 17:56:46 INFO     	 * (global step 19450: loss: 0.4412422552704811, lr: 5e-05
2024-01-05 17:57:00 INFO     	 * (global step 19500: loss: 0.47675251215696335, lr: 5e-05
2024-01-05 17:57:14 INFO     	 * (global step 19550: loss: 0.47736433148384094, lr: 5e-05
2024-01-05 17:57:28 INFO     	 * (global step 19600: loss: 0.5228470042347908, lr: 5e-05
2024-01-05 17:57:42 INFO     	 * (global step 19650: loss: 0.46128831803798676, lr: 5e-05
2024-01-05 17:57:55 INFO     	 * (global step 19700: loss: 0.4078725650906563, lr: 5e-05
2024-01-05 17:58:09 INFO     	 * (global step 19750: loss: 0.6647704616189003, lr: 5e-05
2024-01-05 17:58:23 INFO     	 * (global step 19800: loss: 0.6812268793582916, lr: 5e-05
2024-01-05 17:58:35 INFO     	 * (global step 19850: loss: 0.5243202149868011, lr: 5e-05
2024-01-05 17:58:46 INFO     	 * (global step 19900: loss: 0.7156301736831665, lr: 5e-05
2024-01-05 17:58:59 INFO     	 * (global step 19950: loss: 0.4331349954009056, lr: 5e-05
2024-01-05 17:59:14 INFO     	 * (global step 20000: loss: 0.6095520183444023, lr: 5e-05
2024-01-05 17:59:28 INFO     	 * (global step 20050: loss: 0.5101815909147263, lr: 5e-05
2024-01-05 17:59:42 INFO     	 * (global step 20100: loss: 0.5254767537117004, lr: 5e-05
2024-01-05 17:59:57 INFO     	 * (global step 20150: loss: 0.4058639332652092, lr: 5e-05
2024-01-05 18:00:11 INFO     	 * (global step 20200: loss: 0.4595636650919914, lr: 5e-05
2024-01-05 18:00:23 INFO     [epoch 9/15] average loss: 0.524, lr: 5e-05
2024-01-05 18:00:23 INFO     saving model related files
2024-01-05 18:00:23 INFO     saving model
2024-01-05 18:00:24 INFO     saving tokenizer
2024-01-05 18:00:24 INFO     saving optimizer
2024-01-05 18:00:25 INFO     remove old optimizer files
2024-01-05 18:00:25 INFO     complete training: model ckpt was saved at small_recreated_ckpt/model_mntyya
2024-01-05 18:00:26 INFO     ## 1st RUN: Configuration 5/12 ##
2024-01-05 18:00:26 INFO     initialize model trainer
2024-01-05 18:00:26 INFO     initialize checkpoint at small_recreated_ckpt/model_woixzh
2024-01-05 18:00:26 INFO     hyperparameters
2024-01-05 18:00:26 INFO     	 * dataset_path: lmqg/qag_squad
2024-01-05 18:00:26 INFO     	 * dataset_name: default
2024-01-05 18:00:26 INFO     	 * input_types: ['paragraph']
2024-01-05 18:00:26 INFO     	 * output_types: ['questions_answers']
2024-01-05 18:00:26 INFO     	 * prefix_types: ['qag']
2024-01-05 18:00:26 INFO     	 * model: t5-small
2024-01-05 18:00:26 INFO     	 * max_length: 512
2024-01-05 18:00:26 INFO     	 * max_length_output: 256
2024-01-05 18:00:26 INFO     	 * epoch: 15
2024-01-05 18:00:26 INFO     	 * batch: 2
2024-01-05 18:00:26 INFO     	 * lr: 5e-05
2024-01-05 18:00:26 INFO     	 * fp16: False
2024-01-05 18:00:26 INFO     	 * random_seed: 1
2024-01-05 18:00:26 INFO     	 * gradient_accumulation_steps: 2
2024-01-05 18:00:26 INFO     	 * label_smoothing: 0.15
2024-01-05 18:00:26 INFO     initialize checkpoint with t5-small
2024-01-05 18:00:30 INFO     use spaCy answer extraction model: positionrank
2024-01-05 18:00:30 INFO     Model `t5-small`
2024-01-05 18:00:30 INFO     	 * Num of GPU in use: 1
2024-01-05 18:00:30 INFO     	 * Prefix: True
2024-01-05 18:00:30 INFO     	 * Language: en (ignore at the training phase)
2024-01-05 18:00:30 INFO     dataset preprocessing
2024-01-05 18:00:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featurelmqg/qag_squad/t5-small.512.256.paragraph.questions_answers.train.qag.pkl
2024-01-05 18:00:36 INFO     start model training
2024-01-05 18:00:43 INFO     	 * (global step 50: loss: 1.7728207111358643, lr: 5e-05
2024-01-05 18:00:51 INFO     	 * (global step 100: loss: 1.4806039333343506, lr: 5e-05
2024-01-05 18:00:59 INFO     	 * (global step 150: loss: 0.834697961807251, lr: 5e-05
2024-01-05 18:01:06 INFO     	 * (global step 200: loss: 1.2803834676742554, lr: 5e-05
2024-01-05 18:01:13 INFO     	 * (global step 250: loss: 0.9451367855072021, lr: 5e-05
2024-01-05 18:01:21 INFO     	 * (global step 300: loss: 0.7630171179771423, lr: 5e-05
2024-01-05 18:01:29 INFO     	 * (global step 350: loss: 0.9719663858413696, lr: 5e-05
2024-01-05 18:01:35 INFO     	 * (global step 400: loss: 0.8381746411323547, lr: 5e-05
2024-01-05 18:01:41 INFO     	 * (global step 450: loss: 0.7142868936061859, lr: 5e-05
2024-01-05 18:01:47 INFO     	 * (global step 500: loss: 0.7305144369602203, lr: 5e-05
2024-01-05 18:01:53 INFO     	 * (global step 550: loss: 0.845378190279007, lr: 5e-05
2024-01-05 18:01:59 INFO     	 * (global step 600: loss: 0.950349897146225, lr: 5e-05
2024-01-05 18:02:06 INFO     	 * (global step 650: loss: 0.7694126665592194, lr: 5e-05
2024-01-05 18:02:14 INFO     	 * (global step 700: loss: 0.9067750871181488, lr: 5e-05
2024-01-05 18:02:21 INFO     	 * (global step 750: loss: 0.9591509699821472, lr: 5e-05
2024-01-05 18:02:29 INFO     	 * (global step 800: loss: 1.0849194824695587, lr: 5e-05
2024-01-05 18:02:36 INFO     	 * (global step 850: loss: 0.9280251562595367, lr: 5e-05
2024-01-05 18:02:43 INFO     	 * (global step 900: loss: 0.6631471812725067, lr: 5e-05
2024-01-05 18:02:51 INFO     	 * (global step 950: loss: 0.590509295463562, lr: 5e-05
2024-01-05 18:02:58 INFO     	 * (global step 1000: loss: 0.7870047688484192, lr: 5e-05
2024-01-05 18:03:05 INFO     	 * (global step 1050: loss: 0.6737493872642517, lr: 5e-05
2024-01-05 18:03:12 INFO     	 * (global step 1100: loss: 0.5685000717639923, lr: 5e-05
2024-01-05 18:03:20 INFO     	 * (global step 1150: loss: 0.726594090461731, lr: 5e-05
2024-01-05 18:03:28 INFO     	 * (global step 1200: loss: 1.2113128304481506, lr: 5e-05
2024-01-05 18:03:35 INFO     	 * (global step 1250: loss: 0.7749879658222198, lr: 5e-05
2024-01-05 18:03:42 INFO     	 * (global step 1300: loss: 0.6940084397792816, lr: 5e-05
2024-01-05 18:03:50 INFO     	 * (global step 1350: loss: 0.9254187047481537, lr: 5e-05
2024-01-05 18:03:57 INFO     	 * (global step 1400: loss: 0.5173830986022949, lr: 5e-05
2024-01-05 18:04:04 INFO     	 * (global step 1450: loss: 0.5974380075931549, lr: 5e-05
2024-01-05 18:04:10 INFO     	 * (global step 1500: loss: 0.7022018730640411, lr: 5e-05
2024-01-05 18:04:16 INFO     	 * (global step 1550: loss: 0.8868816196918488, lr: 5e-05
2024-01-05 18:04:22 INFO     	 * (global step 1600: loss: 0.5598921179771423, lr: 5e-05
2024-01-05 18:04:28 INFO     	 * (global step 1650: loss: 0.5945923626422882, lr: 5e-05
2024-01-05 18:04:35 INFO     	 * (global step 1700: loss: 0.7687244415283203, lr: 5e-05
2024-01-05 18:04:43 INFO     	 * (global step 1750: loss: 0.794844776391983, lr: 5e-05
2024-01-05 18:04:51 INFO     	 * (global step 1800: loss: 0.7669158577919006, lr: 5e-05
2024-01-05 18:04:59 INFO     	 * (global step 1850: loss: 0.7317101955413818, lr: 5e-05
2024-01-05 18:05:07 INFO     	 * (global step 1900: loss: 0.4653861075639725, lr: 5e-05
2024-01-05 18:05:14 INFO     	 * (global step 1950: loss: 0.6711544692516327, lr: 5e-05
2024-01-05 18:05:22 INFO     	 * (global step 2000: loss: 1.0128650665283203, lr: 5e-05
2024-01-05 18:05:30 INFO     	 * (global step 2050: loss: 0.7001715898513794, lr: 5e-05
2024-01-05 18:05:38 INFO     	 * (global step 2100: loss: 0.5389806032180786, lr: 5e-05
2024-01-05 18:05:45 INFO     	 * (global step 2150: loss: 0.6141254901885986, lr: 5e-05
2024-01-05 18:05:53 INFO     	 * (global step 2200: loss: 0.929119735956192, lr: 5e-05
2024-01-05 18:06:01 INFO     	 * (global step 2250: loss: 0.56471386551857, lr: 5e-05
2024-01-05 18:06:09 INFO     	 * (global step 2300: loss: 1.4072681963443756, lr: 5e-05
2024-01-05 18:06:16 INFO     	 * (global step 2350: loss: 0.39802253246307373, lr: 5e-05
2024-01-05 18:06:24 INFO     	 * (global step 2400: loss: 0.5917661339044571, lr: 5e-05
2024-01-05 18:06:32 INFO     	 * (global step 2450: loss: 0.7275049984455109, lr: 5e-05
2024-01-05 18:06:39 INFO     	 * (global step 2500: loss: 0.5243711322546005, lr: 5e-05
2024-01-05 18:06:47 INFO     	 * (global step 2550: loss: 0.8317373096942902, lr: 5e-05
2024-01-05 18:06:55 INFO     	 * (global step 2600: loss: 0.4627355486154556, lr: 5e-05
2024-01-05 18:07:03 INFO     	 * (global step 2650: loss: 0.5493955761194229, lr: 5e-05
2024-01-05 18:07:10 INFO     	 * (global step 2700: loss: 0.643118292093277, lr: 5e-05
2024-01-05 18:07:16 INFO     	 * (global step 2750: loss: 0.49111802875995636, lr: 5e-05
2024-01-05 18:07:22 INFO     	 * (global step 2800: loss: 0.5104269832372665, lr: 5e-05
2024-01-05 18:07:28 INFO     	 * (global step 2850: loss: 0.6455878615379333, lr: 5e-05
2024-01-05 18:07:34 INFO     	 * (global step 2900: loss: 0.7618739902973175, lr: 5e-05
2024-01-05 18:07:41 INFO     	 * (global step 2950: loss: 0.33078615367412567, lr: 5e-05
2024-01-05 18:07:48 INFO     	 * (global step 3000: loss: 0.8618737161159515, lr: 5e-05
2024-01-05 18:07:55 INFO     	 * (global step 3050: loss: 0.7278066277503967, lr: 5e-05
2024-01-05 18:08:02 INFO     	 * (global step 3100: loss: 0.7514806687831879, lr: 5e-05
2024-01-05 18:08:10 INFO     	 * (global step 3150: loss: 0.5057901740074158, lr: 5e-05
2024-01-05 18:08:17 INFO     	 * (global step 3200: loss: 0.7809823453426361, lr: 5e-05
2024-01-05 18:08:24 INFO     	 * (global step 3250: loss: 0.49129651486873627, lr: 5e-05
2024-01-05 18:08:31 INFO     	 * (global step 3300: loss: 0.6056638360023499, lr: 5e-05
2024-01-05 18:08:39 INFO     	 * (global step 3350: loss: 0.5823533833026886, lr: 5e-05
2024-01-05 18:08:46 INFO     	 * (global step 3400: loss: 0.584520012140274, lr: 5e-05
2024-01-05 18:08:53 INFO     	 * (global step 3450: loss: 0.7049095928668976, lr: 5e-05
2024-01-05 18:09:01 INFO     	 * (global step 3500: loss: 0.6558476090431213, lr: 5e-05
2024-01-05 18:09:08 INFO     	 * (global step 3550: loss: 0.8971989154815674, lr: 5e-05
2024-01-05 18:09:15 INFO     	 * (global step 3600: loss: 0.6755787283182144, lr: 5e-05
2024-01-05 18:09:22 INFO     	 * (global step 3650: loss: 0.6755453944206238, lr: 5e-05
2024-01-05 18:09:30 INFO     	 * (global step 3700: loss: 0.3991265892982483, lr: 5e-05
2024-01-05 18:09:37 INFO     	 * (global step 3750: loss: 0.5845209956169128, lr: 5e-05
2024-01-05 18:09:43 INFO     	 * (global step 3800: loss: 0.41593407094478607, lr: 5e-05
2024-01-05 18:09:48 INFO     	 * (global step 3850: loss: 0.6116517782211304, lr: 5e-05
2024-01-05 18:09:54 INFO     	 * (global step 3900: loss: 0.820709228515625, lr: 5e-05
2024-01-05 18:10:00 INFO     	 * (global step 3950: loss: 0.6212145686149597, lr: 5e-05
2024-01-05 18:10:07 INFO     	 * (global step 4000: loss: 0.5396171659231186, lr: 5e-05
2024-01-05 18:10:14 INFO     [epoch 0/15] average loss: 0.774, lr: 5e-05
2024-01-05 18:10:14 INFO     saving model related files
2024-01-05 18:10:14 INFO     saving model
2024-01-05 18:10:15 INFO     saving tokenizer
2024-01-05 18:10:15 INFO     saving optimizer
2024-01-05 18:10:17 INFO     remove old optimizer files
2024-01-05 18:10:17 INFO     	 * (global step 4050: loss: 0.6212760806083679, lr: 5e-05
2024-01-05 18:10:25 INFO     	 * (global step 4100: loss: 0.5358265265822411, lr: 5e-05
2024-01-05 18:10:33 INFO     	 * (global step 4150: loss: 0.6276861727237701, lr: 5e-05
2024-01-05 18:10:41 INFO     	 * (global step 4200: loss: 0.49918070435523987, lr: 5e-05
2024-01-05 18:10:48 INFO     	 * (global step 4250: loss: 0.725459098815918, lr: 5e-05
2024-01-05 18:10:56 INFO     	 * (global step 4300: loss: 0.9624768644571304, lr: 5e-05
2024-01-05 18:11:04 INFO     	 * (global step 4350: loss: 0.45126648247241974, lr: 5e-05
2024-01-05 18:11:12 INFO     	 * (global step 4400: loss: 0.40210068225860596, lr: 5e-05
2024-01-05 18:11:20 INFO     	 * (global step 4450: loss: 0.8811017572879791, lr: 5e-05
2024-01-05 18:11:27 INFO     	 * (global step 4500: loss: 0.4579108953475952, lr: 5e-05
2024-01-05 18:11:35 INFO     	 * (global step 4550: loss: 0.5260932147502899, lr: 5e-05
2024-01-05 18:11:43 INFO     	 * (global step 4600: loss: 0.5844783186912537, lr: 5e-05
2024-01-05 18:11:50 INFO     	 * (global step 4650: loss: 0.8014101684093475, lr: 5e-05
2024-01-05 18:11:58 INFO     	 * (global step 4700: loss: 0.8268274962902069, lr: 5e-05
2024-01-05 18:12:06 INFO     	 * (global step 4750: loss: 0.784092366695404, lr: 5e-05
2024-01-05 18:12:14 INFO     	 * (global step 4800: loss: 0.6164823025465012, lr: 5e-05
2024-01-05 18:12:22 INFO     	 * (global step 4850: loss: 0.6800673305988312, lr: 5e-05
2024-01-05 18:12:30 INFO     	 * (global step 4900: loss: 0.6093034446239471, lr: 5e-05
2024-01-05 18:12:38 INFO     	 * (global step 4950: loss: 0.8133270144462585, lr: 5e-05
2024-01-05 18:12:44 INFO     	 * (global step 5000: loss: 0.7636942863464355, lr: 5e-05
2024-01-05 18:12:50 INFO     	 * (global step 5050: loss: 0.6965290904045105, lr: 5e-05
2024-01-05 18:12:56 INFO     	 * (global step 5100: loss: 0.5521915704011917, lr: 5e-05
2024-01-05 18:13:02 INFO     	 * (global step 5150: loss: 0.6535124480724335, lr: 5e-05
2024-01-05 18:13:08 INFO     	 * (global step 5200: loss: 0.533741295337677, lr: 5e-05
2024-01-05 18:13:16 INFO     	 * (global step 5250: loss: 0.5819418579339981, lr: 5e-05
2024-01-05 18:13:24 INFO     	 * (global step 5300: loss: 0.8350092470645905, lr: 5e-05
2024-01-05 18:13:31 INFO     	 * (global step 5350: loss: 0.652933806180954, lr: 5e-05
2024-01-05 18:13:40 INFO     	 * (global step 5400: loss: 0.5551742911338806, lr: 5e-05
2024-01-05 18:13:47 INFO     	 * (global step 5450: loss: 0.5698099732398987, lr: 5e-05
2024-01-05 18:13:55 INFO     	 * (global step 5500: loss: 0.49667710065841675, lr: 5e-05
2024-01-05 18:14:02 INFO     	 * (global step 5550: loss: 0.6869542896747589, lr: 5e-05
2024-01-05 18:14:10 INFO     	 * (global step 5600: loss: 0.7293442487716675, lr: 5e-05
2024-01-05 18:14:17 INFO     	 * (global step 5650: loss: 0.7041328549385071, lr: 5e-05
2024-01-05 18:14:25 INFO     	 * (global step 5700: loss: 0.9760475158691406, lr: 5e-05
2024-01-05 18:14:33 INFO     	 * (global step 5750: loss: 0.49803633987903595, lr: 5e-05
2024-01-05 18:14:40 INFO     	 * (global step 5800: loss: 0.6791359782218933, lr: 5e-05
2024-01-05 18:14:48 INFO     	 * (global step 5850: loss: 0.5326066017150879, lr: 5e-05
2024-01-05 18:14:56 INFO     	 * (global step 5900: loss: 0.46371152997016907, lr: 5e-05
2024-01-05 18:15:04 INFO     	 * (global step 5950: loss: 0.7941720485687256, lr: 5e-05
2024-01-05 18:15:11 INFO     	 * (global step 6000: loss: 0.5657046139240265, lr: 5e-05
2024-01-05 18:15:19 INFO     	 * (global step 6050: loss: 0.5114542543888092, lr: 5e-05
2024-01-05 18:15:26 INFO     	 * (global step 6100: loss: 0.5040881037712097, lr: 5e-05
2024-01-05 18:15:34 INFO     	 * (global step 6150: loss: 0.5752341747283936, lr: 5e-05
2024-01-05 18:15:42 INFO     	 * (global step 6200: loss: 0.6323198974132538, lr: 5e-05
2024-01-05 18:15:48 INFO     	 * (global step 6250: loss: 0.4661755710840225, lr: 5e-05
2024-01-05 18:15:54 INFO     	 * (global step 6300: loss: 0.6270259320735931, lr: 5e-05
2024-01-05 18:16:00 INFO     	 * (global step 6350: loss: 0.6606756150722504, lr: 5e-05
2024-01-05 18:16:06 INFO     	 * (global step 6400: loss: 0.5412724912166595, lr: 5e-05
2024-01-05 18:16:12 INFO     	 * (global step 6450: loss: 0.4673963934183121, lr: 5e-05
2024-01-05 18:16:20 INFO     	 * (global step 6500: loss: 0.7092888057231903, lr: 5e-05
2024-01-05 18:16:28 INFO     	 * (global step 6550: loss: 0.7437098622322083, lr: 5e-05
2024-01-05 18:16:36 INFO     	 * (global step 6600: loss: 0.6809025406837463, lr: 5e-05
2024-01-05 18:16:44 INFO     	 * (global step 6650: loss: 0.5624637305736542, lr: 5e-05
2024-01-05 18:16:52 INFO     	 * (global step 6700: loss: 0.6595516204833984, lr: 5e-05
2024-01-05 18:17:00 INFO     	 * (global step 6750: loss: 0.8111045658588409, lr: 5e-05
2024-01-05 18:17:08 INFO     	 * (global step 6800: loss: 0.7831457257270813, lr: 5e-05
2024-01-05 18:17:16 INFO     	 * (global step 6850: loss: 0.6016977727413177, lr: 5e-05
2024-01-05 18:17:24 INFO     	 * (global step 6900: loss: 0.640748918056488, lr: 5e-05
2024-01-05 18:17:32 INFO     	 * (global step 6950: loss: 0.5533333122730255, lr: 5e-05
2024-01-05 18:17:40 INFO     	 * (global step 7000: loss: 0.580885261297226, lr: 5e-05
2024-01-05 18:17:47 INFO     	 * (global step 7050: loss: 0.6707504987716675, lr: 5e-05
2024-01-05 18:17:54 INFO     	 * (global step 7100: loss: 0.46960708498954773, lr: 5e-05
2024-01-05 18:18:02 INFO     	 * (global step 7150: loss: 0.6405892074108124, lr: 5e-05
2024-01-05 18:18:10 INFO     	 * (global step 7200: loss: 0.6053761094808578, lr: 5e-05
2024-01-05 18:18:19 INFO     	 * (global step 7250: loss: 0.7904725074768066, lr: 5e-05
2024-01-05 18:18:26 INFO     	 * (global step 7300: loss: 0.7501441836357117, lr: 5e-05
2024-01-05 18:18:35 INFO     	 * (global step 7350: loss: 0.6497674286365509, lr: 5e-05
2024-01-05 18:18:43 INFO     	 * (global step 7400: loss: 0.7280196249485016, lr: 5e-05
2024-01-05 18:18:50 INFO     	 * (global step 7450: loss: 0.5839199721813202, lr: 5e-05
2024-01-05 18:18:57 INFO     	 * (global step 7500: loss: 0.5376323014497757, lr: 5e-05
2024-01-05 18:19:02 INFO     	 * (global step 7550: loss: 0.801628589630127, lr: 5e-05
2024-01-05 18:19:08 INFO     	 * (global step 7600: loss: 0.595958411693573, lr: 5e-05
2024-01-05 18:19:14 INFO     	 * (global step 7650: loss: 0.8478047847747803, lr: 5e-05
2024-01-05 18:19:21 INFO     	 * (global step 7700: loss: 0.9519955217838287, lr: 5e-05
2024-01-05 18:19:28 INFO     	 * (global step 7750: loss: 0.6134634017944336, lr: 5e-05
2024-01-05 18:19:36 INFO     	 * (global step 7800: loss: 0.6380950510501862, lr: 5e-05
2024-01-05 18:19:43 INFO     	 * (global step 7850: loss: 0.5500684678554535, lr: 5e-05
2024-01-05 18:19:50 INFO     	 * (global step 7900: loss: 0.5222214758396149, lr: 5e-05
2024-01-05 18:19:57 INFO     	 * (global step 7950: loss: 0.5258458256721497, lr: 5e-05
2024-01-05 18:20:05 INFO     	 * (global step 8000: loss: 0.5086855590343475, lr: 5e-05
2024-01-05 18:20:12 INFO     	 * (global step 8050: loss: 0.8101849853992462, lr: 5e-05
2024-01-05 18:20:19 INFO     [epoch 1/15] average loss: 0.622, lr: 5e-05
2024-01-05 18:20:19 INFO     saving model related files
2024-01-05 18:20:19 INFO     saving model
2024-01-05 18:20:20 INFO     saving tokenizer
2024-01-05 18:20:20 INFO     saving optimizer
2024-01-05 18:20:21 INFO     remove old optimizer files
2024-01-05 18:20:22 INFO     	 * (global step 8100: loss: 0.5031428188085556, lr: 5e-05
2024-01-05 18:20:30 INFO     	 * (global step 8150: loss: 0.43479932844638824, lr: 5e-05
2024-01-05 18:20:37 INFO     	 * (global step 8200: loss: 0.46873751282691956, lr: 5e-05
2024-01-05 18:20:44 INFO     	 * (global step 8250: loss: 0.3500138968229294, lr: 5e-05
2024-01-05 18:20:52 INFO     	 * (global step 8300: loss: 0.5961377918720245, lr: 5e-05
2024-01-05 18:20:59 INFO     	 * (global step 8350: loss: 0.7994186878204346, lr: 5e-05
2024-01-05 18:21:06 INFO     	 * (global step 8400: loss: 0.43882104754447937, lr: 5e-05
2024-01-05 18:21:13 INFO     	 * (global step 8450: loss: 0.5020786970853806, lr: 5e-05
2024-01-05 18:21:20 INFO     	 * (global step 8500: loss: 0.7248212397098541, lr: 5e-05
2024-01-05 18:21:26 INFO     	 * (global step 8550: loss: 0.640448123216629, lr: 5e-05
2024-01-05 18:21:32 INFO     	 * (global step 8600: loss: 0.33646489679813385, lr: 5e-05
2024-01-05 18:21:38 INFO     	 * (global step 8650: loss: 0.43023133277893066, lr: 5e-05
2024-01-05 18:21:44 INFO     	 * (global step 8700: loss: 0.6685518324375153, lr: 5e-05
2024-01-05 18:21:51 INFO     	 * (global step 8750: loss: 0.5577477812767029, lr: 5e-05
2024-01-05 18:22:00 INFO     	 * (global step 8800: loss: 0.30815258622169495, lr: 5e-05
2024-01-05 18:22:07 INFO     	 * (global step 8850: loss: 0.5744191259145737, lr: 5e-05
2024-01-05 18:22:14 INFO     	 * (global step 8900: loss: 0.454338014125824, lr: 5e-05
2024-01-05 18:22:22 INFO     	 * (global step 8950: loss: 0.374255433678627, lr: 5e-05
2024-01-05 18:22:30 INFO     	 * (global step 9000: loss: 0.5465766042470932, lr: 5e-05
2024-01-05 18:22:38 INFO     	 * (global step 9050: loss: 0.6778773963451385, lr: 5e-05
2024-01-05 18:22:46 INFO     	 * (global step 9100: loss: 0.6441956013441086, lr: 5e-05
2024-01-05 18:22:53 INFO     	 * (global step 9150: loss: 0.6076233386993408, lr: 5e-05
2024-01-05 18:23:01 INFO     	 * (global step 9200: loss: 0.6264724135398865, lr: 5e-05
2024-01-05 18:23:09 INFO     	 * (global step 9250: loss: 0.4673958271741867, lr: 5e-05
2024-01-05 18:23:16 INFO     	 * (global step 9300: loss: 0.7088388353586197, lr: 5e-05
2024-01-05 18:23:24 INFO     	 * (global step 9350: loss: 0.41339702904224396, lr: 5e-05
2024-01-05 18:23:32 INFO     	 * (global step 9400: loss: 0.5844441652297974, lr: 5e-05
2024-01-05 18:23:40 INFO     	 * (global step 9450: loss: 0.4547169506549835, lr: 5e-05
2024-01-05 18:23:48 INFO     	 * (global step 9500: loss: 0.5287989228963852, lr: 5e-05
2024-01-05 18:23:56 INFO     	 * (global step 9550: loss: 0.6232616901397705, lr: 5e-05
2024-01-05 18:24:03 INFO     	 * (global step 9600: loss: 0.5635882914066315, lr: 5e-05
2024-01-05 18:24:11 INFO     	 * (global step 9650: loss: 0.6848865449428558, lr: 5e-05
2024-01-05 18:24:18 INFO     	 * (global step 9700: loss: 0.7133422791957855, lr: 5e-05
2024-01-05 18:24:25 INFO     	 * (global step 9750: loss: 0.730372816324234, lr: 5e-05
2024-01-05 18:24:31 INFO     	 * (global step 9800: loss: 0.47140154242515564, lr: 5e-05
2024-01-05 18:24:36 INFO     	 * (global step 9850: loss: 0.7122045457363129, lr: 5e-05
2024-01-05 18:24:43 INFO     	 * (global step 9900: loss: 0.4679810404777527, lr: 5e-05
2024-01-05 18:24:49 INFO     	 * (global step 9950: loss: 0.437240868806839, lr: 5e-05
2024-01-05 18:24:56 INFO     	 * (global step 10000: loss: 0.48066528141498566, lr: 5e-05
2024-01-05 18:25:03 INFO     	 * (global step 10050: loss: 0.5525582134723663, lr: 5e-05
2024-01-05 18:25:11 INFO     	 * (global step 10100: loss: 0.6672464311122894, lr: 5e-05
2024-01-05 18:25:18 INFO     	 * (global step 10150: loss: 0.41902731359004974, lr: 5e-05
2024-01-05 18:25:26 INFO     	 * (global step 10200: loss: 0.49809880554676056, lr: 5e-05
2024-01-05 18:25:33 INFO     	 * (global step 10250: loss: 0.6047763526439667, lr: 5e-05
2024-01-05 18:25:41 INFO     	 * (global step 10300: loss: 0.6751914918422699, lr: 5e-05
2024-01-05 18:25:48 INFO     	 * (global step 10350: loss: 0.3407731130719185, lr: 5e-05
2024-01-05 18:25:56 INFO     	 * (global step 10400: loss: 0.5288295149803162, lr: 5e-05
2024-01-05 18:26:03 INFO     	 * (global step 10450: loss: 0.6551196575164795, lr: 5e-05
2024-01-05 18:26:11 INFO     	 * (global step 10500: loss: 0.5853196978569031, lr: 5e-05
2024-01-05 18:26:18 INFO     	 * (global step 10550: loss: 0.5716466307640076, lr: 5e-05
2024-01-05 18:26:26 INFO     	 * (global step 10600: loss: 0.29834482818841934, lr: 5e-05
2024-01-05 18:26:33 INFO     	 * (global step 10650: loss: 0.523980125784874, lr: 5e-05
2024-01-05 18:26:40 INFO     	 * (global step 10700: loss: 0.5693625658750534, lr: 5e-05
2024-01-05 18:26:48 INFO     	 * (global step 10750: loss: 0.597249761223793, lr: 5e-05
2024-01-05 18:26:55 INFO     	 * (global step 10800: loss: 0.5679447799921036, lr: 5e-05
2024-01-05 18:27:02 INFO     	 * (global step 10850: loss: 0.5178949683904648, lr: 5e-05
2024-01-05 18:27:08 INFO     	 * (global step 10900: loss: 0.5375060439109802, lr: 5e-05
2024-01-05 18:27:13 INFO     	 * (global step 10950: loss: 0.4795081168413162, lr: 5e-05
2024-01-05 18:27:20 INFO     	 * (global step 11000: loss: 0.43903225660324097, lr: 5e-05
2024-01-05 18:27:26 INFO     	 * (global step 11050: loss: 0.6631976366043091, lr: 5e-05
2024-01-05 18:27:34 INFO     	 * (global step 11100: loss: 0.5419610440731049, lr: 5e-05
2024-01-05 18:27:41 INFO     	 * (global step 11150: loss: 0.5794217586517334, lr: 5e-05
2024-01-05 18:27:48 INFO     	 * (global step 11200: loss: 0.48514844477176666, lr: 5e-05
2024-01-05 18:27:56 INFO     	 * (global step 11250: loss: 0.6179113388061523, lr: 5e-05
2024-01-05 18:28:03 INFO     	 * (global step 11300: loss: 0.43329499661922455, lr: 5e-05
2024-01-05 18:28:11 INFO     	 * (global step 11350: loss: 0.6539437174797058, lr: 5e-05
2024-01-05 18:28:19 INFO     	 * (global step 11400: loss: 0.6998924612998962, lr: 5e-05
2024-01-05 18:28:26 INFO     	 * (global step 11450: loss: 0.849172830581665, lr: 5e-05
2024-01-05 18:28:33 INFO     	 * (global step 11500: loss: 0.5793159604072571, lr: 5e-05
2024-01-05 18:28:41 INFO     	 * (global step 11550: loss: 0.6916426718235016, lr: 5e-05
2024-01-05 18:28:49 INFO     	 * (global step 11600: loss: 0.5714187324047089, lr: 5e-05
2024-01-05 18:28:56 INFO     	 * (global step 11650: loss: 0.863503485918045, lr: 5e-05
2024-01-05 18:29:03 INFO     	 * (global step 11700: loss: 0.6274743378162384, lr: 5e-05
2024-01-05 18:29:11 INFO     	 * (global step 11750: loss: 0.6086751222610474, lr: 5e-05
2024-01-05 18:29:18 INFO     	 * (global step 11800: loss: 0.4023337811231613, lr: 5e-05
2024-01-05 18:29:25 INFO     	 * (global step 11850: loss: 0.5576734691858292, lr: 5e-05
2024-01-05 18:29:32 INFO     	 * (global step 11900: loss: 0.44527116417884827, lr: 5e-05
2024-01-05 18:29:38 INFO     	 * (global step 11950: loss: 0.38245606422424316, lr: 5e-05
2024-01-05 18:29:44 INFO     	 * (global step 12000: loss: 0.6781930029392242, lr: 5e-05
2024-01-05 18:29:50 INFO     	 * (global step 12050: loss: 0.4924023300409317, lr: 5e-05
2024-01-05 18:29:57 INFO     	 * (global step 12100: loss: 0.6245290338993073, lr: 5e-05
2024-01-05 18:30:02 INFO     [epoch 2/15] average loss: 0.589, lr: 5e-05
2024-01-05 18:30:02 INFO     saving model related files
2024-01-05 18:30:02 INFO     saving model
2024-01-05 18:30:03 INFO     saving tokenizer
2024-01-05 18:30:03 INFO     saving optimizer
2024-01-05 18:30:04 INFO     remove old optimizer files
2024-01-05 18:30:06 INFO     	 * (global step 12150: loss: 0.46997807919979095, lr: 5e-05
2024-01-05 18:30:14 INFO     	 * (global step 12200: loss: 0.590578705072403, lr: 5e-05
2024-01-05 18:30:21 INFO     	 * (global step 12250: loss: 0.4514467865228653, lr: 5e-05
2024-01-05 18:30:29 INFO     	 * (global step 12300: loss: 0.5570529699325562, lr: 5e-05
2024-01-05 18:30:36 INFO     	 * (global step 12350: loss: 0.6549874246120453, lr: 5e-05
2024-01-05 18:30:44 INFO     	 * (global step 12400: loss: 0.7109085619449615, lr: 5e-05
2024-01-05 18:30:51 INFO     	 * (global step 12450: loss: 0.42122088372707367, lr: 5e-05
2024-01-05 18:30:59 INFO     	 * (global step 12500: loss: 0.6425885707139969, lr: 5e-05
2024-01-05 18:31:07 INFO     	 * (global step 12550: loss: 0.49082084000110626, lr: 5e-05
2024-01-05 18:31:14 INFO     	 * (global step 12600: loss: 0.4650540202856064, lr: 5e-05
2024-01-05 18:31:22 INFO     	 * (global step 12650: loss: 0.5674548745155334, lr: 5e-05
2024-01-05 18:31:30 INFO     	 * (global step 12700: loss: 0.7927940785884857, lr: 5e-05
2024-01-05 18:31:38 INFO     	 * (global step 12750: loss: 0.7353690266609192, lr: 5e-05
2024-01-05 18:31:46 INFO     	 * (global step 12800: loss: 0.6902090013027191, lr: 5e-05
2024-01-05 18:31:54 INFO     	 * (global step 12850: loss: 0.7650718688964844, lr: 5e-05
2024-01-05 18:32:02 INFO     	 * (global step 12900: loss: 0.6184815913438797, lr: 5e-05
2024-01-05 18:32:10 INFO     	 * (global step 12950: loss: 0.6269949674606323, lr: 5e-05
2024-01-05 18:32:18 INFO     	 * (global step 13000: loss: 0.7528863251209259, lr: 5e-05
2024-01-05 18:32:26 INFO     	 * (global step 13050: loss: 0.5491375476121902, lr: 5e-05
2024-01-05 18:32:34 INFO     	 * (global step 13100: loss: 0.45849646627902985, lr: 5e-05
2024-01-05 18:32:40 INFO     	 * (global step 13150: loss: 0.5636303424835205, lr: 5e-05
2024-01-05 18:32:45 INFO     	 * (global step 13200: loss: 0.5319546610116959, lr: 5e-05
2024-01-05 18:32:51 INFO     	 * (global step 13250: loss: 1.0894460082054138, lr: 5e-05
2024-01-05 18:32:57 INFO     	 * (global step 13300: loss: 0.3581560403108597, lr: 5e-05
2024-01-05 18:33:03 INFO     	 * (global step 13350: loss: 0.5586295425891876, lr: 5e-05
2024-01-05 18:33:09 INFO     	 * (global step 13400: loss: 0.6823767423629761, lr: 5e-05
2024-01-05 18:33:15 INFO     	 * (global step 13450: loss: 0.5173718631267548, lr: 5e-05
2024-01-05 18:33:21 INFO     	 * (global step 13500: loss: 0.6970040053129196, lr: 5e-05
2024-01-05 18:33:27 INFO     	 * (global step 13550: loss: 0.586225152015686, lr: 5e-05
2024-01-05 18:33:33 INFO     	 * (global step 13600: loss: 0.7228224277496338, lr: 5e-05
2024-01-05 18:33:39 INFO     	 * (global step 13650: loss: 0.579307347536087, lr: 5e-05
2024-01-05 18:33:45 INFO     	 * (global step 13700: loss: 0.5584477484226227, lr: 5e-05
2024-01-05 18:33:51 INFO     	 * (global step 13750: loss: 0.5521587133407593, lr: 5e-05
2024-01-05 18:33:57 INFO     	 * (global step 13800: loss: 0.5625022053718567, lr: 5e-05
2024-01-05 18:34:03 INFO     	 * (global step 13850: loss: 0.9084119200706482, lr: 5e-05
2024-01-05 18:34:09 INFO     	 * (global step 13900: loss: 0.3707684129476547, lr: 5e-05
2024-01-05 18:34:15 INFO     	 * (global step 13950: loss: 0.5934241563081741, lr: 5e-05
2024-01-05 18:34:21 INFO     	 * (global step 14000: loss: 0.4516250938177109, lr: 5e-05
2024-01-05 18:34:27 INFO     	 * (global step 14050: loss: 0.48008115589618683, lr: 5e-05
2024-01-05 18:34:33 INFO     	 * (global step 14100: loss: 0.6093092560768127, lr: 5e-05
2024-01-05 18:34:39 INFO     	 * (global step 14150: loss: 0.39487719535827637, lr: 5e-05
2024-01-05 18:34:45 INFO     	 * (global step 14200: loss: 0.5685767829418182, lr: 5e-05
2024-01-05 18:34:51 INFO     	 * (global step 14250: loss: 0.6788336038589478, lr: 5e-05
2024-01-05 18:34:57 INFO     	 * (global step 14300: loss: 0.44008517265319824, lr: 5e-05
2024-01-05 18:35:03 INFO     	 * (global step 14350: loss: 1.0162698924541473, lr: 5e-05
2024-01-05 18:35:09 INFO     	 * (global step 14400: loss: 0.6317028701305389, lr: 5e-05
2024-01-05 18:35:15 INFO     	 * (global step 14450: loss: 0.38286443054676056, lr: 5e-05
2024-01-05 18:35:21 INFO     	 * (global step 14500: loss: 0.5357243716716766, lr: 5e-05
2024-01-05 18:35:27 INFO     	 * (global step 14550: loss: 0.5084021538496017, lr: 5e-05
2024-01-05 18:35:33 INFO     	 * (global step 14600: loss: 0.5442650318145752, lr: 5e-05
2024-01-05 18:35:39 INFO     	 * (global step 14650: loss: 0.4917743504047394, lr: 5e-05
2024-01-05 18:35:45 INFO     	 * (global step 14700: loss: 0.4121423810720444, lr: 5e-05
2024-01-05 18:35:51 INFO     	 * (global step 14750: loss: 0.5609699785709381, lr: 5e-05
2024-01-05 18:35:57 INFO     	 * (global step 14800: loss: 0.5220165550708771, lr: 5e-05
2024-01-05 18:36:03 INFO     	 * (global step 14850: loss: 0.5213421285152435, lr: 5e-05
2024-01-05 18:36:10 INFO     	 * (global step 14900: loss: 0.44446803629398346, lr: 5e-05
2024-01-05 18:36:16 INFO     	 * (global step 14950: loss: 0.4438655823469162, lr: 5e-05
2024-01-05 18:36:22 INFO     	 * (global step 15000: loss: 0.4276035949587822, lr: 5e-05
2024-01-05 18:36:28 INFO     	 * (global step 15050: loss: 0.6470908224582672, lr: 5e-05
2024-01-05 18:36:34 INFO     	 * (global step 15100: loss: 0.5362251400947571, lr: 5e-05
2024-01-05 18:36:40 INFO     	 * (global step 15150: loss: 0.5609377473592758, lr: 5e-05
2024-01-05 18:36:46 INFO     	 * (global step 15200: loss: 0.5040234327316284, lr: 5e-05
2024-01-05 18:36:52 INFO     	 * (global step 15250: loss: 0.4136938601732254, lr: 5e-05
2024-01-05 18:36:58 INFO     	 * (global step 15300: loss: 0.5359200984239578, lr: 5e-05
2024-01-05 18:37:04 INFO     	 * (global step 15350: loss: 0.7246540784835815, lr: 5e-05
2024-01-05 18:37:10 INFO     	 * (global step 15400: loss: 0.4122749865055084, lr: 5e-05
2024-01-05 18:37:16 INFO     	 * (global step 15450: loss: 0.43079236149787903, lr: 5e-05
2024-01-05 18:37:22 INFO     	 * (global step 15500: loss: 0.4580902233719826, lr: 5e-05
2024-01-05 18:37:28 INFO     	 * (global step 15550: loss: 0.6715556085109711, lr: 5e-05
2024-01-05 18:37:34 INFO     	 * (global step 15600: loss: 0.5698687136173248, lr: 5e-05
2024-01-05 18:37:40 INFO     	 * (global step 15650: loss: 0.6933708786964417, lr: 5e-05
2024-01-05 18:37:46 INFO     	 * (global step 15700: loss: 0.6730014085769653, lr: 5e-05
2024-01-05 18:37:52 INFO     	 * (global step 15750: loss: 0.5481641441583633, lr: 5e-05
2024-01-05 18:37:58 INFO     	 * (global step 15800: loss: 0.3131759315729141, lr: 5e-05
2024-01-05 18:38:04 INFO     	 * (global step 15850: loss: 0.5440447330474854, lr: 5e-05
2024-01-05 18:38:10 INFO     	 * (global step 15900: loss: 0.6334898769855499, lr: 5e-05
2024-01-05 18:38:16 INFO     	 * (global step 15950: loss: 0.7333014011383057, lr: 5e-05
2024-01-05 18:38:22 INFO     	 * (global step 16000: loss: 0.4172879159450531, lr: 5e-05
2024-01-05 18:38:28 INFO     	 * (global step 16050: loss: 0.39793506264686584, lr: 5e-05
2024-01-05 18:38:34 INFO     	 * (global step 16100: loss: 0.7571995556354523, lr: 5e-05
2024-01-05 18:38:40 INFO     	 * (global step 16150: loss: 0.45338091254234314, lr: 5e-05
2024-01-05 18:38:45 INFO     [epoch 3/15] average loss: 0.567, lr: 5e-05
2024-01-05 18:38:45 INFO     saving model related files
2024-01-05 18:38:45 INFO     saving model
2024-01-05 18:38:46 INFO     saving tokenizer
2024-01-05 18:38:46 INFO     saving optimizer
2024-01-05 18:38:47 INFO     remove old optimizer files
2024-01-05 18:38:48 INFO     	 * (global step 16200: loss: 0.6994669437408447, lr: 5e-05
2024-01-05 18:38:54 INFO     	 * (global step 16250: loss: 0.5695769190788269, lr: 5e-05
2024-01-05 18:39:00 INFO     	 * (global step 16300: loss: 0.44873782992362976, lr: 5e-05
2024-01-05 18:39:06 INFO     	 * (global step 16350: loss: 0.4839346557855606, lr: 5e-05
2024-01-05 18:39:12 INFO     	 * (global step 16400: loss: 0.2948429137468338, lr: 5e-05
2024-01-05 18:39:18 INFO     	 * (global step 16450: loss: 0.4207216799259186, lr: 5e-05
2024-01-05 18:39:24 INFO     	 * (global step 16500: loss: 0.4596109241247177, lr: 5e-05
2024-01-05 18:39:30 INFO     	 * (global step 16550: loss: 0.6105993837118149, lr: 5e-05
2024-01-05 18:39:36 INFO     	 * (global step 16600: loss: 0.47415582835674286, lr: 5e-05
2024-01-05 18:39:42 INFO     	 * (global step 16650: loss: 0.5475552976131439, lr: 5e-05
2024-01-05 18:39:48 INFO     	 * (global step 16700: loss: 0.5431016832590103, lr: 5e-05
2024-01-05 18:39:54 INFO     	 * (global step 16750: loss: 0.46252959966659546, lr: 5e-05
2024-01-05 18:40:01 INFO     	 * (global step 16800: loss: 0.6752725690603256, lr: 5e-05
2024-01-05 18:40:07 INFO     	 * (global step 16850: loss: 0.3578701466321945, lr: 5e-05
2024-01-05 18:40:13 INFO     	 * (global step 16900: loss: 0.6113190352916718, lr: 5e-05
2024-01-05 18:40:19 INFO     	 * (global step 16950: loss: 0.5768514275550842, lr: 5e-05
2024-01-05 18:40:25 INFO     	 * (global step 17000: loss: 0.628000795841217, lr: 5e-05
2024-01-05 18:40:31 INFO     	 * (global step 17050: loss: 0.5581961274147034, lr: 5e-05
2024-01-05 18:40:37 INFO     	 * (global step 17100: loss: 0.27039792388677597, lr: 5e-05
2024-01-05 18:40:43 INFO     	 * (global step 17150: loss: 0.5361394584178925, lr: 5e-05
2024-01-05 18:40:49 INFO     	 * (global step 17200: loss: 0.4632294178009033, lr: 5e-05
2024-01-05 18:40:55 INFO     	 * (global step 17250: loss: 0.3576876223087311, lr: 5e-05
2024-01-05 18:41:01 INFO     	 * (global step 17300: loss: 0.6876709163188934, lr: 5e-05
2024-01-05 18:41:07 INFO     	 * (global step 17350: loss: 0.7147607207298279, lr: 5e-05
2024-01-05 18:41:12 INFO     	 * (global step 17400: loss: 0.7333292365074158, lr: 5e-05
2024-01-05 18:41:19 INFO     	 * (global step 17450: loss: 0.4800317734479904, lr: 5e-05
2024-01-05 18:41:25 INFO     	 * (global step 17500: loss: 0.4645994305610657, lr: 5e-05
2024-01-05 18:41:31 INFO     	 * (global step 17550: loss: 0.46876150369644165, lr: 5e-05
2024-01-05 18:41:37 INFO     	 * (global step 17600: loss: 0.6682968586683273, lr: 5e-05
2024-01-05 18:41:43 INFO     	 * (global step 17650: loss: 0.45770205557346344, lr: 5e-05
2024-01-05 18:41:49 INFO     	 * (global step 17700: loss: 0.41179291158914566, lr: 5e-05
2024-01-05 18:41:55 INFO     	 * (global step 17750: loss: 0.6526602208614349, lr: 5e-05
2024-01-05 18:42:01 INFO     	 * (global step 17800: loss: 0.7147769778966904, lr: 5e-05
2024-01-05 18:42:07 INFO     	 * (global step 17850: loss: 0.692357987165451, lr: 5e-05
2024-01-05 18:42:13 INFO     	 * (global step 17900: loss: 0.45851339399814606, lr: 5e-05
2024-01-05 18:42:19 INFO     	 * (global step 17950: loss: 0.4234555661678314, lr: 5e-05
2024-01-05 18:42:25 INFO     	 * (global step 18000: loss: 0.5554556250572205, lr: 5e-05
2024-01-05 18:42:31 INFO     	 * (global step 18050: loss: 0.4933253079652786, lr: 5e-05
2024-01-05 18:42:37 INFO     	 * (global step 18100: loss: 0.5830363482236862, lr: 5e-05
2024-01-05 18:42:43 INFO     	 * (global step 18150: loss: 0.6089453399181366, lr: 5e-05
2024-01-05 18:42:49 INFO     	 * (global step 18200: loss: 0.5051837712526321, lr: 5e-05
2024-01-05 18:42:55 INFO     	 * (global step 18250: loss: 0.4967948645353317, lr: 5e-05
2024-01-05 18:43:01 INFO     	 * (global step 18300: loss: 0.334022156894207, lr: 5e-05
2024-01-05 18:43:07 INFO     	 * (global step 18350: loss: 0.8171179294586182, lr: 5e-05
2024-01-05 18:43:13 INFO     	 * (global step 18400: loss: 0.6498093605041504, lr: 5e-05
2024-01-05 18:43:19 INFO     	 * (global step 18450: loss: 0.5118217766284943, lr: 5e-05
2024-01-05 18:43:25 INFO     	 * (global step 18500: loss: 0.5142627507448196, lr: 5e-05
2024-01-05 18:43:31 INFO     	 * (global step 18550: loss: 0.8175262808799744, lr: 5e-05
2024-01-05 18:43:37 INFO     	 * (global step 18600: loss: 0.46452879905700684, lr: 5e-05
2024-01-05 18:43:43 INFO     	 * (global step 18650: loss: 0.7319866418838501, lr: 5e-05
2024-01-05 18:43:49 INFO     	 * (global step 18700: loss: 0.6394864618778229, lr: 5e-05
2024-01-05 18:43:55 INFO     	 * (global step 18750: loss: 0.4869874268770218, lr: 5e-05
2024-01-05 18:44:01 INFO     	 * (global step 18800: loss: 0.48868024349212646, lr: 5e-05
2024-01-05 18:44:07 INFO     	 * (global step 18850: loss: 0.6120167970657349, lr: 5e-05
2024-01-05 18:44:13 INFO     	 * (global step 18900: loss: 0.5051442086696625, lr: 5e-05
2024-01-05 18:44:19 INFO     	 * (global step 18950: loss: 0.5527180731296539, lr: 5e-05
2024-01-05 18:44:25 INFO     	 * (global step 19000: loss: 0.553688257932663, lr: 5e-05
2024-01-05 18:44:31 INFO     	 * (global step 19050: loss: 0.7489485442638397, lr: 5e-05
2024-01-05 18:44:37 INFO     	 * (global step 19100: loss: 0.6470401585102081, lr: 5e-05
2024-01-05 18:44:43 INFO     	 * (global step 19150: loss: 0.6065012812614441, lr: 5e-05
2024-01-05 18:44:49 INFO     	 * (global step 19200: loss: 0.4856550395488739, lr: 5e-05
2024-01-05 18:44:55 INFO     	 * (global step 19250: loss: 0.5987414419651031, lr: 5e-05
2024-01-05 18:45:01 INFO     	 * (global step 19300: loss: 0.43593260645866394, lr: 5e-05
2024-01-05 18:45:07 INFO     	 * (global step 19350: loss: 0.7200592756271362, lr: 5e-05
2024-01-05 18:45:14 INFO     	 * (global step 19400: loss: 0.6837153732776642, lr: 5e-05
2024-01-05 18:45:20 INFO     	 * (global step 19450: loss: 0.4628619998693466, lr: 5e-05
2024-01-05 18:45:26 INFO     	 * (global step 19500: loss: 0.4755839407444, lr: 5e-05
2024-01-05 18:45:32 INFO     	 * (global step 19550: loss: 0.33304470777511597, lr: 5e-05
2024-01-05 18:45:38 INFO     	 * (global step 19600: loss: 0.5505829155445099, lr: 5e-05
2024-01-05 18:45:44 INFO     	 * (global step 19650: loss: 0.43183885514736176, lr: 5e-05
2024-01-05 18:45:49 INFO     	 * (global step 19700: loss: 0.4810732901096344, lr: 5e-05
2024-01-05 18:45:55 INFO     	 * (global step 19750: loss: 0.40454868972301483, lr: 5e-05
2024-01-05 18:46:01 INFO     	 * (global step 19800: loss: 0.4765483886003494, lr: 5e-05
2024-01-05 18:46:07 INFO     	 * (global step 19850: loss: 0.32845285534858704, lr: 5e-05
2024-01-05 18:46:13 INFO     	 * (global step 19900: loss: 0.4790724813938141, lr: 5e-05
2024-01-05 18:46:19 INFO     	 * (global step 19950: loss: 0.498140811920166, lr: 5e-05
2024-01-05 18:46:25 INFO     	 * (global step 20000: loss: 0.42338964343070984, lr: 5e-05
2024-01-05 18:46:31 INFO     	 * (global step 20050: loss: 0.6261186897754669, lr: 5e-05
2024-01-05 18:46:37 INFO     	 * (global step 20100: loss: 0.6063357293605804, lr: 5e-05
2024-01-05 18:46:43 INFO     	 * (global step 20150: loss: 0.4702247381210327, lr: 5e-05
2024-01-05 18:46:50 INFO     	 * (global step 20200: loss: 0.4078124314546585, lr: 5e-05
2024-01-05 18:46:54 INFO     [epoch 4/15] average loss: 0.551, lr: 5e-05
2024-01-05 18:46:54 INFO     saving model related files
2024-01-05 18:46:54 INFO     saving model
2024-01-05 18:46:55 INFO     saving tokenizer
2024-01-05 18:46:55 INFO     saving optimizer
2024-01-05 18:46:56 INFO     remove old optimizer files
2024-01-05 18:46:57 INFO     	 * (global step 20250: loss: 0.4820857495069504, lr: 5e-05
2024-01-05 18:47:03 INFO     	 * (global step 20300: loss: 0.4908215403556824, lr: 5e-05
2024-01-05 18:47:09 INFO     	 * (global step 20350: loss: 0.6529531180858612, lr: 5e-05
2024-01-05 18:47:15 INFO     	 * (global step 20400: loss: 0.4667331874370575, lr: 5e-05
2024-01-05 18:47:21 INFO     	 * (global step 20450: loss: 0.385873019695282, lr: 5e-05
2024-01-05 18:47:27 INFO     	 * (global step 20500: loss: 0.4410432279109955, lr: 5e-05
2024-01-05 18:47:33 INFO     	 * (global step 20550: loss: 0.5382973998785019, lr: 5e-05
2024-01-05 18:47:39 INFO     	 * (global step 20600: loss: 0.5782226622104645, lr: 5e-05
2024-01-05 18:47:45 INFO     	 * (global step 20650: loss: 0.6924096643924713, lr: 5e-05
2024-01-05 18:47:51 INFO     	 * (global step 20700: loss: 0.7837211787700653, lr: 5e-05
2024-01-05 18:47:57 INFO     	 * (global step 20750: loss: 0.3605235666036606, lr: 5e-05
2024-01-05 18:48:03 INFO     	 * (global step 20800: loss: 0.659844309091568, lr: 5e-05
2024-01-05 18:48:09 INFO     	 * (global step 20850: loss: 0.5531357228755951, lr: 5e-05
2024-01-05 18:48:15 INFO     	 * (global step 20900: loss: 0.3968196362257004, lr: 5e-05
2024-01-05 18:48:21 INFO     	 * (global step 20950: loss: 0.6161742806434631, lr: 5e-05
2024-01-05 18:48:27 INFO     	 * (global step 21000: loss: 0.35321806371212006, lr: 5e-05
2024-01-05 18:48:33 INFO     	 * (global step 21050: loss: 0.6639601588249207, lr: 5e-05
2024-01-05 18:48:39 INFO     	 * (global step 21100: loss: 0.5311940163373947, lr: 5e-05
2024-01-05 18:48:46 INFO     	 * (global step 21150: loss: 0.474666103720665, lr: 5e-05
2024-01-05 18:48:52 INFO     	 * (global step 21200: loss: 0.38298913836479187, lr: 5e-05
2024-01-05 18:48:58 INFO     	 * (global step 21250: loss: 0.36362943053245544, lr: 5e-05
2024-01-05 18:49:04 INFO     	 * (global step 21300: loss: 0.616132989525795, lr: 5e-05
2024-01-05 18:49:10 INFO     	 * (global step 21350: loss: 0.5930428504943848, lr: 5e-05
2024-01-05 18:49:16 INFO     	 * (global step 21400: loss: 0.5265643000602722, lr: 5e-05
2024-01-05 18:49:22 INFO     	 * (global step 21450: loss: 0.45351535081863403, lr: 5e-05
2024-01-05 18:49:28 INFO     	 * (global step 21500: loss: 0.3961985856294632, lr: 5e-05
2024-01-05 18:49:34 INFO     	 * (global step 21550: loss: 0.38214901089668274, lr: 5e-05
2024-01-05 18:49:39 INFO     	 * (global step 21600: loss: 0.4900987893342972, lr: 5e-05
2024-01-05 18:49:46 INFO     	 * (global step 21650: loss: 0.4182358533143997, lr: 5e-05
2024-01-05 18:49:52 INFO     	 * (global step 21700: loss: 0.5552859902381897, lr: 5e-05
2024-01-05 18:49:58 INFO     	 * (global step 21750: loss: 0.5838029980659485, lr: 5e-05
2024-01-05 18:50:04 INFO     	 * (global step 21800: loss: 0.6409906446933746, lr: 5e-05
2024-01-05 18:50:10 INFO     	 * (global step 21850: loss: 0.7316073179244995, lr: 5e-05
2024-01-05 18:50:16 INFO     	 * (global step 21900: loss: 0.650953859090805, lr: 5e-05
2024-01-05 18:50:22 INFO     	 * (global step 21950: loss: 0.6721255630254745, lr: 5e-05
2024-01-05 18:50:28 INFO     	 * (global step 22000: loss: 0.5831395387649536, lr: 5e-05
2024-01-05 18:50:34 INFO     	 * (global step 22050: loss: 0.5460630357265472, lr: 5e-05
2024-01-05 18:50:40 INFO     	 * (global step 22100: loss: 0.5456413924694061, lr: 5e-05
2024-01-05 18:50:46 INFO     	 * (global step 22150: loss: 0.422413170337677, lr: 5e-05
2024-01-05 18:50:52 INFO     	 * (global step 22200: loss: 0.5704731047153473, lr: 5e-05
2024-01-05 18:50:58 INFO     	 * (global step 22250: loss: 0.6998651027679443, lr: 5e-05
2024-01-05 18:51:05 INFO     	 * (global step 22300: loss: 0.5774864703416824, lr: 5e-05
2024-01-05 18:51:11 INFO     	 * (global step 22350: loss: 0.49842286109924316, lr: 5e-05
2024-01-05 18:51:17 INFO     	 * (global step 22400: loss: 0.5346681773662567, lr: 5e-05
2024-01-05 18:51:23 INFO     	 * (global step 22450: loss: 0.5025866627693176, lr: 5e-05
2024-01-05 18:51:29 INFO     	 * (global step 22500: loss: 0.6432874202728271, lr: 5e-05
2024-01-05 18:51:35 INFO     	 * (global step 22550: loss: 0.5384916663169861, lr: 5e-05
2024-01-05 18:51:41 INFO     	 * (global step 22600: loss: 0.5985692739486694, lr: 5e-05
2024-01-05 18:51:48 INFO     	 * (global step 22650: loss: 0.4561682641506195, lr: 5e-05
2024-01-05 18:51:54 INFO     	 * (global step 22700: loss: 0.42172250151634216, lr: 5e-05
2024-01-05 18:52:00 INFO     	 * (global step 22750: loss: 0.4118986576795578, lr: 5e-05
2024-01-05 18:52:06 INFO     	 * (global step 22800: loss: 0.4639781415462494, lr: 5e-05
2024-01-05 18:52:12 INFO     	 * (global step 22850: loss: 0.5575112700462341, lr: 5e-05
2024-01-05 18:52:18 INFO     	 * (global step 22900: loss: 0.3954193890094757, lr: 5e-05
2024-01-05 18:52:24 INFO     	 * (global step 22950: loss: 0.5375876724720001, lr: 5e-05
2024-01-05 18:52:30 INFO     	 * (global step 23000: loss: 0.6219237744808197, lr: 5e-05
2024-01-05 18:52:37 INFO     	 * (global step 23050: loss: 0.3811981528997421, lr: 5e-05
2024-01-05 18:52:43 INFO     	 * (global step 23100: loss: 0.5794939398765564, lr: 5e-05
2024-01-05 18:52:49 INFO     	 * (global step 23150: loss: 0.606695219874382, lr: 5e-05
2024-01-05 18:52:55 INFO     	 * (global step 23200: loss: 0.4266982078552246, lr: 5e-05
2024-01-05 18:53:01 INFO     	 * (global step 23250: loss: 0.5884765684604645, lr: 5e-05
2024-01-05 18:53:07 INFO     	 * (global step 23300: loss: 0.32217881083488464, lr: 5e-05
2024-01-05 18:53:13 INFO     	 * (global step 23350: loss: 0.46643830835819244, lr: 5e-05
2024-01-05 18:53:20 INFO     	 * (global step 23400: loss: 0.42130210995674133, lr: 5e-05
2024-01-05 18:53:26 INFO     	 * (global step 23450: loss: 0.5038267970085144, lr: 5e-05
2024-01-05 18:53:32 INFO     	 * (global step 23500: loss: 0.519778773188591, lr: 5e-05
2024-01-05 18:53:38 INFO     	 * (global step 23550: loss: 0.44699743390083313, lr: 5e-05
2024-01-05 18:53:44 INFO     	 * (global step 23600: loss: 0.6196277439594269, lr: 5e-05
2024-01-05 18:53:51 INFO     	 * (global step 23650: loss: 0.3191501200199127, lr: 5e-05
2024-01-05 18:53:57 INFO     	 * (global step 23700: loss: 0.5066084265708923, lr: 5e-05
2024-01-05 18:54:03 INFO     	 * (global step 23750: loss: 0.608670249581337, lr: 5e-05
2024-01-05 18:54:09 INFO     	 * (global step 23800: loss: 0.5007766634225845, lr: 5e-05
2024-01-05 18:54:15 INFO     	 * (global step 23850: loss: 0.48967090249061584, lr: 5e-05
2024-01-05 18:54:21 INFO     	 * (global step 23900: loss: 0.420108437538147, lr: 5e-05
2024-01-05 18:54:27 INFO     	 * (global step 23950: loss: 0.881172388792038, lr: 5e-05
2024-01-05 18:54:34 INFO     	 * (global step 24000: loss: 0.4405318349599838, lr: 5e-05
2024-01-05 18:54:40 INFO     	 * (global step 24050: loss: 0.7287659347057343, lr: 5e-05
2024-01-05 18:54:46 INFO     	 * (global step 24100: loss: 0.4741858094930649, lr: 5e-05
2024-01-05 18:54:52 INFO     	 * (global step 24150: loss: 0.5508250743150711, lr: 5e-05
2024-01-05 18:54:59 INFO     	 * (global step 24200: loss: 0.5945158451795578, lr: 5e-05
2024-01-05 18:55:05 INFO     	 * (global step 24250: loss: 0.6539730429649353, lr: 5e-05
2024-01-05 18:55:09 INFO     [epoch 5/15] average loss: 0.539, lr: 5e-05
2024-01-05 18:55:09 INFO     saving model related files
2024-01-05 18:55:09 INFO     saving model
2024-01-05 18:55:10 INFO     saving tokenizer
2024-01-05 18:55:10 INFO     saving optimizer
2024-01-05 18:55:11 INFO     remove old optimizer files
2024-01-05 18:55:13 INFO     	 * (global step 24300: loss: 0.4685405492782593, lr: 5e-05
2024-01-05 18:55:19 INFO     	 * (global step 24350: loss: 0.4873732328414917, lr: 5e-05
2024-01-05 18:55:25 INFO     	 * (global step 24400: loss: 0.4472053050994873, lr: 5e-05
2024-01-05 18:55:31 INFO     	 * (global step 24450: loss: 0.4202863425016403, lr: 5e-05
2024-01-05 18:55:37 INFO     	 * (global step 24500: loss: 0.5910156816244125, lr: 5e-05
2024-01-05 18:55:43 INFO     	 * (global step 24550: loss: 0.47678765654563904, lr: 5e-05
2024-01-05 18:55:50 INFO     	 * (global step 24600: loss: 0.5381410419940948, lr: 5e-05
2024-01-05 18:55:56 INFO     	 * (global step 24650: loss: 0.5009232759475708, lr: 5e-05
2024-01-05 18:56:02 INFO     	 * (global step 24700: loss: 0.3881427198648453, lr: 5e-05
2024-01-05 18:56:08 INFO     	 * (global step 24750: loss: 0.5970287322998047, lr: 5e-05
2024-01-05 18:56:15 INFO     	 * (global step 24800: loss: 0.6984204351902008, lr: 5e-05
2024-01-05 18:56:21 INFO     	 * (global step 24850: loss: 0.5279259383678436, lr: 5e-05
2024-01-05 18:56:27 INFO     	 * (global step 24900: loss: 0.6296549439430237, lr: 5e-05
2024-01-05 18:56:33 INFO     	 * (global step 24950: loss: 0.7213798761367798, lr: 5e-05
2024-01-05 18:56:40 INFO     	 * (global step 25000: loss: 0.42939595878124237, lr: 5e-05
2024-01-05 18:56:46 INFO     	 * (global step 25050: loss: 0.6558059751987457, lr: 5e-05
2024-01-05 18:56:52 INFO     	 * (global step 25100: loss: 0.4965302497148514, lr: 5e-05
2024-01-05 18:56:58 INFO     	 * (global step 25150: loss: 0.579458549618721, lr: 5e-05
2024-01-05 18:57:04 INFO     	 * (global step 25200: loss: 0.7476368248462677, lr: 5e-05
2024-01-05 18:57:11 INFO     	 * (global step 25250: loss: 0.4606103450059891, lr: 5e-05
2024-01-05 18:57:17 INFO     	 * (global step 25300: loss: 0.8408240675926208, lr: 5e-05
2024-01-05 18:57:23 INFO     	 * (global step 25350: loss: 0.4388556033372879, lr: 5e-05
2024-01-05 18:57:29 INFO     	 * (global step 25400: loss: 0.5756542831659317, lr: 5e-05
2024-01-05 18:57:35 INFO     	 * (global step 25450: loss: 0.7282440811395645, lr: 5e-05
2024-01-05 18:57:41 INFO     	 * (global step 25500: loss: 0.6247721016407013, lr: 5e-05
2024-01-05 18:57:47 INFO     	 * (global step 25550: loss: 0.3961319923400879, lr: 5e-05
2024-01-05 18:57:53 INFO     	 * (global step 25600: loss: 0.8527546226978302, lr: 5e-05
2024-01-05 18:57:59 INFO     	 * (global step 25650: loss: 0.6259143352508545, lr: 5e-05
2024-01-05 18:58:05 INFO     	 * (global step 25700: loss: 0.6130285859107971, lr: 5e-05
2024-01-05 18:58:12 INFO     	 * (global step 25750: loss: 0.29760652780532837, lr: 5e-05
2024-01-05 18:58:18 INFO     	 * (global step 25800: loss: 0.47596077620983124, lr: 5e-05
2024-01-05 18:58:24 INFO     	 * (global step 25850: loss: 0.4946361929178238, lr: 5e-05
2024-01-05 18:58:30 INFO     	 * (global step 25900: loss: 0.5674674957990646, lr: 5e-05
2024-01-05 18:58:36 INFO     	 * (global step 25950: loss: 0.509112611413002, lr: 5e-05
2024-01-05 18:58:42 INFO     	 * (global step 26000: loss: 0.3747406005859375, lr: 5e-05
2024-01-05 18:58:48 INFO     	 * (global step 26050: loss: 0.4334963262081146, lr: 5e-05
2024-01-05 18:58:54 INFO     	 * (global step 26100: loss: 0.4437602311372757, lr: 5e-05
2024-01-05 18:59:01 INFO     	 * (global step 26150: loss: 0.49222713708877563, lr: 5e-05
2024-01-05 18:59:07 INFO     	 * (global step 26200: loss: 0.6263065040111542, lr: 5e-05
2024-01-05 18:59:13 INFO     	 * (global step 26250: loss: 0.39671868085861206, lr: 5e-05
2024-01-05 18:59:19 INFO     	 * (global step 26300: loss: 0.510232537984848, lr: 5e-05
2024-01-05 18:59:25 INFO     	 * (global step 26350: loss: 0.5186064839363098, lr: 5e-05
2024-01-05 18:59:31 INFO     	 * (global step 26400: loss: 0.5124475061893463, lr: 5e-05
2024-01-05 18:59:37 INFO     	 * (global step 26450: loss: 0.4266674518585205, lr: 5e-05
2024-01-05 18:59:44 INFO     	 * (global step 26500: loss: 0.8361595571041107, lr: 5e-05
2024-01-05 18:59:50 INFO     	 * (global step 26550: loss: 0.5822503417730331, lr: 5e-05
2024-01-05 18:59:56 INFO     	 * (global step 26600: loss: 0.5778754502534866, lr: 5e-05
2024-01-05 19:00:02 INFO     	 * (global step 26650: loss: 0.44293348491191864, lr: 5e-05
2024-01-05 19:00:08 INFO     	 * (global step 26700: loss: 0.6754543632268906, lr: 5e-05
2024-01-05 19:00:14 INFO     	 * (global step 26750: loss: 0.4079846739768982, lr: 5e-05
2024-01-05 19:00:20 INFO     	 * (global step 26800: loss: 0.6207956075668335, lr: 5e-05
2024-01-05 19:00:26 INFO     	 * (global step 26850: loss: 0.7369159162044525, lr: 5e-05
2024-01-05 19:00:32 INFO     	 * (global step 26900: loss: 0.4049811363220215, lr: 5e-05
2024-01-05 19:00:38 INFO     	 * (global step 26950: loss: 0.5030632317066193, lr: 5e-05
2024-01-05 19:00:44 INFO     	 * (global step 27000: loss: 0.5065327882766724, lr: 5e-05
2024-01-05 19:00:50 INFO     	 * (global step 27050: loss: 0.41642678529024124, lr: 5e-05
2024-01-05 19:00:57 INFO     	 * (global step 27100: loss: 0.42270366847515106, lr: 5e-05
2024-01-05 19:01:03 INFO     	 * (global step 27150: loss: 0.5459201782941818, lr: 5e-05
2024-01-05 19:01:09 INFO     	 * (global step 27200: loss: 0.44194647669792175, lr: 5e-05
2024-01-05 19:01:15 INFO     	 * (global step 27250: loss: 0.3586021810770035, lr: 5e-05
2024-01-05 19:01:21 INFO     	 * (global step 27300: loss: 0.5773689150810242, lr: 5e-05
2024-01-05 19:01:27 INFO     	 * (global step 27350: loss: 0.4459088444709778, lr: 5e-05
2024-01-05 19:01:33 INFO     	 * (global step 27400: loss: 0.5855796039104462, lr: 5e-05
2024-01-05 19:01:39 INFO     	 * (global step 27450: loss: 0.4336130768060684, lr: 5e-05
2024-01-05 19:01:45 INFO     	 * (global step 27500: loss: 0.44784075021743774, lr: 5e-05
2024-01-05 19:01:51 INFO     	 * (global step 27550: loss: 0.3926994800567627, lr: 5e-05
2024-01-05 19:01:57 INFO     	 * (global step 27600: loss: 0.41815805435180664, lr: 5e-05
2024-01-05 19:02:03 INFO     	 * (global step 27650: loss: 0.5512478947639465, lr: 5e-05
2024-01-05 19:02:10 INFO     	 * (global step 27700: loss: 0.5498455464839935, lr: 5e-05
2024-01-05 19:02:16 INFO     	 * (global step 27750: loss: 0.718279629945755, lr: 5e-05
2024-01-05 19:02:22 INFO     	 * (global step 27800: loss: 0.5471982806921005, lr: 5e-05
2024-01-05 19:02:28 INFO     	 * (global step 27850: loss: 0.607463151216507, lr: 5e-05
2024-01-05 19:02:34 INFO     	 * (global step 27900: loss: 0.5757426023483276, lr: 5e-05
2024-01-05 19:02:40 INFO     	 * (global step 27950: loss: 0.4230239689350128, lr: 5e-05
2024-01-05 19:02:46 INFO     	 * (global step 28000: loss: 0.4504931569099426, lr: 5e-05
2024-01-05 19:02:52 INFO     	 * (global step 28050: loss: 0.7412215173244476, lr: 5e-05
2024-01-05 19:02:58 INFO     	 * (global step 28100: loss: 0.6036347299814224, lr: 5e-05
2024-01-05 19:03:04 INFO     	 * (global step 28150: loss: 0.5380952209234238, lr: 5e-05
2024-01-05 19:03:11 INFO     	 * (global step 28200: loss: 0.6055789738893509, lr: 5e-05
2024-01-05 19:03:17 INFO     	 * (global step 28250: loss: 0.5615822672843933, lr: 5e-05
2024-01-05 19:03:23 INFO     	 * (global step 28300: loss: 0.6975025832653046, lr: 5e-05
2024-01-05 19:03:28 INFO     [epoch 6/15] average loss: 0.527, lr: 5e-05
2024-01-05 19:03:28 INFO     saving model related files
2024-01-05 19:03:28 INFO     saving model
2024-01-05 19:03:28 INFO     saving tokenizer
2024-01-05 19:03:29 INFO     saving optimizer
2024-01-05 19:03:30 INFO     remove old optimizer files
2024-01-05 19:03:32 INFO     	 * (global step 28350: loss: 0.6176671087741852, lr: 5e-05
2024-01-05 19:03:38 INFO     	 * (global step 28400: loss: 0.4403996169567108, lr: 5e-05
2024-01-05 19:03:44 INFO     	 * (global step 28450: loss: 0.5284785628318787, lr: 5e-05
2024-01-05 19:03:50 INFO     	 * (global step 28500: loss: 0.5590457022190094, lr: 5e-05
2024-01-05 19:03:56 INFO     	 * (global step 28550: loss: 0.5584724247455597, lr: 5e-05
2024-01-05 19:04:02 INFO     	 * (global step 28600: loss: 0.582430511713028, lr: 5e-05
2024-01-05 19:04:08 INFO     	 * (global step 28650: loss: 0.6318022608757019, lr: 5e-05
2024-01-05 19:04:14 INFO     	 * (global step 28700: loss: 0.5052514225244522, lr: 5e-05
2024-01-05 19:04:20 INFO     	 * (global step 28750: loss: 0.5454187989234924, lr: 5e-05
2024-01-05 19:04:26 INFO     	 * (global step 28800: loss: 0.4473949819803238, lr: 5e-05
2024-01-05 19:04:32 INFO     	 * (global step 28850: loss: 0.6053162515163422, lr: 5e-05
2024-01-05 19:04:38 INFO     	 * (global step 28900: loss: 0.5817747712135315, lr: 5e-05
2024-01-05 19:04:43 INFO     	 * (global step 28950: loss: 0.4989069104194641, lr: 5e-05
2024-01-05 19:04:49 INFO     	 * (global step 29000: loss: 0.4953674077987671, lr: 5e-05
2024-01-05 19:04:55 INFO     	 * (global step 29050: loss: 0.4037279635667801, lr: 5e-05
2024-01-05 19:05:01 INFO     	 * (global step 29100: loss: 0.5695659816265106, lr: 5e-05
2024-01-05 19:05:07 INFO     	 * (global step 29150: loss: 0.7133282721042633, lr: 5e-05
2024-01-05 19:05:14 INFO     	 * (global step 29200: loss: 0.626591145992279, lr: 5e-05
2024-01-05 19:05:20 INFO     	 * (global step 29250: loss: 0.47327058017253876, lr: 5e-05
2024-01-05 19:05:26 INFO     	 * (global step 29300: loss: 0.3798090070486069, lr: 5e-05
2024-01-05 19:05:33 INFO     	 * (global step 29350: loss: 0.5239095985889435, lr: 5e-05
2024-01-05 19:05:39 INFO     	 * (global step 29400: loss: 0.3847843259572983, lr: 5e-05
2024-01-05 19:05:45 INFO     	 * (global step 29450: loss: 0.5419279932975769, lr: 5e-05
2024-01-05 19:05:51 INFO     	 * (global step 29500: loss: 0.3396333009004593, lr: 5e-05
2024-01-05 19:05:58 INFO     	 * (global step 29550: loss: 0.4013364613056183, lr: 5e-05
2024-01-05 19:06:04 INFO     	 * (global step 29600: loss: 0.33788876235485077, lr: 5e-05
2024-01-05 19:06:10 INFO     	 * (global step 29650: loss: 0.41988736391067505, lr: 5e-05
2024-01-05 19:06:17 INFO     	 * (global step 29700: loss: 0.50160051882267, lr: 5e-05
2024-01-05 19:06:23 INFO     	 * (global step 29750: loss: 0.7913614213466644, lr: 5e-05
2024-01-05 19:06:29 INFO     	 * (global step 29800: loss: 0.539066880941391, lr: 5e-05
2024-01-05 19:06:36 INFO     	 * (global step 29850: loss: 0.5064335465431213, lr: 5e-05
2024-01-05 19:06:43 INFO     	 * (global step 29900: loss: 0.468238964676857, lr: 5e-05
2024-01-05 19:06:49 INFO     	 * (global step 29950: loss: 0.5347343981266022, lr: 5e-05
2024-01-05 19:06:56 INFO     	 * (global step 30000: loss: 0.5191867351531982, lr: 5e-05
2024-01-05 19:07:02 INFO     	 * (global step 30050: loss: 0.5250914990901947, lr: 5e-05
2024-01-05 19:07:09 INFO     	 * (global step 30100: loss: 0.5118107199668884, lr: 5e-05
2024-01-05 19:07:15 INFO     	 * (global step 30150: loss: 0.4801890105009079, lr: 5e-05
2024-01-05 19:07:22 INFO     	 * (global step 30200: loss: 0.5973563641309738, lr: 5e-05
2024-01-05 19:07:29 INFO     	 * (global step 30250: loss: 0.5698439478874207, lr: 5e-05
2024-01-05 19:07:35 INFO     	 * (global step 30300: loss: 0.6721609234809875, lr: 5e-05
2024-01-05 19:07:42 INFO     	 * (global step 30350: loss: 0.3417530506849289, lr: 5e-05
2024-01-05 19:07:48 INFO     	 * (global step 30400: loss: 0.5526855289936066, lr: 5e-05
2024-01-05 19:07:55 INFO     	 * (global step 30450: loss: 0.9992088079452515, lr: 5e-05
2024-01-05 19:08:02 INFO     	 * (global step 30500: loss: 0.5401129722595215, lr: 5e-05
2024-01-05 19:08:08 INFO     	 * (global step 30550: loss: 0.3136191666126251, lr: 5e-05
2024-01-05 19:08:15 INFO     	 * (global step 30600: loss: 0.640632688999176, lr: 5e-05
2024-01-05 19:08:22 INFO     	 * (global step 30650: loss: 0.7288487255573273, lr: 5e-05
2024-01-05 19:08:29 INFO     	 * (global step 30700: loss: 0.3679448962211609, lr: 5e-05
2024-01-05 19:08:35 INFO     	 * (global step 30750: loss: 0.5083967596292496, lr: 5e-05
2024-01-05 19:08:42 INFO     	 * (global step 30800: loss: 0.3978351205587387, lr: 5e-05
2024-01-05 19:08:49 INFO     	 * (global step 30850: loss: 0.3572954684495926, lr: 5e-05
2024-01-05 19:08:56 INFO     	 * (global step 30900: loss: 0.6616310477256775, lr: 5e-05
2024-01-05 19:09:02 INFO     	 * (global step 30950: loss: 0.4279777407646179, lr: 5e-05
2024-01-05 19:09:09 INFO     	 * (global step 31000: loss: 0.6502305269241333, lr: 5e-05
2024-01-05 19:09:16 INFO     	 * (global step 31050: loss: 0.3856862485408783, lr: 5e-05
2024-01-05 19:09:22 INFO     	 * (global step 31100: loss: 0.38906237483024597, lr: 5e-05
2024-01-05 19:09:29 INFO     	 * (global step 31150: loss: 0.4524857848882675, lr: 5e-05
2024-01-05 19:09:36 INFO     	 * (global step 31200: loss: 0.4942668378353119, lr: 5e-05
2024-01-05 19:09:42 INFO     	 * (global step 31250: loss: 0.476217657327652, lr: 5e-05
2024-01-05 19:09:48 INFO     	 * (global step 31300: loss: 0.7278552353382111, lr: 5e-05
2024-01-05 19:09:54 INFO     	 * (global step 31350: loss: 0.524539440870285, lr: 5e-05
2024-01-05 19:10:00 INFO     	 * (global step 31400: loss: 0.7485175132751465, lr: 5e-05
2024-01-05 19:10:07 INFO     	 * (global step 31450: loss: 0.5850009471178055, lr: 5e-05
2024-01-05 19:10:14 INFO     	 * (global step 31500: loss: 0.37022434175014496, lr: 5e-05
2024-01-05 19:10:22 INFO     	 * (global step 31550: loss: 0.4097634553909302, lr: 5e-05
2024-01-05 19:10:30 INFO     	 * (global step 31600: loss: 0.5083413571119308, lr: 5e-05
2024-01-05 19:10:37 INFO     	 * (global step 31650: loss: 0.5804620385169983, lr: 5e-05
2024-01-05 19:10:45 INFO     	 * (global step 31700: loss: 0.5946210622787476, lr: 5e-05
2024-01-05 19:10:52 INFO     	 * (global step 31750: loss: 0.5423876643180847, lr: 5e-05
2024-01-05 19:11:00 INFO     	 * (global step 31800: loss: 0.40947510302066803, lr: 5e-05
2024-01-05 19:11:07 INFO     	 * (global step 31850: loss: 0.571436882019043, lr: 5e-05
2024-01-05 19:11:15 INFO     	 * (global step 31900: loss: 0.4681064635515213, lr: 5e-05
2024-01-05 19:11:22 INFO     	 * (global step 31950: loss: 0.4526914805173874, lr: 5e-05
2024-01-05 19:11:30 INFO     	 * (global step 32000: loss: 0.5555878281593323, lr: 5e-05
2024-01-05 19:11:37 INFO     	 * (global step 32050: loss: 0.4968770146369934, lr: 5e-05
2024-01-05 19:11:44 INFO     	 * (global step 32100: loss: 0.5425785779953003, lr: 5e-05
2024-01-05 19:11:52 INFO     	 * (global step 32150: loss: 0.3382249027490616, lr: 5e-05
2024-01-05 19:11:59 INFO     	 * (global step 32200: loss: 0.46097899973392487, lr: 5e-05
2024-01-05 19:12:07 INFO     	 * (global step 32250: loss: 0.3662757873535156, lr: 5e-05
2024-01-05 19:12:14 INFO     	 * (global step 32300: loss: 0.5234182476997375, lr: 5e-05
2024-01-05 19:12:23 INFO     	 * (global step 32350: loss: 0.41644370555877686, lr: 5e-05
2024-01-05 19:12:28 INFO     [epoch 7/15] average loss: 0.517, lr: 5e-05
2024-01-05 19:12:28 INFO     saving model related files
2024-01-05 19:12:28 INFO     saving model
2024-01-05 19:12:29 INFO     saving tokenizer
2024-01-05 19:12:29 INFO     saving optimizer
2024-01-05 19:12:30 INFO     remove old optimizer files
2024-01-05 19:12:32 INFO     	 * (global step 32400: loss: 0.651443213224411, lr: 5e-05
2024-01-05 19:12:38 INFO     	 * (global step 32450: loss: 0.30893197655677795, lr: 5e-05
2024-01-05 19:12:44 INFO     	 * (global step 32500: loss: 0.48735371232032776, lr: 5e-05
2024-01-05 19:12:50 INFO     	 * (global step 32550: loss: 0.424004390835762, lr: 5e-05
2024-01-05 19:12:57 INFO     	 * (global step 32600: loss: 0.5125104039907455, lr: 5e-05
2024-01-05 19:13:05 INFO     	 * (global step 32650: loss: 0.46145007014274597, lr: 5e-05
2024-01-05 19:13:12 INFO     	 * (global step 32700: loss: 0.35756194591522217, lr: 5e-05
2024-01-05 19:13:20 INFO     	 * (global step 32750: loss: 0.4265541136264801, lr: 5e-05
2024-01-05 19:13:27 INFO     	 * (global step 32800: loss: 0.5785884708166122, lr: 5e-05
2024-01-05 19:13:35 INFO     	 * (global step 32850: loss: 0.4118805378675461, lr: 5e-05
2024-01-05 19:13:42 INFO     	 * (global step 32900: loss: 0.47565653920173645, lr: 5e-05
2024-01-05 19:13:50 INFO     	 * (global step 32950: loss: 0.5629478693008423, lr: 5e-05
2024-01-05 19:13:57 INFO     	 * (global step 33000: loss: 0.29821673035621643, lr: 5e-05
2024-01-05 19:14:04 INFO     	 * (global step 33050: loss: 0.4978630989789963, lr: 5e-05
2024-01-05 19:14:12 INFO     	 * (global step 33100: loss: 0.5789423137903214, lr: 5e-05
2024-01-05 19:14:19 INFO     	 * (global step 33150: loss: 0.4091591387987137, lr: 5e-05
2024-01-05 19:14:27 INFO     	 * (global step 33200: loss: 0.44858258962631226, lr: 5e-05
2024-01-05 19:14:34 INFO     	 * (global step 33250: loss: 0.4951242059469223, lr: 5e-05
2024-01-05 19:14:41 INFO     	 * (global step 33300: loss: 0.1875440664589405, lr: 5e-05
2024-01-05 19:14:49 INFO     	 * (global step 33350: loss: 0.5962259471416473, lr: 5e-05
2024-01-05 19:14:56 INFO     	 * (global step 33400: loss: 0.44214875996112823, lr: 5e-05
2024-01-05 19:15:03 INFO     	 * (global step 33450: loss: 0.4621477425098419, lr: 5e-05
2024-01-05 19:15:11 INFO     	 * (global step 33500: loss: 0.5718341618776321, lr: 5e-05
2024-01-05 19:15:19 INFO     	 * (global step 33550: loss: 0.4353422001004219, lr: 5e-05
2024-01-05 19:15:26 INFO     	 * (global step 33600: loss: 0.5502960532903671, lr: 5e-05
2024-01-05 19:15:32 INFO     	 * (global step 33650: loss: 0.6077868491411209, lr: 5e-05
2024-01-05 19:15:37 INFO     	 * (global step 33700: loss: 0.34685809165239334, lr: 5e-05
2024-01-05 19:15:44 INFO     	 * (global step 33750: loss: 0.367112934589386, lr: 5e-05
2024-01-05 19:15:50 INFO     	 * (global step 33800: loss: 0.5559539198875427, lr: 5e-05
2024-01-05 19:15:58 INFO     	 * (global step 33850: loss: 0.45134437084198, lr: 5e-05
2024-01-05 19:16:05 INFO     	 * (global step 33900: loss: 0.4484339654445648, lr: 5e-05
2024-01-05 19:16:13 INFO     	 * (global step 33950: loss: 0.4183300584554672, lr: 5e-05
2024-01-05 19:16:21 INFO     	 * (global step 34000: loss: 0.6397307217121124, lr: 5e-05
2024-01-05 19:16:28 INFO     	 * (global step 34050: loss: 0.8699377328157425, lr: 5e-05
2024-01-05 19:16:36 INFO     	 * (global step 34100: loss: 0.5934635102748871, lr: 5e-05
2024-01-05 19:16:43 INFO     	 * (global step 34150: loss: 0.4198372960090637, lr: 5e-05
2024-01-05 19:16:51 INFO     	 * (global step 34200: loss: 0.5117095857858658, lr: 5e-05
2024-01-05 19:16:58 INFO     	 * (global step 34250: loss: 0.5234377384185791, lr: 5e-05
2024-01-05 19:17:06 INFO     	 * (global step 34300: loss: 0.32630710303783417, lr: 5e-05
2024-01-05 19:17:13 INFO     	 * (global step 34350: loss: 0.5033961534500122, lr: 5e-05
2024-01-05 19:17:21 INFO     	 * (global step 34400: loss: 0.32152844965457916, lr: 5e-05
2024-01-05 19:17:28 INFO     	 * (global step 34450: loss: 0.5436675548553467, lr: 5e-05
2024-01-05 19:17:36 INFO     	 * (global step 34500: loss: 0.31451788544654846, lr: 5e-05
2024-01-05 19:17:44 INFO     	 * (global step 34550: loss: 0.40818409621715546, lr: 5e-05
2024-01-05 19:17:51 INFO     	 * (global step 34600: loss: 0.5860623866319656, lr: 5e-05
2024-01-05 19:17:59 INFO     	 * (global step 34650: loss: 0.45104464888572693, lr: 5e-05
2024-01-05 19:18:06 INFO     	 * (global step 34700: loss: 0.63995561003685, lr: 5e-05
2024-01-05 19:18:13 INFO     	 * (global step 34750: loss: 0.588903397321701, lr: 5e-05
2024-01-05 19:18:19 INFO     	 * (global step 34800: loss: 0.4494805932044983, lr: 5e-05
2024-01-05 19:18:25 INFO     	 * (global step 34850: loss: 0.6520035266876221, lr: 5e-05
2024-01-05 19:18:31 INFO     	 * (global step 34900: loss: 0.4779351204633713, lr: 5e-05
2024-01-05 19:18:37 INFO     	 * (global step 34950: loss: 0.43794772028923035, lr: 5e-05
2024-01-05 19:18:45 INFO     	 * (global step 35000: loss: 0.4578457921743393, lr: 5e-05
2024-01-05 19:18:52 INFO     	 * (global step 35050: loss: 0.8305965662002563, lr: 5e-05
2024-01-05 19:19:00 INFO     	 * (global step 35100: loss: 0.5553458333015442, lr: 5e-05
2024-01-05 19:19:08 INFO     	 * (global step 35150: loss: 0.5152894705533981, lr: 5e-05
2024-01-05 19:19:15 INFO     	 * (global step 35200: loss: 0.5567398816347122, lr: 5e-05
2024-01-05 19:19:23 INFO     	 * (global step 35250: loss: 0.47264181077480316, lr: 5e-05
2024-01-05 19:19:30 INFO     	 * (global step 35300: loss: 0.4514678716659546, lr: 5e-05
2024-01-05 19:19:38 INFO     	 * (global step 35350: loss: 0.4258258789777756, lr: 5e-05
2024-01-05 19:19:45 INFO     	 * (global step 35400: loss: 0.47314949333667755, lr: 5e-05
2024-01-05 19:19:53 INFO     	 * (global step 35450: loss: 0.48427166044712067, lr: 5e-05
2024-01-05 19:20:00 INFO     	 * (global step 35500: loss: 0.4505365192890167, lr: 5e-05
2024-01-05 19:20:08 INFO     	 * (global step 35550: loss: 0.47417183220386505, lr: 5e-05
2024-01-05 19:20:15 INFO     	 * (global step 35600: loss: 0.48448561131954193, lr: 5e-05
2024-01-05 19:20:23 INFO     	 * (global step 35650: loss: 0.30252163857221603, lr: 5e-05
2024-01-05 19:20:30 INFO     	 * (global step 35700: loss: 0.6196186542510986, lr: 5e-05
2024-01-05 19:20:38 INFO     	 * (global step 35750: loss: 0.5236770659685135, lr: 5e-05
2024-01-05 19:20:45 INFO     	 * (global step 35800: loss: 0.45757368206977844, lr: 5e-05
2024-01-05 19:20:53 INFO     	 * (global step 35850: loss: 0.502726212143898, lr: 5e-05
2024-01-05 19:21:01 INFO     	 * (global step 35900: loss: 0.7099941670894623, lr: 5e-05
2024-01-05 19:21:08 INFO     	 * (global step 35950: loss: 0.8206248879432678, lr: 5e-05
2024-01-05 19:21:14 INFO     	 * (global step 36000: loss: 0.5606225281953812, lr: 5e-05
2024-01-05 19:21:20 INFO     	 * (global step 36050: loss: 0.6715207993984222, lr: 5e-05
2024-01-05 19:21:26 INFO     	 * (global step 36100: loss: 0.43218179047107697, lr: 5e-05
2024-01-05 19:21:32 INFO     	 * (global step 36150: loss: 0.5286659896373749, lr: 5e-05
2024-01-05 19:21:39 INFO     	 * (global step 36200: loss: 0.5031812787055969, lr: 5e-05
2024-01-05 19:21:46 INFO     	 * (global step 36250: loss: 0.44554053246974945, lr: 5e-05
2024-01-05 19:21:54 INFO     	 * (global step 36300: loss: 0.4201575517654419, lr: 5e-05
2024-01-05 19:22:01 INFO     	 * (global step 36350: loss: 0.3782188445329666, lr: 5e-05
2024-01-05 19:22:09 INFO     	 * (global step 36400: loss: 0.43410322070121765, lr: 5e-05
2024-01-05 19:22:13 INFO     [epoch 8/15] average loss: 0.508, lr: 5e-05
2024-01-05 19:22:13 INFO     saving model related files
2024-01-05 19:22:13 INFO     saving model
2024-01-05 19:22:14 INFO     saving tokenizer
2024-01-05 19:22:14 INFO     saving optimizer
2024-01-05 19:22:16 INFO     remove old optimizer files
2024-01-05 19:22:19 INFO     	 * (global step 36450: loss: 0.31098659336566925, lr: 5e-05
2024-01-05 19:22:26 INFO     	 * (global step 36500: loss: 0.5610938966274261, lr: 5e-05
2024-01-05 19:22:33 INFO     	 * (global step 36550: loss: 0.553243488073349, lr: 5e-05
2024-01-05 19:22:41 INFO     	 * (global step 36600: loss: 0.35882094502449036, lr: 5e-05
2024-01-05 19:22:48 INFO     	 * (global step 36650: loss: 0.4110525846481323, lr: 5e-05
2024-01-05 19:22:55 INFO     	 * (global step 36700: loss: 0.5497956275939941, lr: 5e-05
2024-01-05 19:23:02 INFO     	 * (global step 36750: loss: 0.4877392649650574, lr: 5e-05
2024-01-05 19:23:09 INFO     	 * (global step 36800: loss: 0.6710655689239502, lr: 5e-05
2024-01-05 19:23:17 INFO     	 * (global step 36850: loss: 0.4855130463838577, lr: 5e-05
2024-01-05 19:23:24 INFO     	 * (global step 36900: loss: 0.4911147505044937, lr: 5e-05
2024-01-05 19:23:31 INFO     	 * (global step 36950: loss: 0.38425345718860626, lr: 5e-05
2024-01-05 19:23:38 INFO     	 * (global step 37000: loss: 0.628566101193428, lr: 5e-05
2024-01-05 19:23:46 INFO     	 * (global step 37050: loss: 0.45005129277706146, lr: 5e-05
2024-01-05 19:23:52 INFO     	 * (global step 37100: loss: 0.5400534421205521, lr: 5e-05
2024-01-05 19:23:58 INFO     	 * (global step 37150: loss: 0.8065824508666992, lr: 5e-05
2024-01-05 19:24:04 INFO     	 * (global step 37200: loss: 0.6228852868080139, lr: 5e-05
2024-01-05 19:24:10 INFO     	 * (global step 37250: loss: 0.5587629228830338, lr: 5e-05
2024-01-05 19:24:17 INFO     	 * (global step 37300: loss: 0.43044161796569824, lr: 5e-05
2024-01-05 19:24:24 INFO     	 * (global step 37350: loss: 0.35935020446777344, lr: 5e-05
2024-01-05 19:24:32 INFO     	 * (global step 37400: loss: 0.7237937450408936, lr: 5e-05
2024-01-05 19:24:40 INFO     	 * (global step 37450: loss: 0.29351821541786194, lr: 5e-05
2024-01-05 19:24:47 INFO     	 * (global step 37500: loss: 0.7380918264389038, lr: 5e-05
2024-01-05 19:24:55 INFO     	 * (global step 37550: loss: 0.3642144054174423, lr: 5e-05
2024-01-05 19:25:03 INFO     	 * (global step 37600: loss: 0.5320923626422882, lr: 5e-05
2024-01-05 19:25:11 INFO     	 * (global step 37650: loss: 0.46813103556632996, lr: 5e-05
2024-01-05 19:25:19 INFO     	 * (global step 37700: loss: 0.5398252308368683, lr: 5e-05
2024-01-05 19:25:26 INFO     	 * (global step 37750: loss: 0.5443337261676788, lr: 5e-05
2024-01-05 19:25:33 INFO     	 * (global step 37800: loss: 0.4412463903427124, lr: 5e-05
2024-01-05 19:25:41 INFO     	 * (global step 37850: loss: 0.39708420634269714, lr: 5e-05
2024-01-05 19:25:49 INFO     	 * (global step 37900: loss: 0.4024782180786133, lr: 5e-05
2024-01-05 19:25:56 INFO     	 * (global step 37950: loss: 0.2976549565792084, lr: 5e-05
2024-01-05 19:26:03 INFO     	 * (global step 38000: loss: 0.5432971119880676, lr: 5e-05
2024-01-05 19:26:11 INFO     	 * (global step 38050: loss: 0.5379088073968887, lr: 5e-05
2024-01-05 19:26:19 INFO     	 * (global step 38100: loss: 0.4481382668018341, lr: 5e-05
2024-01-05 19:26:27 INFO     	 * (global step 38150: loss: 0.5476585924625397, lr: 5e-05
2024-01-05 19:26:34 INFO     	 * (global step 38200: loss: 0.4266364052891731, lr: 5e-05
2024-01-05 19:26:42 INFO     	 * (global step 38250: loss: 0.5597740560770035, lr: 5e-05
2024-01-05 19:26:49 INFO     	 * (global step 38300: loss: 0.4921102076768875, lr: 5e-05
2024-01-05 19:26:57 INFO     	 * (global step 38350: loss: 0.5969580709934235, lr: 5e-05
2024-01-05 19:27:05 INFO     	 * (global step 38400: loss: 0.7170390188694, lr: 5e-05
2024-01-05 19:27:12 INFO     	 * (global step 38450: loss: 0.4849344938993454, lr: 5e-05
2024-01-05 19:27:19 INFO     	 * (global step 38500: loss: 0.4459362328052521, lr: 5e-05
2024-01-05 19:27:25 INFO     	 * (global step 38550: loss: 0.2614109441637993, lr: 5e-05
2024-01-05 19:27:31 INFO     	 * (global step 38600: loss: 0.25843607634305954, lr: 5e-05
2024-01-05 19:27:37 INFO     	 * (global step 38650: loss: 0.7377847731113434, lr: 5e-05
2024-01-05 19:27:43 INFO     	 * (global step 38700: loss: 0.4332466423511505, lr: 5e-05
2024-01-05 19:27:49 INFO     	 * (global step 38750: loss: 0.4669334292411804, lr: 5e-05
2024-01-05 19:27:57 INFO     	 * (global step 38800: loss: 0.25827692449092865, lr: 5e-05
2024-01-05 19:28:05 INFO     	 * (global step 38850: loss: 0.4814508557319641, lr: 5e-05
2024-01-05 19:28:12 INFO     	 * (global step 38900: loss: 0.39576978981494904, lr: 5e-05
2024-01-05 19:28:19 INFO     	 * (global step 38950: loss: 0.6023375988006592, lr: 5e-05
2024-01-05 19:28:27 INFO     	 * (global step 39000: loss: 0.43332114815711975, lr: 5e-05
2024-01-05 19:28:35 INFO     	 * (global step 39050: loss: 0.4910185933113098, lr: 5e-05
2024-01-05 19:28:42 INFO     	 * (global step 39100: loss: 0.47948455810546875, lr: 5e-05
2024-01-05 19:28:49 INFO     	 * (global step 39150: loss: 0.6761064380407333, lr: 5e-05
2024-01-05 19:28:57 INFO     	 * (global step 39200: loss: 0.4303726404905319, lr: 5e-05
2024-01-05 19:29:04 INFO     	 * (global step 39250: loss: 0.5020192712545395, lr: 5e-05
2024-01-05 19:29:12 INFO     	 * (global step 39300: loss: 0.4448082000017166, lr: 5e-05
2024-01-05 19:29:19 INFO     	 * (global step 39350: loss: 0.4415454715490341, lr: 5e-05
2024-01-05 19:29:27 INFO     	 * (global step 39400: loss: 0.42278580367565155, lr: 5e-05
2024-01-05 19:29:34 INFO     	 * (global step 39450: loss: 0.5251545757055283, lr: 5e-05
2024-01-05 19:29:42 INFO     	 * (global step 39500: loss: 0.6206365823745728, lr: 5e-05
2024-01-05 19:29:49 INFO     	 * (global step 39550: loss: 0.5933529138565063, lr: 5e-05
2024-01-05 19:29:57 INFO     	 * (global step 39600: loss: 0.7226487696170807, lr: 5e-05
2024-01-05 19:30:03 INFO     	 * (global step 39650: loss: 0.48430904746055603, lr: 5e-05
2024-01-05 19:30:09 INFO     	 * (global step 39700: loss: 0.44084060192108154, lr: 5e-05
2024-01-05 19:30:15 INFO     	 * (global step 39750: loss: 0.5957741141319275, lr: 5e-05
2024-01-05 19:30:21 INFO     	 * (global step 39800: loss: 0.9193267226219177, lr: 5e-05
2024-01-05 19:30:27 INFO     	 * (global step 39850: loss: 0.762859046459198, lr: 5e-05
2024-01-05 19:30:35 INFO     	 * (global step 39900: loss: 0.4024251252412796, lr: 5e-05
2024-01-05 19:30:42 INFO     	 * (global step 39950: loss: 0.374126672744751, lr: 5e-05
2024-01-05 19:30:50 INFO     	 * (global step 40000: loss: 0.7758389115333557, lr: 5e-05
2024-01-05 19:30:57 INFO     	 * (global step 40050: loss: 0.4864044487476349, lr: 5e-05
2024-01-05 19:31:05 INFO     	 * (global step 40100: loss: 0.5760277807712555, lr: 5e-05
2024-01-05 19:31:12 INFO     	 * (global step 40150: loss: 0.6504160463809967, lr: 5e-05
2024-01-05 19:31:20 INFO     	 * (global step 40200: loss: 0.6027494668960571, lr: 5e-05
2024-01-05 19:31:27 INFO     	 * (global step 40250: loss: 0.6552129089832306, lr: 5e-05
2024-01-05 19:31:35 INFO     	 * (global step 40300: loss: 0.37995152175426483, lr: 5e-05
2024-01-05 19:31:42 INFO     	 * (global step 40350: loss: 0.6231561601161957, lr: 5e-05
2024-01-05 19:31:50 INFO     	 * (global step 40400: loss: 0.3702329844236374, lr: 5e-05
2024-01-05 19:31:57 INFO     	 * (global step 40450: loss: 0.3664312958717346, lr: 5e-05
2024-01-05 19:32:02 INFO     [epoch 9/15] average loss: 0.5, lr: 5e-05
2024-01-05 19:32:02 INFO     saving model related files
2024-01-05 19:32:02 INFO     saving model
2024-01-05 19:32:03 INFO     saving tokenizer
2024-01-05 19:32:03 INFO     saving optimizer
2024-01-05 19:32:05 INFO     remove old optimizer files
2024-01-05 19:32:05 INFO     complete training: model ckpt was saved at small_recreated_ckpt/model_woixzh
2024-01-05 19:32:06 INFO     ## 1st RUN: Configuration 6/12 ##
2024-01-05 19:32:06 INFO     initialize model trainer
2024-01-05 19:32:06 INFO     initialize checkpoint at small_recreated_ckpt/model_sdkaaa
2024-01-05 19:32:06 INFO     hyperparameters
2024-01-05 19:32:06 INFO     	 * dataset_path: lmqg/qag_squad
2024-01-05 19:32:06 INFO     	 * dataset_name: default
2024-01-05 19:32:06 INFO     	 * input_types: ['paragraph']
2024-01-05 19:32:06 INFO     	 * output_types: ['questions_answers']
2024-01-05 19:32:06 INFO     	 * prefix_types: ['qag']
2024-01-05 19:32:06 INFO     	 * model: t5-small
2024-01-05 19:32:06 INFO     	 * max_length: 512
2024-01-05 19:32:06 INFO     	 * max_length_output: 256
2024-01-05 19:32:06 INFO     	 * epoch: 15
2024-01-05 19:32:06 INFO     	 * batch: 2
2024-01-05 19:32:06 INFO     	 * lr: 5e-05
2024-01-05 19:32:06 INFO     	 * fp16: False
2024-01-05 19:32:06 INFO     	 * random_seed: 1
2024-01-05 19:32:06 INFO     	 * gradient_accumulation_steps: 4
2024-01-05 19:32:06 INFO     	 * label_smoothing: 0.0
2024-01-05 19:32:06 INFO     initialize checkpoint with t5-small
2024-01-05 19:32:14 INFO     use spaCy answer extraction model: positionrank
2024-01-05 19:32:17 INFO     Model `t5-small`
2024-01-05 19:32:17 INFO     	 * Num of GPU in use: 1
2024-01-05 19:32:17 INFO     	 * Prefix: True
2024-01-05 19:32:17 INFO     	 * Language: en (ignore at the training phase)
2024-01-05 19:32:17 INFO     dataset preprocessing
2024-01-05 19:32:21 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featurelmqg/qag_squad/t5-small.512.256.paragraph.questions_answers.train.qag.pkl
2024-01-05 19:32:28 INFO     start model training
2024-01-05 19:32:42 INFO     	 * (global step 50: loss: 1.9683461487293243, lr: 5e-05
2024-01-05 19:32:57 INFO     	 * (global step 100: loss: 1.356455773115158, lr: 5e-05
2024-01-05 19:33:09 INFO     	 * (global step 150: loss: 1.171513020992279, lr: 5e-05
2024-01-05 19:33:21 INFO     	 * (global step 200: loss: 0.9567255824804306, lr: 5e-05
2024-01-05 19:33:32 INFO     	 * (global step 250: loss: 0.9093507677316666, lr: 5e-05
2024-01-05 19:33:46 INFO     	 * (global step 300: loss: 0.9120561629533768, lr: 5e-05
2024-01-05 19:34:00 INFO     	 * (global step 350: loss: 0.9905566275119781, lr: 5e-05
2024-01-05 19:34:13 INFO     	 * (global step 400: loss: 1.1026918292045593, lr: 5e-05
2024-01-05 19:34:27 INFO     	 * (global step 450: loss: 0.7788396924734116, lr: 5e-05
2024-01-05 19:34:42 INFO     	 * (global step 500: loss: 0.9390723407268524, lr: 5e-05
2024-01-05 19:34:55 INFO     	 * (global step 550: loss: 0.8875989615917206, lr: 5e-05
2024-01-05 19:35:09 INFO     	 * (global step 600: loss: 1.0437647700309753, lr: 5e-05
2024-01-05 19:35:23 INFO     	 * (global step 650: loss: 0.8059968650341034, lr: 5e-05
2024-01-05 19:35:37 INFO     	 * (global step 700: loss: 0.7046518325805664, lr: 5e-05
2024-01-05 19:35:49 INFO     	 * (global step 750: loss: 0.7406651377677917, lr: 5e-05
2024-01-05 19:36:01 INFO     	 * (global step 800: loss: 0.5896588414907455, lr: 5e-05
2024-01-05 19:36:12 INFO     	 * (global step 850: loss: 0.7721086889505386, lr: 5e-05
2024-01-05 19:36:27 INFO     	 * (global step 900: loss: 0.8177237212657928, lr: 5e-05
2024-01-05 19:36:40 INFO     	 * (global step 950: loss: 0.6391644515097141, lr: 5e-05
2024-01-05 19:36:54 INFO     	 * (global step 1000: loss: 1.1417423635721207, lr: 5e-05
2024-01-05 19:37:07 INFO     	 * (global step 1050: loss: 0.63432177901268, lr: 5e-05
2024-01-05 19:37:21 INFO     	 * (global step 1100: loss: 0.749979630112648, lr: 5e-05
2024-01-05 19:37:35 INFO     	 * (global step 1150: loss: 1.1428949683904648, lr: 5e-05
2024-01-05 19:37:49 INFO     	 * (global step 1200: loss: 0.6139503717422485, lr: 5e-05
2024-01-05 19:38:03 INFO     	 * (global step 1250: loss: 0.5974080041050911, lr: 5e-05
2024-01-05 19:38:16 INFO     	 * (global step 1300: loss: 0.527667723596096, lr: 5e-05
2024-01-05 19:38:28 INFO     	 * (global step 1350: loss: 0.7422125041484833, lr: 5e-05
2024-01-05 19:38:39 INFO     	 * (global step 1400: loss: 0.7432255148887634, lr: 5e-05
2024-01-05 19:38:53 INFO     	 * (global step 1450: loss: 0.8126007616519928, lr: 5e-05
2024-01-05 19:39:07 INFO     	 * (global step 1500: loss: 0.6822608560323715, lr: 5e-05
2024-01-05 19:39:21 INFO     	 * (global step 1550: loss: 0.7468858361244202, lr: 5e-05
2024-01-05 19:39:35 INFO     	 * (global step 1600: loss: 0.7782741039991379, lr: 5e-05
2024-01-05 19:39:49 INFO     	 * (global step 1650: loss: 0.6408639475703239, lr: 5e-05
2024-01-05 19:40:03 INFO     	 * (global step 1700: loss: 0.7089401185512543, lr: 5e-05
2024-01-05 19:40:17 INFO     	 * (global step 1750: loss: 0.6033055335283279, lr: 5e-05
2024-01-05 19:40:32 INFO     	 * (global step 1800: loss: 0.6442025750875473, lr: 5e-05
2024-01-05 19:40:46 INFO     	 * (global step 1850: loss: 0.4581522196531296, lr: 5e-05
2024-01-05 19:41:00 INFO     	 * (global step 1900: loss: 0.5260118767619133, lr: 5e-05
2024-01-05 19:41:14 INFO     	 * (global step 1950: loss: 0.8001906424760818, lr: 5e-05
2024-01-05 19:41:26 INFO     	 * (global step 2000: loss: 0.9214302077889442, lr: 5e-05
2024-01-05 19:41:31 INFO     [epoch 0/15] average loss: 0.844, lr: 5e-05
2024-01-05 19:41:31 INFO     saving model related files
2024-01-05 19:41:31 INFO     saving model
2024-01-05 19:41:32 INFO     saving tokenizer
2024-01-05 19:41:32 INFO     saving optimizer
2024-01-05 19:41:33 INFO     remove old optimizer files
2024-01-05 19:41:39 INFO     	 * (global step 2050: loss: 0.5207161530852318, lr: 5e-05
2024-01-05 19:41:53 INFO     	 * (global step 2100: loss: 0.6235640570521355, lr: 5e-05
2024-01-05 19:42:08 INFO     	 * (global step 2150: loss: 0.8029514849185944, lr: 5e-05
2024-01-05 19:42:23 INFO     	 * (global step 2200: loss: 0.572873480618, lr: 5e-05
2024-01-05 19:42:38 INFO     	 * (global step 2250: loss: 0.5722057521343231, lr: 5e-05
2024-01-05 19:42:54 INFO     	 * (global step 2300: loss: 0.7193123772740364, lr: 5e-05
2024-01-05 19:43:09 INFO     	 * (global step 2350: loss: 0.6858168840408325, lr: 5e-05
2024-01-05 19:43:23 INFO     	 * (global step 2400: loss: 0.6333549693226814, lr: 5e-05
2024-01-05 19:43:38 INFO     	 * (global step 2450: loss: 0.6030596643686295, lr: 5e-05
2024-01-05 19:43:53 INFO     	 * (global step 2500: loss: 0.7331376671791077, lr: 5e-05
2024-01-05 19:44:07 INFO     	 * (global step 2550: loss: 0.5679949596524239, lr: 5e-05
2024-01-05 19:44:21 INFO     	 * (global step 2600: loss: 0.6219414174556732, lr: 5e-05
2024-01-05 19:44:36 INFO     	 * (global step 2650: loss: 0.7452861070632935, lr: 5e-05
2024-01-05 19:44:50 INFO     	 * (global step 2700: loss: 0.5476554483175278, lr: 5e-05
2024-01-05 19:45:02 INFO     	 * (global step 2750: loss: 0.6060774028301239, lr: 5e-05
2024-01-05 19:45:13 INFO     	 * (global step 2800: loss: 0.6290666311979294, lr: 5e-05
2024-01-05 19:45:26 INFO     	 * (global step 2850: loss: 0.7195910513401031, lr: 5e-05
2024-01-05 19:45:41 INFO     	 * (global step 2900: loss: 0.7823194116353989, lr: 5e-05
2024-01-05 19:45:56 INFO     	 * (global step 2950: loss: 0.7296600267291069, lr: 5e-05
2024-01-05 19:46:10 INFO     	 * (global step 3000: loss: 0.6369543671607971, lr: 5e-05
2024-01-05 19:46:25 INFO     	 * (global step 3050: loss: 0.594692274928093, lr: 5e-05
2024-01-05 19:46:39 INFO     	 * (global step 3100: loss: 0.5493701174855232, lr: 5e-05
2024-01-05 19:46:53 INFO     	 * (global step 3150: loss: 0.6956853121519089, lr: 5e-05
2024-01-05 19:47:08 INFO     	 * (global step 3200: loss: 0.546018086373806, lr: 5e-05
2024-01-05 19:47:23 INFO     	 * (global step 3250: loss: 0.7384836450219154, lr: 5e-05
2024-01-05 19:47:37 INFO     	 * (global step 3300: loss: 0.5285576358437538, lr: 5e-05
2024-01-05 19:47:52 INFO     	 * (global step 3350: loss: 0.6356758028268814, lr: 5e-05
2024-01-05 19:48:06 INFO     	 * (global step 3400: loss: 0.7273606508970261, lr: 5e-05
2024-01-05 19:48:19 INFO     	 * (global step 3450: loss: 0.6776612401008606, lr: 5e-05
2024-01-05 19:48:30 INFO     	 * (global step 3500: loss: 0.6288524344563484, lr: 5e-05
2024-01-05 19:48:42 INFO     	 * (global step 3550: loss: 0.5964941903948784, lr: 5e-05
2024-01-05 19:48:56 INFO     	 * (global step 3600: loss: 0.6519294083118439, lr: 5e-05
2024-01-05 19:49:10 INFO     	 * (global step 3650: loss: 0.7636253237724304, lr: 5e-05
2024-01-05 19:49:25 INFO     	 * (global step 3700: loss: 0.9449896961450577, lr: 5e-05
2024-01-05 19:49:39 INFO     	 * (global step 3750: loss: 0.6208747550845146, lr: 5e-05
2024-01-05 19:49:53 INFO     	 * (global step 3800: loss: 0.5737085565924644, lr: 5e-05
2024-01-05 19:50:08 INFO     	 * (global step 3850: loss: 0.8724498301744461, lr: 5e-05
2024-01-05 19:50:22 INFO     	 * (global step 3900: loss: 0.5949727147817612, lr: 5e-05
2024-01-05 19:50:36 INFO     	 * (global step 3950: loss: 0.46728190034627914, lr: 5e-05
2024-01-05 19:50:50 INFO     	 * (global step 4000: loss: 0.5433912053704262, lr: 5e-05
2024-01-05 19:51:04 INFO     [epoch 1/15] average loss: 0.649, lr: 5e-05
2024-01-05 19:51:04 INFO     saving model related files
2024-01-05 19:51:04 INFO     saving model
2024-01-05 19:51:05 INFO     saving tokenizer
2024-01-05 19:51:05 INFO     saving optimizer
2024-01-05 19:51:06 INFO     remove old optimizer files
2024-01-05 19:51:07 INFO     	 * (global step 4050: loss: 0.46088119223713875, lr: 5e-05
2024-01-05 19:51:19 INFO     	 * (global step 4100: loss: 0.5883832946419716, lr: 5e-05
2024-01-05 19:51:30 INFO     	 * (global step 4150: loss: 0.5952820628881454, lr: 5e-05
2024-01-05 19:51:43 INFO     	 * (global step 4200: loss: 0.8200926184654236, lr: 5e-05
2024-01-05 19:51:57 INFO     	 * (global step 4250: loss: 0.6265074834227562, lr: 5e-05
2024-01-05 19:52:11 INFO     	 * (global step 4300: loss: 0.45802123844623566, lr: 5e-05
2024-01-05 19:52:25 INFO     	 * (global step 4350: loss: 0.6519149094820023, lr: 5e-05
2024-01-05 19:52:39 INFO     	 * (global step 4400: loss: 0.4869554936885834, lr: 5e-05
2024-01-05 19:52:53 INFO     	 * (global step 4450: loss: 0.5146061778068542, lr: 5e-05
2024-01-05 19:53:07 INFO     	 * (global step 4500: loss: 0.6014266535639763, lr: 5e-05
2024-01-05 19:53:21 INFO     	 * (global step 4550: loss: 0.5808875784277916, lr: 5e-05
2024-01-05 19:53:34 INFO     	 * (global step 4600: loss: 0.5335203930735588, lr: 5e-05
2024-01-05 19:53:48 INFO     	 * (global step 4650: loss: 0.8149071931838989, lr: 5e-05
2024-01-05 19:53:59 INFO     	 * (global step 4700: loss: 0.6368646919727325, lr: 5e-05
2024-01-05 19:54:11 INFO     	 * (global step 4750: loss: 0.5615806430578232, lr: 5e-05
2024-01-05 19:54:24 INFO     	 * (global step 4800: loss: 0.5962616503238678, lr: 5e-05
2024-01-05 19:54:39 INFO     	 * (global step 4850: loss: 0.6198903918266296, lr: 5e-05
2024-01-05 19:54:54 INFO     	 * (global step 4900: loss: 0.515105664730072, lr: 5e-05
2024-01-05 19:55:09 INFO     	 * (global step 4950: loss: 0.48833467066287994, lr: 5e-05
2024-01-05 19:55:23 INFO     	 * (global step 5000: loss: 0.5380774512887001, lr: 5e-05
2024-01-05 19:55:37 INFO     	 * (global step 5050: loss: 0.7409514486789703, lr: 5e-05
2024-01-05 19:55:51 INFO     	 * (global step 5100: loss: 0.5544579476118088, lr: 5e-05
2024-01-05 19:56:06 INFO     	 * (global step 5150: loss: 0.656633049249649, lr: 5e-05
2024-01-05 19:56:20 INFO     	 * (global step 5200: loss: 0.5846219807863235, lr: 5e-05
2024-01-05 19:56:34 INFO     	 * (global step 5250: loss: 0.5705863386392593, lr: 5e-05
2024-01-05 19:56:48 INFO     	 * (global step 5300: loss: 0.5374017357826233, lr: 5e-05
2024-01-05 19:56:59 INFO     	 * (global step 5350: loss: 0.6392663568258286, lr: 5e-05
2024-01-05 19:57:11 INFO     	 * (global step 5400: loss: 0.6311511918902397, lr: 5e-05
2024-01-05 19:57:24 INFO     	 * (global step 5450: loss: 0.5647475272417068, lr: 5e-05
2024-01-05 19:57:38 INFO     	 * (global step 5500: loss: 0.5222877115011215, lr: 5e-05
2024-01-05 19:57:53 INFO     	 * (global step 5550: loss: 0.6356829479336739, lr: 5e-05
2024-01-05 19:58:07 INFO     	 * (global step 5600: loss: 0.6626552864909172, lr: 5e-05
2024-01-05 19:58:22 INFO     	 * (global step 5650: loss: 0.5433778166770935, lr: 5e-05
2024-01-05 19:58:36 INFO     	 * (global step 5700: loss: 0.6358453035354614, lr: 5e-05
2024-01-05 19:58:50 INFO     	 * (global step 5750: loss: 0.5996612757444382, lr: 5e-05
2024-01-05 19:59:05 INFO     	 * (global step 5800: loss: 0.6132113113999367, lr: 5e-05
2024-01-05 19:59:20 INFO     	 * (global step 5850: loss: 0.7014027386903763, lr: 5e-05
2024-01-05 19:59:34 INFO     	 * (global step 5900: loss: 0.5967185944318771, lr: 5e-05
2024-01-05 19:59:46 INFO     	 * (global step 5950: loss: 0.5329428091645241, lr: 5e-05
2024-01-05 19:59:58 INFO     	 * (global step 6000: loss: 0.5410545133054256, lr: 5e-05
2024-01-05 20:00:09 INFO     	 * (global step 6050: loss: 0.7441586256027222, lr: 5e-05
2024-01-05 20:00:16 INFO     [epoch 2/15] average loss: 0.613, lr: 5e-05
2024-01-05 20:00:16 INFO     saving model related files
2024-01-05 20:00:16 INFO     saving model
2024-01-05 20:00:17 INFO     saving tokenizer
2024-01-05 20:00:17 INFO     saving optimizer
2024-01-05 20:00:18 INFO     remove old optimizer files
2024-01-05 20:00:26 INFO     	 * (global step 6100: loss: 0.6218877583742142, lr: 5e-05
2024-01-05 20:00:40 INFO     	 * (global step 6150: loss: 0.5763398557901382, lr: 5e-05
2024-01-05 20:00:54 INFO     	 * (global step 6200: loss: 0.6379717662930489, lr: 5e-05
2024-01-05 20:01:08 INFO     	 * (global step 6250: loss: 0.6541084200143814, lr: 5e-05
2024-01-05 20:01:22 INFO     	 * (global step 6300: loss: 0.49488699436187744, lr: 5e-05
2024-01-05 20:01:36 INFO     	 * (global step 6350: loss: 0.7998945713043213, lr: 5e-05
2024-01-05 20:01:50 INFO     	 * (global step 6400: loss: 0.6304809153079987, lr: 5e-05
2024-01-05 20:02:04 INFO     	 * (global step 6450: loss: 0.7122280895709991, lr: 5e-05
2024-01-05 20:02:18 INFO     	 * (global step 6500: loss: 0.6272619813680649, lr: 5e-05
2024-01-05 20:02:30 INFO     	 * (global step 6550: loss: 0.4856308326125145, lr: 5e-05
2024-01-05 20:02:42 INFO     	 * (global step 6600: loss: 0.5010330080986023, lr: 5e-05
2024-01-05 20:02:54 INFO     	 * (global step 6650: loss: 0.46497008204460144, lr: 5e-05
2024-01-05 20:03:08 INFO     	 * (global step 6700: loss: 0.5598894581198692, lr: 5e-05
2024-01-05 20:03:22 INFO     	 * (global step 6750: loss: 0.5743438377976418, lr: 5e-05
2024-01-05 20:03:36 INFO     	 * (global step 6800: loss: 0.6616055890917778, lr: 5e-05
2024-01-05 20:03:50 INFO     	 * (global step 6850: loss: 0.5218518674373627, lr: 5e-05
2024-01-05 20:04:04 INFO     	 * (global step 6900: loss: 0.5212390869855881, lr: 5e-05
2024-01-05 20:04:17 INFO     	 * (global step 6950: loss: 0.5328209474682808, lr: 5e-05
2024-01-05 20:04:31 INFO     	 * (global step 7000: loss: 0.435153491795063, lr: 5e-05
2024-01-05 20:04:45 INFO     	 * (global step 7050: loss: 0.6832427158951759, lr: 5e-05
2024-01-05 20:04:59 INFO     	 * (global step 7100: loss: 0.6048837974667549, lr: 5e-05
2024-01-05 20:05:11 INFO     	 * (global step 7150: loss: 0.5163929164409637, lr: 5e-05
2024-01-05 20:05:22 INFO     	 * (global step 7200: loss: 0.6065421998500824, lr: 5e-05
2024-01-05 20:05:35 INFO     	 * (global step 7250: loss: 0.6047801673412323, lr: 5e-05
2024-01-05 20:05:49 INFO     	 * (global step 7300: loss: 0.5213684663176537, lr: 5e-05
2024-01-05 20:06:03 INFO     	 * (global step 7350: loss: 0.5975173935294151, lr: 5e-05
2024-01-05 20:06:17 INFO     	 * (global step 7400: loss: 0.516302801668644, lr: 5e-05
2024-01-05 20:06:30 INFO     	 * (global step 7450: loss: 0.5236732587218285, lr: 5e-05
2024-01-05 20:06:45 INFO     	 * (global step 7500: loss: 0.6193896606564522, lr: 5e-05
2024-01-05 20:06:59 INFO     	 * (global step 7550: loss: 0.706953763961792, lr: 5e-05
2024-01-05 20:07:13 INFO     	 * (global step 7600: loss: 0.5242600440979004, lr: 5e-05
2024-01-05 20:07:27 INFO     	 * (global step 7650: loss: 0.5692737624049187, lr: 5e-05
2024-01-05 20:07:39 INFO     	 * (global step 7700: loss: 0.48906778544187546, lr: 5e-05
2024-01-05 20:07:51 INFO     	 * (global step 7750: loss: 0.47431453317403793, lr: 5e-05
2024-01-05 20:08:03 INFO     	 * (global step 7800: loss: 0.5315188467502594, lr: 5e-05
2024-01-05 20:08:17 INFO     	 * (global step 7850: loss: 0.6419678330421448, lr: 5e-05
2024-01-05 20:08:31 INFO     	 * (global step 7900: loss: 0.510166697204113, lr: 5e-05
2024-01-05 20:08:45 INFO     	 * (global step 7950: loss: 0.6362283602356911, lr: 5e-05
2024-01-05 20:08:59 INFO     	 * (global step 8000: loss: 0.6043230593204498, lr: 5e-05
2024-01-05 20:09:13 INFO     	 * (global step 8050: loss: 0.6638045907020569, lr: 5e-05
2024-01-05 20:09:25 INFO     [epoch 3/15] average loss: 0.59, lr: 5e-05
2024-01-05 20:09:25 INFO     saving model related files
2024-01-05 20:09:25 INFO     saving model
2024-01-05 20:09:26 INFO     saving tokenizer
2024-01-05 20:09:26 INFO     saving optimizer
2024-01-05 20:09:28 INFO     remove old optimizer files
2024-01-05 20:09:29 INFO     	 * (global step 8100: loss: 0.6232620179653168, lr: 5e-05
2024-01-05 20:09:42 INFO     	 * (global step 8150: loss: 0.6177085340023041, lr: 5e-05
2024-01-05 20:09:56 INFO     	 * (global step 8200: loss: 0.4246470481157303, lr: 5e-05
2024-01-05 20:10:09 INFO     	 * (global step 8250: loss: 0.524510957300663, lr: 5e-05
2024-01-05 20:10:21 INFO     	 * (global step 8300: loss: 0.51724524050951, lr: 5e-05
2024-01-05 20:10:32 INFO     	 * (global step 8350: loss: 0.7287378311157227, lr: 5e-05
2024-01-05 20:10:45 INFO     	 * (global step 8400: loss: 0.6151045486330986, lr: 5e-05
2024-01-05 20:10:59 INFO     	 * (global step 8450: loss: 0.5654485523700714, lr: 5e-05
2024-01-05 20:11:12 INFO     	 * (global step 8500: loss: 0.6520281881093979, lr: 5e-05
2024-01-05 20:11:26 INFO     	 * (global step 8550: loss: 0.4243975132703781, lr: 5e-05
2024-01-05 20:11:41 INFO     	 * (global step 8600: loss: 0.41564276069402695, lr: 5e-05
2024-01-05 20:11:55 INFO     	 * (global step 8650: loss: 0.5498397052288055, lr: 5e-05
2024-01-05 20:12:08 INFO     	 * (global step 8700: loss: 0.7100074589252472, lr: 5e-05
2024-01-05 20:12:22 INFO     	 * (global step 8750: loss: 0.5010459050536156, lr: 5e-05
2024-01-05 20:12:35 INFO     	 * (global step 8800: loss: 0.6550595536828041, lr: 5e-05
2024-01-05 20:12:48 INFO     	 * (global step 8850: loss: 0.6281488686800003, lr: 5e-05
2024-01-05 20:12:59 INFO     	 * (global step 8900: loss: 0.5527530536055565, lr: 5e-05
2024-01-05 20:13:11 INFO     	 * (global step 8950: loss: 0.5842651724815369, lr: 5e-05
2024-01-05 20:13:25 INFO     	 * (global step 9000: loss: 0.5590618699789047, lr: 5e-05
2024-01-05 20:13:40 INFO     	 * (global step 9050: loss: 0.5390918105840683, lr: 5e-05
2024-01-05 20:13:56 INFO     	 * (global step 9100: loss: 0.5032876878976822, lr: 5e-05
2024-01-05 20:14:10 INFO     	 * (global step 9150: loss: 0.35514408349990845, lr: 5e-05
2024-01-05 20:14:25 INFO     	 * (global step 9200: loss: 0.7019302397966385, lr: 5e-05
2024-01-05 20:14:40 INFO     	 * (global step 9250: loss: 0.4671875834465027, lr: 5e-05
2024-01-05 20:14:55 INFO     	 * (global step 9300: loss: 0.5589302405714989, lr: 5e-05
2024-01-05 20:15:11 INFO     	 * (global step 9350: loss: 0.5889337733387947, lr: 5e-05
2024-01-05 20:15:26 INFO     	 * (global step 9400: loss: 0.7247853353619576, lr: 5e-05
2024-01-05 20:15:41 INFO     	 * (global step 9450: loss: 0.5807426869869232, lr: 5e-05
2024-01-05 20:15:56 INFO     	 * (global step 9500: loss: 0.5442078039050102, lr: 5e-05
2024-01-05 20:16:09 INFO     	 * (global step 9550: loss: 0.5547611266374588, lr: 5e-05
2024-01-05 20:16:20 INFO     	 * (global step 9600: loss: 0.4682481214404106, lr: 5e-05
2024-01-05 20:16:32 INFO     	 * (global step 9650: loss: 0.48092224448919296, lr: 5e-05
2024-01-05 20:16:46 INFO     	 * (global step 9700: loss: 0.5899596810340881, lr: 5e-05
2024-01-05 20:17:00 INFO     	 * (global step 9750: loss: 0.5030924901366234, lr: 5e-05
2024-01-05 20:17:14 INFO     	 * (global step 9800: loss: 0.5874130502343178, lr: 5e-05
2024-01-05 20:17:28 INFO     	 * (global step 9850: loss: 0.4291035383939743, lr: 5e-05
2024-01-05 20:17:42 INFO     	 * (global step 9900: loss: 0.5209133699536324, lr: 5e-05
2024-01-05 20:17:57 INFO     	 * (global step 9950: loss: 0.5146957412362099, lr: 5e-05
2024-01-05 20:18:11 INFO     	 * (global step 10000: loss: 0.4942067041993141, lr: 5e-05
2024-01-05 20:18:25 INFO     	 * (global step 10050: loss: 0.5716823264956474, lr: 5e-05
2024-01-05 20:18:39 INFO     	 * (global step 10100: loss: 0.49755043536424637, lr: 5e-05
2024-01-05 20:18:44 INFO     [epoch 4/15] average loss: 0.574, lr: 5e-05
2024-01-05 20:18:44 INFO     saving model related files
2024-01-05 20:18:44 INFO     saving model
2024-01-05 20:18:45 INFO     saving tokenizer
2024-01-05 20:18:45 INFO     saving optimizer
2024-01-05 20:18:47 INFO     remove old optimizer files
2024-01-05 20:18:55 INFO     	 * (global step 10150: loss: 0.44564832001924515, lr: 5e-05
2024-01-05 20:19:06 INFO     	 * (global step 10200: loss: 0.503976546227932, lr: 5e-05
2024-01-05 20:19:18 INFO     	 * (global step 10250: loss: 0.43054844439029694, lr: 5e-05
2024-01-05 20:19:32 INFO     	 * (global step 10300: loss: 0.49593163281679153, lr: 5e-05
2024-01-05 20:19:47 INFO     	 * (global step 10350: loss: 0.653833232820034, lr: 5e-05
2024-01-05 20:20:01 INFO     	 * (global step 10400: loss: 0.5750895440578461, lr: 5e-05
2024-01-05 20:20:16 INFO     	 * (global step 10450: loss: 0.4785066992044449, lr: 5e-05
2024-01-05 20:20:30 INFO     	 * (global step 10500: loss: 0.5268029272556305, lr: 5e-05
2024-01-05 20:20:46 INFO     	 * (global step 10550: loss: 0.5192427709698677, lr: 5e-05
2024-01-05 20:21:00 INFO     	 * (global step 10600: loss: 0.5693782269954681, lr: 5e-05
2024-01-05 20:21:14 INFO     	 * (global step 10650: loss: 0.6229522675275803, lr: 5e-05
2024-01-05 20:21:29 INFO     	 * (global step 10700: loss: 0.5297592207789421, lr: 5e-05
2024-01-05 20:21:42 INFO     	 * (global step 10750: loss: 0.4118359386920929, lr: 5e-05
2024-01-05 20:21:54 INFO     	 * (global step 10800: loss: 0.5673370212316513, lr: 5e-05
2024-01-05 20:22:05 INFO     	 * (global step 10850: loss: 0.542991153895855, lr: 5e-05
2024-01-05 20:22:19 INFO     	 * (global step 10900: loss: 0.6412922739982605, lr: 5e-05
2024-01-05 20:22:33 INFO     	 * (global step 10950: loss: 0.5749920159578323, lr: 5e-05
2024-01-05 20:22:47 INFO     	 * (global step 11000: loss: 0.49513351917266846, lr: 5e-05
2024-01-05 20:23:01 INFO     	 * (global step 11050: loss: 0.5275279432535172, lr: 5e-05
2024-01-05 20:23:16 INFO     	 * (global step 11100: loss: 0.5223701596260071, lr: 5e-05
2024-01-05 20:23:30 INFO     	 * (global step 11150: loss: 0.501884363591671, lr: 5e-05
2024-01-05 20:23:43 INFO     	 * (global step 11200: loss: 0.5176438987255096, lr: 5e-05
2024-01-05 20:23:57 INFO     	 * (global step 11250: loss: 0.687283992767334, lr: 5e-05
2024-01-05 20:24:12 INFO     	 * (global step 11300: loss: 0.5218319520354271, lr: 5e-05
2024-01-05 20:24:26 INFO     	 * (global step 11350: loss: 0.5056614577770233, lr: 5e-05
2024-01-05 20:24:38 INFO     	 * (global step 11400: loss: 0.5320426225662231, lr: 5e-05
2024-01-05 20:24:50 INFO     	 * (global step 11450: loss: 0.4070732370018959, lr: 5e-05
2024-01-05 20:25:02 INFO     	 * (global step 11500: loss: 0.5191408917307854, lr: 5e-05
2024-01-05 20:25:16 INFO     	 * (global step 11550: loss: 0.5881900787353516, lr: 5e-05
2024-01-05 20:25:30 INFO     	 * (global step 11600: loss: 0.4889931157231331, lr: 5e-05
2024-01-05 20:25:44 INFO     	 * (global step 11650: loss: 0.335394985973835, lr: 5e-05
2024-01-05 20:25:58 INFO     	 * (global step 11700: loss: 0.5078701227903366, lr: 5e-05
2024-01-05 20:26:13 INFO     	 * (global step 11750: loss: 0.6092711389064789, lr: 5e-05
2024-01-05 20:26:27 INFO     	 * (global step 11800: loss: 0.5973800718784332, lr: 5e-05
2024-01-05 20:26:41 INFO     	 * (global step 11850: loss: 0.617163609713316, lr: 5e-05
2024-01-05 20:26:56 INFO     	 * (global step 11900: loss: 0.44471127539873123, lr: 5e-05
2024-01-05 20:27:10 INFO     	 * (global step 11950: loss: 0.45790495723485947, lr: 5e-05
2024-01-05 20:27:22 INFO     	 * (global step 12000: loss: 0.4564303681254387, lr: 5e-05
2024-01-05 20:27:33 INFO     	 * (global step 12050: loss: 0.4400981366634369, lr: 5e-05
2024-01-05 20:27:46 INFO     	 * (global step 12100: loss: 0.5254119858145714, lr: 5e-05
2024-01-05 20:27:58 INFO     [epoch 5/15] average loss: 0.561, lr: 5e-05
2024-01-05 20:27:58 INFO     saving model related files
2024-01-05 20:27:58 INFO     saving model
2024-01-05 20:27:59 INFO     saving tokenizer
2024-01-05 20:27:59 INFO     saving optimizer
2024-01-05 20:28:01 INFO     remove old optimizer files
2024-01-05 20:28:03 INFO     	 * (global step 12150: loss: 0.5068404898047447, lr: 5e-05
2024-01-05 20:28:17 INFO     	 * (global step 12200: loss: 0.5357495695352554, lr: 5e-05
2024-01-05 20:28:30 INFO     	 * (global step 12250: loss: 0.6559005677700043, lr: 5e-05
2024-01-05 20:28:44 INFO     	 * (global step 12300: loss: 0.649157777428627, lr: 5e-05
2024-01-05 20:28:58 INFO     	 * (global step 12350: loss: 0.47106895223259926, lr: 5e-05
2024-01-05 20:29:12 INFO     	 * (global step 12400: loss: 0.5785598680377007, lr: 5e-05
2024-01-05 20:29:26 INFO     	 * (global step 12450: loss: 0.7053924798965454, lr: 5e-05
2024-01-05 20:29:39 INFO     	 * (global step 12500: loss: 0.42895985022187233, lr: 5e-05
2024-01-05 20:29:53 INFO     	 * (global step 12550: loss: 0.5375899225473404, lr: 5e-05
2024-01-05 20:30:05 INFO     	 * (global step 12600: loss: 0.6656161397695541, lr: 5e-05
2024-01-05 20:30:16 INFO     	 * (global step 12650: loss: 0.6518242806196213, lr: 5e-05
2024-01-05 20:30:29 INFO     	 * (global step 12700: loss: 0.5939182788133621, lr: 5e-05
2024-01-05 20:30:43 INFO     	 * (global step 12750: loss: 0.5751826018095016, lr: 5e-05
2024-01-05 20:30:57 INFO     	 * (global step 12800: loss: 0.8242700397968292, lr: 5e-05
2024-01-05 20:31:11 INFO     	 * (global step 12850: loss: 0.5723149254918098, lr: 5e-05
2024-01-05 20:31:24 INFO     	 * (global step 12900: loss: 0.5638207793235779, lr: 5e-05
2024-01-05 20:31:38 INFO     	 * (global step 12950: loss: 0.560040608048439, lr: 5e-05
2024-01-05 20:31:53 INFO     	 * (global step 13000: loss: 0.4722724184393883, lr: 5e-05
2024-01-05 20:32:06 INFO     	 * (global step 13050: loss: 0.5525238662958145, lr: 5e-05
2024-01-05 20:32:20 INFO     	 * (global step 13100: loss: 0.5438164919614792, lr: 5e-05
2024-01-05 20:32:34 INFO     	 * (global step 13150: loss: 0.6028380766510963, lr: 5e-05
2024-01-05 20:32:47 INFO     	 * (global step 13200: loss: 0.49883243441581726, lr: 5e-05
2024-01-05 20:32:59 INFO     	 * (global step 13250: loss: 0.6048965081572533, lr: 5e-05
2024-01-05 20:33:11 INFO     	 * (global step 13300: loss: 0.5580820590257645, lr: 5e-05
2024-01-05 20:33:23 INFO     	 * (global step 13350: loss: 0.6987747922539711, lr: 5e-05
2024-01-05 20:33:38 INFO     	 * (global step 13400: loss: 0.5896581709384918, lr: 5e-05
2024-01-05 20:33:52 INFO     	 * (global step 13450: loss: 0.5946943610906601, lr: 5e-05
2024-01-05 20:34:07 INFO     	 * (global step 13500: loss: 0.5631847307085991, lr: 5e-05
2024-01-05 20:34:21 INFO     	 * (global step 13550: loss: 0.4597732722759247, lr: 5e-05
2024-01-05 20:34:35 INFO     	 * (global step 13600: loss: 0.4189523681998253, lr: 5e-05
2024-01-05 20:34:50 INFO     	 * (global step 13650: loss: 0.5318679139018059, lr: 5e-05
2024-01-05 20:35:04 INFO     	 * (global step 13700: loss: 0.6411689594388008, lr: 5e-05
2024-01-05 20:35:18 INFO     	 * (global step 13750: loss: 0.5305882692337036, lr: 5e-05
2024-01-05 20:35:33 INFO     	 * (global step 13800: loss: 0.4361007437109947, lr: 5e-05
2024-01-05 20:35:47 INFO     	 * (global step 13850: loss: 0.5908961296081543, lr: 5e-05
2024-01-05 20:35:59 INFO     	 * (global step 13900: loss: 0.5012756437063217, lr: 5e-05
2024-01-05 20:36:11 INFO     	 * (global step 13950: loss: 0.533865325152874, lr: 5e-05
2024-01-05 20:36:22 INFO     	 * (global step 14000: loss: 0.5828364193439484, lr: 5e-05
2024-01-05 20:36:37 INFO     	 * (global step 14050: loss: 0.6213707253336906, lr: 5e-05
2024-01-05 20:36:52 INFO     	 * (global step 14100: loss: 0.5626535639166832, lr: 5e-05
2024-01-05 20:37:05 INFO     	 * (global step 14150: loss: 0.5792265012860298, lr: 5e-05
2024-01-05 20:37:10 INFO     [epoch 6/15] average loss: 0.55, lr: 5e-05
2024-01-05 20:37:10 INFO     saving model related files
2024-01-05 20:37:10 INFO     saving model
2024-01-05 20:37:11 INFO     saving tokenizer
2024-01-05 20:37:11 INFO     saving optimizer
2024-01-05 20:37:13 INFO     remove old optimizer files
2024-01-05 20:37:23 INFO     	 * (global step 14200: loss: 0.5344368442893028, lr: 5e-05
2024-01-05 20:37:37 INFO     	 * (global step 14250: loss: 0.5642394572496414, lr: 5e-05
2024-01-05 20:37:50 INFO     	 * (global step 14300: loss: 0.5555459782481194, lr: 5e-05
2024-01-05 20:38:05 INFO     	 * (global step 14350: loss: 0.632681094110012, lr: 5e-05
2024-01-05 20:38:19 INFO     	 * (global step 14400: loss: 0.49595674127340317, lr: 5e-05
2024-01-05 20:38:33 INFO     	 * (global step 14450: loss: 0.5849487483501434, lr: 5e-05
2024-01-05 20:38:45 INFO     	 * (global step 14500: loss: 0.5514007359743118, lr: 5e-05
2024-01-05 20:38:57 INFO     	 * (global step 14550: loss: 0.4994504302740097, lr: 5e-05
2024-01-05 20:39:09 INFO     	 * (global step 14600: loss: 0.465929064899683, lr: 5e-05
2024-01-05 20:39:23 INFO     	 * (global step 14650: loss: 0.4974125176668167, lr: 5e-05
2024-01-05 20:39:37 INFO     	 * (global step 14700: loss: 0.5133092626929283, lr: 5e-05
2024-01-05 20:39:52 INFO     	 * (global step 14750: loss: 0.39549680054187775, lr: 5e-05
2024-01-05 20:40:05 INFO     	 * (global step 14800: loss: 0.49810267984867096, lr: 5e-05
2024-01-05 20:40:20 INFO     	 * (global step 14850: loss: 0.4990215599536896, lr: 5e-05
2024-01-05 20:40:33 INFO     	 * (global step 14900: loss: 0.6242067441344261, lr: 5e-05
2024-01-05 20:40:47 INFO     	 * (global step 14950: loss: 0.5966302901506424, lr: 5e-05
2024-01-05 20:41:00 INFO     	 * (global step 15000: loss: 0.616851694881916, lr: 5e-05
2024-01-05 20:41:14 INFO     	 * (global step 15050: loss: 0.5198159143328667, lr: 5e-05
2024-01-05 20:41:27 INFO     	 * (global step 15100: loss: 0.5804509446024895, lr: 5e-05
2024-01-05 20:41:38 INFO     	 * (global step 15150: loss: 0.8575648814439774, lr: 5e-05
2024-01-05 20:41:49 INFO     	 * (global step 15200: loss: 0.5521436110138893, lr: 5e-05
2024-01-05 20:42:03 INFO     	 * (global step 15250: loss: 0.4405636563897133, lr: 5e-05
2024-01-05 20:42:18 INFO     	 * (global step 15300: loss: 0.579118549823761, lr: 5e-05
2024-01-05 20:42:32 INFO     	 * (global step 15350: loss: 0.5518776699900627, lr: 5e-05
2024-01-05 20:42:45 INFO     	 * (global step 15400: loss: 0.4687693640589714, lr: 5e-05
2024-01-05 20:43:00 INFO     	 * (global step 15450: loss: 0.6268031224608421, lr: 5e-05
2024-01-05 20:43:13 INFO     	 * (global step 15500: loss: 0.5686560124158859, lr: 5e-05
2024-01-05 20:43:26 INFO     	 * (global step 15550: loss: 0.48154035210609436, lr: 5e-05
2024-01-05 20:43:40 INFO     	 * (global step 15600: loss: 0.5493120886385441, lr: 5e-05
2024-01-05 20:43:54 INFO     	 * (global step 15650: loss: 0.6336802244186401, lr: 5e-05
2024-01-05 20:44:07 INFO     	 * (global step 15700: loss: 0.6124756410717964, lr: 5e-05
2024-01-05 20:44:18 INFO     	 * (global step 15750: loss: 0.4567703753709793, lr: 5e-05
2024-01-05 20:44:30 INFO     	 * (global step 15800: loss: 0.49711647629737854, lr: 5e-05
2024-01-05 20:44:44 INFO     	 * (global step 15850: loss: 0.6824605017900467, lr: 5e-05
2024-01-05 20:44:59 INFO     	 * (global step 15900: loss: 0.44706766307353973, lr: 5e-05
2024-01-05 20:45:13 INFO     	 * (global step 15950: loss: 0.47531720995903015, lr: 5e-05
2024-01-05 20:45:28 INFO     	 * (global step 16000: loss: 0.5765651911497116, lr: 5e-05
2024-01-05 20:45:42 INFO     	 * (global step 16050: loss: 0.5534645915031433, lr: 5e-05
2024-01-05 20:45:57 INFO     	 * (global step 16100: loss: 0.5119264498353004, lr: 5e-05
2024-01-05 20:46:12 INFO     	 * (global step 16150: loss: 0.6435621157288551, lr: 5e-05
2024-01-05 20:46:24 INFO     [epoch 7/15] average loss: 0.541, lr: 5e-05
2024-01-05 20:46:24 INFO     saving model related files
2024-01-05 20:46:24 INFO     saving model
2024-01-05 20:46:25 INFO     saving tokenizer
2024-01-05 20:46:25 INFO     saving optimizer
2024-01-05 20:46:27 INFO     remove old optimizer files
2024-01-05 20:46:29 INFO     	 * (global step 16200: loss: 0.5465739220380783, lr: 5e-05
2024-01-05 20:46:44 INFO     	 * (global step 16250: loss: 0.49447036534547806, lr: 5e-05
2024-01-05 20:46:59 INFO     	 * (global step 16300: loss: 0.5240076035261154, lr: 5e-05
2024-01-05 20:47:14 INFO     	 * (global step 16350: loss: 0.48291415721178055, lr: 5e-05
2024-01-05 20:47:25 INFO     	 * (global step 16400: loss: 0.5534755140542984, lr: 5e-05
2024-01-05 20:47:37 INFO     	 * (global step 16450: loss: 0.6505303084850311, lr: 5e-05
2024-01-05 20:47:50 INFO     	 * (global step 16500: loss: 0.42265684902668, lr: 5e-05
2024-01-05 20:48:04 INFO     	 * (global step 16550: loss: 0.6246493011713028, lr: 5e-05
2024-01-05 20:48:18 INFO     	 * (global step 16600: loss: 0.5493596196174622, lr: 5e-05
2024-01-05 20:48:33 INFO     	 * (global step 16650: loss: 0.4142032749950886, lr: 5e-05
2024-01-05 20:48:48 INFO     	 * (global step 16700: loss: 0.5697811394929886, lr: 5e-05
2024-01-05 20:49:02 INFO     	 * (global step 16750: loss: 0.6078511029481888, lr: 5e-05
2024-01-05 20:49:17 INFO     	 * (global step 16800: loss: 0.7577462419867516, lr: 5e-05
2024-01-05 20:49:31 INFO     	 * (global step 16850: loss: 0.4399362727999687, lr: 5e-05
2024-01-05 20:49:45 INFO     	 * (global step 16900: loss: 0.5620894059538841, lr: 5e-05
2024-01-05 20:49:59 INFO     	 * (global step 16950: loss: 0.45349733904004097, lr: 5e-05
2024-01-05 20:50:13 INFO     	 * (global step 17000: loss: 0.6014223471283913, lr: 5e-05
2024-01-05 20:50:25 INFO     	 * (global step 17050: loss: 0.5005240924656391, lr: 5e-05
2024-01-05 20:50:36 INFO     	 * (global step 17100: loss: 0.47662822902202606, lr: 5e-05
2024-01-05 20:50:50 INFO     	 * (global step 17150: loss: 0.3417856767773628, lr: 5e-05
2024-01-05 20:51:05 INFO     	 * (global step 17200: loss: 0.33910978212952614, lr: 5e-05
2024-01-05 20:51:19 INFO     	 * (global step 17250: loss: 0.4500102251768112, lr: 5e-05
2024-01-05 20:51:34 INFO     	 * (global step 17300: loss: 0.5251582339406013, lr: 5e-05
2024-01-05 20:51:48 INFO     	 * (global step 17350: loss: 0.5502899810671806, lr: 5e-05
2024-01-05 20:52:03 INFO     	 * (global step 17400: loss: 0.4036504626274109, lr: 5e-05
2024-01-05 20:52:16 INFO     	 * (global step 17450: loss: 0.5359626784920692, lr: 5e-05
2024-01-05 20:52:30 INFO     	 * (global step 17500: loss: 0.5474370568990707, lr: 5e-05
2024-01-05 20:52:44 INFO     	 * (global step 17550: loss: 0.518701832741499, lr: 5e-05
2024-01-05 20:52:58 INFO     	 * (global step 17600: loss: 0.5615285709500313, lr: 5e-05
2024-01-05 20:53:12 INFO     	 * (global step 17650: loss: 0.48077288269996643, lr: 5e-05
2024-01-05 20:53:23 INFO     	 * (global step 17700: loss: 0.40827593952417374, lr: 5e-05
2024-01-05 20:53:35 INFO     	 * (global step 17750: loss: 0.46244344115257263, lr: 5e-05
2024-01-05 20:53:49 INFO     	 * (global step 17800: loss: 0.45966511964797974, lr: 5e-05
2024-01-05 20:54:03 INFO     	 * (global step 17850: loss: 0.5712070167064667, lr: 5e-05
2024-01-05 20:54:17 INFO     	 * (global step 17900: loss: 0.47534503042697906, lr: 5e-05
2024-01-05 20:54:31 INFO     	 * (global step 17950: loss: 0.5748903378844261, lr: 5e-05
2024-01-05 20:54:45 INFO     	 * (global step 18000: loss: 0.45468973368406296, lr: 5e-05
2024-01-05 20:54:59 INFO     	 * (global step 18050: loss: 0.40883680060505867, lr: 5e-05
2024-01-05 20:55:12 INFO     	 * (global step 18100: loss: 0.6017995998263359, lr: 5e-05
2024-01-05 20:55:27 INFO     	 * (global step 18150: loss: 0.4542306289076805, lr: 5e-05
2024-01-05 20:55:41 INFO     	 * (global step 18200: loss: 0.5188814103603363, lr: 5e-05
2024-01-05 20:55:45 INFO     [epoch 8/15] average loss: 0.532, lr: 5e-05
2024-01-05 20:55:45 INFO     saving model related files
2024-01-05 20:55:45 INFO     saving model
2024-01-05 20:55:46 INFO     saving tokenizer
2024-01-05 20:55:46 INFO     saving optimizer
2024-01-05 20:55:48 INFO     remove old optimizer files
2024-01-05 20:55:57 INFO     	 * (global step 18250: loss: 0.5599981620907784, lr: 5e-05
2024-01-05 20:56:09 INFO     	 * (global step 18300: loss: 0.4851861000061035, lr: 5e-05
2024-01-05 20:56:20 INFO     	 * (global step 18350: loss: 0.5620520859956741, lr: 5e-05
2024-01-05 20:56:34 INFO     	 * (global step 18400: loss: 0.5030723661184311, lr: 5e-05
2024-01-05 20:56:48 INFO     	 * (global step 18450: loss: 0.6144862473011017, lr: 5e-05
2024-01-05 20:57:02 INFO     	 * (global step 18500: loss: 0.5153355598449707, lr: 5e-05
2024-01-05 20:57:16 INFO     	 * (global step 18550: loss: 0.5005007907748222, lr: 5e-05
2024-01-05 20:57:31 INFO     	 * (global step 18600: loss: 0.5539601072669029, lr: 5e-05
2024-01-05 20:57:45 INFO     	 * (global step 18650: loss: 0.49128683656454086, lr: 5e-05
2024-01-05 20:57:59 INFO     	 * (global step 18700: loss: 0.7187148630619049, lr: 5e-05
2024-01-05 20:58:13 INFO     	 * (global step 18750: loss: 0.6831805109977722, lr: 5e-05
2024-01-05 20:58:27 INFO     	 * (global step 18800: loss: 0.5157729014754295, lr: 5e-05
2024-01-05 20:58:41 INFO     	 * (global step 18850: loss: 0.504826731979847, lr: 5e-05
2024-01-05 20:58:53 INFO     	 * (global step 18900: loss: 0.4098557233810425, lr: 5e-05
2024-01-05 20:59:04 INFO     	 * (global step 18950: loss: 0.4368209168314934, lr: 5e-05
2024-01-05 20:59:17 INFO     	 * (global step 19000: loss: 0.4945560023188591, lr: 5e-05
2024-01-05 20:59:31 INFO     	 * (global step 19050: loss: 0.5157957077026367, lr: 5e-05
2024-01-05 20:59:45 INFO     	 * (global step 19100: loss: 0.5627866461873055, lr: 5e-05
2024-01-05 20:59:59 INFO     	 * (global step 19150: loss: 0.47884541749954224, lr: 5e-05
2024-01-05 21:00:13 INFO     	 * (global step 19200: loss: 0.6287148147821426, lr: 5e-05
2024-01-05 21:00:27 INFO     	 * (global step 19250: loss: 0.4514346867799759, lr: 5e-05
2024-01-05 21:00:41 INFO     	 * (global step 19300: loss: 0.38860030844807625, lr: 5e-05
2024-01-05 21:00:54 INFO     	 * (global step 19350: loss: 0.5419337004423141, lr: 5e-05
2024-01-05 21:01:08 INFO     	 * (global step 19400: loss: 0.45823315158486366, lr: 5e-05
2024-01-05 21:01:21 INFO     	 * (global step 19450: loss: 0.4412422552704811, lr: 5e-05
2024-01-05 21:01:33 INFO     	 * (global step 19500: loss: 0.47675251215696335, lr: 5e-05
2024-01-05 21:01:44 INFO     	 * (global step 19550: loss: 0.47736433148384094, lr: 5e-05
2024-01-05 21:01:55 INFO     	 * (global step 19600: loss: 0.5228470042347908, lr: 5e-05
2024-01-05 21:02:07 INFO     	 * (global step 19650: loss: 0.46128831803798676, lr: 5e-05
2024-01-05 21:02:18 INFO     	 * (global step 19700: loss: 0.4078725650906563, lr: 5e-05
2024-01-05 21:02:29 INFO     	 * (global step 19750: loss: 0.6647704616189003, lr: 5e-05
2024-01-05 21:02:41 INFO     	 * (global step 19800: loss: 0.6812268793582916, lr: 5e-05
2024-01-05 21:02:52 INFO     	 * (global step 19850: loss: 0.5243202149868011, lr: 5e-05
2024-01-05 21:03:03 INFO     	 * (global step 19900: loss: 0.7156301736831665, lr: 5e-05
2024-01-05 21:03:15 INFO     	 * (global step 19950: loss: 0.4331349954009056, lr: 5e-05
2024-01-05 21:03:26 INFO     	 * (global step 20000: loss: 0.6095520183444023, lr: 5e-05
2024-01-05 21:03:37 INFO     	 * (global step 20050: loss: 0.5101815909147263, lr: 5e-05
2024-01-05 21:03:49 INFO     	 * (global step 20100: loss: 0.5254767537117004, lr: 5e-05
2024-01-05 21:04:00 INFO     	 * (global step 20150: loss: 0.4058639332652092, lr: 5e-05
2024-01-05 21:04:11 INFO     	 * (global step 20200: loss: 0.4595636650919914, lr: 5e-05
2024-01-05 21:04:20 INFO     [epoch 9/15] average loss: 0.524, lr: 5e-05
2024-01-05 21:04:21 INFO     saving model related files
2024-01-05 21:04:21 INFO     saving model
2024-01-05 21:04:21 INFO     saving tokenizer
2024-01-05 21:04:21 INFO     saving optimizer
2024-01-05 21:04:22 INFO     remove old optimizer files
2024-01-05 21:04:22 INFO     complete training: model ckpt was saved at small_recreated_ckpt/model_sdkaaa
2024-01-05 21:04:23 INFO     ## 1st RUN: Configuration 7/12 ##
2024-01-05 21:04:23 INFO     initialize model trainer
2024-01-05 21:04:23 INFO     initialize checkpoint at small_recreated_ckpt/model_uramvg
2024-01-05 21:04:23 INFO     hyperparameters
2024-01-05 21:04:23 INFO     	 * dataset_path: lmqg/qag_squad
2024-01-05 21:04:23 INFO     	 * dataset_name: default
2024-01-05 21:04:23 INFO     	 * input_types: ['paragraph']
2024-01-05 21:04:23 INFO     	 * output_types: ['questions_answers']
2024-01-05 21:04:23 INFO     	 * prefix_types: ['qag']
2024-01-05 21:04:23 INFO     	 * model: t5-small
2024-01-05 21:04:23 INFO     	 * max_length: 512
2024-01-05 21:04:23 INFO     	 * max_length_output: 256
2024-01-05 21:04:23 INFO     	 * epoch: 15
2024-01-05 21:04:23 INFO     	 * batch: 2
2024-01-05 21:04:23 INFO     	 * lr: 5e-05
2024-01-05 21:04:23 INFO     	 * fp16: False
2024-01-05 21:04:23 INFO     	 * random_seed: 1
2024-01-05 21:04:23 INFO     	 * gradient_accumulation_steps: 2
2024-01-05 21:04:23 INFO     	 * label_smoothing: 0.0
2024-01-05 21:04:23 INFO     initialize checkpoint with t5-small
2024-01-05 21:04:24 INFO     use spaCy answer extraction model: positionrank
2024-01-05 21:04:24 INFO     Model `t5-small`
2024-01-05 21:04:24 INFO     	 * Num of GPU in use: 1
2024-01-05 21:04:24 INFO     	 * Prefix: True
2024-01-05 21:04:24 INFO     	 * Language: en (ignore at the training phase)
2024-01-05 21:04:24 INFO     dataset preprocessing
2024-01-05 21:04:26 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featurelmqg/qag_squad/t5-small.512.256.paragraph.questions_answers.train.qag.pkl
2024-01-05 21:04:28 INFO     start model training
2024-01-05 21:04:34 INFO     	 * (global step 50: loss: 1.7728207111358643, lr: 5e-05
2024-01-05 21:04:40 INFO     	 * (global step 100: loss: 1.4806039333343506, lr: 5e-05
2024-01-05 21:04:46 INFO     	 * (global step 150: loss: 0.834697961807251, lr: 5e-05
2024-01-05 21:04:52 INFO     	 * (global step 200: loss: 1.2803834676742554, lr: 5e-05
2024-01-05 21:04:58 INFO     	 * (global step 250: loss: 0.9451367855072021, lr: 5e-05
2024-01-05 21:05:04 INFO     	 * (global step 300: loss: 0.7630171179771423, lr: 5e-05
2024-01-05 21:05:10 INFO     	 * (global step 350: loss: 0.9719663858413696, lr: 5e-05
2024-01-05 21:05:16 INFO     	 * (global step 400: loss: 0.8381746411323547, lr: 5e-05
2024-01-05 21:05:22 INFO     	 * (global step 450: loss: 0.7142868936061859, lr: 5e-05
2024-01-05 21:05:28 INFO     	 * (global step 500: loss: 0.7305144369602203, lr: 5e-05
2024-01-05 21:05:34 INFO     	 * (global step 550: loss: 0.845378190279007, lr: 5e-05
2024-01-05 21:05:40 INFO     	 * (global step 600: loss: 0.950349897146225, lr: 5e-05
2024-01-05 21:05:46 INFO     	 * (global step 650: loss: 0.7694126665592194, lr: 5e-05
2024-01-05 21:05:52 INFO     	 * (global step 700: loss: 0.9067750871181488, lr: 5e-05
2024-01-05 21:05:58 INFO     	 * (global step 750: loss: 0.9591509699821472, lr: 5e-05
2024-01-05 21:06:04 INFO     	 * (global step 800: loss: 1.0849194824695587, lr: 5e-05
2024-01-05 21:06:10 INFO     	 * (global step 850: loss: 0.9280251562595367, lr: 5e-05
2024-01-05 21:06:16 INFO     	 * (global step 900: loss: 0.6631471812725067, lr: 5e-05
2024-01-05 21:06:22 INFO     	 * (global step 950: loss: 0.590509295463562, lr: 5e-05
2024-01-05 21:06:28 INFO     	 * (global step 1000: loss: 0.7870047688484192, lr: 5e-05
2024-01-05 21:06:34 INFO     	 * (global step 1050: loss: 0.6737493872642517, lr: 5e-05
2024-01-05 21:06:40 INFO     	 * (global step 1100: loss: 0.5685000717639923, lr: 5e-05
2024-01-05 21:06:46 INFO     	 * (global step 1150: loss: 0.726594090461731, lr: 5e-05
2024-01-05 21:06:52 INFO     	 * (global step 1200: loss: 1.2113128304481506, lr: 5e-05
2024-01-05 21:06:58 INFO     	 * (global step 1250: loss: 0.7749879658222198, lr: 5e-05
2024-01-05 21:07:04 INFO     	 * (global step 1300: loss: 0.6940084397792816, lr: 5e-05
2024-01-05 21:07:10 INFO     	 * (global step 1350: loss: 0.9254187047481537, lr: 5e-05
2024-01-05 21:07:16 INFO     	 * (global step 1400: loss: 0.5173830986022949, lr: 5e-05
2024-01-05 21:07:22 INFO     	 * (global step 1450: loss: 0.5974380075931549, lr: 5e-05
2024-01-05 21:07:28 INFO     	 * (global step 1500: loss: 0.7022018730640411, lr: 5e-05
2024-01-05 21:07:34 INFO     	 * (global step 1550: loss: 0.8868816196918488, lr: 5e-05
2024-01-05 21:07:40 INFO     	 * (global step 1600: loss: 0.5598921179771423, lr: 5e-05
2024-01-05 21:07:46 INFO     	 * (global step 1650: loss: 0.5945923626422882, lr: 5e-05
2024-01-05 21:07:52 INFO     	 * (global step 1700: loss: 0.7687244415283203, lr: 5e-05
2024-01-05 21:07:58 INFO     	 * (global step 1750: loss: 0.794844776391983, lr: 5e-05
2024-01-05 21:08:04 INFO     	 * (global step 1800: loss: 0.7669158577919006, lr: 5e-05
2024-01-05 21:08:10 INFO     	 * (global step 1850: loss: 0.7317101955413818, lr: 5e-05
2024-01-05 21:08:16 INFO     	 * (global step 1900: loss: 0.4653861075639725, lr: 5e-05
2024-01-05 21:08:22 INFO     	 * (global step 1950: loss: 0.6711544692516327, lr: 5e-05
2024-01-05 21:08:28 INFO     	 * (global step 2000: loss: 1.0128650665283203, lr: 5e-05
2024-01-05 21:08:34 INFO     	 * (global step 2050: loss: 0.7001715898513794, lr: 5e-05
2024-01-05 21:08:40 INFO     	 * (global step 2100: loss: 0.5389806032180786, lr: 5e-05
2024-01-05 21:08:46 INFO     	 * (global step 2150: loss: 0.6141254901885986, lr: 5e-05
2024-01-05 21:08:52 INFO     	 * (global step 2200: loss: 0.929119735956192, lr: 5e-05
2024-01-05 21:08:58 INFO     	 * (global step 2250: loss: 0.56471386551857, lr: 5e-05
2024-01-05 21:09:04 INFO     	 * (global step 2300: loss: 1.4072681963443756, lr: 5e-05
2024-01-05 21:09:10 INFO     	 * (global step 2350: loss: 0.39802253246307373, lr: 5e-05
2024-01-05 21:09:16 INFO     	 * (global step 2400: loss: 0.5917661339044571, lr: 5e-05
2024-01-05 21:09:22 INFO     	 * (global step 2450: loss: 0.7275049984455109, lr: 5e-05
2024-01-05 21:09:28 INFO     	 * (global step 2500: loss: 0.5243711322546005, lr: 5e-05
2024-01-05 21:09:34 INFO     	 * (global step 2550: loss: 0.8317373096942902, lr: 5e-05
2024-01-05 21:09:40 INFO     	 * (global step 2600: loss: 0.4627355486154556, lr: 5e-05
2024-01-05 21:09:46 INFO     	 * (global step 2650: loss: 0.5493955761194229, lr: 5e-05
2024-01-05 21:09:52 INFO     	 * (global step 2700: loss: 0.643118292093277, lr: 5e-05
2024-01-05 21:09:58 INFO     	 * (global step 2750: loss: 0.49111802875995636, lr: 5e-05
2024-01-05 21:10:04 INFO     	 * (global step 2800: loss: 0.5104269832372665, lr: 5e-05
2024-01-05 21:10:10 INFO     	 * (global step 2850: loss: 0.6455878615379333, lr: 5e-05
2024-01-05 21:10:16 INFO     	 * (global step 2900: loss: 0.7618739902973175, lr: 5e-05
2024-01-05 21:10:22 INFO     	 * (global step 2950: loss: 0.33078615367412567, lr: 5e-05
2024-01-05 21:10:28 INFO     	 * (global step 3000: loss: 0.8618737161159515, lr: 5e-05
2024-01-05 21:10:34 INFO     	 * (global step 3050: loss: 0.7278066277503967, lr: 5e-05
2024-01-05 21:10:40 INFO     	 * (global step 3100: loss: 0.7514806687831879, lr: 5e-05
2024-01-05 21:10:46 INFO     	 * (global step 3150: loss: 0.5057901740074158, lr: 5e-05
2024-01-05 21:10:52 INFO     	 * (global step 3200: loss: 0.7809823453426361, lr: 5e-05
2024-01-05 21:10:58 INFO     	 * (global step 3250: loss: 0.49129651486873627, lr: 5e-05
2024-01-05 21:11:04 INFO     	 * (global step 3300: loss: 0.6056638360023499, lr: 5e-05
2024-01-05 21:11:10 INFO     	 * (global step 3350: loss: 0.5823533833026886, lr: 5e-05
2024-01-05 21:11:16 INFO     	 * (global step 3400: loss: 0.584520012140274, lr: 5e-05
2024-01-05 21:11:22 INFO     	 * (global step 3450: loss: 0.7049095928668976, lr: 5e-05
2024-01-05 21:11:28 INFO     	 * (global step 3500: loss: 0.6558476090431213, lr: 5e-05
2024-01-05 21:11:34 INFO     	 * (global step 3550: loss: 0.8971989154815674, lr: 5e-05
2024-01-05 21:11:40 INFO     	 * (global step 3600: loss: 0.6755787283182144, lr: 5e-05
2024-01-05 21:11:46 INFO     	 * (global step 3650: loss: 0.6755453944206238, lr: 5e-05
2024-01-05 21:11:52 INFO     	 * (global step 3700: loss: 0.3991265892982483, lr: 5e-05
2024-01-05 21:11:58 INFO     	 * (global step 3750: loss: 0.5845209956169128, lr: 5e-05
2024-01-05 21:12:04 INFO     	 * (global step 3800: loss: 0.41593407094478607, lr: 5e-05
2024-01-05 21:12:10 INFO     	 * (global step 3850: loss: 0.6116517782211304, lr: 5e-05
2024-01-05 21:12:16 INFO     	 * (global step 3900: loss: 0.820709228515625, lr: 5e-05
2024-01-05 21:12:22 INFO     	 * (global step 3950: loss: 0.6212145686149597, lr: 5e-05
2024-01-05 21:12:28 INFO     	 * (global step 4000: loss: 0.5396171659231186, lr: 5e-05
2024-01-05 21:12:34 INFO     [epoch 0/15] average loss: 0.774, lr: 5e-05
2024-01-05 21:12:34 INFO     saving model related files
2024-01-05 21:12:34 INFO     saving model
2024-01-05 21:12:34 INFO     saving tokenizer
2024-01-05 21:12:34 INFO     saving optimizer
2024-01-05 21:12:35 INFO     remove old optimizer files
2024-01-05 21:12:36 INFO     	 * (global step 4050: loss: 0.6212760806083679, lr: 5e-05
2024-01-05 21:12:42 INFO     	 * (global step 4100: loss: 0.5358265265822411, lr: 5e-05
2024-01-05 21:12:48 INFO     	 * (global step 4150: loss: 0.6276861727237701, lr: 5e-05
2024-01-05 21:12:54 INFO     	 * (global step 4200: loss: 0.49918070435523987, lr: 5e-05
2024-01-05 21:13:00 INFO     	 * (global step 4250: loss: 0.725459098815918, lr: 5e-05
2024-01-05 21:13:06 INFO     	 * (global step 4300: loss: 0.9624768644571304, lr: 5e-05
2024-01-05 21:13:12 INFO     	 * (global step 4350: loss: 0.45126648247241974, lr: 5e-05
2024-01-05 21:13:18 INFO     	 * (global step 4400: loss: 0.40210068225860596, lr: 5e-05
2024-01-05 21:13:24 INFO     	 * (global step 4450: loss: 0.8811017572879791, lr: 5e-05
2024-01-05 21:13:30 INFO     	 * (global step 4500: loss: 0.4579108953475952, lr: 5e-05
2024-01-05 21:13:36 INFO     	 * (global step 4550: loss: 0.5260932147502899, lr: 5e-05
2024-01-05 21:13:42 INFO     	 * (global step 4600: loss: 0.5844783186912537, lr: 5e-05
2024-01-05 21:13:48 INFO     	 * (global step 4650: loss: 0.8014101684093475, lr: 5e-05
2024-01-05 21:13:54 INFO     	 * (global step 4700: loss: 0.8268274962902069, lr: 5e-05
2024-01-05 21:14:00 INFO     	 * (global step 4750: loss: 0.784092366695404, lr: 5e-05
2024-01-05 21:14:06 INFO     	 * (global step 4800: loss: 0.6164823025465012, lr: 5e-05
2024-01-05 21:14:12 INFO     	 * (global step 4850: loss: 0.6800673305988312, lr: 5e-05
2024-01-05 21:14:18 INFO     	 * (global step 4900: loss: 0.6093034446239471, lr: 5e-05
2024-01-05 21:14:24 INFO     	 * (global step 4950: loss: 0.8133270144462585, lr: 5e-05
2024-01-05 21:14:30 INFO     	 * (global step 5000: loss: 0.7636942863464355, lr: 5e-05
2024-01-05 21:14:36 INFO     	 * (global step 5050: loss: 0.6965290904045105, lr: 5e-05
2024-01-05 21:14:42 INFO     	 * (global step 5100: loss: 0.5521915704011917, lr: 5e-05
2024-01-05 21:14:48 INFO     	 * (global step 5150: loss: 0.6535124480724335, lr: 5e-05
2024-01-05 21:14:54 INFO     	 * (global step 5200: loss: 0.533741295337677, lr: 5e-05
2024-01-05 21:15:00 INFO     	 * (global step 5250: loss: 0.5819418579339981, lr: 5e-05
2024-01-05 21:15:06 INFO     	 * (global step 5300: loss: 0.8350092470645905, lr: 5e-05
2024-01-05 21:15:12 INFO     	 * (global step 5350: loss: 0.652933806180954, lr: 5e-05
2024-01-05 21:15:18 INFO     	 * (global step 5400: loss: 0.5551742911338806, lr: 5e-05
2024-01-05 21:15:24 INFO     	 * (global step 5450: loss: 0.5698099732398987, lr: 5e-05
2024-01-05 21:15:30 INFO     	 * (global step 5500: loss: 0.49667710065841675, lr: 5e-05
2024-01-05 21:15:36 INFO     	 * (global step 5550: loss: 0.6869542896747589, lr: 5e-05
2024-01-05 21:15:42 INFO     	 * (global step 5600: loss: 0.7293442487716675, lr: 5e-05
2024-01-05 21:15:48 INFO     	 * (global step 5650: loss: 0.7041328549385071, lr: 5e-05
2024-01-05 21:15:54 INFO     	 * (global step 5700: loss: 0.9760475158691406, lr: 5e-05
2024-01-05 21:16:00 INFO     	 * (global step 5750: loss: 0.49803633987903595, lr: 5e-05
2024-01-05 21:16:06 INFO     	 * (global step 5800: loss: 0.6791359782218933, lr: 5e-05
2024-01-05 21:16:12 INFO     	 * (global step 5850: loss: 0.5326066017150879, lr: 5e-05
2024-01-05 21:16:18 INFO     	 * (global step 5900: loss: 0.46371152997016907, lr: 5e-05
2024-01-05 21:16:25 INFO     	 * (global step 5950: loss: 0.7941720485687256, lr: 5e-05
2024-01-05 21:16:31 INFO     	 * (global step 6000: loss: 0.5657046139240265, lr: 5e-05
2024-01-05 21:16:37 INFO     	 * (global step 6050: loss: 0.5114542543888092, lr: 5e-05
2024-01-05 21:16:43 INFO     	 * (global step 6100: loss: 0.5040881037712097, lr: 5e-05
2024-01-05 21:16:49 INFO     	 * (global step 6150: loss: 0.5752341747283936, lr: 5e-05
2024-01-05 21:16:55 INFO     	 * (global step 6200: loss: 0.6323198974132538, lr: 5e-05
2024-01-05 21:17:01 INFO     	 * (global step 6250: loss: 0.4661755710840225, lr: 5e-05
2024-01-05 21:17:07 INFO     	 * (global step 6300: loss: 0.6270259320735931, lr: 5e-05
2024-01-05 21:17:13 INFO     	 * (global step 6350: loss: 0.6606756150722504, lr: 5e-05
2024-01-05 21:17:19 INFO     	 * (global step 6400: loss: 0.5412724912166595, lr: 5e-05
2024-01-05 21:17:25 INFO     	 * (global step 6450: loss: 0.4673963934183121, lr: 5e-05
2024-01-05 21:17:31 INFO     	 * (global step 6500: loss: 0.7092888057231903, lr: 5e-05
2024-01-05 21:17:37 INFO     	 * (global step 6550: loss: 0.7437098622322083, lr: 5e-05
2024-01-05 21:17:43 INFO     	 * (global step 6600: loss: 0.6809025406837463, lr: 5e-05
2024-01-05 21:17:49 INFO     	 * (global step 6650: loss: 0.5624637305736542, lr: 5e-05
2024-01-05 21:17:55 INFO     	 * (global step 6700: loss: 0.6595516204833984, lr: 5e-05
2024-01-05 21:18:01 INFO     	 * (global step 6750: loss: 0.8111045658588409, lr: 5e-05
2024-01-05 21:18:07 INFO     	 * (global step 6800: loss: 0.7831457257270813, lr: 5e-05
2024-01-05 21:18:13 INFO     	 * (global step 6850: loss: 0.6016977727413177, lr: 5e-05
2024-01-05 21:18:19 INFO     	 * (global step 6900: loss: 0.640748918056488, lr: 5e-05
2024-01-05 21:18:25 INFO     	 * (global step 6950: loss: 0.5533333122730255, lr: 5e-05
2024-01-05 21:18:31 INFO     	 * (global step 7000: loss: 0.580885261297226, lr: 5e-05
2024-01-05 21:18:37 INFO     	 * (global step 7050: loss: 0.6707504987716675, lr: 5e-05
2024-01-05 21:18:43 INFO     	 * (global step 7100: loss: 0.46960708498954773, lr: 5e-05
2024-01-05 21:18:49 INFO     	 * (global step 7150: loss: 0.6405892074108124, lr: 5e-05
2024-01-05 21:18:55 INFO     	 * (global step 7200: loss: 0.6053761094808578, lr: 5e-05
2024-01-05 21:19:01 INFO     	 * (global step 7250: loss: 0.7904725074768066, lr: 5e-05
2024-01-05 21:19:07 INFO     	 * (global step 7300: loss: 0.7501441836357117, lr: 5e-05
2024-01-05 21:19:13 INFO     	 * (global step 7350: loss: 0.6497674286365509, lr: 5e-05
2024-01-05 21:19:19 INFO     	 * (global step 7400: loss: 0.7280196249485016, lr: 5e-05
2024-01-05 21:19:26 INFO     	 * (global step 7450: loss: 0.5839199721813202, lr: 5e-05
2024-01-05 21:19:32 INFO     	 * (global step 7500: loss: 0.5376323014497757, lr: 5e-05
2024-01-05 21:19:38 INFO     	 * (global step 7550: loss: 0.801628589630127, lr: 5e-05
2024-01-05 21:19:44 INFO     	 * (global step 7600: loss: 0.595958411693573, lr: 5e-05
2024-01-05 21:19:50 INFO     	 * (global step 7650: loss: 0.8478047847747803, lr: 5e-05
2024-01-05 21:19:56 INFO     	 * (global step 7700: loss: 0.9519955217838287, lr: 5e-05
2024-01-05 21:20:02 INFO     	 * (global step 7750: loss: 0.6134634017944336, lr: 5e-05
2024-01-05 21:20:08 INFO     	 * (global step 7800: loss: 0.6380950510501862, lr: 5e-05
2024-01-05 21:20:14 INFO     	 * (global step 7850: loss: 0.5500684678554535, lr: 5e-05
2024-01-05 21:20:20 INFO     	 * (global step 7900: loss: 0.5222214758396149, lr: 5e-05
2024-01-05 21:20:26 INFO     	 * (global step 7950: loss: 0.5258458256721497, lr: 5e-05
2024-01-05 21:20:32 INFO     	 * (global step 8000: loss: 0.5086855590343475, lr: 5e-05
2024-01-05 21:20:38 INFO     	 * (global step 8050: loss: 0.8101849853992462, lr: 5e-05
2024-01-05 21:20:43 INFO     [epoch 1/15] average loss: 0.622, lr: 5e-05
2024-01-05 21:20:43 INFO     saving model related files
2024-01-05 21:20:43 INFO     saving model
2024-01-05 21:20:44 INFO     saving tokenizer
2024-01-05 21:20:44 INFO     saving optimizer
2024-01-05 21:20:45 INFO     remove old optimizer files
2024-01-05 21:20:46 INFO     	 * (global step 8100: loss: 0.5031428188085556, lr: 5e-05
2024-01-05 21:20:52 INFO     	 * (global step 8150: loss: 0.43479932844638824, lr: 5e-05
2024-01-05 21:20:58 INFO     	 * (global step 8200: loss: 0.46873751282691956, lr: 5e-05
2024-01-05 21:21:04 INFO     	 * (global step 8250: loss: 0.3500138968229294, lr: 5e-05
2024-01-05 21:21:10 INFO     	 * (global step 8300: loss: 0.5961377918720245, lr: 5e-05
2024-01-05 21:21:16 INFO     	 * (global step 8350: loss: 0.7994186878204346, lr: 5e-05
2024-01-05 21:21:22 INFO     	 * (global step 8400: loss: 0.43882104754447937, lr: 5e-05
2024-01-05 21:21:28 INFO     	 * (global step 8450: loss: 0.5020786970853806, lr: 5e-05
2024-01-05 21:21:34 INFO     	 * (global step 8500: loss: 0.7248212397098541, lr: 5e-05
2024-01-05 21:21:40 INFO     	 * (global step 8550: loss: 0.640448123216629, lr: 5e-05
2024-01-05 21:21:46 INFO     	 * (global step 8600: loss: 0.33646489679813385, lr: 5e-05
2024-01-05 21:21:52 INFO     	 * (global step 8650: loss: 0.43023133277893066, lr: 5e-05
2024-01-05 21:21:58 INFO     	 * (global step 8700: loss: 0.6685518324375153, lr: 5e-05
2024-01-05 21:22:04 INFO     	 * (global step 8750: loss: 0.5577477812767029, lr: 5e-05
2024-01-05 21:22:10 INFO     	 * (global step 8800: loss: 0.30815258622169495, lr: 5e-05
2024-01-05 21:22:16 INFO     	 * (global step 8850: loss: 0.5744191259145737, lr: 5e-05
2024-01-05 21:22:23 INFO     	 * (global step 8900: loss: 0.454338014125824, lr: 5e-05
2024-01-05 21:22:29 INFO     	 * (global step 8950: loss: 0.374255433678627, lr: 5e-05
2024-01-05 21:22:35 INFO     	 * (global step 9000: loss: 0.5465766042470932, lr: 5e-05
2024-01-05 21:22:41 INFO     	 * (global step 9050: loss: 0.6778773963451385, lr: 5e-05
2024-01-05 21:22:47 INFO     	 * (global step 9100: loss: 0.6441956013441086, lr: 5e-05
2024-01-05 21:22:53 INFO     	 * (global step 9150: loss: 0.6076233386993408, lr: 5e-05
2024-01-05 21:22:59 INFO     	 * (global step 9200: loss: 0.6264724135398865, lr: 5e-05
2024-01-05 21:23:05 INFO     	 * (global step 9250: loss: 0.4673958271741867, lr: 5e-05
2024-01-05 21:23:11 INFO     	 * (global step 9300: loss: 0.7088388353586197, lr: 5e-05
2024-01-05 21:23:17 INFO     	 * (global step 9350: loss: 0.41339702904224396, lr: 5e-05
2024-01-05 21:23:23 INFO     	 * (global step 9400: loss: 0.5844441652297974, lr: 5e-05
2024-01-05 21:23:29 INFO     	 * (global step 9450: loss: 0.4547169506549835, lr: 5e-05
2024-01-05 21:23:35 INFO     	 * (global step 9500: loss: 0.5287989228963852, lr: 5e-05
2024-01-05 21:23:41 INFO     	 * (global step 9550: loss: 0.6232616901397705, lr: 5e-05
2024-01-05 21:23:47 INFO     	 * (global step 9600: loss: 0.5635882914066315, lr: 5e-05
2024-01-05 21:23:53 INFO     	 * (global step 9650: loss: 0.6848865449428558, lr: 5e-05
2024-01-05 21:23:59 INFO     	 * (global step 9700: loss: 0.7133422791957855, lr: 5e-05
2024-01-05 21:24:05 INFO     	 * (global step 9750: loss: 0.730372816324234, lr: 5e-05
2024-01-05 21:24:11 INFO     	 * (global step 9800: loss: 0.47140154242515564, lr: 5e-05
2024-01-05 21:24:17 INFO     	 * (global step 9850: loss: 0.7122045457363129, lr: 5e-05
2024-01-05 21:24:23 INFO     	 * (global step 9900: loss: 0.4679810404777527, lr: 5e-05
2024-01-05 21:24:29 INFO     	 * (global step 9950: loss: 0.437240868806839, lr: 5e-05
2024-01-05 21:24:35 INFO     	 * (global step 10000: loss: 0.48066528141498566, lr: 5e-05
2024-01-05 21:24:41 INFO     	 * (global step 10050: loss: 0.5525582134723663, lr: 5e-05
2024-01-05 21:24:47 INFO     	 * (global step 10100: loss: 0.6672464311122894, lr: 5e-05
2024-01-05 21:24:53 INFO     	 * (global step 10150: loss: 0.41902731359004974, lr: 5e-05
2024-01-05 21:24:59 INFO     	 * (global step 10200: loss: 0.49809880554676056, lr: 5e-05
2024-01-05 21:25:05 INFO     	 * (global step 10250: loss: 0.6047763526439667, lr: 5e-05
2024-01-05 21:25:11 INFO     	 * (global step 10300: loss: 0.6751914918422699, lr: 5e-05
2024-01-05 21:25:17 INFO     	 * (global step 10350: loss: 0.3407731130719185, lr: 5e-05
2024-01-05 21:25:23 INFO     	 * (global step 10400: loss: 0.5288295149803162, lr: 5e-05
2024-01-05 21:25:29 INFO     	 * (global step 10450: loss: 0.6551196575164795, lr: 5e-05
2024-01-05 21:25:35 INFO     	 * (global step 10500: loss: 0.5853196978569031, lr: 5e-05
2024-01-05 21:25:41 INFO     	 * (global step 10550: loss: 0.5716466307640076, lr: 5e-05
2024-01-05 21:25:47 INFO     	 * (global step 10600: loss: 0.29834482818841934, lr: 5e-05
2024-01-05 21:25:53 INFO     	 * (global step 10650: loss: 0.523980125784874, lr: 5e-05
2024-01-05 21:25:59 INFO     	 * (global step 10700: loss: 0.5693625658750534, lr: 5e-05
2024-01-05 21:26:05 INFO     	 * (global step 10750: loss: 0.597249761223793, lr: 5e-05
2024-01-05 21:26:11 INFO     	 * (global step 10800: loss: 0.5679447799921036, lr: 5e-05
2024-01-05 21:26:17 INFO     	 * (global step 10850: loss: 0.5178949683904648, lr: 5e-05
2024-01-05 21:26:23 INFO     	 * (global step 10900: loss: 0.5375060439109802, lr: 5e-05
2024-01-05 21:26:29 INFO     	 * (global step 10950: loss: 0.4795081168413162, lr: 5e-05
2024-01-05 21:26:35 INFO     	 * (global step 11000: loss: 0.43903225660324097, lr: 5e-05
2024-01-05 21:26:41 INFO     	 * (global step 11050: loss: 0.6631976366043091, lr: 5e-05
2024-01-05 21:26:47 INFO     	 * (global step 11100: loss: 0.5419610440731049, lr: 5e-05
2024-01-05 21:26:53 INFO     	 * (global step 11150: loss: 0.5794217586517334, lr: 5e-05
2024-01-05 21:26:59 INFO     	 * (global step 11200: loss: 0.48514844477176666, lr: 5e-05
2024-01-05 21:27:05 INFO     	 * (global step 11250: loss: 0.6179113388061523, lr: 5e-05
2024-01-05 21:27:11 INFO     	 * (global step 11300: loss: 0.43329499661922455, lr: 5e-05
2024-01-05 21:27:17 INFO     	 * (global step 11350: loss: 0.6539437174797058, lr: 5e-05
2024-01-05 21:27:23 INFO     	 * (global step 11400: loss: 0.6998924612998962, lr: 5e-05
2024-01-05 21:27:29 INFO     	 * (global step 11450: loss: 0.849172830581665, lr: 5e-05
2024-01-05 21:27:35 INFO     	 * (global step 11500: loss: 0.5793159604072571, lr: 5e-05
2024-01-05 21:27:41 INFO     	 * (global step 11550: loss: 0.6916426718235016, lr: 5e-05
2024-01-05 21:27:47 INFO     	 * (global step 11600: loss: 0.5714187324047089, lr: 5e-05
2024-01-05 21:27:53 INFO     	 * (global step 11650: loss: 0.863503485918045, lr: 5e-05
2024-01-05 21:27:59 INFO     	 * (global step 11700: loss: 0.6274743378162384, lr: 5e-05
2024-01-05 21:28:05 INFO     	 * (global step 11750: loss: 0.6086751222610474, lr: 5e-05
2024-01-05 21:28:11 INFO     	 * (global step 11800: loss: 0.4023337811231613, lr: 5e-05
2024-01-05 21:28:17 INFO     	 * (global step 11850: loss: 0.5576734691858292, lr: 5e-05
2024-01-05 21:28:23 INFO     	 * (global step 11900: loss: 0.44527116417884827, lr: 5e-05
2024-01-05 21:28:29 INFO     	 * (global step 11950: loss: 0.38245606422424316, lr: 5e-05
2024-01-05 21:28:35 INFO     	 * (global step 12000: loss: 0.6781930029392242, lr: 5e-05
2024-01-05 21:28:41 INFO     	 * (global step 12050: loss: 0.4924023300409317, lr: 5e-05
2024-01-05 21:28:47 INFO     	 * (global step 12100: loss: 0.6245290338993073, lr: 5e-05
2024-01-05 21:28:53 INFO     [epoch 2/15] average loss: 0.589, lr: 5e-05
2024-01-05 21:28:53 INFO     saving model related files
2024-01-05 21:28:53 INFO     saving model
2024-01-05 21:28:54 INFO     saving tokenizer
2024-01-05 21:28:54 INFO     saving optimizer
2024-01-05 21:28:55 INFO     remove old optimizer files
2024-01-05 21:28:55 INFO     	 * (global step 12150: loss: 0.46997807919979095, lr: 5e-05
2024-01-05 21:29:01 INFO     	 * (global step 12200: loss: 0.590578705072403, lr: 5e-05
2024-01-05 21:29:07 INFO     	 * (global step 12250: loss: 0.4514467865228653, lr: 5e-05
2024-01-05 21:29:14 INFO     	 * (global step 12300: loss: 0.5570529699325562, lr: 5e-05
2024-01-05 21:29:20 INFO     	 * (global step 12350: loss: 0.6549874246120453, lr: 5e-05
2024-01-05 21:29:26 INFO     	 * (global step 12400: loss: 0.7109085619449615, lr: 5e-05
2024-01-05 21:29:32 INFO     	 * (global step 12450: loss: 0.42122088372707367, lr: 5e-05
2024-01-05 21:29:38 INFO     	 * (global step 12500: loss: 0.6425885707139969, lr: 5e-05
2024-01-05 21:29:44 INFO     	 * (global step 12550: loss: 0.49082084000110626, lr: 5e-05
2024-01-05 21:29:50 INFO     	 * (global step 12600: loss: 0.4650540202856064, lr: 5e-05
2024-01-05 21:29:56 INFO     	 * (global step 12650: loss: 0.5674548745155334, lr: 5e-05
2024-01-05 21:30:02 INFO     	 * (global step 12700: loss: 0.7927940785884857, lr: 5e-05
2024-01-05 21:30:08 INFO     	 * (global step 12750: loss: 0.7353690266609192, lr: 5e-05
2024-01-05 21:30:14 INFO     	 * (global step 12800: loss: 0.6902090013027191, lr: 5e-05
2024-01-05 21:30:20 INFO     	 * (global step 12850: loss: 0.7650718688964844, lr: 5e-05
2024-01-05 21:30:26 INFO     	 * (global step 12900: loss: 0.6184815913438797, lr: 5e-05
2024-01-05 21:30:32 INFO     	 * (global step 12950: loss: 0.6269949674606323, lr: 5e-05
2024-01-05 21:30:38 INFO     	 * (global step 13000: loss: 0.7528863251209259, lr: 5e-05
2024-01-05 21:30:44 INFO     	 * (global step 13050: loss: 0.5491375476121902, lr: 5e-05
2024-01-05 21:30:50 INFO     	 * (global step 13100: loss: 0.45849646627902985, lr: 5e-05
2024-01-05 21:30:56 INFO     	 * (global step 13150: loss: 0.5636303424835205, lr: 5e-05
2024-01-05 21:31:02 INFO     	 * (global step 13200: loss: 0.5319546610116959, lr: 5e-05
2024-01-05 21:31:08 INFO     	 * (global step 13250: loss: 1.0894460082054138, lr: 5e-05
2024-01-05 21:31:14 INFO     	 * (global step 13300: loss: 0.3581560403108597, lr: 5e-05
2024-01-05 21:31:20 INFO     	 * (global step 13350: loss: 0.5586295425891876, lr: 5e-05
2024-01-05 21:31:26 INFO     	 * (global step 13400: loss: 0.6823767423629761, lr: 5e-05
2024-01-05 21:31:32 INFO     	 * (global step 13450: loss: 0.5173718631267548, lr: 5e-05
2024-01-05 21:31:38 INFO     	 * (global step 13500: loss: 0.6970040053129196, lr: 5e-05
2024-01-05 21:31:43 INFO     	 * (global step 13550: loss: 0.586225152015686, lr: 5e-05
2024-01-05 21:31:49 INFO     	 * (global step 13600: loss: 0.7228224277496338, lr: 5e-05
2024-01-05 21:31:55 INFO     	 * (global step 13650: loss: 0.579307347536087, lr: 5e-05
2024-01-05 21:32:01 INFO     	 * (global step 13700: loss: 0.5584477484226227, lr: 5e-05
2024-01-05 21:32:07 INFO     	 * (global step 13750: loss: 0.5521587133407593, lr: 5e-05
2024-01-05 21:32:13 INFO     	 * (global step 13800: loss: 0.5625022053718567, lr: 5e-05
2024-01-05 21:32:19 INFO     	 * (global step 13850: loss: 0.9084119200706482, lr: 5e-05
2024-01-05 21:32:25 INFO     	 * (global step 13900: loss: 0.3707684129476547, lr: 5e-05
2024-01-05 21:32:31 INFO     	 * (global step 13950: loss: 0.5934241563081741, lr: 5e-05
2024-01-05 21:32:37 INFO     	 * (global step 14000: loss: 0.4516250938177109, lr: 5e-05
2024-01-05 21:32:43 INFO     	 * (global step 14050: loss: 0.48008115589618683, lr: 5e-05
2024-01-05 21:32:49 INFO     	 * (global step 14100: loss: 0.6093092560768127, lr: 5e-05
2024-01-05 21:32:55 INFO     	 * (global step 14150: loss: 0.39487719535827637, lr: 5e-05
2024-01-05 21:33:01 INFO     	 * (global step 14200: loss: 0.5685767829418182, lr: 5e-05
2024-01-05 21:33:07 INFO     	 * (global step 14250: loss: 0.6788336038589478, lr: 5e-05
2024-01-05 21:33:13 INFO     	 * (global step 14300: loss: 0.44008517265319824, lr: 5e-05
2024-01-05 21:33:19 INFO     	 * (global step 14350: loss: 1.0162698924541473, lr: 5e-05
2024-01-05 21:33:25 INFO     	 * (global step 14400: loss: 0.6317028701305389, lr: 5e-05
2024-01-05 21:33:31 INFO     	 * (global step 14450: loss: 0.38286443054676056, lr: 5e-05
2024-01-05 21:33:37 INFO     	 * (global step 14500: loss: 0.5357243716716766, lr: 5e-05
2024-01-05 21:33:43 INFO     	 * (global step 14550: loss: 0.5084021538496017, lr: 5e-05
2024-01-05 21:33:49 INFO     	 * (global step 14600: loss: 0.5442650318145752, lr: 5e-05
2024-01-05 21:33:55 INFO     	 * (global step 14650: loss: 0.4917743504047394, lr: 5e-05
2024-01-05 21:34:01 INFO     	 * (global step 14700: loss: 0.4121423810720444, lr: 5e-05
2024-01-05 21:34:07 INFO     	 * (global step 14750: loss: 0.5609699785709381, lr: 5e-05
2024-01-05 21:34:13 INFO     	 * (global step 14800: loss: 0.5220165550708771, lr: 5e-05
2024-01-05 21:34:19 INFO     	 * (global step 14850: loss: 0.5213421285152435, lr: 5e-05
2024-01-05 21:34:25 INFO     	 * (global step 14900: loss: 0.44446803629398346, lr: 5e-05
2024-01-05 21:34:31 INFO     	 * (global step 14950: loss: 0.4438655823469162, lr: 5e-05
2024-01-05 21:34:37 INFO     	 * (global step 15000: loss: 0.4276035949587822, lr: 5e-05
2024-01-05 21:34:43 INFO     	 * (global step 15050: loss: 0.6470908224582672, lr: 5e-05
2024-01-05 21:34:49 INFO     	 * (global step 15100: loss: 0.5362251400947571, lr: 5e-05
2024-01-05 21:34:54 INFO     	 * (global step 15150: loss: 0.5609377473592758, lr: 5e-05
2024-01-05 21:35:00 INFO     	 * (global step 15200: loss: 0.5040234327316284, lr: 5e-05
2024-01-05 21:35:06 INFO     	 * (global step 15250: loss: 0.4136938601732254, lr: 5e-05
2024-01-05 21:35:12 INFO     	 * (global step 15300: loss: 0.5359200984239578, lr: 5e-05
2024-01-05 21:35:18 INFO     	 * (global step 15350: loss: 0.7246540784835815, lr: 5e-05
2024-01-05 21:35:24 INFO     	 * (global step 15400: loss: 0.4122749865055084, lr: 5e-05
2024-01-05 21:35:30 INFO     	 * (global step 15450: loss: 0.43079236149787903, lr: 5e-05
2024-01-05 21:35:36 INFO     	 * (global step 15500: loss: 0.4580902233719826, lr: 5e-05
2024-01-05 21:35:42 INFO     	 * (global step 15550: loss: 0.6715556085109711, lr: 5e-05
2024-01-05 21:35:48 INFO     	 * (global step 15600: loss: 0.5698687136173248, lr: 5e-05
2024-01-05 21:35:54 INFO     	 * (global step 15650: loss: 0.6933708786964417, lr: 5e-05
2024-01-05 21:36:00 INFO     	 * (global step 15700: loss: 0.6730014085769653, lr: 5e-05
2024-01-05 21:36:06 INFO     	 * (global step 15750: loss: 0.5481641441583633, lr: 5e-05
2024-01-05 21:36:12 INFO     	 * (global step 15800: loss: 0.3131759315729141, lr: 5e-05
2024-01-05 21:36:18 INFO     	 * (global step 15850: loss: 0.5440447330474854, lr: 5e-05
2024-01-05 21:36:24 INFO     	 * (global step 15900: loss: 0.6334898769855499, lr: 5e-05
2024-01-05 21:36:30 INFO     	 * (global step 15950: loss: 0.7333014011383057, lr: 5e-05
2024-01-05 21:36:36 INFO     	 * (global step 16000: loss: 0.4172879159450531, lr: 5e-05
2024-01-05 21:36:42 INFO     	 * (global step 16050: loss: 0.39793506264686584, lr: 5e-05
2024-01-05 21:36:48 INFO     	 * (global step 16100: loss: 0.7571995556354523, lr: 5e-05
2024-01-05 21:36:54 INFO     	 * (global step 16150: loss: 0.45338091254234314, lr: 5e-05
2024-01-05 21:36:59 INFO     [epoch 3/15] average loss: 0.567, lr: 5e-05
2024-01-05 21:36:59 INFO     saving model related files
2024-01-05 21:36:59 INFO     saving model
2024-01-05 21:37:00 INFO     saving tokenizer
2024-01-05 21:37:00 INFO     saving optimizer
2024-01-05 21:37:01 INFO     remove old optimizer files
2024-01-05 21:37:02 INFO     	 * (global step 16200: loss: 0.6994669437408447, lr: 5e-05
2024-01-05 21:37:08 INFO     	 * (global step 16250: loss: 0.5695769190788269, lr: 5e-05
2024-01-05 21:37:14 INFO     	 * (global step 16300: loss: 0.44873782992362976, lr: 5e-05
2024-01-05 21:37:20 INFO     	 * (global step 16350: loss: 0.4839346557855606, lr: 5e-05
2024-01-05 21:37:26 INFO     	 * (global step 16400: loss: 0.2948429137468338, lr: 5e-05
2024-01-05 21:37:32 INFO     	 * (global step 16450: loss: 0.4207216799259186, lr: 5e-05
2024-01-05 21:37:38 INFO     	 * (global step 16500: loss: 0.4596109241247177, lr: 5e-05
2024-01-05 21:37:44 INFO     	 * (global step 16550: loss: 0.6105993837118149, lr: 5e-05
2024-01-05 21:37:50 INFO     	 * (global step 16600: loss: 0.47415582835674286, lr: 5e-05
2024-01-05 21:37:56 INFO     	 * (global step 16650: loss: 0.5475552976131439, lr: 5e-05
2024-01-05 21:38:02 INFO     	 * (global step 16700: loss: 0.5431016832590103, lr: 5e-05
2024-01-05 21:38:08 INFO     	 * (global step 16750: loss: 0.46252959966659546, lr: 5e-05
2024-01-05 21:38:14 INFO     	 * (global step 16800: loss: 0.6752725690603256, lr: 5e-05
2024-01-05 21:38:20 INFO     	 * (global step 16850: loss: 0.3578701466321945, lr: 5e-05
2024-01-05 21:38:26 INFO     	 * (global step 16900: loss: 0.6113190352916718, lr: 5e-05
2024-01-05 21:38:32 INFO     	 * (global step 16950: loss: 0.5768514275550842, lr: 5e-05
2024-01-05 21:38:38 INFO     	 * (global step 17000: loss: 0.628000795841217, lr: 5e-05
2024-01-05 21:38:44 INFO     	 * (global step 17050: loss: 0.5581961274147034, lr: 5e-05
2024-01-05 21:38:50 INFO     	 * (global step 17100: loss: 0.27039792388677597, lr: 5e-05
2024-01-05 21:38:56 INFO     	 * (global step 17150: loss: 0.5361394584178925, lr: 5e-05
2024-01-05 21:39:02 INFO     	 * (global step 17200: loss: 0.4632294178009033, lr: 5e-05
2024-01-05 21:39:08 INFO     	 * (global step 17250: loss: 0.3576876223087311, lr: 5e-05
2024-01-05 21:39:14 INFO     	 * (global step 17300: loss: 0.6876709163188934, lr: 5e-05
2024-01-05 21:39:20 INFO     	 * (global step 17350: loss: 0.7147607207298279, lr: 5e-05
2024-01-05 21:39:26 INFO     	 * (global step 17400: loss: 0.7333292365074158, lr: 5e-05
2024-01-05 21:39:32 INFO     	 * (global step 17450: loss: 0.4800317734479904, lr: 5e-05
2024-01-05 21:39:38 INFO     	 * (global step 17500: loss: 0.4645994305610657, lr: 5e-05
2024-01-05 21:39:44 INFO     	 * (global step 17550: loss: 0.46876150369644165, lr: 5e-05
2024-01-05 21:39:50 INFO     	 * (global step 17600: loss: 0.6682968586683273, lr: 5e-05
2024-01-05 21:39:56 INFO     	 * (global step 17650: loss: 0.45770205557346344, lr: 5e-05
2024-01-05 21:40:02 INFO     	 * (global step 17700: loss: 0.41179291158914566, lr: 5e-05
2024-01-05 21:40:07 INFO     	 * (global step 17750: loss: 0.6526602208614349, lr: 5e-05
2024-01-05 21:40:13 INFO     	 * (global step 17800: loss: 0.7147769778966904, lr: 5e-05
2024-01-05 21:40:19 INFO     	 * (global step 17850: loss: 0.692357987165451, lr: 5e-05
2024-01-05 21:40:25 INFO     	 * (global step 17900: loss: 0.45851339399814606, lr: 5e-05
2024-01-05 21:40:31 INFO     	 * (global step 17950: loss: 0.4234555661678314, lr: 5e-05
2024-01-05 21:40:37 INFO     	 * (global step 18000: loss: 0.5554556250572205, lr: 5e-05
2024-01-05 21:40:43 INFO     	 * (global step 18050: loss: 0.4933253079652786, lr: 5e-05
2024-01-05 21:40:49 INFO     	 * (global step 18100: loss: 0.5830363482236862, lr: 5e-05
2024-01-05 21:40:55 INFO     	 * (global step 18150: loss: 0.6089453399181366, lr: 5e-05
2024-01-05 21:41:01 INFO     	 * (global step 18200: loss: 0.5051837712526321, lr: 5e-05
2024-01-05 21:41:07 INFO     	 * (global step 18250: loss: 0.4967948645353317, lr: 5e-05
2024-01-05 21:41:13 INFO     	 * (global step 18300: loss: 0.334022156894207, lr: 5e-05
2024-01-05 21:41:18 INFO     	 * (global step 18350: loss: 0.8171179294586182, lr: 5e-05
2024-01-05 21:41:24 INFO     	 * (global step 18400: loss: 0.6498093605041504, lr: 5e-05
2024-01-05 21:41:30 INFO     	 * (global step 18450: loss: 0.5118217766284943, lr: 5e-05
2024-01-05 21:41:36 INFO     	 * (global step 18500: loss: 0.5142627507448196, lr: 5e-05
2024-01-05 21:41:42 INFO     	 * (global step 18550: loss: 0.8175262808799744, lr: 5e-05
2024-01-05 21:41:48 INFO     	 * (global step 18600: loss: 0.46452879905700684, lr: 5e-05
2024-01-05 21:41:54 INFO     	 * (global step 18650: loss: 0.7319866418838501, lr: 5e-05
2024-01-05 21:42:00 INFO     	 * (global step 18700: loss: 0.6394864618778229, lr: 5e-05
2024-01-05 21:42:06 INFO     	 * (global step 18750: loss: 0.4869874268770218, lr: 5e-05
2024-01-05 21:42:12 INFO     	 * (global step 18800: loss: 0.48868024349212646, lr: 5e-05
2024-01-05 21:42:18 INFO     	 * (global step 18850: loss: 0.6120167970657349, lr: 5e-05
2024-01-05 21:42:24 INFO     	 * (global step 18900: loss: 0.5051442086696625, lr: 5e-05
2024-01-05 21:42:30 INFO     	 * (global step 18950: loss: 0.5527180731296539, lr: 5e-05
2024-01-05 21:42:36 INFO     	 * (global step 19000: loss: 0.553688257932663, lr: 5e-05
2024-01-05 21:42:42 INFO     	 * (global step 19050: loss: 0.7489485442638397, lr: 5e-05
2024-01-05 21:42:48 INFO     	 * (global step 19100: loss: 0.6470401585102081, lr: 5e-05
2024-01-05 21:42:54 INFO     	 * (global step 19150: loss: 0.6065012812614441, lr: 5e-05
2024-01-05 21:43:00 INFO     	 * (global step 19200: loss: 0.4856550395488739, lr: 5e-05
2024-01-05 21:43:06 INFO     	 * (global step 19250: loss: 0.5987414419651031, lr: 5e-05
2024-01-05 21:43:12 INFO     	 * (global step 19300: loss: 0.43593260645866394, lr: 5e-05
2024-01-05 21:43:18 INFO     	 * (global step 19350: loss: 0.7200592756271362, lr: 5e-05
2024-01-05 21:43:25 INFO     	 * (global step 19400: loss: 0.6837153732776642, lr: 5e-05
2024-01-05 21:43:31 INFO     	 * (global step 19450: loss: 0.4628619998693466, lr: 5e-05
2024-01-05 21:43:37 INFO     	 * (global step 19500: loss: 0.4755839407444, lr: 5e-05
2024-01-05 21:43:43 INFO     	 * (global step 19550: loss: 0.33304470777511597, lr: 5e-05
2024-01-05 21:43:49 INFO     	 * (global step 19600: loss: 0.5505829155445099, lr: 5e-05
2024-01-05 21:43:55 INFO     	 * (global step 19650: loss: 0.43183885514736176, lr: 5e-05
2024-01-05 21:44:01 INFO     	 * (global step 19700: loss: 0.4810732901096344, lr: 5e-05
2024-01-05 21:44:07 INFO     	 * (global step 19750: loss: 0.40454868972301483, lr: 5e-05
2024-01-05 21:44:13 INFO     	 * (global step 19800: loss: 0.4765483886003494, lr: 5e-05
2024-01-05 21:44:19 INFO     	 * (global step 19850: loss: 0.32845285534858704, lr: 5e-05
2024-01-05 21:44:25 INFO     	 * (global step 19900: loss: 0.4790724813938141, lr: 5e-05
2024-01-05 21:44:31 INFO     	 * (global step 19950: loss: 0.498140811920166, lr: 5e-05
2024-01-05 21:44:37 INFO     	 * (global step 20000: loss: 0.42338964343070984, lr: 5e-05
2024-01-05 21:44:43 INFO     	 * (global step 20050: loss: 0.6261186897754669, lr: 5e-05
2024-01-05 21:44:50 INFO     	 * (global step 20100: loss: 0.6063357293605804, lr: 5e-05
2024-01-05 21:44:56 INFO     	 * (global step 20150: loss: 0.4702247381210327, lr: 5e-05
2024-01-05 21:45:02 INFO     	 * (global step 20200: loss: 0.4078124314546585, lr: 5e-05
2024-01-05 21:45:07 INFO     [epoch 4/15] average loss: 0.551, lr: 5e-05
2024-01-05 21:45:07 INFO     saving model related files
2024-01-05 21:45:07 INFO     saving model
2024-01-05 21:45:07 INFO     saving tokenizer
2024-01-05 21:45:07 INFO     saving optimizer
2024-01-05 21:45:09 INFO     remove old optimizer files
2024-01-05 21:45:10 INFO     	 * (global step 20250: loss: 0.4820857495069504, lr: 5e-05
2024-01-05 21:45:16 INFO     	 * (global step 20300: loss: 0.4908215403556824, lr: 5e-05
2024-01-05 21:45:22 INFO     	 * (global step 20350: loss: 0.6529531180858612, lr: 5e-05
2024-01-05 21:45:28 INFO     	 * (global step 20400: loss: 0.4667331874370575, lr: 5e-05
2024-01-05 21:45:35 INFO     	 * (global step 20450: loss: 0.385873019695282, lr: 5e-05
2024-01-05 21:45:41 INFO     	 * (global step 20500: loss: 0.4410432279109955, lr: 5e-05
2024-01-05 21:45:47 INFO     	 * (global step 20550: loss: 0.5382973998785019, lr: 5e-05
2024-01-05 21:45:53 INFO     	 * (global step 20600: loss: 0.5782226622104645, lr: 5e-05
2024-01-05 21:45:59 INFO     	 * (global step 20650: loss: 0.6924096643924713, lr: 5e-05
2024-01-05 21:46:06 INFO     	 * (global step 20700: loss: 0.7837211787700653, lr: 5e-05
2024-01-05 21:46:12 INFO     	 * (global step 20750: loss: 0.3605235666036606, lr: 5e-05
2024-01-05 21:46:18 INFO     	 * (global step 20800: loss: 0.659844309091568, lr: 5e-05
2024-01-05 21:46:24 INFO     	 * (global step 20850: loss: 0.5531357228755951, lr: 5e-05
2024-01-05 21:46:30 INFO     	 * (global step 20900: loss: 0.3968196362257004, lr: 5e-05
2024-01-05 21:46:37 INFO     	 * (global step 20950: loss: 0.6161742806434631, lr: 5e-05
2024-01-05 21:46:43 INFO     	 * (global step 21000: loss: 0.35321806371212006, lr: 5e-05
2024-01-05 21:46:49 INFO     	 * (global step 21050: loss: 0.6639601588249207, lr: 5e-05
2024-01-05 21:46:55 INFO     	 * (global step 21100: loss: 0.5311940163373947, lr: 5e-05
2024-01-05 21:47:01 INFO     	 * (global step 21150: loss: 0.474666103720665, lr: 5e-05
2024-01-05 21:47:07 INFO     	 * (global step 21200: loss: 0.38298913836479187, lr: 5e-05
2024-01-05 21:47:13 INFO     	 * (global step 21250: loss: 0.36362943053245544, lr: 5e-05
2024-01-05 21:47:19 INFO     	 * (global step 21300: loss: 0.616132989525795, lr: 5e-05
2024-01-05 21:47:25 INFO     	 * (global step 21350: loss: 0.5930428504943848, lr: 5e-05
2024-01-05 21:47:31 INFO     	 * (global step 21400: loss: 0.5265643000602722, lr: 5e-05
2024-01-05 21:47:39 INFO     	 * (global step 21450: loss: 0.45351535081863403, lr: 5e-05
2024-01-05 21:47:47 INFO     	 * (global step 21500: loss: 0.3961985856294632, lr: 5e-05
2024-01-05 21:47:54 INFO     	 * (global step 21550: loss: 0.38214901089668274, lr: 5e-05
2024-01-05 21:48:02 INFO     	 * (global step 21600: loss: 0.4900987893342972, lr: 5e-05
2024-01-05 21:48:10 INFO     	 * (global step 21650: loss: 0.4182358533143997, lr: 5e-05
2024-01-05 21:48:17 INFO     	 * (global step 21700: loss: 0.5552859902381897, lr: 5e-05
2024-01-05 21:48:25 INFO     	 * (global step 21750: loss: 0.5838029980659485, lr: 5e-05
2024-01-05 21:48:32 INFO     	 * (global step 21800: loss: 0.6409906446933746, lr: 5e-05
2024-01-05 21:48:40 INFO     	 * (global step 21850: loss: 0.7316073179244995, lr: 5e-05
2024-01-05 21:48:48 INFO     	 * (global step 21900: loss: 0.650953859090805, lr: 5e-05
2024-01-05 21:48:55 INFO     	 * (global step 21950: loss: 0.6721255630254745, lr: 5e-05
2024-01-05 21:49:03 INFO     	 * (global step 22000: loss: 0.5831395387649536, lr: 5e-05
2024-01-05 21:49:11 INFO     	 * (global step 22050: loss: 0.5460630357265472, lr: 5e-05
2024-01-05 21:49:19 INFO     	 * (global step 22100: loss: 0.5456413924694061, lr: 5e-05
2024-01-05 21:49:26 INFO     	 * (global step 22150: loss: 0.422413170337677, lr: 5e-05
2024-01-05 21:49:34 INFO     	 * (global step 22200: loss: 0.5704731047153473, lr: 5e-05
2024-01-05 21:49:41 INFO     	 * (global step 22250: loss: 0.6998651027679443, lr: 5e-05
2024-01-05 21:49:49 INFO     	 * (global step 22300: loss: 0.5774864703416824, lr: 5e-05
2024-01-05 21:49:57 INFO     	 * (global step 22350: loss: 0.49842286109924316, lr: 5e-05
2024-01-05 21:50:04 INFO     	 * (global step 22400: loss: 0.5346681773662567, lr: 5e-05
2024-01-05 21:50:11 INFO     	 * (global step 22450: loss: 0.5025866627693176, lr: 5e-05
2024-01-05 21:50:17 INFO     	 * (global step 22500: loss: 0.6432874202728271, lr: 5e-05
2024-01-05 21:50:23 INFO     	 * (global step 22550: loss: 0.5384916663169861, lr: 5e-05
2024-01-05 21:50:29 INFO     	 * (global step 22600: loss: 0.5985692739486694, lr: 5e-05
2024-01-05 21:50:35 INFO     	 * (global step 22650: loss: 0.4561682641506195, lr: 5e-05
2024-01-05 21:50:43 INFO     	 * (global step 22700: loss: 0.42172250151634216, lr: 5e-05
2024-01-05 21:50:50 INFO     	 * (global step 22750: loss: 0.4118986576795578, lr: 5e-05
2024-01-05 21:50:58 INFO     	 * (global step 22800: loss: 0.4639781415462494, lr: 5e-05
2024-01-05 21:51:06 INFO     	 * (global step 22850: loss: 0.5575112700462341, lr: 5e-05
2024-01-05 21:51:13 INFO     	 * (global step 22900: loss: 0.3954193890094757, lr: 5e-05
2024-01-05 21:51:21 INFO     	 * (global step 22950: loss: 0.5375876724720001, lr: 5e-05
2024-01-05 21:51:30 INFO     	 * (global step 23000: loss: 0.6219237744808197, lr: 5e-05
2024-01-05 21:51:37 INFO     	 * (global step 23050: loss: 0.3811981528997421, lr: 5e-05
2024-01-05 21:51:45 INFO     	 * (global step 23100: loss: 0.5794939398765564, lr: 5e-05
2024-01-05 21:51:53 INFO     	 * (global step 23150: loss: 0.606695219874382, lr: 5e-05
2024-01-05 21:52:01 INFO     	 * (global step 23200: loss: 0.4266982078552246, lr: 5e-05
2024-01-05 21:52:08 INFO     	 * (global step 23250: loss: 0.5884765684604645, lr: 5e-05
2024-01-05 21:52:16 INFO     	 * (global step 23300: loss: 0.32217881083488464, lr: 5e-05
2024-01-05 21:52:24 INFO     	 * (global step 23350: loss: 0.46643830835819244, lr: 5e-05
2024-01-05 21:52:31 INFO     	 * (global step 23400: loss: 0.42130210995674133, lr: 5e-05
2024-01-05 21:52:39 INFO     	 * (global step 23450: loss: 0.5038267970085144, lr: 5e-05
2024-01-05 21:52:46 INFO     	 * (global step 23500: loss: 0.519778773188591, lr: 5e-05
2024-01-05 21:52:54 INFO     	 * (global step 23550: loss: 0.44699743390083313, lr: 5e-05
2024-01-05 21:53:02 INFO     	 * (global step 23600: loss: 0.6196277439594269, lr: 5e-05
2024-01-05 21:53:10 INFO     	 * (global step 23650: loss: 0.3191501200199127, lr: 5e-05
2024-01-05 21:53:17 INFO     	 * (global step 23700: loss: 0.5066084265708923, lr: 5e-05
2024-01-05 21:53:23 INFO     	 * (global step 23750: loss: 0.608670249581337, lr: 5e-05
2024-01-05 21:53:28 INFO     	 * (global step 23800: loss: 0.5007766634225845, lr: 5e-05
2024-01-05 21:53:34 INFO     	 * (global step 23850: loss: 0.48967090249061584, lr: 5e-05
2024-01-05 21:53:40 INFO     	 * (global step 23900: loss: 0.420108437538147, lr: 5e-05
2024-01-05 21:53:47 INFO     	 * (global step 23950: loss: 0.881172388792038, lr: 5e-05
2024-01-05 21:53:55 INFO     	 * (global step 24000: loss: 0.4405318349599838, lr: 5e-05
2024-01-05 21:54:03 INFO     	 * (global step 24050: loss: 0.7287659347057343, lr: 5e-05
2024-01-05 21:54:11 INFO     	 * (global step 24100: loss: 0.4741858094930649, lr: 5e-05
2024-01-05 21:54:19 INFO     	 * (global step 24150: loss: 0.5508250743150711, lr: 5e-05
2024-01-05 21:54:26 INFO     	 * (global step 24200: loss: 0.5945158451795578, lr: 5e-05
2024-01-05 21:54:34 INFO     	 * (global step 24250: loss: 0.6539730429649353, lr: 5e-05
2024-01-05 21:54:40 INFO     [epoch 5/15] average loss: 0.539, lr: 5e-05
2024-01-05 21:54:40 INFO     saving model related files
2024-01-05 21:54:40 INFO     saving model
2024-01-05 21:54:41 INFO     saving tokenizer
2024-01-05 21:54:41 INFO     saving optimizer
2024-01-05 21:54:43 INFO     remove old optimizer files
2024-01-05 21:54:45 INFO     	 * (global step 24300: loss: 0.4685405492782593, lr: 5e-05
2024-01-05 21:54:53 INFO     	 * (global step 24350: loss: 0.4873732328414917, lr: 5e-05
2024-01-05 21:55:00 INFO     	 * (global step 24400: loss: 0.4472053050994873, lr: 5e-05
2024-01-05 21:55:08 INFO     	 * (global step 24450: loss: 0.4202863425016403, lr: 5e-05
2024-01-05 21:55:16 INFO     	 * (global step 24500: loss: 0.5910156816244125, lr: 5e-05
2024-01-05 21:55:24 INFO     	 * (global step 24550: loss: 0.47678765654563904, lr: 5e-05
2024-01-05 21:55:31 INFO     	 * (global step 24600: loss: 0.5381410419940948, lr: 5e-05
2024-01-05 21:55:39 INFO     	 * (global step 24650: loss: 0.5009232759475708, lr: 5e-05
2024-01-05 21:55:46 INFO     	 * (global step 24700: loss: 0.3881427198648453, lr: 5e-05
2024-01-05 21:55:54 INFO     	 * (global step 24750: loss: 0.5970287322998047, lr: 5e-05
2024-01-05 21:56:01 INFO     	 * (global step 24800: loss: 0.6984204351902008, lr: 5e-05
2024-01-05 21:56:09 INFO     	 * (global step 24850: loss: 0.5279259383678436, lr: 5e-05
2024-01-05 21:56:16 INFO     	 * (global step 24900: loss: 0.6296549439430237, lr: 5e-05
2024-01-05 21:56:22 INFO     	 * (global step 24950: loss: 0.7213798761367798, lr: 5e-05
2024-01-05 21:56:28 INFO     	 * (global step 25000: loss: 0.42939595878124237, lr: 5e-05
2024-01-05 21:56:34 INFO     	 * (global step 25050: loss: 0.6558059751987457, lr: 5e-05
2024-01-05 21:56:40 INFO     	 * (global step 25100: loss: 0.4965302497148514, lr: 5e-05
2024-01-05 21:56:47 INFO     	 * (global step 25150: loss: 0.579458549618721, lr: 5e-05
2024-01-05 21:56:55 INFO     	 * (global step 25200: loss: 0.7476368248462677, lr: 5e-05
2024-01-05 21:57:03 INFO     	 * (global step 25250: loss: 0.4606103450059891, lr: 5e-05
2024-01-05 21:57:10 INFO     	 * (global step 25300: loss: 0.8408240675926208, lr: 5e-05
2024-01-05 21:57:18 INFO     	 * (global step 25350: loss: 0.4388556033372879, lr: 5e-05
2024-01-05 21:57:26 INFO     	 * (global step 25400: loss: 0.5756542831659317, lr: 5e-05
2024-01-05 21:57:34 INFO     	 * (global step 25450: loss: 0.7282440811395645, lr: 5e-05
2024-01-05 21:57:41 INFO     	 * (global step 25500: loss: 0.6247721016407013, lr: 5e-05
2024-01-05 21:57:48 INFO     	 * (global step 25550: loss: 0.3961319923400879, lr: 5e-05
2024-01-05 21:57:56 INFO     	 * (global step 25600: loss: 0.8527546226978302, lr: 5e-05
2024-01-05 21:58:04 INFO     	 * (global step 25650: loss: 0.6259143352508545, lr: 5e-05
2024-01-05 21:58:11 INFO     	 * (global step 25700: loss: 0.6130285859107971, lr: 5e-05
2024-01-05 21:58:19 INFO     	 * (global step 25750: loss: 0.29760652780532837, lr: 5e-05
2024-01-05 21:58:27 INFO     	 * (global step 25800: loss: 0.47596077620983124, lr: 5e-05
2024-01-05 21:58:35 INFO     	 * (global step 25850: loss: 0.4946361929178238, lr: 5e-05
2024-01-05 21:58:43 INFO     	 * (global step 25900: loss: 0.5674674957990646, lr: 5e-05
2024-01-05 21:58:51 INFO     	 * (global step 25950: loss: 0.509112611413002, lr: 5e-05
2024-01-05 21:58:58 INFO     	 * (global step 26000: loss: 0.3747406005859375, lr: 5e-05
2024-01-05 21:59:06 INFO     	 * (global step 26050: loss: 0.4334963262081146, lr: 5e-05
2024-01-05 21:59:13 INFO     	 * (global step 26100: loss: 0.4437602311372757, lr: 5e-05
2024-01-05 21:59:21 INFO     	 * (global step 26150: loss: 0.49222713708877563, lr: 5e-05
2024-01-05 21:59:27 INFO     	 * (global step 26200: loss: 0.6263065040111542, lr: 5e-05
2024-01-05 21:59:33 INFO     	 * (global step 26250: loss: 0.39671868085861206, lr: 5e-05
2024-01-05 21:59:39 INFO     	 * (global step 26300: loss: 0.510232537984848, lr: 5e-05
2024-01-05 21:59:45 INFO     	 * (global step 26350: loss: 0.5186064839363098, lr: 5e-05
2024-01-05 21:59:52 INFO     	 * (global step 26400: loss: 0.5124475061893463, lr: 5e-05
2024-01-05 21:59:59 INFO     	 * (global step 26450: loss: 0.4266674518585205, lr: 5e-05
2024-01-05 22:00:07 INFO     	 * (global step 26500: loss: 0.8361595571041107, lr: 5e-05
2024-01-05 22:00:15 INFO     	 * (global step 26550: loss: 0.5822503417730331, lr: 5e-05
2024-01-05 22:00:22 INFO     	 * (global step 26600: loss: 0.5778754502534866, lr: 5e-05
2024-01-05 22:00:30 INFO     	 * (global step 26650: loss: 0.44293348491191864, lr: 5e-05
2024-01-05 22:00:37 INFO     	 * (global step 26700: loss: 0.6754543632268906, lr: 5e-05
2024-01-05 22:00:45 INFO     	 * (global step 26750: loss: 0.4079846739768982, lr: 5e-05
2024-01-05 22:00:52 INFO     	 * (global step 26800: loss: 0.6207956075668335, lr: 5e-05
2024-01-05 22:01:00 INFO     	 * (global step 26850: loss: 0.7369159162044525, lr: 5e-05
2024-01-05 22:01:07 INFO     	 * (global step 26900: loss: 0.4049811363220215, lr: 5e-05
2024-01-05 22:01:15 INFO     	 * (global step 26950: loss: 0.5030632317066193, lr: 5e-05
2024-01-05 22:01:22 INFO     	 * (global step 27000: loss: 0.5065327882766724, lr: 5e-05
2024-01-05 22:01:30 INFO     	 * (global step 27050: loss: 0.41642678529024124, lr: 5e-05
2024-01-05 22:01:38 INFO     	 * (global step 27100: loss: 0.42270366847515106, lr: 5e-05
2024-01-05 22:01:45 INFO     	 * (global step 27150: loss: 0.5459201782941818, lr: 5e-05
2024-01-05 22:01:53 INFO     	 * (global step 27200: loss: 0.44194647669792175, lr: 5e-05
2024-01-05 22:02:00 INFO     	 * (global step 27250: loss: 0.3586021810770035, lr: 5e-05
2024-01-05 22:02:08 INFO     	 * (global step 27300: loss: 0.5773689150810242, lr: 5e-05
2024-01-05 22:02:15 INFO     	 * (global step 27350: loss: 0.4459088444709778, lr: 5e-05
2024-01-05 22:02:22 INFO     	 * (global step 27400: loss: 0.5855796039104462, lr: 5e-05
2024-01-05 22:02:28 INFO     	 * (global step 27450: loss: 0.4336130768060684, lr: 5e-05
2024-01-05 22:02:34 INFO     	 * (global step 27500: loss: 0.44784075021743774, lr: 5e-05
2024-01-05 22:02:40 INFO     	 * (global step 27550: loss: 0.3926994800567627, lr: 5e-05
2024-01-05 22:02:46 INFO     	 * (global step 27600: loss: 0.41815805435180664, lr: 5e-05
2024-01-05 22:02:53 INFO     	 * (global step 27650: loss: 0.5512478947639465, lr: 5e-05
2024-01-05 22:03:01 INFO     	 * (global step 27700: loss: 0.5498455464839935, lr: 5e-05
2024-01-05 22:03:09 INFO     	 * (global step 27750: loss: 0.718279629945755, lr: 5e-05
2024-01-05 22:03:17 INFO     	 * (global step 27800: loss: 0.5471982806921005, lr: 5e-05
2024-01-05 22:03:25 INFO     	 * (global step 27850: loss: 0.607463151216507, lr: 5e-05
2024-01-05 22:03:32 INFO     	 * (global step 27900: loss: 0.5757426023483276, lr: 5e-05
2024-01-05 22:03:40 INFO     	 * (global step 27950: loss: 0.4230239689350128, lr: 5e-05
2024-01-05 22:03:48 INFO     	 * (global step 28000: loss: 0.4504931569099426, lr: 5e-05
2024-01-05 22:03:56 INFO     	 * (global step 28050: loss: 0.7412215173244476, lr: 5e-05
2024-01-05 22:04:04 INFO     	 * (global step 28100: loss: 0.6036347299814224, lr: 5e-05
2024-01-05 22:04:11 INFO     	 * (global step 28150: loss: 0.5380952209234238, lr: 5e-05
2024-01-05 22:04:19 INFO     	 * (global step 28200: loss: 0.6055789738893509, lr: 5e-05
2024-01-05 22:04:26 INFO     	 * (global step 28250: loss: 0.5615822672843933, lr: 5e-05
2024-01-05 22:04:33 INFO     	 * (global step 28300: loss: 0.6975025832653046, lr: 5e-05
2024-01-05 22:04:39 INFO     [epoch 6/15] average loss: 0.527, lr: 5e-05
2024-01-05 22:04:39 INFO     saving model related files
2024-01-05 22:04:39 INFO     saving model
2024-01-05 22:04:40 INFO     saving tokenizer
2024-01-05 22:04:40 INFO     saving optimizer
2024-01-05 22:04:41 INFO     remove old optimizer files
2024-01-05 22:04:43 INFO     	 * (global step 28350: loss: 0.6176671087741852, lr: 5e-05
2024-01-05 22:04:50 INFO     	 * (global step 28400: loss: 0.4403996169567108, lr: 5e-05
2024-01-05 22:04:57 INFO     	 * (global step 28450: loss: 0.5284785628318787, lr: 5e-05
2024-01-05 22:05:04 INFO     	 * (global step 28500: loss: 0.5590457022190094, lr: 5e-05
2024-01-05 22:05:12 INFO     	 * (global step 28550: loss: 0.5584724247455597, lr: 5e-05
2024-01-05 22:05:19 INFO     	 * (global step 28600: loss: 0.582430511713028, lr: 5e-05
2024-01-05 22:05:27 INFO     	 * (global step 28650: loss: 0.6318022608757019, lr: 5e-05
2024-01-05 22:05:34 INFO     	 * (global step 28700: loss: 0.5052514225244522, lr: 5e-05
2024-01-05 22:05:40 INFO     	 * (global step 28750: loss: 0.5454187989234924, lr: 5e-05
2024-01-05 22:05:46 INFO     	 * (global step 28800: loss: 0.4473949819803238, lr: 5e-05
2024-01-05 22:05:52 INFO     	 * (global step 28850: loss: 0.6053162515163422, lr: 5e-05
2024-01-05 22:05:58 INFO     	 * (global step 28900: loss: 0.5817747712135315, lr: 5e-05
2024-01-05 22:06:05 INFO     	 * (global step 28950: loss: 0.4989069104194641, lr: 5e-05
2024-01-05 22:06:12 INFO     	 * (global step 29000: loss: 0.4953674077987671, lr: 5e-05
2024-01-05 22:06:20 INFO     	 * (global step 29050: loss: 0.4037279635667801, lr: 5e-05
2024-01-05 22:06:27 INFO     	 * (global step 29100: loss: 0.5695659816265106, lr: 5e-05
2024-01-05 22:06:35 INFO     	 * (global step 29150: loss: 0.7133282721042633, lr: 5e-05
2024-01-05 22:06:42 INFO     	 * (global step 29200: loss: 0.626591145992279, lr: 5e-05
2024-01-05 22:06:50 INFO     	 * (global step 29250: loss: 0.47327058017253876, lr: 5e-05
2024-01-05 22:06:57 INFO     	 * (global step 29300: loss: 0.3798090070486069, lr: 5e-05
2024-01-05 22:07:05 INFO     	 * (global step 29350: loss: 0.5239095985889435, lr: 5e-05
2024-01-05 22:07:12 INFO     	 * (global step 29400: loss: 0.3847843259572983, lr: 5e-05
2024-01-05 22:07:20 INFO     	 * (global step 29450: loss: 0.5419279932975769, lr: 5e-05
2024-01-05 22:07:27 INFO     	 * (global step 29500: loss: 0.3396333009004593, lr: 5e-05
2024-01-05 22:07:35 INFO     	 * (global step 29550: loss: 0.4013364613056183, lr: 5e-05
2024-01-05 22:07:42 INFO     	 * (global step 29600: loss: 0.33788876235485077, lr: 5e-05
2024-01-05 22:07:49 INFO     	 * (global step 29650: loss: 0.41988736391067505, lr: 5e-05
2024-01-05 22:07:57 INFO     	 * (global step 29700: loss: 0.50160051882267, lr: 5e-05
2024-01-05 22:08:05 INFO     	 * (global step 29750: loss: 0.7913614213466644, lr: 5e-05
2024-01-05 22:08:13 INFO     	 * (global step 29800: loss: 0.539066880941391, lr: 5e-05
2024-01-05 22:08:20 INFO     	 * (global step 29850: loss: 0.5064335465431213, lr: 5e-05
2024-01-05 22:08:28 INFO     	 * (global step 29900: loss: 0.468238964676857, lr: 5e-05
2024-01-05 22:08:35 INFO     	 * (global step 29950: loss: 0.5347343981266022, lr: 5e-05
2024-01-05 22:08:41 INFO     	 * (global step 30000: loss: 0.5191867351531982, lr: 5e-05
2024-01-05 22:08:46 INFO     	 * (global step 30050: loss: 0.5250914990901947, lr: 5e-05
2024-01-05 22:08:52 INFO     	 * (global step 30100: loss: 0.5118107199668884, lr: 5e-05
2024-01-05 22:08:59 INFO     	 * (global step 30150: loss: 0.4801890105009079, lr: 5e-05
2024-01-05 22:09:06 INFO     	 * (global step 30200: loss: 0.5973563641309738, lr: 5e-05
2024-01-05 22:09:13 INFO     	 * (global step 30250: loss: 0.5698439478874207, lr: 5e-05
2024-01-05 22:09:20 INFO     	 * (global step 30300: loss: 0.6721609234809875, lr: 5e-05
2024-01-05 22:09:28 INFO     	 * (global step 30350: loss: 0.3417530506849289, lr: 5e-05
2024-01-05 22:09:35 INFO     	 * (global step 30400: loss: 0.5526855289936066, lr: 5e-05
2024-01-05 22:09:43 INFO     	 * (global step 30450: loss: 0.9992088079452515, lr: 5e-05
2024-01-05 22:09:51 INFO     	 * (global step 30500: loss: 0.5401129722595215, lr: 5e-05
2024-01-05 22:09:58 INFO     	 * (global step 30550: loss: 0.3136191666126251, lr: 5e-05
2024-01-05 22:10:06 INFO     	 * (global step 30600: loss: 0.640632688999176, lr: 5e-05
2024-01-05 22:10:13 INFO     	 * (global step 30650: loss: 0.7288487255573273, lr: 5e-05
2024-01-05 22:10:21 INFO     	 * (global step 30700: loss: 0.3679448962211609, lr: 5e-05
2024-01-05 22:10:28 INFO     	 * (global step 30750: loss: 0.5083967596292496, lr: 5e-05
2024-01-05 22:10:36 INFO     	 * (global step 30800: loss: 0.3978351205587387, lr: 5e-05
2024-01-05 22:10:44 INFO     	 * (global step 30850: loss: 0.3572954684495926, lr: 5e-05
2024-01-05 22:10:51 INFO     	 * (global step 30900: loss: 0.6616310477256775, lr: 5e-05
2024-01-05 22:10:59 INFO     	 * (global step 30950: loss: 0.4279777407646179, lr: 5e-05
2024-01-05 22:11:07 INFO     	 * (global step 31000: loss: 0.6502305269241333, lr: 5e-05
2024-01-05 22:11:15 INFO     	 * (global step 31050: loss: 0.3856862485408783, lr: 5e-05
2024-01-05 22:11:22 INFO     	 * (global step 31100: loss: 0.38906237483024597, lr: 5e-05
2024-01-05 22:11:29 INFO     	 * (global step 31150: loss: 0.4524857848882675, lr: 5e-05
2024-01-05 22:11:37 INFO     	 * (global step 31200: loss: 0.4942668378353119, lr: 5e-05
2024-01-05 22:11:43 INFO     	 * (global step 31250: loss: 0.476217657327652, lr: 5e-05
2024-01-05 22:11:48 INFO     	 * (global step 31300: loss: 0.7278552353382111, lr: 5e-05
2024-01-05 22:11:55 INFO     	 * (global step 31350: loss: 0.524539440870285, lr: 5e-05
2024-01-05 22:12:01 INFO     	 * (global step 31400: loss: 0.7485175132751465, lr: 5e-05
2024-01-05 22:12:08 INFO     	 * (global step 31450: loss: 0.5850009471178055, lr: 5e-05
2024-01-05 22:12:15 INFO     	 * (global step 31500: loss: 0.37022434175014496, lr: 5e-05
2024-01-05 22:12:23 INFO     	 * (global step 31550: loss: 0.4097634553909302, lr: 5e-05
2024-01-05 22:12:32 INFO     	 * (global step 31600: loss: 0.5083413571119308, lr: 5e-05
2024-01-05 22:12:39 INFO     	 * (global step 31650: loss: 0.5804620385169983, lr: 5e-05
2024-01-05 22:12:48 INFO     	 * (global step 31700: loss: 0.5946210622787476, lr: 5e-05
2024-01-05 22:12:55 INFO     	 * (global step 31750: loss: 0.5423876643180847, lr: 5e-05
2024-01-05 22:13:03 INFO     	 * (global step 31800: loss: 0.40947510302066803, lr: 5e-05
2024-01-05 22:13:11 INFO     	 * (global step 31850: loss: 0.571436882019043, lr: 5e-05
2024-01-05 22:13:18 INFO     	 * (global step 31900: loss: 0.4681064635515213, lr: 5e-05
2024-01-05 22:13:26 INFO     	 * (global step 31950: loss: 0.4526914805173874, lr: 5e-05
2024-01-05 22:13:34 INFO     	 * (global step 32000: loss: 0.5555878281593323, lr: 5e-05
2024-01-05 22:13:41 INFO     	 * (global step 32050: loss: 0.4968770146369934, lr: 5e-05
2024-01-05 22:13:49 INFO     	 * (global step 32100: loss: 0.5425785779953003, lr: 5e-05
2024-01-05 22:13:57 INFO     	 * (global step 32150: loss: 0.3382249027490616, lr: 5e-05
2024-01-05 22:14:05 INFO     	 * (global step 32200: loss: 0.46097899973392487, lr: 5e-05
2024-01-05 22:14:13 INFO     	 * (global step 32250: loss: 0.3662757873535156, lr: 5e-05
2024-01-05 22:14:21 INFO     	 * (global step 32300: loss: 0.5234182476997375, lr: 5e-05
2024-01-05 22:14:29 INFO     	 * (global step 32350: loss: 0.41644370555877686, lr: 5e-05
2024-01-05 22:14:34 INFO     [epoch 7/15] average loss: 0.517, lr: 5e-05
2024-01-05 22:14:34 INFO     saving model related files
2024-01-05 22:14:34 INFO     saving model
2024-01-05 22:14:35 INFO     saving tokenizer
2024-01-05 22:14:35 INFO     saving optimizer
2024-01-05 22:14:36 INFO     remove old optimizer files
2024-01-05 22:14:39 INFO     	 * (global step 32400: loss: 0.651443213224411, lr: 5e-05
2024-01-05 22:14:46 INFO     	 * (global step 32450: loss: 0.30893197655677795, lr: 5e-05
2024-01-05 22:14:53 INFO     	 * (global step 32500: loss: 0.48735371232032776, lr: 5e-05
2024-01-05 22:14:59 INFO     	 * (global step 32550: loss: 0.424004390835762, lr: 5e-05
2024-01-05 22:15:05 INFO     	 * (global step 32600: loss: 0.5125104039907455, lr: 5e-05
2024-01-05 22:15:11 INFO     	 * (global step 32650: loss: 0.46145007014274597, lr: 5e-05
2024-01-05 22:15:17 INFO     	 * (global step 32700: loss: 0.35756194591522217, lr: 5e-05
2024-01-05 22:15:24 INFO     	 * (global step 32750: loss: 0.4265541136264801, lr: 5e-05
2024-01-05 22:15:31 INFO     	 * (global step 32800: loss: 0.5785884708166122, lr: 5e-05
2024-01-05 22:15:39 INFO     	 * (global step 32850: loss: 0.4118805378675461, lr: 5e-05
2024-01-05 22:15:47 INFO     	 * (global step 32900: loss: 0.47565653920173645, lr: 5e-05
2024-01-05 22:15:55 INFO     	 * (global step 32950: loss: 0.5629478693008423, lr: 5e-05
2024-01-05 22:16:02 INFO     	 * (global step 33000: loss: 0.29821673035621643, lr: 5e-05
2024-01-05 22:16:09 INFO     	 * (global step 33050: loss: 0.4978630989789963, lr: 5e-05
2024-01-05 22:16:16 INFO     	 * (global step 33100: loss: 0.5789423137903214, lr: 5e-05
2024-01-05 22:16:24 INFO     	 * (global step 33150: loss: 0.4091591387987137, lr: 5e-05
2024-01-05 22:16:30 INFO     	 * (global step 33200: loss: 0.44858258962631226, lr: 5e-05
2024-01-05 22:16:38 INFO     	 * (global step 33250: loss: 0.4951242059469223, lr: 5e-05
2024-01-05 22:16:45 INFO     	 * (global step 33300: loss: 0.1875440664589405, lr: 5e-05
2024-01-05 22:16:52 INFO     	 * (global step 33350: loss: 0.5962259471416473, lr: 5e-05
2024-01-05 22:17:00 INFO     	 * (global step 33400: loss: 0.44214875996112823, lr: 5e-05
2024-01-05 22:17:07 INFO     	 * (global step 33450: loss: 0.4621477425098419, lr: 5e-05
2024-01-05 22:17:15 INFO     	 * (global step 33500: loss: 0.5718341618776321, lr: 5e-05
2024-01-05 22:17:22 INFO     	 * (global step 33550: loss: 0.4353422001004219, lr: 5e-05
2024-01-05 22:17:29 INFO     	 * (global step 33600: loss: 0.5502960532903671, lr: 5e-05
2024-01-05 22:17:36 INFO     	 * (global step 33650: loss: 0.6077868491411209, lr: 5e-05
2024-01-05 22:17:43 INFO     	 * (global step 33700: loss: 0.34685809165239334, lr: 5e-05
2024-01-05 22:17:49 INFO     	 * (global step 33750: loss: 0.367112934589386, lr: 5e-05
2024-01-05 22:17:54 INFO     	 * (global step 33800: loss: 0.5559539198875427, lr: 5e-05
2024-01-05 22:18:00 INFO     	 * (global step 33850: loss: 0.45134437084198, lr: 5e-05
2024-01-05 22:18:06 INFO     	 * (global step 33900: loss: 0.4484339654445648, lr: 5e-05
2024-01-05 22:18:14 INFO     	 * (global step 33950: loss: 0.4183300584554672, lr: 5e-05
2024-01-05 22:18:21 INFO     	 * (global step 34000: loss: 0.6397307217121124, lr: 5e-05
2024-01-05 22:18:28 INFO     	 * (global step 34050: loss: 0.8699377328157425, lr: 5e-05
2024-01-05 22:18:36 INFO     	 * (global step 34100: loss: 0.5934635102748871, lr: 5e-05
2024-01-05 22:18:43 INFO     	 * (global step 34150: loss: 0.4198372960090637, lr: 5e-05
2024-01-05 22:18:50 INFO     	 * (global step 34200: loss: 0.5117095857858658, lr: 5e-05
2024-01-05 22:18:57 INFO     	 * (global step 34250: loss: 0.5234377384185791, lr: 5e-05
2024-01-05 22:19:05 INFO     	 * (global step 34300: loss: 0.32630710303783417, lr: 5e-05
2024-01-05 22:19:12 INFO     	 * (global step 34350: loss: 0.5033961534500122, lr: 5e-05
2024-01-05 22:19:20 INFO     	 * (global step 34400: loss: 0.32152844965457916, lr: 5e-05
2024-01-05 22:19:27 INFO     	 * (global step 34450: loss: 0.5436675548553467, lr: 5e-05
2024-01-05 22:19:35 INFO     	 * (global step 34500: loss: 0.31451788544654846, lr: 5e-05
2024-01-05 22:19:43 INFO     	 * (global step 34550: loss: 0.40818409621715546, lr: 5e-05
2024-01-05 22:19:50 INFO     	 * (global step 34600: loss: 0.5860623866319656, lr: 5e-05
2024-01-05 22:19:58 INFO     	 * (global step 34650: loss: 0.45104464888572693, lr: 5e-05
2024-01-05 22:20:05 INFO     	 * (global step 34700: loss: 0.63995561003685, lr: 5e-05
2024-01-05 22:20:13 INFO     	 * (global step 34750: loss: 0.588903397321701, lr: 5e-05
2024-01-05 22:20:20 INFO     	 * (global step 34800: loss: 0.4494805932044983, lr: 5e-05
2024-01-05 22:20:27 INFO     	 * (global step 34850: loss: 0.6520035266876221, lr: 5e-05
2024-01-05 22:20:33 INFO     	 * (global step 34900: loss: 0.4779351204633713, lr: 5e-05
2024-01-05 22:20:39 INFO     	 * (global step 34950: loss: 0.43794772028923035, lr: 5e-05
2024-01-05 22:20:46 INFO     	 * (global step 35000: loss: 0.4578457921743393, lr: 5e-05
2024-01-05 22:20:51 INFO     	 * (global step 35050: loss: 0.8305965662002563, lr: 5e-05
2024-01-05 22:20:59 INFO     	 * (global step 35100: loss: 0.5553458333015442, lr: 5e-05
2024-01-05 22:21:06 INFO     	 * (global step 35150: loss: 0.5152894705533981, lr: 5e-05
2024-01-05 22:21:14 INFO     	 * (global step 35200: loss: 0.5567398816347122, lr: 5e-05
2024-01-05 22:21:21 INFO     	 * (global step 35250: loss: 0.47264181077480316, lr: 5e-05
2024-01-05 22:21:29 INFO     	 * (global step 35300: loss: 0.4514678716659546, lr: 5e-05
2024-01-05 22:21:36 INFO     	 * (global step 35350: loss: 0.4258258789777756, lr: 5e-05
2024-01-05 22:21:44 INFO     	 * (global step 35400: loss: 0.47314949333667755, lr: 5e-05
2024-01-05 22:21:51 INFO     	 * (global step 35450: loss: 0.48427166044712067, lr: 5e-05
2024-01-05 22:21:59 INFO     	 * (global step 35500: loss: 0.4505365192890167, lr: 5e-05
2024-01-05 22:22:06 INFO     	 * (global step 35550: loss: 0.47417183220386505, lr: 5e-05
2024-01-05 22:22:14 INFO     	 * (global step 35600: loss: 0.48448561131954193, lr: 5e-05
2024-01-05 22:22:21 INFO     	 * (global step 35650: loss: 0.30252163857221603, lr: 5e-05
2024-01-05 22:22:29 INFO     	 * (global step 35700: loss: 0.6196186542510986, lr: 5e-05
2024-01-05 22:22:36 INFO     	 * (global step 35750: loss: 0.5236770659685135, lr: 5e-05
2024-01-05 22:22:44 INFO     	 * (global step 35800: loss: 0.45757368206977844, lr: 5e-05
2024-01-05 22:22:51 INFO     	 * (global step 35850: loss: 0.502726212143898, lr: 5e-05
2024-01-05 22:22:59 INFO     	 * (global step 35900: loss: 0.7099941670894623, lr: 5e-05
2024-01-05 22:23:07 INFO     	 * (global step 35950: loss: 0.8206248879432678, lr: 5e-05
2024-01-05 22:23:14 INFO     	 * (global step 36000: loss: 0.5606225281953812, lr: 5e-05
2024-01-05 22:23:21 INFO     	 * (global step 36050: loss: 0.6715207993984222, lr: 5e-05
2024-01-05 22:23:27 INFO     	 * (global step 36100: loss: 0.43218179047107697, lr: 5e-05
2024-01-05 22:23:33 INFO     	 * (global step 36150: loss: 0.5286659896373749, lr: 5e-05
2024-01-05 22:23:39 INFO     	 * (global step 36200: loss: 0.5031812787055969, lr: 5e-05
2024-01-05 22:23:45 INFO     	 * (global step 36250: loss: 0.44554053246974945, lr: 5e-05
2024-01-05 22:23:51 INFO     	 * (global step 36300: loss: 0.4201575517654419, lr: 5e-05
2024-01-05 22:23:59 INFO     	 * (global step 36350: loss: 0.3782188445329666, lr: 5e-05
2024-01-05 22:24:06 INFO     	 * (global step 36400: loss: 0.43410322070121765, lr: 5e-05
2024-01-05 22:24:11 INFO     [epoch 8/15] average loss: 0.508, lr: 5e-05
2024-01-05 22:24:11 INFO     saving model related files
2024-01-05 22:24:11 INFO     saving model
2024-01-05 22:24:12 INFO     saving tokenizer
2024-01-05 22:24:12 INFO     saving optimizer
2024-01-05 22:24:14 INFO     remove old optimizer files
2024-01-05 22:24:17 INFO     	 * (global step 36450: loss: 0.31098659336566925, lr: 5e-05
2024-01-05 22:24:24 INFO     	 * (global step 36500: loss: 0.5610938966274261, lr: 5e-05
2024-01-05 22:24:31 INFO     	 * (global step 36550: loss: 0.553243488073349, lr: 5e-05
2024-01-05 22:24:39 INFO     	 * (global step 36600: loss: 0.35882094502449036, lr: 5e-05
2024-01-05 22:24:46 INFO     	 * (global step 36650: loss: 0.4110525846481323, lr: 5e-05
2024-01-05 22:24:54 INFO     	 * (global step 36700: loss: 0.5497956275939941, lr: 5e-05
2024-01-05 22:25:01 INFO     	 * (global step 36750: loss: 0.4877392649650574, lr: 5e-05
2024-01-05 22:25:08 INFO     	 * (global step 36800: loss: 0.6710655689239502, lr: 5e-05
2024-01-05 22:25:16 INFO     	 * (global step 36850: loss: 0.4855130463838577, lr: 5e-05
2024-01-05 22:25:24 INFO     	 * (global step 36900: loss: 0.4911147505044937, lr: 5e-05
2024-01-05 22:25:31 INFO     	 * (global step 36950: loss: 0.38425345718860626, lr: 5e-05
2024-01-05 22:25:39 INFO     	 * (global step 37000: loss: 0.628566101193428, lr: 5e-05
2024-01-05 22:25:46 INFO     	 * (global step 37050: loss: 0.45005129277706146, lr: 5e-05
2024-01-05 22:25:54 INFO     	 * (global step 37100: loss: 0.5400534421205521, lr: 5e-05
2024-01-05 22:26:01 INFO     	 * (global step 37150: loss: 0.8065824508666992, lr: 5e-05
2024-01-05 22:26:08 INFO     	 * (global step 37200: loss: 0.6228852868080139, lr: 5e-05
2024-01-05 22:26:14 INFO     	 * (global step 37250: loss: 0.5587629228830338, lr: 5e-05
2024-01-05 22:26:20 INFO     	 * (global step 37300: loss: 0.43044161796569824, lr: 5e-05
2024-01-05 22:26:26 INFO     	 * (global step 37350: loss: 0.35935020446777344, lr: 5e-05
2024-01-05 22:26:32 INFO     	 * (global step 37400: loss: 0.7237937450408936, lr: 5e-05
2024-01-05 22:26:39 INFO     	 * (global step 37450: loss: 0.29351821541786194, lr: 5e-05
2024-01-05 22:26:47 INFO     	 * (global step 37500: loss: 0.7380918264389038, lr: 5e-05
2024-01-05 22:26:54 INFO     	 * (global step 37550: loss: 0.3642144054174423, lr: 5e-05
2024-01-05 22:27:02 INFO     	 * (global step 37600: loss: 0.5320923626422882, lr: 5e-05
2024-01-05 22:27:09 INFO     	 * (global step 37650: loss: 0.46813103556632996, lr: 5e-05
2024-01-05 22:27:16 INFO     	 * (global step 37700: loss: 0.5398252308368683, lr: 5e-05
2024-01-05 22:27:23 INFO     	 * (global step 37750: loss: 0.5443337261676788, lr: 5e-05
2024-01-05 22:27:31 INFO     	 * (global step 37800: loss: 0.4412463903427124, lr: 5e-05
2024-01-05 22:27:38 INFO     	 * (global step 37850: loss: 0.39708420634269714, lr: 5e-05
2024-01-05 22:27:46 INFO     	 * (global step 37900: loss: 0.4024782180786133, lr: 5e-05
2024-01-05 22:27:54 INFO     	 * (global step 37950: loss: 0.2976549565792084, lr: 5e-05
2024-01-05 22:28:01 INFO     	 * (global step 38000: loss: 0.5432971119880676, lr: 5e-05
2024-01-05 22:28:09 INFO     	 * (global step 38050: loss: 0.5379088073968887, lr: 5e-05
2024-01-05 22:28:16 INFO     	 * (global step 38100: loss: 0.4481382668018341, lr: 5e-05
2024-01-05 22:28:24 INFO     	 * (global step 38150: loss: 0.5476585924625397, lr: 5e-05
2024-01-05 22:28:31 INFO     	 * (global step 38200: loss: 0.4266364052891731, lr: 5e-05
2024-01-05 22:28:39 INFO     	 * (global step 38250: loss: 0.5597740560770035, lr: 5e-05
2024-01-05 22:28:46 INFO     	 * (global step 38300: loss: 0.4921102076768875, lr: 5e-05
2024-01-05 22:28:54 INFO     	 * (global step 38350: loss: 0.5969580709934235, lr: 5e-05
2024-01-05 22:29:01 INFO     	 * (global step 38400: loss: 0.7170390188694, lr: 5e-05
2024-01-05 22:29:07 INFO     	 * (global step 38450: loss: 0.4849344938993454, lr: 5e-05
2024-01-05 22:29:12 INFO     	 * (global step 38500: loss: 0.4459362328052521, lr: 5e-05
2024-01-05 22:29:18 INFO     	 * (global step 38550: loss: 0.2614109441637993, lr: 5e-05
2024-01-05 22:29:24 INFO     	 * (global step 38600: loss: 0.25843607634305954, lr: 5e-05
2024-01-05 22:29:31 INFO     	 * (global step 38650: loss: 0.7377847731113434, lr: 5e-05
2024-01-05 22:29:39 INFO     	 * (global step 38700: loss: 0.4332466423511505, lr: 5e-05
2024-01-05 22:29:47 INFO     	 * (global step 38750: loss: 0.4669334292411804, lr: 5e-05
2024-01-05 22:29:54 INFO     	 * (global step 38800: loss: 0.25827692449092865, lr: 5e-05
2024-01-05 22:30:01 INFO     	 * (global step 38850: loss: 0.4814508557319641, lr: 5e-05
2024-01-05 22:30:09 INFO     	 * (global step 38900: loss: 0.39576978981494904, lr: 5e-05
2024-01-05 22:30:16 INFO     	 * (global step 38950: loss: 0.6023375988006592, lr: 5e-05
2024-01-05 22:30:24 INFO     	 * (global step 39000: loss: 0.43332114815711975, lr: 5e-05
2024-01-05 22:30:31 INFO     	 * (global step 39050: loss: 0.4910185933113098, lr: 5e-05
2024-01-05 22:30:38 INFO     	 * (global step 39100: loss: 0.47948455810546875, lr: 5e-05
2024-01-05 22:30:46 INFO     	 * (global step 39150: loss: 0.6761064380407333, lr: 5e-05
2024-01-05 22:30:54 INFO     	 * (global step 39200: loss: 0.4303726404905319, lr: 5e-05
2024-01-05 22:31:01 INFO     	 * (global step 39250: loss: 0.5020192712545395, lr: 5e-05
2024-01-05 22:31:09 INFO     	 * (global step 39300: loss: 0.4448082000017166, lr: 5e-05
2024-01-05 22:31:16 INFO     	 * (global step 39350: loss: 0.4415454715490341, lr: 5e-05
2024-01-05 22:31:23 INFO     	 * (global step 39400: loss: 0.42278580367565155, lr: 5e-05
2024-01-05 22:31:31 INFO     	 * (global step 39450: loss: 0.5251545757055283, lr: 5e-05
2024-01-05 22:31:39 INFO     	 * (global step 39500: loss: 0.6206365823745728, lr: 5e-05
2024-01-05 22:31:46 INFO     	 * (global step 39550: loss: 0.5933529138565063, lr: 5e-05
2024-01-05 22:31:53 INFO     	 * (global step 39600: loss: 0.7226487696170807, lr: 5e-05
2024-01-05 22:31:59 INFO     	 * (global step 39650: loss: 0.48430904746055603, lr: 5e-05
2024-01-05 22:32:04 INFO     	 * (global step 39700: loss: 0.44084060192108154, lr: 5e-05
2024-01-05 22:32:11 INFO     	 * (global step 39750: loss: 0.5957741141319275, lr: 5e-05
2024-01-05 22:32:17 INFO     	 * (global step 39800: loss: 0.9193267226219177, lr: 5e-05
2024-01-05 22:32:25 INFO     	 * (global step 39850: loss: 0.762859046459198, lr: 5e-05
2024-01-05 22:32:33 INFO     	 * (global step 39900: loss: 0.4024251252412796, lr: 5e-05
2024-01-05 22:32:41 INFO     	 * (global step 39950: loss: 0.374126672744751, lr: 5e-05
2024-01-05 22:32:49 INFO     	 * (global step 40000: loss: 0.7758389115333557, lr: 5e-05
2024-01-05 22:32:57 INFO     	 * (global step 40050: loss: 0.4864044487476349, lr: 5e-05
2024-01-05 22:33:05 INFO     	 * (global step 40100: loss: 0.5760277807712555, lr: 5e-05
2024-01-05 22:33:13 INFO     	 * (global step 40150: loss: 0.6504160463809967, lr: 5e-05
2024-01-05 22:33:20 INFO     	 * (global step 40200: loss: 0.6027494668960571, lr: 5e-05
2024-01-05 22:33:28 INFO     	 * (global step 40250: loss: 0.6552129089832306, lr: 5e-05
2024-01-05 22:33:36 INFO     	 * (global step 40300: loss: 0.37995152175426483, lr: 5e-05
2024-01-05 22:33:43 INFO     	 * (global step 40350: loss: 0.6231561601161957, lr: 5e-05
2024-01-05 22:33:51 INFO     	 * (global step 40400: loss: 0.3702329844236374, lr: 5e-05
2024-01-05 22:33:59 INFO     	 * (global step 40450: loss: 0.3664312958717346, lr: 5e-05
2024-01-05 22:34:04 INFO     [epoch 9/15] average loss: 0.5, lr: 5e-05
2024-01-05 22:34:04 INFO     saving model related files
2024-01-05 22:34:04 INFO     saving model
2024-01-05 22:34:05 INFO     saving tokenizer
2024-01-05 22:34:05 INFO     saving optimizer
2024-01-05 22:34:06 INFO     remove old optimizer files
2024-01-05 22:34:06 INFO     complete training: model ckpt was saved at small_recreated_ckpt/model_uramvg
2024-01-05 22:34:07 INFO     ## 1st RUN: Configuration 8/12 ##
2024-01-05 22:34:07 INFO     initialize model trainer
2024-01-05 22:34:07 INFO     initialize checkpoint at small_recreated_ckpt/model_nxaqhy
2024-01-05 22:34:07 INFO     hyperparameters
2024-01-05 22:34:07 INFO     	 * dataset_path: lmqg/qag_squad
2024-01-05 22:34:07 INFO     	 * dataset_name: default
2024-01-05 22:34:07 INFO     	 * input_types: ['paragraph']
2024-01-05 22:34:07 INFO     	 * output_types: ['questions_answers']
2024-01-05 22:34:07 INFO     	 * prefix_types: ['qag']
2024-01-05 22:34:07 INFO     	 * model: t5-small
2024-01-05 22:34:07 INFO     	 * max_length: 512
2024-01-05 22:34:07 INFO     	 * max_length_output: 256
2024-01-05 22:34:07 INFO     	 * epoch: 15
2024-01-05 22:34:07 INFO     	 * batch: 2
2024-01-05 22:34:07 INFO     	 * lr: 1e-05
2024-01-05 22:34:07 INFO     	 * fp16: False
2024-01-05 22:34:07 INFO     	 * random_seed: 1
2024-01-05 22:34:07 INFO     	 * gradient_accumulation_steps: 4
2024-01-05 22:34:07 INFO     	 * label_smoothing: 0.15
2024-01-05 22:34:07 INFO     initialize checkpoint with t5-small
2024-01-05 22:34:19 INFO     use spaCy answer extraction model: positionrank
2024-01-05 22:34:23 INFO     Model `t5-small`
2024-01-05 22:34:23 INFO     	 * Num of GPU in use: 1
2024-01-05 22:34:23 INFO     	 * Prefix: True
2024-01-05 22:34:23 INFO     	 * Language: en (ignore at the training phase)
2024-01-05 22:34:24 INFO     dataset preprocessing
2024-01-05 22:34:28 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featurelmqg/qag_squad/t5-small.512.256.paragraph.questions_answers.train.qag.pkl
2024-01-05 22:34:36 INFO     start model training
2024-01-05 22:34:50 INFO     	 * (global step 50: loss: 4.917056858539581, lr: 1e-05
2024-01-05 22:35:03 INFO     	 * (global step 100: loss: 2.7308374643325806, lr: 1e-05
2024-01-05 22:35:14 INFO     	 * (global step 150: loss: 2.106336772441864, lr: 1e-05
2024-01-05 22:35:26 INFO     	 * (global step 200: loss: 1.856107771396637, lr: 1e-05
2024-01-05 22:35:40 INFO     	 * (global step 250: loss: 1.7061720788478851, lr: 1e-05
2024-01-05 22:35:54 INFO     	 * (global step 300: loss: 1.8264073729515076, lr: 1e-05
2024-01-05 22:36:09 INFO     	 * (global step 350: loss: 1.6261934638023376, lr: 1e-05
2024-01-05 22:36:24 INFO     	 * (global step 400: loss: 1.7227388322353363, lr: 1e-05
2024-01-05 22:36:38 INFO     	 * (global step 450: loss: 1.404605358839035, lr: 1e-05
2024-01-05 22:36:53 INFO     	 * (global step 500: loss: 1.3987509608268738, lr: 1e-05
2024-01-05 22:37:07 INFO     	 * (global step 550: loss: 1.3692759275436401, lr: 1e-05
2024-01-05 22:37:22 INFO     	 * (global step 600: loss: 1.4955163300037384, lr: 1e-05
2024-01-05 22:37:37 INFO     	 * (global step 650: loss: 1.2817923426628113, lr: 1e-05
2024-01-05 22:37:52 INFO     	 * (global step 700: loss: 1.0510715544223785, lr: 1e-05
2024-01-05 22:38:06 INFO     	 * (global step 750: loss: 1.1471454203128815, lr: 1e-05
2024-01-05 22:38:21 INFO     	 * (global step 800: loss: 0.8920959532260895, lr: 1e-05
2024-01-05 22:38:33 INFO     	 * (global step 850: loss: 1.0804439783096313, lr: 1e-05
2024-01-05 22:38:44 INFO     	 * (global step 900: loss: 1.141614705324173, lr: 1e-05
2024-01-05 22:38:57 INFO     	 * (global step 950: loss: 0.9120609313249588, lr: 1e-05
2024-01-05 22:39:11 INFO     	 * (global step 1000: loss: 1.5201641023159027, lr: 1e-05
2024-01-05 22:39:26 INFO     	 * (global step 1050: loss: 0.933428019285202, lr: 1e-05
2024-01-05 22:39:41 INFO     	 * (global step 1100: loss: 1.0352755784988403, lr: 1e-05
2024-01-05 22:39:55 INFO     	 * (global step 1150: loss: 1.443696141242981, lr: 1e-05
2024-01-05 22:40:10 INFO     	 * (global step 1200: loss: 0.8777892887592316, lr: 1e-05
2024-01-05 22:40:25 INFO     	 * (global step 1250: loss: 0.8706676214933395, lr: 1e-05
2024-01-05 22:40:40 INFO     	 * (global step 1300: loss: 0.7320647090673447, lr: 1e-05
2024-01-05 22:40:54 INFO     	 * (global step 1350: loss: 0.9739631712436676, lr: 1e-05
2024-01-05 22:41:09 INFO     	 * (global step 1400: loss: 0.9850045442581177, lr: 1e-05
2024-01-05 22:41:23 INFO     	 * (global step 1450: loss: 1.0478754043579102, lr: 1e-05
2024-01-05 22:41:35 INFO     	 * (global step 1500: loss: 0.8902233242988586, lr: 1e-05
2024-01-05 22:41:46 INFO     	 * (global step 1550: loss: 0.938806802034378, lr: 1e-05
2024-01-05 22:41:59 INFO     	 * (global step 1600: loss: 1.000888854265213, lr: 1e-05
2024-01-05 22:42:14 INFO     	 * (global step 1650: loss: 0.8252349942922592, lr: 1e-05
2024-01-05 22:42:30 INFO     	 * (global step 1700: loss: 0.9014475047588348, lr: 1e-05
2024-01-05 22:42:45 INFO     	 * (global step 1750: loss: 0.7600937187671661, lr: 1e-05
2024-01-05 22:43:00 INFO     	 * (global step 1800: loss: 0.775558814406395, lr: 1e-05
2024-01-05 22:43:14 INFO     	 * (global step 1850: loss: 0.5890153869986534, lr: 1e-05
2024-01-05 22:43:29 INFO     	 * (global step 1900: loss: 0.6798195242881775, lr: 1e-05
2024-01-05 22:43:43 INFO     	 * (global step 1950: loss: 0.9999025613069534, lr: 1e-05
2024-01-05 22:43:58 INFO     	 * (global step 2000: loss: 1.114661067724228, lr: 1e-05
2024-01-05 22:44:05 INFO     [epoch 0/15] average loss: 1.328, lr: 1e-05
2024-01-05 22:44:05 INFO     saving model related files
2024-01-05 22:44:05 INFO     saving model
2024-01-05 22:44:06 INFO     saving tokenizer
2024-01-05 22:44:06 INFO     saving optimizer
2024-01-05 22:44:08 INFO     remove old optimizer files
2024-01-05 22:44:15 INFO     	 * (global step 2050: loss: 0.6726418286561966, lr: 1e-05
2024-01-05 22:44:30 INFO     	 * (global step 2100: loss: 0.7638778537511826, lr: 1e-05
2024-01-05 22:44:45 INFO     	 * (global step 2150: loss: 0.9632978439331055, lr: 1e-05
2024-01-05 22:44:59 INFO     	 * (global step 2200: loss: 0.681011512875557, lr: 1e-05
2024-01-05 22:45:12 INFO     	 * (global step 2250: loss: 0.7123786360025406, lr: 1e-05
2024-01-05 22:45:23 INFO     	 * (global step 2300: loss: 0.8452218472957611, lr: 1e-05
2024-01-05 22:45:35 INFO     	 * (global step 2350: loss: 0.7948266267776489, lr: 1e-05
2024-01-05 22:45:49 INFO     	 * (global step 2400: loss: 0.7758551985025406, lr: 1e-05
2024-01-05 22:46:03 INFO     	 * (global step 2450: loss: 0.7368334829807281, lr: 1e-05
2024-01-05 22:46:17 INFO     	 * (global step 2500: loss: 0.8734922260046005, lr: 1e-05
2024-01-05 22:46:31 INFO     	 * (global step 2550: loss: 0.699472039937973, lr: 1e-05
2024-01-05 22:46:45 INFO     	 * (global step 2600: loss: 0.7330448180437088, lr: 1e-05
2024-01-05 22:46:59 INFO     	 * (global step 2650: loss: 0.8527432680130005, lr: 1e-05
2024-01-05 22:47:13 INFO     	 * (global step 2700: loss: 0.6770761460065842, lr: 1e-05
2024-01-05 22:47:27 INFO     	 * (global step 2750: loss: 0.7258691340684891, lr: 1e-05
2024-01-05 22:47:41 INFO     	 * (global step 2800: loss: 0.7496861219406128, lr: 1e-05
2024-01-05 22:47:52 INFO     	 * (global step 2850: loss: 0.85659608989954, lr: 1e-05
2024-01-05 22:48:04 INFO     	 * (global step 2900: loss: 0.9362447261810303, lr: 1e-05
2024-01-05 22:48:17 INFO     	 * (global step 2950: loss: 0.8823091983795166, lr: 1e-05
2024-01-05 22:48:31 INFO     	 * (global step 3000: loss: 0.7433388531208038, lr: 1e-05
2024-01-05 22:48:45 INFO     	 * (global step 3050: loss: 0.6827368438243866, lr: 1e-05
2024-01-05 22:48:59 INFO     	 * (global step 3100: loss: 0.67319405823946, lr: 1e-05
2024-01-05 22:49:13 INFO     	 * (global step 3150: loss: 0.8194966018199921, lr: 1e-05
2024-01-05 22:49:27 INFO     	 * (global step 3200: loss: 0.6534003838896751, lr: 1e-05
2024-01-05 22:49:41 INFO     	 * (global step 3250: loss: 0.8691475093364716, lr: 1e-05
2024-01-05 22:49:55 INFO     	 * (global step 3300: loss: 0.6192482113838196, lr: 1e-05
2024-01-05 22:50:09 INFO     	 * (global step 3350: loss: 0.759856715798378, lr: 1e-05
2024-01-05 22:50:22 INFO     	 * (global step 3400: loss: 0.8464123904705048, lr: 1e-05
2024-01-05 22:50:34 INFO     	 * (global step 3450: loss: 0.7899551093578339, lr: 1e-05
2024-01-05 22:50:45 INFO     	 * (global step 3500: loss: 0.74592025578022, lr: 1e-05
2024-01-05 22:50:58 INFO     	 * (global step 3550: loss: 0.6957978010177612, lr: 1e-05
2024-01-05 22:51:12 INFO     	 * (global step 3600: loss: 0.7696995735168457, lr: 1e-05
2024-01-05 22:51:26 INFO     	 * (global step 3650: loss: 0.8989294022321701, lr: 1e-05
2024-01-05 22:51:40 INFO     	 * (global step 3700: loss: 1.0762335062026978, lr: 1e-05
2024-01-05 22:51:54 INFO     	 * (global step 3750: loss: 0.7246150225400925, lr: 1e-05
2024-01-05 22:52:08 INFO     	 * (global step 3800: loss: 0.6502870097756386, lr: 1e-05
2024-01-05 22:52:22 INFO     	 * (global step 3850: loss: 1.0039863735437393, lr: 1e-05
2024-01-05 22:52:35 INFO     	 * (global step 3900: loss: 0.7163811177015305, lr: 1e-05
2024-01-05 22:52:49 INFO     	 * (global step 3950: loss: 0.5419599786400795, lr: 1e-05
2024-01-05 22:53:02 INFO     	 * (global step 4000: loss: 0.6173883900046349, lr: 1e-05
2024-01-05 22:53:13 INFO     [epoch 1/15] average loss: 0.775, lr: 1e-05
2024-01-05 22:53:13 INFO     saving model related files
2024-01-05 22:53:13 INFO     saving model
2024-01-05 22:53:13 INFO     saving tokenizer
2024-01-05 22:53:13 INFO     saving optimizer
2024-01-05 22:53:14 INFO     remove old optimizer files
2024-01-05 22:53:15 INFO     	 * (global step 4050: loss: 0.5648557469248772, lr: 1e-05
2024-01-05 22:53:27 INFO     	 * (global step 4100: loss: 0.6937594786286354, lr: 1e-05
2024-01-05 22:53:41 INFO     	 * (global step 4150: loss: 0.7252419143915176, lr: 1e-05
2024-01-05 22:53:56 INFO     	 * (global step 4200: loss: 0.9115630015730858, lr: 1e-05
2024-01-05 22:54:11 INFO     	 * (global step 4250: loss: 0.7358650714159012, lr: 1e-05
2024-01-05 22:54:25 INFO     	 * (global step 4300: loss: 0.5365257114171982, lr: 1e-05
2024-01-05 22:54:39 INFO     	 * (global step 4350: loss: 0.7823908627033234, lr: 1e-05
2024-01-05 22:54:54 INFO     	 * (global step 4400: loss: 0.6044096797704697, lr: 1e-05
2024-01-05 22:55:08 INFO     	 * (global step 4450: loss: 0.6188656911253929, lr: 1e-05
2024-01-05 22:55:22 INFO     	 * (global step 4500: loss: 0.6962577551603317, lr: 1e-05
2024-01-05 22:55:36 INFO     	 * (global step 4550: loss: 0.6810564994812012, lr: 1e-05
2024-01-05 22:55:50 INFO     	 * (global step 4600: loss: 0.6351006701588631, lr: 1e-05
2024-01-05 22:56:05 INFO     	 * (global step 4650: loss: 0.9325925558805466, lr: 1e-05
2024-01-05 22:56:16 INFO     	 * (global step 4700: loss: 0.7275092005729675, lr: 1e-05
2024-01-05 22:56:27 INFO     	 * (global step 4750: loss: 0.6485116258263588, lr: 1e-05
2024-01-05 22:56:40 INFO     	 * (global step 4800: loss: 0.7171027138829231, lr: 1e-05
2024-01-05 22:56:55 INFO     	 * (global step 4850: loss: 0.7244978919625282, lr: 1e-05
2024-01-05 22:57:10 INFO     	 * (global step 4900: loss: 0.6252811402082443, lr: 1e-05
2024-01-05 22:57:23 INFO     	 * (global step 4950: loss: 0.5599473789334297, lr: 1e-05
2024-01-05 22:57:38 INFO     	 * (global step 5000: loss: 0.6457046270370483, lr: 1e-05
2024-01-05 22:57:52 INFO     	 * (global step 5050: loss: 0.882525771856308, lr: 1e-05
2024-01-05 22:58:06 INFO     	 * (global step 5100: loss: 0.6508330851793289, lr: 1e-05
2024-01-05 22:58:21 INFO     	 * (global step 5150: loss: 0.7427068054676056, lr: 1e-05
2024-01-05 22:58:35 INFO     	 * (global step 5200: loss: 0.7183885276317596, lr: 1e-05
2024-01-05 22:58:50 INFO     	 * (global step 5250: loss: 0.6594568192958832, lr: 1e-05
2024-01-05 22:59:04 INFO     	 * (global step 5300: loss: 0.6329094246029854, lr: 1e-05
2024-01-05 22:59:16 INFO     	 * (global step 5350: loss: 0.7458727210760117, lr: 1e-05
2024-01-05 22:59:28 INFO     	 * (global step 5400: loss: 0.7246618717908859, lr: 1e-05
2024-01-05 22:59:40 INFO     	 * (global step 5450: loss: 0.6491982638835907, lr: 1e-05
2024-01-05 22:59:54 INFO     	 * (global step 5500: loss: 0.6265072748064995, lr: 1e-05
2024-01-05 23:00:08 INFO     	 * (global step 5550: loss: 0.7363491058349609, lr: 1e-05
2024-01-05 23:00:22 INFO     	 * (global step 5600: loss: 0.7672411650419235, lr: 1e-05
2024-01-05 23:00:37 INFO     	 * (global step 5650: loss: 0.6386456862092018, lr: 1e-05
2024-01-05 23:00:51 INFO     	 * (global step 5700: loss: 0.747886523604393, lr: 1e-05
2024-01-05 23:01:05 INFO     	 * (global step 5750: loss: 0.6988334208726883, lr: 1e-05
2024-01-05 23:01:19 INFO     	 * (global step 5800: loss: 0.7111552804708481, lr: 1e-05
2024-01-05 23:01:33 INFO     	 * (global step 5850: loss: 0.7872537672519684, lr: 1e-05
2024-01-05 23:01:47 INFO     	 * (global step 5900: loss: 0.7127583622932434, lr: 1e-05
2024-01-05 23:02:01 INFO     	 * (global step 5950: loss: 0.631389670073986, lr: 1e-05
2024-01-05 23:02:12 INFO     	 * (global step 6000: loss: 0.6011367961764336, lr: 1e-05
2024-01-05 23:02:24 INFO     	 * (global step 6050: loss: 0.8623236790299416, lr: 1e-05
2024-01-05 23:02:30 INFO     [epoch 2/15] average loss: 0.715, lr: 1e-05
2024-01-05 23:02:30 INFO     saving model related files
2024-01-05 23:02:30 INFO     saving model
2024-01-05 23:02:31 INFO     saving tokenizer
2024-01-05 23:02:31 INFO     saving optimizer
2024-01-05 23:02:32 INFO     remove old optimizer files
2024-01-05 23:02:40 INFO     	 * (global step 6100: loss: 0.735960528254509, lr: 1e-05
2024-01-05 23:02:54 INFO     	 * (global step 6150: loss: 0.7127228081226349, lr: 1e-05
2024-01-05 23:03:08 INFO     	 * (global step 6200: loss: 0.7244098037481308, lr: 1e-05
2024-01-05 23:03:22 INFO     	 * (global step 6250: loss: 0.7647132724523544, lr: 1e-05
2024-01-05 23:03:36 INFO     	 * (global step 6300: loss: 0.5888963341712952, lr: 1e-05
2024-01-05 23:03:50 INFO     	 * (global step 6350: loss: 0.9317487180233002, lr: 1e-05
2024-01-05 23:04:04 INFO     	 * (global step 6400: loss: 0.735124409198761, lr: 1e-05
2024-01-05 23:04:19 INFO     	 * (global step 6450: loss: 0.8297348767518997, lr: 1e-05
2024-01-05 23:04:33 INFO     	 * (global step 6500: loss: 0.7223718464374542, lr: 1e-05
2024-01-05 23:04:47 INFO     	 * (global step 6550: loss: 0.5765669718384743, lr: 1e-05
2024-01-05 23:04:59 INFO     	 * (global step 6600: loss: 0.5992936566472054, lr: 1e-05
2024-01-05 23:05:10 INFO     	 * (global step 6650: loss: 0.5552413165569305, lr: 1e-05
2024-01-05 23:05:22 INFO     	 * (global step 6700: loss: 0.63963283598423, lr: 1e-05
2024-01-05 23:05:36 INFO     	 * (global step 6750: loss: 0.6551429107785225, lr: 1e-05
2024-01-05 23:05:50 INFO     	 * (global step 6800: loss: 0.7615717574954033, lr: 1e-05
2024-01-05 23:06:04 INFO     	 * (global step 6850: loss: 0.6369536891579628, lr: 1e-05
2024-01-05 23:06:18 INFO     	 * (global step 6900: loss: 0.6057853251695633, lr: 1e-05
2024-01-05 23:06:32 INFO     	 * (global step 6950: loss: 0.6160698533058167, lr: 1e-05
2024-01-05 23:06:46 INFO     	 * (global step 7000: loss: 0.50897166877985, lr: 1e-05
2024-01-05 23:07:00 INFO     	 * (global step 7050: loss: 0.8022587150335312, lr: 1e-05
2024-01-05 23:07:14 INFO     	 * (global step 7100: loss: 0.7024240791797638, lr: 1e-05
2024-01-05 23:07:27 INFO     	 * (global step 7150: loss: 0.6125230714678764, lr: 1e-05
2024-01-05 23:07:38 INFO     	 * (global step 7200: loss: 0.6980392038822174, lr: 1e-05
2024-01-05 23:07:50 INFO     	 * (global step 7250: loss: 0.7074588388204575, lr: 1e-05
2024-01-05 23:08:04 INFO     	 * (global step 7300: loss: 0.5754370093345642, lr: 1e-05
2024-01-05 23:08:17 INFO     	 * (global step 7350: loss: 0.6828550845384598, lr: 1e-05
2024-01-05 23:08:31 INFO     	 * (global step 7400: loss: 0.6110261380672455, lr: 1e-05
2024-01-05 23:08:45 INFO     	 * (global step 7450: loss: 0.6022157296538353, lr: 1e-05
2024-01-05 23:08:59 INFO     	 * (global step 7500: loss: 0.7133044898509979, lr: 1e-05
2024-01-05 23:09:13 INFO     	 * (global step 7550: loss: 0.8386452198028564, lr: 1e-05
2024-01-05 23:09:27 INFO     	 * (global step 7600: loss: 0.5993479043245316, lr: 1e-05
2024-01-05 23:09:41 INFO     	 * (global step 7650: loss: 0.6471939533948898, lr: 1e-05
2024-01-05 23:09:55 INFO     	 * (global step 7700: loss: 0.5592780560255051, lr: 1e-05
2024-01-05 23:10:07 INFO     	 * (global step 7750: loss: 0.5581077411770821, lr: 1e-05
2024-01-05 23:10:18 INFO     	 * (global step 7800: loss: 0.6375931948423386, lr: 1e-05
2024-01-05 23:10:30 INFO     	 * (global step 7850: loss: 0.713587298989296, lr: 1e-05
2024-01-05 23:10:45 INFO     	 * (global step 7900: loss: 0.5942520648241043, lr: 1e-05
2024-01-05 23:10:59 INFO     	 * (global step 7950: loss: 0.7596396654844284, lr: 1e-05
2024-01-05 23:11:13 INFO     	 * (global step 8000: loss: 0.6840367615222931, lr: 1e-05
2024-01-05 23:11:27 INFO     	 * (global step 8050: loss: 0.7688783407211304, lr: 1e-05
2024-01-05 23:11:41 INFO     [epoch 3/15] average loss: 0.683, lr: 1e-05
2024-01-05 23:11:41 INFO     saving model related files
2024-01-05 23:11:41 INFO     saving model
2024-01-05 23:11:41 INFO     saving tokenizer
2024-01-05 23:11:41 INFO     saving optimizer
2024-01-05 23:11:43 INFO     remove old optimizer files
2024-01-05 23:11:44 INFO     	 * (global step 8100: loss: 0.7291631400585175, lr: 1e-05
2024-01-05 23:11:58 INFO     	 * (global step 8150: loss: 0.7110144346952438, lr: 1e-05
2024-01-05 23:12:12 INFO     	 * (global step 8200: loss: 0.5088981986045837, lr: 1e-05
2024-01-05 23:12:26 INFO     	 * (global step 8250: loss: 0.6202326491475105, lr: 1e-05
2024-01-05 23:12:40 INFO     	 * (global step 8300: loss: 0.5925181955099106, lr: 1e-05
2024-01-05 23:12:52 INFO     	 * (global step 8350: loss: 0.8191884458065033, lr: 1e-05
2024-01-05 23:13:03 INFO     	 * (global step 8400: loss: 0.7024769186973572, lr: 1e-05
2024-01-05 23:13:15 INFO     	 * (global step 8450: loss: 0.6171766519546509, lr: 1e-05
2024-01-05 23:13:29 INFO     	 * (global step 8500: loss: 0.7453739494085312, lr: 1e-05
2024-01-05 23:13:42 INFO     	 * (global step 8550: loss: 0.5140477269887924, lr: 1e-05
2024-01-05 23:13:56 INFO     	 * (global step 8600: loss: 0.481462724506855, lr: 1e-05
2024-01-05 23:14:10 INFO     	 * (global step 8650: loss: 0.6372040435671806, lr: 1e-05
2024-01-05 23:14:24 INFO     	 * (global step 8700: loss: 0.8323901146650314, lr: 1e-05
2024-01-05 23:14:38 INFO     	 * (global step 8750: loss: 0.5940345898270607, lr: 1e-05
2024-01-05 23:14:52 INFO     	 * (global step 8800: loss: 0.7248432040214539, lr: 1e-05
2024-01-05 23:15:06 INFO     	 * (global step 8850: loss: 0.7205767929553986, lr: 1e-05
2024-01-05 23:15:18 INFO     	 * (global step 8900: loss: 0.6436376571655273, lr: 1e-05
2024-01-05 23:15:30 INFO     	 * (global step 8950: loss: 0.7234655022621155, lr: 1e-05
2024-01-05 23:15:41 INFO     	 * (global step 9000: loss: 0.6368720382452011, lr: 1e-05
2024-01-05 23:15:55 INFO     	 * (global step 9050: loss: 0.6172456964850426, lr: 1e-05
2024-01-05 23:16:09 INFO     	 * (global step 9100: loss: 0.5773298293352127, lr: 1e-05
2024-01-05 23:16:23 INFO     	 * (global step 9150: loss: 0.3987183906137943, lr: 1e-05
2024-01-05 23:16:37 INFO     	 * (global step 9200: loss: 0.8227074891328812, lr: 1e-05
2024-01-05 23:16:51 INFO     	 * (global step 9250: loss: 0.5394781827926636, lr: 1e-05
2024-01-05 23:17:04 INFO     	 * (global step 9300: loss: 0.6383713185787201, lr: 1e-05
2024-01-05 23:17:18 INFO     	 * (global step 9350: loss: 0.6643656641244888, lr: 1e-05
2024-01-05 23:17:32 INFO     	 * (global step 9400: loss: 0.8370367586612701, lr: 1e-05
2024-01-05 23:17:46 INFO     	 * (global step 9450: loss: 0.662355899810791, lr: 1e-05
2024-01-05 23:17:57 INFO     	 * (global step 9500: loss: 0.6243843734264374, lr: 1e-05
2024-01-05 23:18:08 INFO     	 * (global step 9550: loss: 0.6363013088703156, lr: 1e-05
2024-01-05 23:18:22 INFO     	 * (global step 9600: loss: 0.534560352563858, lr: 1e-05
2024-01-05 23:18:36 INFO     	 * (global step 9650: loss: 0.5438896715641022, lr: 1e-05
2024-01-05 23:18:49 INFO     	 * (global step 9700: loss: 0.666669674217701, lr: 1e-05
2024-01-05 23:19:03 INFO     	 * (global step 9750: loss: 0.5776769369840622, lr: 1e-05
2024-01-05 23:19:18 INFO     	 * (global step 9800: loss: 0.6674968749284744, lr: 1e-05
2024-01-05 23:19:32 INFO     	 * (global step 9850: loss: 0.48477648198604584, lr: 1e-05
2024-01-05 23:19:46 INFO     	 * (global step 9900: loss: 0.6117071360349655, lr: 1e-05
2024-01-05 23:20:00 INFO     	 * (global step 9950: loss: 0.5901485085487366, lr: 1e-05
2024-01-05 23:20:14 INFO     	 * (global step 10000: loss: 0.5915016159415245, lr: 1e-05
2024-01-05 23:20:25 INFO     	 * (global step 10050: loss: 0.6286198496818542, lr: 1e-05
2024-01-05 23:20:37 INFO     	 * (global step 10100: loss: 0.5855629593133926, lr: 1e-05
2024-01-05 23:20:41 INFO     [epoch 4/15] average loss: 0.662, lr: 1e-05
2024-01-05 23:20:41 INFO     saving model related files
2024-01-05 23:20:41 INFO     saving model
2024-01-05 23:20:42 INFO     saving tokenizer
2024-01-05 23:20:42 INFO     saving optimizer
2024-01-05 23:20:43 INFO     remove old optimizer files
2024-01-05 23:20:51 INFO     	 * (global step 10150: loss: 0.5365713089704514, lr: 1e-05
2024-01-05 23:21:05 INFO     	 * (global step 10200: loss: 0.5981417670845985, lr: 1e-05
2024-01-05 23:21:19 INFO     	 * (global step 10250: loss: 0.5027329921722412, lr: 1e-05
2024-01-05 23:21:33 INFO     	 * (global step 10300: loss: 0.5603697448968887, lr: 1e-05
2024-01-05 23:21:47 INFO     	 * (global step 10350: loss: 0.7645251601934433, lr: 1e-05
2024-01-05 23:22:01 INFO     	 * (global step 10400: loss: 0.6583439782261848, lr: 1e-05
2024-01-05 23:22:15 INFO     	 * (global step 10450: loss: 0.5552060380578041, lr: 1e-05
2024-01-05 23:22:28 INFO     	 * (global step 10500: loss: 0.6247033551335335, lr: 1e-05
2024-01-05 23:22:42 INFO     	 * (global step 10550: loss: 0.598400354385376, lr: 1e-05
2024-01-05 23:22:54 INFO     	 * (global step 10600: loss: 0.6415719464421272, lr: 1e-05
2024-01-05 23:23:06 INFO     	 * (global step 10650: loss: 0.7197147905826569, lr: 1e-05
2024-01-05 23:23:17 INFO     	 * (global step 10700: loss: 0.616056889295578, lr: 1e-05
2024-01-05 23:23:31 INFO     	 * (global step 10750: loss: 0.47923510521650314, lr: 1e-05
2024-01-05 23:23:45 INFO     	 * (global step 10800: loss: 0.6526646018028259, lr: 1e-05
2024-01-05 23:23:59 INFO     	 * (global step 10850: loss: 0.6084596514701843, lr: 1e-05
2024-01-05 23:24:13 INFO     	 * (global step 10900: loss: 0.7620401084423065, lr: 1e-05
2024-01-05 23:24:27 INFO     	 * (global step 10950: loss: 0.6686878427863121, lr: 1e-05
2024-01-05 23:24:41 INFO     	 * (global step 11000: loss: 0.5575844347476959, lr: 1e-05
2024-01-05 23:24:54 INFO     	 * (global step 11050: loss: 0.608811043202877, lr: 1e-05
2024-01-05 23:25:08 INFO     	 * (global step 11100: loss: 0.5953408107161522, lr: 1e-05
2024-01-05 23:25:20 INFO     	 * (global step 11150: loss: 0.593337669968605, lr: 1e-05
2024-01-05 23:25:31 INFO     	 * (global step 11200: loss: 0.5952142030000687, lr: 1e-05
2024-01-05 23:25:44 INFO     	 * (global step 11250: loss: 0.7801658809185028, lr: 1e-05
2024-01-05 23:25:58 INFO     	 * (global step 11300: loss: 0.618744470179081, lr: 1e-05
2024-01-05 23:26:12 INFO     	 * (global step 11350: loss: 0.5740041732788086, lr: 1e-05
2024-01-05 23:26:26 INFO     	 * (global step 11400: loss: 0.6156342700123787, lr: 1e-05
2024-01-05 23:26:40 INFO     	 * (global step 11450: loss: 0.4763197898864746, lr: 1e-05
2024-01-05 23:26:54 INFO     	 * (global step 11500: loss: 0.6013584434986115, lr: 1e-05
2024-01-05 23:27:07 INFO     	 * (global step 11550: loss: 0.690862625837326, lr: 1e-05
2024-01-05 23:27:21 INFO     	 * (global step 11600: loss: 0.5763927325606346, lr: 1e-05
2024-01-05 23:27:35 INFO     	 * (global step 11650: loss: 0.3916347101330757, lr: 1e-05
2024-01-05 23:27:47 INFO     	 * (global step 11700: loss: 0.5848798528313637, lr: 1e-05
2024-01-05 23:27:58 INFO     	 * (global step 11750: loss: 0.6985104382038116, lr: 1e-05
2024-01-05 23:28:10 INFO     	 * (global step 11800: loss: 0.6747060120105743, lr: 1e-05
2024-01-05 23:28:24 INFO     	 * (global step 11850: loss: 0.6945604830980301, lr: 1e-05
2024-01-05 23:28:38 INFO     	 * (global step 11900: loss: 0.5199698507785797, lr: 1e-05
2024-01-05 23:28:52 INFO     	 * (global step 11950: loss: 0.5443515628576279, lr: 1e-05
2024-01-05 23:29:07 INFO     	 * (global step 12000: loss: 0.5287073254585266, lr: 1e-05
2024-01-05 23:29:21 INFO     	 * (global step 12050: loss: 0.5203713178634644, lr: 1e-05
2024-01-05 23:29:35 INFO     	 * (global step 12100: loss: 0.5918023511767387, lr: 1e-05
2024-01-05 23:29:48 INFO     [epoch 5/15] average loss: 0.648, lr: 1e-05
2024-01-05 23:29:48 INFO     saving model related files
2024-01-05 23:29:48 INFO     saving model
2024-01-05 23:29:49 INFO     saving tokenizer
2024-01-05 23:29:49 INFO     saving optimizer
2024-01-05 23:29:50 INFO     remove old optimizer files
2024-01-05 23:29:52 INFO     	 * (global step 12150: loss: 0.6178244575858116, lr: 1e-05
2024-01-05 23:30:07 INFO     	 * (global step 12200: loss: 0.6264094933867455, lr: 1e-05
2024-01-05 23:30:21 INFO     	 * (global step 12250: loss: 0.7758216261863708, lr: 1e-05
2024-01-05 23:30:34 INFO     	 * (global step 12300: loss: 0.7248823642730713, lr: 1e-05
2024-01-05 23:30:45 INFO     	 * (global step 12350: loss: 0.5544671639800072, lr: 1e-05
2024-01-05 23:30:56 INFO     	 * (global step 12400: loss: 0.6585743650794029, lr: 1e-05
2024-01-05 23:31:10 INFO     	 * (global step 12450: loss: 0.7839664965867996, lr: 1e-05
2024-01-05 23:31:24 INFO     	 * (global step 12500: loss: 0.5109015926718712, lr: 1e-05
2024-01-05 23:31:38 INFO     	 * (global step 12550: loss: 0.613072507083416, lr: 1e-05
2024-01-05 23:31:52 INFO     	 * (global step 12600: loss: 0.7702076882123947, lr: 1e-05
2024-01-05 23:32:07 INFO     	 * (global step 12650: loss: 0.7338564917445183, lr: 1e-05
2024-01-05 23:32:21 INFO     	 * (global step 12700: loss: 0.6640485674142838, lr: 1e-05
2024-01-05 23:32:35 INFO     	 * (global step 12750: loss: 0.644181452691555, lr: 1e-05
2024-01-05 23:32:50 INFO     	 * (global step 12800: loss: 0.954168513417244, lr: 1e-05
2024-01-05 23:33:04 INFO     	 * (global step 12850: loss: 0.6772991567850113, lr: 1e-05
2024-01-05 23:33:18 INFO     	 * (global step 12900: loss: 0.6498925313353539, lr: 1e-05
2024-01-05 23:33:32 INFO     	 * (global step 12950: loss: 0.6354267299175262, lr: 1e-05
2024-01-05 23:33:44 INFO     	 * (global step 13000: loss: 0.5521663650870323, lr: 1e-05
2024-01-05 23:33:56 INFO     	 * (global step 13050: loss: 0.6302634179592133, lr: 1e-05
2024-01-05 23:34:07 INFO     	 * (global step 13100: loss: 0.6571847349405289, lr: 1e-05
2024-01-05 23:34:21 INFO     	 * (global step 13150: loss: 0.703760102391243, lr: 1e-05
2024-01-05 23:34:35 INFO     	 * (global step 13200: loss: 0.5861966535449028, lr: 1e-05
2024-01-05 23:34:49 INFO     	 * (global step 13250: loss: 0.7165688052773476, lr: 1e-05
2024-01-05 23:35:03 INFO     	 * (global step 13300: loss: 0.6700188592076302, lr: 1e-05
2024-01-05 23:35:17 INFO     	 * (global step 13350: loss: 0.7928519546985626, lr: 1e-05
2024-01-05 23:35:31 INFO     	 * (global step 13400: loss: 0.7039620876312256, lr: 1e-05
2024-01-05 23:35:45 INFO     	 * (global step 13450: loss: 0.712130531668663, lr: 1e-05
2024-01-05 23:35:58 INFO     	 * (global step 13500: loss: 0.6716048866510391, lr: 1e-05
2024-01-05 23:36:11 INFO     	 * (global step 13550: loss: 0.5347146540880203, lr: 1e-05
2024-01-05 23:36:22 INFO     	 * (global step 13600: loss: 0.49653153866529465, lr: 1e-05
2024-01-05 23:36:34 INFO     	 * (global step 13650: loss: 0.5840642526745796, lr: 1e-05
2024-01-05 23:36:48 INFO     	 * (global step 13700: loss: 0.7237854897975922, lr: 1e-05
2024-01-05 23:37:01 INFO     	 * (global step 13750: loss: 0.6451178416609764, lr: 1e-05
2024-01-05 23:37:15 INFO     	 * (global step 13800: loss: 0.4966762810945511, lr: 1e-05
2024-01-05 23:37:29 INFO     	 * (global step 13850: loss: 0.6845104694366455, lr: 1e-05
2024-01-05 23:37:44 INFO     	 * (global step 13900: loss: 0.5661419481039047, lr: 1e-05
2024-01-05 23:37:58 INFO     	 * (global step 13950: loss: 0.6092072576284409, lr: 1e-05
2024-01-05 23:38:11 INFO     	 * (global step 14000: loss: 0.6665326058864594, lr: 1e-05
2024-01-05 23:38:25 INFO     	 * (global step 14050: loss: 0.7417503744363785, lr: 1e-05
2024-01-05 23:38:40 INFO     	 * (global step 14100: loss: 0.6379244327545166, lr: 1e-05
2024-01-05 23:38:53 INFO     	 * (global step 14150: loss: 0.6409206166863441, lr: 1e-05
2024-01-05 23:38:57 INFO     [epoch 6/15] average loss: 0.636, lr: 1e-05
2024-01-05 23:38:57 INFO     saving model related files
2024-01-05 23:38:57 INFO     saving model
2024-01-05 23:38:57 INFO     saving tokenizer
2024-01-05 23:38:58 INFO     saving optimizer
2024-01-05 23:38:58 INFO     remove old optimizer files
2024-01-05 23:39:06 INFO     	 * (global step 14200: loss: 0.6298418045043945, lr: 1e-05
2024-01-05 23:39:17 INFO     	 * (global step 14250: loss: 0.6586759313941002, lr: 1e-05
2024-01-05 23:39:29 INFO     	 * (global step 14300: loss: 0.6467831581830978, lr: 1e-05
2024-01-05 23:39:40 INFO     	 * (global step 14350: loss: 0.7422293424606323, lr: 1e-05
2024-01-05 23:39:52 INFO     	 * (global step 14400: loss: 0.5618665367364883, lr: 1e-05
2024-01-05 23:40:03 INFO     	 * (global step 14450: loss: 0.6666081100702286, lr: 1e-05
2024-01-05 23:40:14 INFO     	 * (global step 14500: loss: 0.6012236177921295, lr: 1e-05
2024-01-05 23:40:26 INFO     	 * (global step 14550: loss: 0.5577360764145851, lr: 1e-05
2024-01-05 23:40:38 INFO     	 * (global step 14600: loss: 0.5406804233789444, lr: 1e-05
2024-01-05 23:40:49 INFO     	 * (global step 14650: loss: 0.6037436723709106, lr: 1e-05
2024-01-05 23:41:01 INFO     	 * (global step 14700: loss: 0.588167130947113, lr: 1e-05
2024-01-05 23:41:13 INFO     	 * (global step 14750: loss: 0.45377787947654724, lr: 1e-05
2024-01-05 23:41:24 INFO     	 * (global step 14800: loss: 0.5996252298355103, lr: 1e-05
2024-01-05 23:41:36 INFO     	 * (global step 14850: loss: 0.5739760622382164, lr: 1e-05
2024-01-05 23:41:47 INFO     	 * (global step 14900: loss: 0.7428903952240944, lr: 1e-05
2024-01-05 23:41:59 INFO     	 * (global step 14950: loss: 0.6910968348383904, lr: 1e-05
2024-01-05 23:42:11 INFO     	 * (global step 15000: loss: 0.7003290951251984, lr: 1e-05
2024-01-05 23:42:22 INFO     	 * (global step 15050: loss: 0.5954434275627136, lr: 1e-05
2024-01-05 23:42:34 INFO     	 * (global step 15100: loss: 0.6867543160915375, lr: 1e-05
2024-01-05 23:42:46 INFO     	 * (global step 15150: loss: 0.9749933630228043, lr: 1e-05
2024-01-05 23:42:58 INFO     	 * (global step 15200: loss: 0.6235017627477646, lr: 1e-05
2024-01-05 23:43:10 INFO     	 * (global step 15250: loss: 0.5019140988588333, lr: 1e-05
2024-01-05 23:43:22 INFO     	 * (global step 15300: loss: 0.6534026935696602, lr: 1e-05
2024-01-05 23:43:34 INFO     	 * (global step 15350: loss: 0.6267775744199753, lr: 1e-05
2024-01-05 23:43:46 INFO     	 * (global step 15400: loss: 0.5535441115498543, lr: 1e-05
2024-01-05 23:43:57 INFO     	 * (global step 15450: loss: 0.7269446402788162, lr: 1e-05
2024-01-05 23:44:09 INFO     	 * (global step 15500: loss: 0.6575583890080452, lr: 1e-05
2024-01-05 23:44:21 INFO     	 * (global step 15550: loss: 0.5714728683233261, lr: 1e-05
2024-01-05 23:44:32 INFO     	 * (global step 15600: loss: 0.6308492124080658, lr: 1e-05
2024-01-05 23:44:44 INFO     	 * (global step 15650: loss: 0.7464992702007294, lr: 1e-05
2024-01-05 23:44:56 INFO     	 * (global step 15700: loss: 0.6934839338064194, lr: 1e-05
2024-01-05 23:45:08 INFO     	 * (global step 15750: loss: 0.5354353189468384, lr: 1e-05
2024-01-05 23:45:19 INFO     	 * (global step 15800: loss: 0.5715443715453148, lr: 1e-05
2024-01-05 23:45:30 INFO     	 * (global step 15850: loss: 0.7788465917110443, lr: 1e-05
2024-01-05 23:45:42 INFO     	 * (global step 15900: loss: 0.5203924775123596, lr: 1e-05
2024-01-05 23:45:53 INFO     	 * (global step 15950: loss: 0.5646328330039978, lr: 1e-05
2024-01-05 23:46:05 INFO     	 * (global step 16000: loss: 0.656474769115448, lr: 1e-05
2024-01-05 23:46:17 INFO     	 * (global step 16050: loss: 0.6464466005563736, lr: 1e-05
2024-01-05 23:46:28 INFO     	 * (global step 16100: loss: 0.6102245301008224, lr: 1e-05
2024-01-05 23:46:40 INFO     	 * (global step 16150: loss: 0.7492943927645683, lr: 1e-05
2024-01-05 23:46:50 INFO     [epoch 7/15] average loss: 0.626, lr: 1e-05
2024-01-05 23:46:50 INFO     saving model related files
2024-01-05 23:46:50 INFO     saving model
2024-01-05 23:46:51 INFO     saving tokenizer
2024-01-05 23:46:51 INFO     saving optimizer
2024-01-05 23:46:52 INFO     remove old optimizer files
2024-01-05 23:46:54 INFO     	 * (global step 16200: loss: 0.6617775335907936, lr: 1e-05
2024-01-05 23:47:05 INFO     	 * (global step 16250: loss: 0.5693766325712204, lr: 1e-05
2024-01-05 23:47:17 INFO     	 * (global step 16300: loss: 0.6218595132231712, lr: 1e-05
2024-01-05 23:47:29 INFO     	 * (global step 16350: loss: 0.5706480294466019, lr: 1e-05
2024-01-05 23:47:40 INFO     	 * (global step 16400: loss: 0.6647273525595665, lr: 1e-05
2024-01-05 23:47:52 INFO     	 * (global step 16450: loss: 0.7445200905203819, lr: 1e-05
2024-01-05 23:48:04 INFO     	 * (global step 16500: loss: 0.4803547337651253, lr: 1e-05
2024-01-05 23:48:15 INFO     	 * (global step 16550: loss: 0.71721450984478, lr: 1e-05
2024-01-05 23:48:27 INFO     	 * (global step 16600: loss: 0.6415634155273438, lr: 1e-05
2024-01-05 23:48:39 INFO     	 * (global step 16650: loss: 0.48508100025355816, lr: 1e-05
2024-01-05 23:48:50 INFO     	 * (global step 16700: loss: 0.6596399545669556, lr: 1e-05
2024-01-05 23:49:02 INFO     	 * (global step 16750: loss: 0.6975103467702866, lr: 1e-05
2024-01-05 23:49:13 INFO     	 * (global step 16800: loss: 0.8571493178606033, lr: 1e-05
2024-01-05 23:49:25 INFO     	 * (global step 16850: loss: 0.5162120014429092, lr: 1e-05
2024-01-05 23:49:36 INFO     	 * (global step 16900: loss: 0.6647935509681702, lr: 1e-05
2024-01-05 23:49:48 INFO     	 * (global step 16950: loss: 0.52748654037714, lr: 1e-05
2024-01-05 23:49:59 INFO     	 * (global step 17000: loss: 0.692290648818016, lr: 1e-05
2024-01-05 23:50:11 INFO     	 * (global step 17050: loss: 0.5990818440914154, lr: 1e-05
2024-01-05 23:50:23 INFO     	 * (global step 17100: loss: 0.5510732010006905, lr: 1e-05
2024-01-05 23:50:35 INFO     	 * (global step 17150: loss: 0.40353479236364365, lr: 1e-05
2024-01-05 23:50:46 INFO     	 * (global step 17200: loss: 0.39056895673274994, lr: 1e-05
2024-01-05 23:50:58 INFO     	 * (global step 17250: loss: 0.5359618365764618, lr: 1e-05
2024-01-05 23:51:09 INFO     	 * (global step 17300: loss: 0.6125803664326668, lr: 1e-05
2024-01-05 23:51:21 INFO     	 * (global step 17350: loss: 0.6262148171663284, lr: 1e-05
2024-01-05 23:51:33 INFO     	 * (global step 17400: loss: 0.45529337227344513, lr: 1e-05
2024-01-05 23:51:44 INFO     	 * (global step 17450: loss: 0.6287525370717049, lr: 1e-05
2024-01-05 23:51:56 INFO     	 * (global step 17500: loss: 0.6469086706638336, lr: 1e-05
2024-01-05 23:52:08 INFO     	 * (global step 17550: loss: 0.6269213184714317, lr: 1e-05
2024-01-05 23:52:19 INFO     	 * (global step 17600: loss: 0.6450587064027786, lr: 1e-05
2024-01-05 23:52:31 INFO     	 * (global step 17650: loss: 0.5624211058020592, lr: 1e-05
2024-01-05 23:52:42 INFO     	 * (global step 17700: loss: 0.48195964843034744, lr: 1e-05
2024-01-05 23:52:54 INFO     	 * (global step 17750: loss: 0.5552472472190857, lr: 1e-05
2024-01-05 23:53:06 INFO     	 * (global step 17800: loss: 0.5289672538638115, lr: 1e-05
2024-01-05 23:53:17 INFO     	 * (global step 17850: loss: 0.6765430271625519, lr: 1e-05
2024-01-05 23:53:29 INFO     	 * (global step 17900: loss: 0.576384924352169, lr: 1e-05
2024-01-05 23:53:40 INFO     	 * (global step 17950: loss: 0.661941684782505, lr: 1e-05
2024-01-05 23:53:52 INFO     	 * (global step 18000: loss: 0.5305807664990425, lr: 1e-05
2024-01-05 23:54:04 INFO     	 * (global step 18050: loss: 0.4542940929532051, lr: 1e-05
2024-01-05 23:54:16 INFO     	 * (global step 18100: loss: 0.6894938498735428, lr: 1e-05
2024-01-05 23:54:27 INFO     	 * (global step 18150: loss: 0.5056900009512901, lr: 1e-05
2024-01-05 23:54:39 INFO     	 * (global step 18200: loss: 0.6067919433116913, lr: 1e-05
2024-01-05 23:54:42 INFO     [epoch 8/15] average loss: 0.618, lr: 1e-05
2024-01-05 23:54:42 INFO     saving model related files
2024-01-05 23:54:42 INFO     saving model
2024-01-05 23:54:43 INFO     saving tokenizer
2024-01-05 23:54:43 INFO     saving optimizer
2024-01-05 23:54:44 INFO     remove old optimizer files
2024-01-05 23:54:52 INFO     	 * (global step 18250: loss: 0.6774251759052277, lr: 1e-05
2024-01-05 23:55:04 INFO     	 * (global step 18300: loss: 0.5895249545574188, lr: 1e-05
2024-01-05 23:55:15 INFO     	 * (global step 18350: loss: 0.6576378047466278, lr: 1e-05
2024-01-05 23:55:27 INFO     	 * (global step 18400: loss: 0.5794257372617722, lr: 1e-05
2024-01-05 23:55:38 INFO     	 * (global step 18450: loss: 0.7037744969129562, lr: 1e-05
2024-01-05 23:55:50 INFO     	 * (global step 18500: loss: 0.6018707007169724, lr: 1e-05
2024-01-05 23:56:01 INFO     	 * (global step 18550: loss: 0.5700216367840767, lr: 1e-05
2024-01-05 23:56:13 INFO     	 * (global step 18600: loss: 0.638168603181839, lr: 1e-05
2024-01-05 23:56:24 INFO     	 * (global step 18650: loss: 0.5606513917446136, lr: 1e-05
2024-01-05 23:56:36 INFO     	 * (global step 18700: loss: 0.8252592980861664, lr: 1e-05
2024-01-05 23:56:48 INFO     	 * (global step 18750: loss: 0.7865589261054993, lr: 1e-05
2024-01-05 23:56:59 INFO     	 * (global step 18800: loss: 0.6121331006288528, lr: 1e-05
2024-01-05 23:57:11 INFO     	 * (global step 18850: loss: 0.568048819899559, lr: 1e-05
2024-01-05 23:57:23 INFO     	 * (global step 18900: loss: 0.4858918711543083, lr: 1e-05
2024-01-05 23:57:34 INFO     	 * (global step 18950: loss: 0.4961651861667633, lr: 1e-05
2024-01-05 23:57:46 INFO     	 * (global step 19000: loss: 0.555709958076477, lr: 1e-05
2024-01-05 23:57:57 INFO     	 * (global step 19050: loss: 0.5746140256524086, lr: 1e-05
2024-01-05 23:58:08 INFO     	 * (global step 19100: loss: 0.6816328391432762, lr: 1e-05
2024-01-05 23:58:20 INFO     	 * (global step 19150: loss: 0.5698116049170494, lr: 1e-05
2024-01-05 23:58:31 INFO     	 * (global step 19200: loss: 0.7265195176005363, lr: 1e-05
2024-01-05 23:58:43 INFO     	 * (global step 19250: loss: 0.5277789309620857, lr: 1e-05
2024-01-05 23:58:54 INFO     	 * (global step 19300: loss: 0.46881040930747986, lr: 1e-05
2024-01-05 23:59:06 INFO     	 * (global step 19350: loss: 0.6235433667898178, lr: 1e-05
2024-01-05 23:59:17 INFO     	 * (global step 19400: loss: 0.5345550030469894, lr: 1e-05
2024-01-05 23:59:29 INFO     	 * (global step 19450: loss: 0.5177184045314789, lr: 1e-05
2024-01-05 23:59:40 INFO     	 * (global step 19500: loss: 0.536976121366024, lr: 1e-05
2024-01-05 23:59:52 INFO     	 * (global step 19550: loss: 0.5635085254907608, lr: 1e-05
2024-01-06 00:00:04 INFO     	 * (global step 19600: loss: 0.6060644313693047, lr: 1e-05
2024-01-06 00:00:15 INFO     	 * (global step 19650: loss: 0.5428312942385674, lr: 1e-05
2024-01-06 00:00:26 INFO     	 * (global step 19700: loss: 0.4856572151184082, lr: 1e-05
2024-01-06 00:00:38 INFO     	 * (global step 19750: loss: 0.7740907296538353, lr: 1e-05
2024-01-06 00:00:49 INFO     	 * (global step 19800: loss: 0.7885249257087708, lr: 1e-05
2024-01-06 00:01:01 INFO     	 * (global step 19850: loss: 0.6259486675262451, lr: 1e-05
2024-01-06 00:01:13 INFO     	 * (global step 19900: loss: 0.8278590962290764, lr: 1e-05
2024-01-06 00:01:24 INFO     	 * (global step 19950: loss: 0.5257948487997055, lr: 1e-05
2024-01-06 00:01:36 INFO     	 * (global step 20000: loss: 0.6993709281086922, lr: 1e-05
2024-01-06 00:01:47 INFO     	 * (global step 20050: loss: 0.6032468527555466, lr: 1e-05
2024-01-06 00:01:59 INFO     	 * (global step 20100: loss: 0.6182452961802483, lr: 1e-05
2024-01-06 00:02:11 INFO     	 * (global step 20150: loss: 0.470481738448143, lr: 1e-05
2024-01-06 00:02:22 INFO     	 * (global step 20200: loss: 0.5231761857867241, lr: 1e-05
2024-01-06 00:02:31 INFO     [epoch 9/15] average loss: 0.611, lr: 1e-05
2024-01-06 00:02:31 INFO     saving model related files
2024-01-06 00:02:31 INFO     saving model
2024-01-06 00:02:32 INFO     saving tokenizer
2024-01-06 00:02:32 INFO     saving optimizer
2024-01-06 00:02:33 INFO     remove old optimizer files
2024-01-06 00:02:34 INFO     complete training: model ckpt was saved at small_recreated_ckpt/model_nxaqhy
2024-01-06 00:02:34 INFO     ## 1st RUN: Configuration 9/12 ##
2024-01-06 00:02:34 INFO     initialize model trainer
2024-01-06 00:02:34 INFO     initialize checkpoint at small_recreated_ckpt/model_oprhlh
2024-01-06 00:02:34 INFO     hyperparameters
2024-01-06 00:02:34 INFO     	 * dataset_path: lmqg/qag_squad
2024-01-06 00:02:34 INFO     	 * dataset_name: default
2024-01-06 00:02:34 INFO     	 * input_types: ['paragraph']
2024-01-06 00:02:34 INFO     	 * output_types: ['questions_answers']
2024-01-06 00:02:34 INFO     	 * prefix_types: ['qag']
2024-01-06 00:02:34 INFO     	 * model: t5-small
2024-01-06 00:02:34 INFO     	 * max_length: 512
2024-01-06 00:02:34 INFO     	 * max_length_output: 256
2024-01-06 00:02:34 INFO     	 * epoch: 15
2024-01-06 00:02:34 INFO     	 * batch: 2
2024-01-06 00:02:34 INFO     	 * lr: 1e-05
2024-01-06 00:02:34 INFO     	 * fp16: False
2024-01-06 00:02:34 INFO     	 * random_seed: 1
2024-01-06 00:02:34 INFO     	 * gradient_accumulation_steps: 2
2024-01-06 00:02:34 INFO     	 * label_smoothing: 0.15
2024-01-06 00:02:35 INFO     initialize checkpoint with t5-small
2024-01-06 00:02:44 INFO     use spaCy answer extraction model: positionrank
2024-01-06 00:02:47 INFO     Model `t5-small`
2024-01-06 00:02:47 INFO     	 * Num of GPU in use: 1
2024-01-06 00:02:47 INFO     	 * Prefix: True
2024-01-06 00:02:47 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 00:02:47 INFO     dataset preprocessing
2024-01-06 00:02:50 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featurelmqg/qag_squad/t5-small.512.256.paragraph.questions_answers.train.qag.pkl
2024-01-06 00:02:56 INFO     start model training
2024-01-06 00:03:02 INFO     	 * (global step 50: loss: 4.310180902481079, lr: 1e-05
2024-01-06 00:03:08 INFO     	 * (global step 100: loss: 3.2145384550094604, lr: 1e-05
2024-01-06 00:03:15 INFO     	 * (global step 150: loss: 1.8389996886253357, lr: 1e-05
2024-01-06 00:03:21 INFO     	 * (global step 200: loss: 2.1822739839553833, lr: 1e-05
2024-01-06 00:03:27 INFO     	 * (global step 250: loss: 1.8631829619407654, lr: 1e-05
2024-01-06 00:03:33 INFO     	 * (global step 300: loss: 1.454402506351471, lr: 1e-05
2024-01-06 00:03:39 INFO     	 * (global step 350: loss: 1.7291598916053772, lr: 1e-05
2024-01-06 00:03:45 INFO     	 * (global step 400: loss: 1.4717230796813965, lr: 1e-05
2024-01-06 00:03:51 INFO     	 * (global step 450: loss: 1.378155767917633, lr: 1e-05
2024-01-06 00:03:57 INFO     	 * (global step 500: loss: 1.2371894121170044, lr: 1e-05
2024-01-06 00:04:03 INFO     	 * (global step 550: loss: 1.3583593368530273, lr: 1e-05
2024-01-06 00:04:09 INFO     	 * (global step 600: loss: 1.4417025446891785, lr: 1e-05
2024-01-06 00:04:15 INFO     	 * (global step 650: loss: 1.272692084312439, lr: 1e-05
2024-01-06 00:04:21 INFO     	 * (global step 700: loss: 1.2847851514816284, lr: 1e-05
2024-01-06 00:04:27 INFO     	 * (global step 750: loss: 1.351819932460785, lr: 1e-05
2024-01-06 00:04:33 INFO     	 * (global step 800: loss: 1.5140576362609863, lr: 1e-05
2024-01-06 00:04:39 INFO     	 * (global step 850: loss: 1.2774102091789246, lr: 1e-05
2024-01-06 00:04:46 INFO     	 * (global step 900: loss: 1.0219069123268127, lr: 1e-05
2024-01-06 00:04:52 INFO     	 * (global step 950: loss: 0.9702576994895935, lr: 1e-05
2024-01-06 00:04:58 INFO     	 * (global step 1000: loss: 1.1033598482608795, lr: 1e-05
2024-01-06 00:05:04 INFO     	 * (global step 1050: loss: 1.0103358626365662, lr: 1e-05
2024-01-06 00:05:10 INFO     	 * (global step 1100: loss: 0.8842679560184479, lr: 1e-05
2024-01-06 00:05:16 INFO     	 * (global step 1150: loss: 1.0694872438907623, lr: 1e-05
2024-01-06 00:05:22 INFO     	 * (global step 1200: loss: 1.5346017479896545, lr: 1e-05
2024-01-06 00:05:28 INFO     	 * (global step 1250: loss: 1.0355868339538574, lr: 1e-05
2024-01-06 00:05:34 INFO     	 * (global step 1300: loss: 0.9869965016841888, lr: 1e-05
2024-01-06 00:05:39 INFO     	 * (global step 1350: loss: 1.2342310547828674, lr: 1e-05
2024-01-06 00:05:45 INFO     	 * (global step 1400: loss: 0.7541961669921875, lr: 1e-05
2024-01-06 00:05:51 INFO     	 * (global step 1450: loss: 0.8561093807220459, lr: 1e-05
2024-01-06 00:05:57 INFO     	 * (global step 1500: loss: 0.9722200334072113, lr: 1e-05
2024-01-06 00:06:03 INFO     	 * (global step 1550: loss: 1.1341052651405334, lr: 1e-05
2024-01-06 00:06:09 INFO     	 * (global step 1600: loss: 0.7686428725719452, lr: 1e-05
2024-01-06 00:06:16 INFO     	 * (global step 1650: loss: 0.7746158242225647, lr: 1e-05
2024-01-06 00:06:22 INFO     	 * (global step 1700: loss: 0.9820490479469299, lr: 1e-05
2024-01-06 00:06:28 INFO     	 * (global step 1750: loss: 1.0439279675483704, lr: 1e-05
2024-01-06 00:06:34 INFO     	 * (global step 1800: loss: 0.9279545545578003, lr: 1e-05
2024-01-06 00:06:40 INFO     	 * (global step 1850: loss: 0.9611206948757172, lr: 1e-05
2024-01-06 00:06:47 INFO     	 * (global step 1900: loss: 0.6188735514879227, lr: 1e-05
2024-01-06 00:06:53 INFO     	 * (global step 1950: loss: 0.8461036682128906, lr: 1e-05
2024-01-06 00:06:59 INFO     	 * (global step 2000: loss: 1.2859495282173157, lr: 1e-05
2024-01-06 00:07:05 INFO     	 * (global step 2050: loss: 0.8467083275318146, lr: 1e-05
2024-01-06 00:07:11 INFO     	 * (global step 2100: loss: 0.6921774446964264, lr: 1e-05
2024-01-06 00:07:17 INFO     	 * (global step 2150: loss: 0.7206156849861145, lr: 1e-05
2024-01-06 00:07:23 INFO     	 * (global step 2200: loss: 1.1742233037948608, lr: 1e-05
2024-01-06 00:07:30 INFO     	 * (global step 2250: loss: 0.7177814245223999, lr: 1e-05
2024-01-06 00:07:36 INFO     	 * (global step 2300: loss: 1.6168610155582428, lr: 1e-05
2024-01-06 00:07:43 INFO     	 * (global step 2350: loss: 0.5287576466798782, lr: 1e-05
2024-01-06 00:07:49 INFO     	 * (global step 2400: loss: 0.7405338287353516, lr: 1e-05
2024-01-06 00:07:55 INFO     	 * (global step 2450: loss: 0.8372392952442169, lr: 1e-05
2024-01-06 00:08:02 INFO     	 * (global step 2500: loss: 0.711274117231369, lr: 1e-05
2024-01-06 00:08:08 INFO     	 * (global step 2550: loss: 1.01499542593956, lr: 1e-05
2024-01-06 00:08:14 INFO     	 * (global step 2600: loss: 0.5622921735048294, lr: 1e-05
2024-01-06 00:08:20 INFO     	 * (global step 2650: loss: 0.6628347486257553, lr: 1e-05
2024-01-06 00:08:26 INFO     	 * (global step 2700: loss: 0.7817330956459045, lr: 1e-05
2024-01-06 00:08:32 INFO     	 * (global step 2750: loss: 0.5740059018135071, lr: 1e-05
2024-01-06 00:08:39 INFO     	 * (global step 2800: loss: 0.6399737298488617, lr: 1e-05
2024-01-06 00:08:45 INFO     	 * (global step 2850: loss: 0.7790201008319855, lr: 1e-05
2024-01-06 00:08:51 INFO     	 * (global step 2900: loss: 0.9098347425460815, lr: 1e-05
2024-01-06 00:08:58 INFO     	 * (global step 2950: loss: 0.40959616005420685, lr: 1e-05
2024-01-06 00:09:04 INFO     	 * (global step 3000: loss: 1.0068418979644775, lr: 1e-05
2024-01-06 00:09:10 INFO     	 * (global step 3050: loss: 0.8817207217216492, lr: 1e-05
2024-01-06 00:09:16 INFO     	 * (global step 3100: loss: 0.8750881850719452, lr: 1e-05
2024-01-06 00:09:22 INFO     	 * (global step 3150: loss: 0.6526415944099426, lr: 1e-05
2024-01-06 00:09:29 INFO     	 * (global step 3200: loss: 0.967170000076294, lr: 1e-05
2024-01-06 00:09:35 INFO     	 * (global step 3250: loss: 0.606879860162735, lr: 1e-05
2024-01-06 00:09:42 INFO     	 * (global step 3300: loss: 0.745899111032486, lr: 1e-05
2024-01-06 00:09:48 INFO     	 * (global step 3350: loss: 0.6714199483394623, lr: 1e-05
2024-01-06 00:09:55 INFO     	 * (global step 3400: loss: 0.7026410400867462, lr: 1e-05
2024-01-06 00:10:01 INFO     	 * (global step 3450: loss: 0.8316719830036163, lr: 1e-05
2024-01-06 00:10:08 INFO     	 * (global step 3500: loss: 0.7728663682937622, lr: 1e-05
2024-01-06 00:10:14 INFO     	 * (global step 3550: loss: 0.9823234379291534, lr: 1e-05
2024-01-06 00:10:21 INFO     	 * (global step 3600: loss: 0.7556896507740021, lr: 1e-05
2024-01-06 00:10:27 INFO     	 * (global step 3650: loss: 0.7879565954208374, lr: 1e-05
2024-01-06 00:10:33 INFO     	 * (global step 3700: loss: 0.4633239656686783, lr: 1e-05
2024-01-06 00:10:39 INFO     	 * (global step 3750: loss: 0.7349064350128174, lr: 1e-05
2024-01-06 00:10:45 INFO     	 * (global step 3800: loss: 0.5121905654668808, lr: 1e-05
2024-01-06 00:10:51 INFO     	 * (global step 3850: loss: 0.6761788427829742, lr: 1e-05
2024-01-06 00:10:58 INFO     	 * (global step 3900: loss: 0.9725692570209503, lr: 1e-05
2024-01-06 00:11:05 INFO     	 * (global step 3950: loss: 0.7598520219326019, lr: 1e-05
2024-01-06 00:11:13 INFO     	 * (global step 4000: loss: 0.645383283495903, lr: 1e-05
2024-01-06 00:11:20 INFO     [epoch 0/15] average loss: 1.111, lr: 1e-05
2024-01-06 00:11:20 INFO     saving model related files
2024-01-06 00:11:20 INFO     saving model
2024-01-06 00:11:21 INFO     saving tokenizer
2024-01-06 00:11:21 INFO     saving optimizer
2024-01-06 00:11:23 INFO     remove old optimizer files
2024-01-06 00:11:23 INFO     	 * (global step 4050: loss: 0.7376749217510223, lr: 1e-05
2024-01-06 00:11:30 INFO     	 * (global step 4100: loss: 0.646093562245369, lr: 1e-05
2024-01-06 00:11:38 INFO     	 * (global step 4150: loss: 0.7444116771221161, lr: 1e-05
2024-01-06 00:11:46 INFO     	 * (global step 4200: loss: 0.5807651877403259, lr: 1e-05
2024-01-06 00:11:53 INFO     	 * (global step 4250: loss: 0.9009491503238678, lr: 1e-05
2024-01-06 00:12:00 INFO     	 * (global step 4300: loss: 1.0807850360870361, lr: 1e-05
2024-01-06 00:12:08 INFO     	 * (global step 4350: loss: 0.5694548785686493, lr: 1e-05
2024-01-06 00:12:15 INFO     	 * (global step 4400: loss: 0.47207552194595337, lr: 1e-05
2024-01-06 00:12:23 INFO     	 * (global step 4450: loss: 0.9785670340061188, lr: 1e-05
2024-01-06 00:12:30 INFO     	 * (global step 4500: loss: 0.5401087701320648, lr: 1e-05
2024-01-06 00:12:38 INFO     	 * (global step 4550: loss: 0.6625156849622726, lr: 1e-05
2024-01-06 00:12:45 INFO     	 * (global step 4600: loss: 0.651030570268631, lr: 1e-05
2024-01-06 00:12:52 INFO     	 * (global step 4650: loss: 0.9020990133285522, lr: 1e-05
2024-01-06 00:12:59 INFO     	 * (global step 4700: loss: 0.9237690567970276, lr: 1e-05
2024-01-06 00:13:06 INFO     	 * (global step 4750: loss: 0.8861579298973083, lr: 1e-05
2024-01-06 00:13:14 INFO     	 * (global step 4800: loss: 0.6965555846691132, lr: 1e-05
2024-01-06 00:13:20 INFO     	 * (global step 4850: loss: 0.7968471646308899, lr: 1e-05
2024-01-06 00:13:26 INFO     	 * (global step 4900: loss: 0.7243223190307617, lr: 1e-05
2024-01-06 00:13:32 INFO     	 * (global step 4950: loss: 0.9789935797452927, lr: 1e-05
2024-01-06 00:13:38 INFO     	 * (global step 5000: loss: 0.855045884847641, lr: 1e-05
2024-01-06 00:13:44 INFO     	 * (global step 5050: loss: 0.7742142379283905, lr: 1e-05
2024-01-06 00:13:51 INFO     	 * (global step 5100: loss: 0.6519080996513367, lr: 1e-05
2024-01-06 00:13:59 INFO     	 * (global step 5150: loss: 0.7964014410972595, lr: 1e-05
2024-01-06 00:14:06 INFO     	 * (global step 5200: loss: 0.6107098162174225, lr: 1e-05
2024-01-06 00:14:13 INFO     	 * (global step 5250: loss: 0.6655118316411972, lr: 1e-05
2024-01-06 00:14:21 INFO     	 * (global step 5300: loss: 0.9067829251289368, lr: 1e-05
2024-01-06 00:14:28 INFO     	 * (global step 5350: loss: 0.7892976105213165, lr: 1e-05
2024-01-06 00:14:36 INFO     	 * (global step 5400: loss: 0.652528703212738, lr: 1e-05
2024-01-06 00:14:43 INFO     	 * (global step 5450: loss: 0.7123818099498749, lr: 1e-05
2024-01-06 00:14:50 INFO     	 * (global step 5500: loss: 0.5820002555847168, lr: 1e-05
2024-01-06 00:14:57 INFO     	 * (global step 5550: loss: 0.812355250120163, lr: 1e-05
2024-01-06 00:15:05 INFO     	 * (global step 5600: loss: 0.8395763039588928, lr: 1e-05
2024-01-06 00:15:12 INFO     	 * (global step 5650: loss: 0.7877157926559448, lr: 1e-05
2024-01-06 00:15:19 INFO     	 * (global step 5700: loss: 1.1168416142463684, lr: 1e-05
2024-01-06 00:15:26 INFO     	 * (global step 5750: loss: 0.5836213231086731, lr: 1e-05
2024-01-06 00:15:33 INFO     	 * (global step 5800: loss: 0.798801064491272, lr: 1e-05
2024-01-06 00:15:41 INFO     	 * (global step 5850: loss: 0.62031289935112, lr: 1e-05
2024-01-06 00:15:48 INFO     	 * (global step 5900: loss: 0.5430846065282822, lr: 1e-05
2024-01-06 00:15:54 INFO     	 * (global step 5950: loss: 0.8888377547264099, lr: 1e-05
2024-01-06 00:15:59 INFO     	 * (global step 6000: loss: 0.6431312263011932, lr: 1e-05
2024-01-06 00:16:06 INFO     	 * (global step 6050: loss: 0.5848158299922943, lr: 1e-05
2024-01-06 00:16:11 INFO     	 * (global step 6100: loss: 0.5732920169830322, lr: 1e-05
2024-01-06 00:16:18 INFO     	 * (global step 6150: loss: 0.679705411195755, lr: 1e-05
2024-01-06 00:16:26 INFO     	 * (global step 6200: loss: 0.7631078958511353, lr: 1e-05
2024-01-06 00:16:33 INFO     	 * (global step 6250: loss: 0.5849902033805847, lr: 1e-05
2024-01-06 00:16:40 INFO     	 * (global step 6300: loss: 0.6938280165195465, lr: 1e-05
2024-01-06 00:16:48 INFO     	 * (global step 6350: loss: 0.7628058791160583, lr: 1e-05
2024-01-06 00:16:55 INFO     	 * (global step 6400: loss: 0.6338317096233368, lr: 1e-05
2024-01-06 00:17:03 INFO     	 * (global step 6450: loss: 0.5529651492834091, lr: 1e-05
2024-01-06 00:17:10 INFO     	 * (global step 6500: loss: 0.7927877902984619, lr: 1e-05
2024-01-06 00:17:17 INFO     	 * (global step 6550: loss: 0.8906002342700958, lr: 1e-05
2024-01-06 00:17:25 INFO     	 * (global step 6600: loss: 0.759628027677536, lr: 1e-05
2024-01-06 00:17:32 INFO     	 * (global step 6650: loss: 0.6428033411502838, lr: 1e-05
2024-01-06 00:17:40 INFO     	 * (global step 6700: loss: 0.7737402021884918, lr: 1e-05
2024-01-06 00:17:47 INFO     	 * (global step 6750: loss: 0.918411374092102, lr: 1e-05
2024-01-06 00:17:54 INFO     	 * (global step 6800: loss: 0.8756024539470673, lr: 1e-05
2024-01-06 00:18:02 INFO     	 * (global step 6850: loss: 0.7042336463928223, lr: 1e-05
2024-01-06 00:18:09 INFO     	 * (global step 6900: loss: 0.7339164614677429, lr: 1e-05
2024-01-06 00:18:17 INFO     	 * (global step 6950: loss: 0.6339384019374847, lr: 1e-05
2024-01-06 00:18:24 INFO     	 * (global step 7000: loss: 0.628993034362793, lr: 1e-05
2024-01-06 00:18:32 INFO     	 * (global step 7050: loss: 0.7744928300380707, lr: 1e-05
2024-01-06 00:18:38 INFO     	 * (global step 7100: loss: 0.537755697965622, lr: 1e-05
2024-01-06 00:18:44 INFO     	 * (global step 7150: loss: 0.785569965839386, lr: 1e-05
2024-01-06 00:18:50 INFO     	 * (global step 7200: loss: 0.7077280580997467, lr: 1e-05
2024-01-06 00:18:56 INFO     	 * (global step 7250: loss: 0.8832387626171112, lr: 1e-05
2024-01-06 00:19:02 INFO     	 * (global step 7300: loss: 0.8710291683673859, lr: 1e-05
2024-01-06 00:19:10 INFO     	 * (global step 7350: loss: 0.7240933179855347, lr: 1e-05
2024-01-06 00:19:17 INFO     	 * (global step 7400: loss: 0.8454223275184631, lr: 1e-05
2024-01-06 00:19:25 INFO     	 * (global step 7450: loss: 0.6885892152786255, lr: 1e-05
2024-01-06 00:19:32 INFO     	 * (global step 7500: loss: 0.629964143037796, lr: 1e-05
2024-01-06 00:19:40 INFO     	 * (global step 7550: loss: 0.8561873137950897, lr: 1e-05
2024-01-06 00:19:47 INFO     	 * (global step 7600: loss: 0.6672124117612839, lr: 1e-05
2024-01-06 00:19:54 INFO     	 * (global step 7650: loss: 0.9518890380859375, lr: 1e-05
2024-01-06 00:20:02 INFO     	 * (global step 7700: loss: 1.0967938005924225, lr: 1e-05
2024-01-06 00:20:09 INFO     	 * (global step 7750: loss: 0.7242786884307861, lr: 1e-05
2024-01-06 00:20:17 INFO     	 * (global step 7800: loss: 0.7380533516407013, lr: 1e-05
2024-01-06 00:20:24 INFO     	 * (global step 7850: loss: 0.6404260247945786, lr: 1e-05
2024-01-06 00:20:31 INFO     	 * (global step 7900: loss: 0.568210557103157, lr: 1e-05
2024-01-06 00:20:38 INFO     	 * (global step 7950: loss: 0.5939228236675262, lr: 1e-05
2024-01-06 00:20:45 INFO     	 * (global step 8000: loss: 0.5719826519489288, lr: 1e-05
2024-01-06 00:20:53 INFO     	 * (global step 8050: loss: 0.9274095892906189, lr: 1e-05
2024-01-06 00:21:00 INFO     [epoch 1/15] average loss: 0.72, lr: 1e-05
2024-01-06 00:21:00 INFO     saving model related files
2024-01-06 00:21:00 INFO     saving model
2024-01-06 00:21:01 INFO     saving tokenizer
2024-01-06 00:21:01 INFO     saving optimizer
2024-01-06 00:21:03 INFO     remove old optimizer files
2024-01-06 00:21:03 INFO     	 * (global step 8100: loss: 0.6304499804973602, lr: 1e-05
2024-01-06 00:21:10 INFO     	 * (global step 8150: loss: 0.5321252346038818, lr: 1e-05
2024-01-06 00:21:16 INFO     	 * (global step 8200: loss: 0.573421061038971, lr: 1e-05
2024-01-06 00:21:22 INFO     	 * (global step 8250: loss: 0.39730481803417206, lr: 1e-05
2024-01-06 00:21:28 INFO     	 * (global step 8300: loss: 0.6931351125240326, lr: 1e-05
2024-01-06 00:21:34 INFO     	 * (global step 8350: loss: 0.9444292783737183, lr: 1e-05
2024-01-06 00:21:41 INFO     	 * (global step 8400: loss: 0.4980928301811218, lr: 1e-05
2024-01-06 00:21:48 INFO     	 * (global step 8450: loss: 0.5977313220500946, lr: 1e-05
2024-01-06 00:21:55 INFO     	 * (global step 8500: loss: 0.8338504731655121, lr: 1e-05
2024-01-06 00:22:02 INFO     	 * (global step 8550: loss: 0.7116146087646484, lr: 1e-05
2024-01-06 00:22:10 INFO     	 * (global step 8600: loss: 0.3732977658510208, lr: 1e-05
2024-01-06 00:22:17 INFO     	 * (global step 8650: loss: 0.49718308448791504, lr: 1e-05
2024-01-06 00:22:24 INFO     	 * (global step 8700: loss: 0.7726496160030365, lr: 1e-05
2024-01-06 00:22:32 INFO     	 * (global step 8750: loss: 0.6465017795562744, lr: 1e-05
2024-01-06 00:22:39 INFO     	 * (global step 8800: loss: 0.3646107465028763, lr: 1e-05
2024-01-06 00:22:47 INFO     	 * (global step 8850: loss: 0.6840067505836487, lr: 1e-05
2024-01-06 00:22:54 INFO     	 * (global step 8900: loss: 0.49875715374946594, lr: 1e-05
2024-01-06 00:23:02 INFO     	 * (global step 8950: loss: 0.4343561679124832, lr: 1e-05
2024-01-06 00:23:09 INFO     	 * (global step 9000: loss: 0.6386762857437134, lr: 1e-05
2024-01-06 00:23:17 INFO     	 * (global step 9050: loss: 0.7807322144508362, lr: 1e-05
2024-01-06 00:23:24 INFO     	 * (global step 9100: loss: 0.7267940044403076, lr: 1e-05
2024-01-06 00:23:31 INFO     	 * (global step 9150: loss: 0.7296931743621826, lr: 1e-05
2024-01-06 00:23:38 INFO     	 * (global step 9200: loss: 0.7230777740478516, lr: 1e-05
2024-01-06 00:23:46 INFO     	 * (global step 9250: loss: 0.5276426374912262, lr: 1e-05
2024-01-06 00:23:52 INFO     	 * (global step 9300: loss: 0.7850725948810577, lr: 1e-05
2024-01-06 00:23:58 INFO     	 * (global step 9350: loss: 0.45228660106658936, lr: 1e-05
2024-01-06 00:24:04 INFO     	 * (global step 9400: loss: 0.6749769449234009, lr: 1e-05
2024-01-06 00:24:10 INFO     	 * (global step 9450: loss: 0.5497872680425644, lr: 1e-05
2024-01-06 00:24:16 INFO     	 * (global step 9500: loss: 0.6223099827766418, lr: 1e-05
2024-01-06 00:24:23 INFO     	 * (global step 9550: loss: 0.7235552668571472, lr: 1e-05
2024-01-06 00:24:31 INFO     	 * (global step 9600: loss: 0.6475785374641418, lr: 1e-05
2024-01-06 00:24:38 INFO     	 * (global step 9650: loss: 0.8175826370716095, lr: 1e-05
2024-01-06 00:24:45 INFO     	 * (global step 9700: loss: 0.8065090477466583, lr: 1e-05
2024-01-06 00:24:52 INFO     	 * (global step 9750: loss: 0.8632806837558746, lr: 1e-05
2024-01-06 00:25:00 INFO     	 * (global step 9800: loss: 0.5577623248100281, lr: 1e-05
2024-01-06 00:25:07 INFO     	 * (global step 9850: loss: 0.8159051835536957, lr: 1e-05
2024-01-06 00:25:14 INFO     	 * (global step 9900: loss: 0.5260604918003082, lr: 1e-05
2024-01-06 00:25:22 INFO     	 * (global step 9950: loss: 0.5109604299068451, lr: 1e-05
2024-01-06 00:25:29 INFO     	 * (global step 10000: loss: 0.5645721554756165, lr: 1e-05
2024-01-06 00:25:37 INFO     	 * (global step 10050: loss: 0.6087363362312317, lr: 1e-05
2024-01-06 00:25:44 INFO     	 * (global step 10100: loss: 0.7942016422748566, lr: 1e-05
2024-01-06 00:25:51 INFO     	 * (global step 10150: loss: 0.45180028676986694, lr: 1e-05
2024-01-06 00:25:59 INFO     	 * (global step 10200: loss: 0.5720157325267792, lr: 1e-05
2024-01-06 00:26:06 INFO     	 * (global step 10250: loss: 0.6904003620147705, lr: 1e-05
2024-01-06 00:26:13 INFO     	 * (global step 10300: loss: 0.7409031689167023, lr: 1e-05
2024-01-06 00:26:21 INFO     	 * (global step 10350: loss: 0.41321877390146255, lr: 1e-05
2024-01-06 00:26:27 INFO     	 * (global step 10400: loss: 0.6042177081108093, lr: 1e-05
2024-01-06 00:26:32 INFO     	 * (global step 10450: loss: 0.7262648642063141, lr: 1e-05
2024-01-06 00:26:38 INFO     	 * (global step 10500: loss: 0.664669543504715, lr: 1e-05
2024-01-06 00:26:45 INFO     	 * (global step 10550: loss: 0.6373715996742249, lr: 1e-05
2024-01-06 00:26:51 INFO     	 * (global step 10600: loss: 0.351612389087677, lr: 1e-05
2024-01-06 00:26:58 INFO     	 * (global step 10650: loss: 0.5860619992017746, lr: 1e-05
2024-01-06 00:27:05 INFO     	 * (global step 10700: loss: 0.6485626399517059, lr: 1e-05
2024-01-06 00:27:13 INFO     	 * (global step 10750: loss: 0.6893987059593201, lr: 1e-05
2024-01-06 00:27:20 INFO     	 * (global step 10800: loss: 0.6429531127214432, lr: 1e-05
2024-01-06 00:27:27 INFO     	 * (global step 10850: loss: 0.5902554094791412, lr: 1e-05
2024-01-06 00:27:34 INFO     	 * (global step 10900: loss: 0.6134313642978668, lr: 1e-05
2024-01-06 00:27:42 INFO     	 * (global step 10950: loss: 0.5794327259063721, lr: 1e-05
2024-01-06 00:27:49 INFO     	 * (global step 11000: loss: 0.5143473148345947, lr: 1e-05
2024-01-06 00:27:57 INFO     	 * (global step 11050: loss: 0.7659961581230164, lr: 1e-05
2024-01-06 00:28:04 INFO     	 * (global step 11100: loss: 0.6084359288215637, lr: 1e-05
2024-01-06 00:28:11 INFO     	 * (global step 11150: loss: 0.6537002325057983, lr: 1e-05
2024-01-06 00:28:18 INFO     	 * (global step 11200: loss: 0.5541139543056488, lr: 1e-05
2024-01-06 00:28:26 INFO     	 * (global step 11250: loss: 0.7025776505470276, lr: 1e-05
2024-01-06 00:28:33 INFO     	 * (global step 11300: loss: 0.5006996840238571, lr: 1e-05
2024-01-06 00:28:40 INFO     	 * (global step 11350: loss: 0.7050058245658875, lr: 1e-05
2024-01-06 00:28:48 INFO     	 * (global step 11400: loss: 0.7989636957645416, lr: 1e-05
2024-01-06 00:28:55 INFO     	 * (global step 11450: loss: 0.942157506942749, lr: 1e-05
2024-01-06 00:29:01 INFO     	 * (global step 11500: loss: 0.6690593957901001, lr: 1e-05
2024-01-06 00:29:07 INFO     	 * (global step 11550: loss: 0.7732374966144562, lr: 1e-05
2024-01-06 00:29:13 INFO     	 * (global step 11600: loss: 0.6631940305233002, lr: 1e-05
2024-01-06 00:29:19 INFO     	 * (global step 11650: loss: 1.0071894824504852, lr: 1e-05
2024-01-06 00:29:25 INFO     	 * (global step 11700: loss: 0.684224545955658, lr: 1e-05
2024-01-06 00:29:33 INFO     	 * (global step 11750: loss: 0.7017152607440948, lr: 1e-05
2024-01-06 00:29:40 INFO     	 * (global step 11800: loss: 0.45590707659721375, lr: 1e-05
2024-01-06 00:29:48 INFO     	 * (global step 11850: loss: 0.6182111948728561, lr: 1e-05
2024-01-06 00:29:55 INFO     	 * (global step 11900: loss: 0.5167608112096786, lr: 1e-05
2024-01-06 00:30:02 INFO     	 * (global step 11950: loss: 0.4628141522407532, lr: 1e-05
2024-01-06 00:30:09 INFO     	 * (global step 12000: loss: 0.759223222732544, lr: 1e-05
2024-01-06 00:30:16 INFO     	 * (global step 12050: loss: 0.5629478394985199, lr: 1e-05
2024-01-06 00:30:24 INFO     	 * (global step 12100: loss: 0.7391658425331116, lr: 1e-05
2024-01-06 00:30:30 INFO     [epoch 2/15] average loss: 0.674, lr: 1e-05
2024-01-06 00:30:30 INFO     saving model related files
2024-01-06 00:30:30 INFO     saving model
2024-01-06 00:30:31 INFO     saving tokenizer
2024-01-06 00:30:31 INFO     saving optimizer
2024-01-06 00:30:33 INFO     remove old optimizer files
2024-01-06 00:30:34 INFO     	 * (global step 12150: loss: 0.5875865817070007, lr: 1e-05
2024-01-06 00:30:41 INFO     	 * (global step 12200: loss: 0.680733859539032, lr: 1e-05
2024-01-06 00:30:49 INFO     	 * (global step 12250: loss: 0.5352017730474472, lr: 1e-05
2024-01-06 00:30:56 INFO     	 * (global step 12300: loss: 0.6676132082939148, lr: 1e-05
2024-01-06 00:31:03 INFO     	 * (global step 12350: loss: 0.7762026786804199, lr: 1e-05
2024-01-06 00:31:11 INFO     	 * (global step 12400: loss: 0.772603303194046, lr: 1e-05
2024-01-06 00:31:18 INFO     	 * (global step 12450: loss: 0.46599043905735016, lr: 1e-05
2024-01-06 00:31:26 INFO     	 * (global step 12500: loss: 0.7661153823137283, lr: 1e-05
2024-01-06 00:31:32 INFO     	 * (global step 12550: loss: 0.5811251699924469, lr: 1e-05
2024-01-06 00:31:38 INFO     	 * (global step 12600: loss: 0.5328499227762222, lr: 1e-05
2024-01-06 00:31:44 INFO     	 * (global step 12650: loss: 0.6941891312599182, lr: 1e-05
2024-01-06 00:31:50 INFO     	 * (global step 12700: loss: 0.9347354471683502, lr: 1e-05
2024-01-06 00:31:56 INFO     	 * (global step 12750: loss: 0.8037879765033722, lr: 1e-05
2024-01-06 00:32:04 INFO     	 * (global step 12800: loss: 0.7995681464672089, lr: 1e-05
2024-01-06 00:32:12 INFO     	 * (global step 12850: loss: 0.834136962890625, lr: 1e-05
2024-01-06 00:32:19 INFO     	 * (global step 12900: loss: 0.7077276706695557, lr: 1e-05
2024-01-06 00:32:26 INFO     	 * (global step 12950: loss: 0.7112230062484741, lr: 1e-05
2024-01-06 00:32:34 INFO     	 * (global step 13000: loss: 0.8410547971725464, lr: 1e-05
2024-01-06 00:32:41 INFO     	 * (global step 13050: loss: 0.7076622545719147, lr: 1e-05
2024-01-06 00:32:49 INFO     	 * (global step 13100: loss: 0.5798610001802444, lr: 1e-05
2024-01-06 00:32:56 INFO     	 * (global step 13150: loss: 0.6620565354824066, lr: 1e-05
2024-01-06 00:33:04 INFO     	 * (global step 13200: loss: 0.6045610010623932, lr: 1e-05
2024-01-06 00:33:12 INFO     	 * (global step 13250: loss: 1.2096529006958008, lr: 1e-05
2024-01-06 00:33:19 INFO     	 * (global step 13300: loss: 0.4451695382595062, lr: 1e-05
2024-01-06 00:33:27 INFO     	 * (global step 13350: loss: 0.6256110668182373, lr: 1e-05
2024-01-06 00:33:34 INFO     	 * (global step 13400: loss: 0.7927678525447845, lr: 1e-05
2024-01-06 00:33:41 INFO     	 * (global step 13450: loss: 0.5888310372829437, lr: 1e-05
2024-01-06 00:33:49 INFO     	 * (global step 13500: loss: 0.7730801105499268, lr: 1e-05
2024-01-06 00:33:56 INFO     	 * (global step 13550: loss: 0.6784630119800568, lr: 1e-05
2024-01-06 00:34:04 INFO     	 * (global step 13600: loss: 0.8087619245052338, lr: 1e-05
2024-01-06 00:34:11 INFO     	 * (global step 13650: loss: 0.6416393667459488, lr: 1e-05
2024-01-06 00:34:17 INFO     	 * (global step 13700: loss: 0.6894341260194778, lr: 1e-05
2024-01-06 00:34:23 INFO     	 * (global step 13750: loss: 0.6331738531589508, lr: 1e-05
2024-01-06 00:34:29 INFO     	 * (global step 13800: loss: 0.6388566344976425, lr: 1e-05
2024-01-06 00:34:35 INFO     	 * (global step 13850: loss: 1.0027463734149933, lr: 1e-05
2024-01-06 00:34:42 INFO     	 * (global step 13900: loss: 0.42834529280662537, lr: 1e-05
2024-01-06 00:34:49 INFO     	 * (global step 13950: loss: 0.6627175807952881, lr: 1e-05
2024-01-06 00:34:57 INFO     	 * (global step 14000: loss: 0.5156283974647522, lr: 1e-05
2024-01-06 00:35:04 INFO     	 * (global step 14050: loss: 0.5304724276065826, lr: 1e-05
2024-01-06 00:35:11 INFO     	 * (global step 14100: loss: 0.7031361758708954, lr: 1e-05
2024-01-06 00:35:18 INFO     	 * (global step 14150: loss: 0.4677681624889374, lr: 1e-05
2024-01-06 00:35:25 INFO     	 * (global step 14200: loss: 0.66867995262146, lr: 1e-05
2024-01-06 00:35:33 INFO     	 * (global step 14250: loss: 0.7229800224304199, lr: 1e-05
2024-01-06 00:35:40 INFO     	 * (global step 14300: loss: 0.5278516262769699, lr: 1e-05
2024-01-06 00:35:48 INFO     	 * (global step 14350: loss: 1.206419587135315, lr: 1e-05
2024-01-06 00:35:55 INFO     	 * (global step 14400: loss: 0.728276401758194, lr: 1e-05
2024-01-06 00:36:01 INFO     	 * (global step 14450: loss: 0.45882923901081085, lr: 1e-05
2024-01-06 00:36:09 INFO     	 * (global step 14500: loss: 0.615014985203743, lr: 1e-05
2024-01-06 00:36:16 INFO     	 * (global step 14550: loss: 0.5631822645664215, lr: 1e-05
2024-01-06 00:36:23 INFO     	 * (global step 14600: loss: 0.6089268028736115, lr: 1e-05
2024-01-06 00:36:30 INFO     	 * (global step 14650: loss: 0.5715464055538177, lr: 1e-05
2024-01-06 00:36:37 INFO     	 * (global step 14700: loss: 0.484764039516449, lr: 1e-05
2024-01-06 00:36:44 INFO     	 * (global step 14750: loss: 0.6623222529888153, lr: 1e-05
2024-01-06 00:36:52 INFO     	 * (global step 14800: loss: 0.5960020124912262, lr: 1e-05
2024-01-06 00:36:59 INFO     	 * (global step 14850: loss: 0.5812821090221405, lr: 1e-05
2024-01-06 00:37:05 INFO     	 * (global step 14900: loss: 0.5230657458305359, lr: 1e-05
2024-01-06 00:37:10 INFO     	 * (global step 14950: loss: 0.5110732913017273, lr: 1e-05
2024-01-06 00:37:17 INFO     	 * (global step 15000: loss: 0.49538448452949524, lr: 1e-05
2024-01-06 00:37:22 INFO     	 * (global step 15050: loss: 0.7446277737617493, lr: 1e-05
2024-01-06 00:37:29 INFO     	 * (global step 15100: loss: 0.6064231097698212, lr: 1e-05
2024-01-06 00:37:37 INFO     	 * (global step 15150: loss: 0.6291553676128387, lr: 1e-05
2024-01-06 00:37:44 INFO     	 * (global step 15200: loss: 0.5818545818328857, lr: 1e-05
2024-01-06 00:37:51 INFO     	 * (global step 15250: loss: 0.48188385367393494, lr: 1e-05
2024-01-06 00:37:58 INFO     	 * (global step 15300: loss: 0.5868925452232361, lr: 1e-05
2024-01-06 00:38:06 INFO     	 * (global step 15350: loss: 0.8002466857433319, lr: 1e-05
2024-01-06 00:38:13 INFO     	 * (global step 15400: loss: 0.47754959762096405, lr: 1e-05
2024-01-06 00:38:20 INFO     	 * (global step 15450: loss: 0.4812116175889969, lr: 1e-05
2024-01-06 00:38:27 INFO     	 * (global step 15500: loss: 0.5175483822822571, lr: 1e-05
2024-01-06 00:38:35 INFO     	 * (global step 15550: loss: 0.7629557549953461, lr: 1e-05
2024-01-06 00:38:42 INFO     	 * (global step 15600: loss: 0.6695467233657837, lr: 1e-05
2024-01-06 00:38:50 INFO     	 * (global step 15650: loss: 0.7831536531448364, lr: 1e-05
2024-01-06 00:38:57 INFO     	 * (global step 15700: loss: 0.7426592111587524, lr: 1e-05
2024-01-06 00:39:05 INFO     	 * (global step 15750: loss: 0.625704288482666, lr: 1e-05
2024-01-06 00:39:12 INFO     	 * (global step 15800: loss: 0.3735455274581909, lr: 1e-05
2024-01-06 00:39:20 INFO     	 * (global step 15850: loss: 0.6159206032752991, lr: 1e-05
2024-01-06 00:39:27 INFO     	 * (global step 15900: loss: 0.746850460767746, lr: 1e-05
2024-01-06 00:39:35 INFO     	 * (global step 15950: loss: 0.8377982974052429, lr: 1e-05
2024-01-06 00:39:41 INFO     	 * (global step 16000: loss: 0.48671433329582214, lr: 1e-05
2024-01-06 00:39:47 INFO     	 * (global step 16050: loss: 0.4866003543138504, lr: 1e-05
2024-01-06 00:39:52 INFO     	 * (global step 16100: loss: 0.8584542274475098, lr: 1e-05
2024-01-06 00:39:59 INFO     	 * (global step 16150: loss: 0.550821840763092, lr: 1e-05
2024-01-06 00:40:04 INFO     [epoch 3/15] average loss: 0.65, lr: 1e-05
2024-01-06 00:40:04 INFO     saving model related files
2024-01-06 00:40:04 INFO     saving model
2024-01-06 00:40:05 INFO     saving tokenizer
2024-01-06 00:40:05 INFO     saving optimizer
2024-01-06 00:40:06 INFO     remove old optimizer files
2024-01-06 00:40:08 INFO     	 * (global step 16200: loss: 0.7858916521072388, lr: 1e-05
2024-01-06 00:40:15 INFO     	 * (global step 16250: loss: 0.6471613645553589, lr: 1e-05
2024-01-06 00:40:22 INFO     	 * (global step 16300: loss: 0.5389630049467087, lr: 1e-05
2024-01-06 00:40:29 INFO     	 * (global step 16350: loss: 0.5627269446849823, lr: 1e-05
2024-01-06 00:40:37 INFO     	 * (global step 16400: loss: 0.36811335384845734, lr: 1e-05
2024-01-06 00:40:44 INFO     	 * (global step 16450: loss: 0.48764684796333313, lr: 1e-05
2024-01-06 00:40:51 INFO     	 * (global step 16500: loss: 0.532894641160965, lr: 1e-05
2024-01-06 00:40:59 INFO     	 * (global step 16550: loss: 0.6911108195781708, lr: 1e-05
2024-01-06 00:41:06 INFO     	 * (global step 16600: loss: 0.5214059054851532, lr: 1e-05
2024-01-06 00:41:14 INFO     	 * (global step 16650: loss: 0.627654641866684, lr: 1e-05
2024-01-06 00:41:21 INFO     	 * (global step 16700: loss: 0.6471098065376282, lr: 1e-05
2024-01-06 00:41:28 INFO     	 * (global step 16750: loss: 0.5400592684745789, lr: 1e-05
2024-01-06 00:41:36 INFO     	 * (global step 16800: loss: 0.7593382894992828, lr: 1e-05
2024-01-06 00:41:43 INFO     	 * (global step 16850: loss: 0.4400273859500885, lr: 1e-05
2024-01-06 00:41:50 INFO     	 * (global step 16900: loss: 0.6745015382766724, lr: 1e-05
2024-01-06 00:41:57 INFO     	 * (global step 16950: loss: 0.6764092445373535, lr: 1e-05
2024-01-06 00:42:05 INFO     	 * (global step 17000: loss: 0.7318916618824005, lr: 1e-05
2024-01-06 00:42:11 INFO     	 * (global step 17050: loss: 0.6370302438735962, lr: 1e-05
2024-01-06 00:42:17 INFO     	 * (global step 17100: loss: 0.3094319850206375, lr: 1e-05
2024-01-06 00:42:23 INFO     	 * (global step 17150: loss: 0.6337432861328125, lr: 1e-05
2024-01-06 00:42:29 INFO     	 * (global step 17200: loss: 0.5307475924491882, lr: 1e-05
2024-01-06 00:42:35 INFO     	 * (global step 17250: loss: 0.43955494463443756, lr: 1e-05
2024-01-06 00:42:43 INFO     	 * (global step 17300: loss: 0.7595079243183136, lr: 1e-05
2024-01-06 00:42:50 INFO     	 * (global step 17350: loss: 0.7784493863582611, lr: 1e-05
2024-01-06 00:42:57 INFO     	 * (global step 17400: loss: 0.873094916343689, lr: 1e-05
2024-01-06 00:43:05 INFO     	 * (global step 17450: loss: 0.5529177486896515, lr: 1e-05
2024-01-06 00:43:12 INFO     	 * (global step 17500: loss: 0.5554143488407135, lr: 1e-05
2024-01-06 00:43:20 INFO     	 * (global step 17550: loss: 0.5525123178958893, lr: 1e-05
2024-01-06 00:43:28 INFO     	 * (global step 17600: loss: 0.7548589706420898, lr: 1e-05
2024-01-06 00:43:35 INFO     	 * (global step 17650: loss: 0.5127246379852295, lr: 1e-05
2024-01-06 00:43:43 INFO     	 * (global step 17700: loss: 0.48008161783218384, lr: 1e-05
2024-01-06 00:43:50 INFO     	 * (global step 17750: loss: 0.7428178489208221, lr: 1e-05
2024-01-06 00:43:58 INFO     	 * (global step 17800: loss: 0.8017680644989014, lr: 1e-05
2024-01-06 00:44:05 INFO     	 * (global step 17850: loss: 0.7969572246074677, lr: 1e-05
2024-01-06 00:44:12 INFO     	 * (global step 17900: loss: 0.5474146604537964, lr: 1e-05
2024-01-06 00:44:20 INFO     	 * (global step 17950: loss: 0.4984758645296097, lr: 1e-05
2024-01-06 00:44:27 INFO     	 * (global step 18000: loss: 0.6218039691448212, lr: 1e-05
2024-01-06 00:44:35 INFO     	 * (global step 18050: loss: 0.5903619080781937, lr: 1e-05
2024-01-06 00:44:42 INFO     	 * (global step 18100: loss: 0.670408695936203, lr: 1e-05
2024-01-06 00:44:50 INFO     	 * (global step 18150: loss: 0.6993608176708221, lr: 1e-05
2024-01-06 00:44:57 INFO     	 * (global step 18200: loss: 0.5831543207168579, lr: 1e-05
2024-01-06 00:45:03 INFO     	 * (global step 18250: loss: 0.5983110070228577, lr: 1e-05
2024-01-06 00:45:08 INFO     	 * (global step 18300: loss: 0.36463073641061783, lr: 1e-05
2024-01-06 00:45:14 INFO     	 * (global step 18350: loss: 0.9474826455116272, lr: 1e-05
2024-01-06 00:45:20 INFO     	 * (global step 18400: loss: 0.748723030090332, lr: 1e-05
2024-01-06 00:45:27 INFO     	 * (global step 18450: loss: 0.5990077555179596, lr: 1e-05
2024-01-06 00:45:35 INFO     	 * (global step 18500: loss: 0.5869184732437134, lr: 1e-05
2024-01-06 00:45:42 INFO     	 * (global step 18550: loss: 0.937500387430191, lr: 1e-05
2024-01-06 00:45:50 INFO     	 * (global step 18600: loss: 0.5397174060344696, lr: 1e-05
2024-01-06 00:45:57 INFO     	 * (global step 18650: loss: 0.834983617067337, lr: 1e-05
2024-01-06 00:46:04 INFO     	 * (global step 18700: loss: 0.719763845205307, lr: 1e-05
2024-01-06 00:46:12 INFO     	 * (global step 18750: loss: 0.5818381458520889, lr: 1e-05
2024-01-06 00:46:19 INFO     	 * (global step 18800: loss: 0.5732537508010864, lr: 1e-05
2024-01-06 00:46:27 INFO     	 * (global step 18850: loss: 0.6908743977546692, lr: 1e-05
2024-01-06 00:46:34 INFO     	 * (global step 18900: loss: 0.5928259044885635, lr: 1e-05
2024-01-06 00:46:41 INFO     	 * (global step 18950: loss: 0.5879128873348236, lr: 1e-05
2024-01-06 00:46:49 INFO     	 * (global step 19000: loss: 0.6485569477081299, lr: 1e-05
2024-01-06 00:46:56 INFO     	 * (global step 19050: loss: 0.8871256411075592, lr: 1e-05
2024-01-06 00:47:03 INFO     	 * (global step 19100: loss: 0.7511786818504333, lr: 1e-05
2024-01-06 00:47:11 INFO     	 * (global step 19150: loss: 0.6920840442180634, lr: 1e-05
2024-01-06 00:47:18 INFO     	 * (global step 19200: loss: 0.5490067005157471, lr: 1e-05
2024-01-06 00:47:25 INFO     	 * (global step 19250: loss: 0.680771678686142, lr: 1e-05
2024-01-06 00:47:33 INFO     	 * (global step 19300: loss: 0.49047665297985077, lr: 1e-05
2024-01-06 00:47:41 INFO     	 * (global step 19350: loss: 0.84290811419487, lr: 1e-05
2024-01-06 00:47:48 INFO     	 * (global step 19400: loss: 0.7483553290367126, lr: 1e-05
2024-01-06 00:47:55 INFO     	 * (global step 19450: loss: 0.5323151051998138, lr: 1e-05
2024-01-06 00:48:01 INFO     	 * (global step 19500: loss: 0.5477095693349838, lr: 1e-05
2024-01-06 00:48:07 INFO     	 * (global step 19550: loss: 0.3854716792702675, lr: 1e-05
2024-01-06 00:48:13 INFO     	 * (global step 19600: loss: 0.6322026252746582, lr: 1e-05
2024-01-06 00:48:19 INFO     	 * (global step 19650: loss: 0.5145425796508789, lr: 1e-05
2024-01-06 00:48:25 INFO     	 * (global step 19700: loss: 0.5438940078020096, lr: 1e-05
2024-01-06 00:48:32 INFO     	 * (global step 19750: loss: 0.4694926589727402, lr: 1e-05
2024-01-06 00:48:40 INFO     	 * (global step 19800: loss: 0.5560320913791656, lr: 1e-05
2024-01-06 00:48:47 INFO     	 * (global step 19850: loss: 0.3800538629293442, lr: 1e-05
2024-01-06 00:48:55 INFO     	 * (global step 19900: loss: 0.5639932453632355, lr: 1e-05
2024-01-06 00:49:02 INFO     	 * (global step 19950: loss: 0.5513802766799927, lr: 1e-05
2024-01-06 00:49:10 INFO     	 * (global step 20000: loss: 0.48593975603580475, lr: 1e-05
2024-01-06 00:49:18 INFO     	 * (global step 20050: loss: 0.7401508092880249, lr: 1e-05
2024-01-06 00:49:25 INFO     	 * (global step 20100: loss: 0.6516757309436798, lr: 1e-05
2024-01-06 00:49:33 INFO     	 * (global step 20150: loss: 0.5345345139503479, lr: 1e-05
2024-01-06 00:49:40 INFO     	 * (global step 20200: loss: 0.46652786433696747, lr: 1e-05
2024-01-06 00:49:47 INFO     [epoch 4/15] average loss: 0.634, lr: 1e-05
2024-01-06 00:49:47 INFO     saving model related files
2024-01-06 00:49:47 INFO     saving model
2024-01-06 00:49:48 INFO     saving tokenizer
2024-01-06 00:49:48 INFO     saving optimizer
2024-01-06 00:49:49 INFO     remove old optimizer files
2024-01-06 00:49:51 INFO     	 * (global step 20250: loss: 0.5476353019475937, lr: 1e-05
2024-01-06 00:49:58 INFO     	 * (global step 20300: loss: 0.5907951593399048, lr: 1e-05
2024-01-06 00:50:06 INFO     	 * (global step 20350: loss: 0.7823721170425415, lr: 1e-05
2024-01-06 00:50:13 INFO     	 * (global step 20400: loss: 0.544745609164238, lr: 1e-05
2024-01-06 00:50:21 INFO     	 * (global step 20450: loss: 0.4462967664003372, lr: 1e-05
2024-01-06 00:50:28 INFO     	 * (global step 20500: loss: 0.5048476010560989, lr: 1e-05
2024-01-06 00:50:36 INFO     	 * (global step 20550: loss: 0.6345032602548599, lr: 1e-05
2024-01-06 00:50:43 INFO     	 * (global step 20600: loss: 0.6477793455123901, lr: 1e-05
2024-01-06 00:50:50 INFO     	 * (global step 20650: loss: 0.8072548508644104, lr: 1e-05
2024-01-06 00:50:56 INFO     	 * (global step 20700: loss: 0.8955800831317902, lr: 1e-05
2024-01-06 00:51:02 INFO     	 * (global step 20750: loss: 0.42130161821842194, lr: 1e-05
2024-01-06 00:51:08 INFO     	 * (global step 20800: loss: 0.7326174080371857, lr: 1e-05
2024-01-06 00:51:14 INFO     	 * (global step 20850: loss: 0.6351051330566406, lr: 1e-05
2024-01-06 00:51:21 INFO     	 * (global step 20900: loss: 0.46602147817611694, lr: 1e-05
2024-01-06 00:51:28 INFO     	 * (global step 20950: loss: 0.7531323432922363, lr: 1e-05
2024-01-06 00:51:36 INFO     	 * (global step 21000: loss: 0.44050541520118713, lr: 1e-05
2024-01-06 00:51:43 INFO     	 * (global step 21050: loss: 0.7676005959510803, lr: 1e-05
2024-01-06 00:51:51 INFO     	 * (global step 21100: loss: 0.5929872393608093, lr: 1e-05
2024-01-06 00:51:59 INFO     	 * (global step 21150: loss: 0.5636177659034729, lr: 1e-05
2024-01-06 00:52:06 INFO     	 * (global step 21200: loss: 0.43471069633960724, lr: 1e-05
2024-01-06 00:52:13 INFO     	 * (global step 21250: loss: 0.4807368367910385, lr: 1e-05
2024-01-06 00:52:21 INFO     	 * (global step 21300: loss: 0.707612007856369, lr: 1e-05
2024-01-06 00:52:29 INFO     	 * (global step 21350: loss: 0.6344861537218094, lr: 1e-05
2024-01-06 00:52:36 INFO     	 * (global step 21400: loss: 0.6181194186210632, lr: 1e-05
2024-01-06 00:52:44 INFO     	 * (global step 21450: loss: 0.5303145349025726, lr: 1e-05
2024-01-06 00:52:52 INFO     	 * (global step 21500: loss: 0.4580710232257843, lr: 1e-05
2024-01-06 00:52:59 INFO     	 * (global step 21550: loss: 0.41372881829738617, lr: 1e-05
2024-01-06 00:53:07 INFO     	 * (global step 21600: loss: 0.5783174484968185, lr: 1e-05
2024-01-06 00:53:14 INFO     	 * (global step 21650: loss: 0.492117777466774, lr: 1e-05
2024-01-06 00:53:22 INFO     	 * (global step 21700: loss: 0.6149419695138931, lr: 1e-05
2024-01-06 00:53:29 INFO     	 * (global step 21750: loss: 0.6585688591003418, lr: 1e-05
2024-01-06 00:53:37 INFO     	 * (global step 21800: loss: 0.7419697940349579, lr: 1e-05
2024-01-06 00:53:44 INFO     	 * (global step 21850: loss: 0.8176344931125641, lr: 1e-05
2024-01-06 00:53:50 INFO     	 * (global step 21900: loss: 0.7662428915500641, lr: 1e-05
2024-01-06 00:53:55 INFO     	 * (global step 21950: loss: 0.7728271484375, lr: 1e-05
2024-01-06 00:54:02 INFO     	 * (global step 22000: loss: 0.6449792832136154, lr: 1e-05
2024-01-06 00:54:08 INFO     	 * (global step 22050: loss: 0.6120304763317108, lr: 1e-05
2024-01-06 00:54:15 INFO     	 * (global step 22100: loss: 0.6284297704696655, lr: 1e-05
2024-01-06 00:54:23 INFO     	 * (global step 22150: loss: 0.4924950450658798, lr: 1e-05
2024-01-06 00:54:31 INFO     	 * (global step 22200: loss: 0.666122704744339, lr: 1e-05
2024-01-06 00:54:38 INFO     	 * (global step 22250: loss: 0.7973608076572418, lr: 1e-05
2024-01-06 00:54:45 INFO     	 * (global step 22300: loss: 0.653135746717453, lr: 1e-05
2024-01-06 00:54:53 INFO     	 * (global step 22350: loss: 0.5968358814716339, lr: 1e-05
2024-01-06 00:55:01 INFO     	 * (global step 22400: loss: 0.6044777035713196, lr: 1e-05
2024-01-06 00:55:09 INFO     	 * (global step 22450: loss: 0.6008381843566895, lr: 1e-05
2024-01-06 00:55:16 INFO     	 * (global step 22500: loss: 0.7058533132076263, lr: 1e-05
2024-01-06 00:55:23 INFO     	 * (global step 22550: loss: 0.5971646904945374, lr: 1e-05
2024-01-06 00:55:31 INFO     	 * (global step 22600: loss: 0.6702375710010529, lr: 1e-05
2024-01-06 00:55:38 INFO     	 * (global step 22650: loss: 0.5158943831920624, lr: 1e-05
2024-01-06 00:55:46 INFO     	 * (global step 22700: loss: 0.48548319935798645, lr: 1e-05
2024-01-06 00:55:53 INFO     	 * (global step 22750: loss: 0.48988692462444305, lr: 1e-05
2024-01-06 00:56:01 INFO     	 * (global step 22800: loss: 0.5343603789806366, lr: 1e-05
2024-01-06 00:56:08 INFO     	 * (global step 22850: loss: 0.649834394454956, lr: 1e-05
2024-01-06 00:56:15 INFO     	 * (global step 22900: loss: 0.45323099195957184, lr: 1e-05
2024-01-06 00:56:23 INFO     	 * (global step 22950: loss: 0.6085619032382965, lr: 1e-05
2024-01-06 00:56:30 INFO     	 * (global step 23000: loss: 0.700621485710144, lr: 1e-05
2024-01-06 00:56:37 INFO     	 * (global step 23050: loss: 0.42957286536693573, lr: 1e-05
2024-01-06 00:56:44 INFO     	 * (global step 23100: loss: 0.6725853383541107, lr: 1e-05
2024-01-06 00:56:50 INFO     	 * (global step 23150: loss: 0.7200808823108673, lr: 1e-05
2024-01-06 00:56:56 INFO     	 * (global step 23200: loss: 0.5016544163227081, lr: 1e-05
2024-01-06 00:57:02 INFO     	 * (global step 23250: loss: 0.729867547750473, lr: 1e-05
2024-01-06 00:57:08 INFO     	 * (global step 23300: loss: 0.36245763301849365, lr: 1e-05
2024-01-06 00:57:16 INFO     	 * (global step 23350: loss: 0.5342501103878021, lr: 1e-05
2024-01-06 00:57:23 INFO     	 * (global step 23400: loss: 0.4733055979013443, lr: 1e-05
2024-01-06 00:57:31 INFO     	 * (global step 23450: loss: 0.5713726580142975, lr: 1e-05
2024-01-06 00:57:38 INFO     	 * (global step 23500: loss: 0.6007705330848694, lr: 1e-05
2024-01-06 00:57:46 INFO     	 * (global step 23550: loss: 0.5124195367097855, lr: 1e-05
2024-01-06 00:57:54 INFO     	 * (global step 23600: loss: 0.6997503936290741, lr: 1e-05
2024-01-06 00:58:01 INFO     	 * (global step 23650: loss: 0.3737707734107971, lr: 1e-05
2024-01-06 00:58:09 INFO     	 * (global step 23700: loss: 0.5732133835554123, lr: 1e-05
2024-01-06 00:58:16 INFO     	 * (global step 23750: loss: 0.7037204504013062, lr: 1e-05
2024-01-06 00:58:24 INFO     	 * (global step 23800: loss: 0.5841688811779022, lr: 1e-05
2024-01-06 00:58:31 INFO     	 * (global step 23850: loss: 0.5360314846038818, lr: 1e-05
2024-01-06 00:58:39 INFO     	 * (global step 23900: loss: 0.4809565246105194, lr: 1e-05
2024-01-06 00:58:47 INFO     	 * (global step 23950: loss: 1.0074420273303986, lr: 1e-05
2024-01-06 00:58:54 INFO     	 * (global step 24000: loss: 0.5058982521295547, lr: 1e-05
2024-01-06 00:59:01 INFO     	 * (global step 24050: loss: 0.8309171497821808, lr: 1e-05
2024-01-06 00:59:09 INFO     	 * (global step 24100: loss: 0.531779482960701, lr: 1e-05
2024-01-06 00:59:16 INFO     	 * (global step 24150: loss: 0.6292117089033127, lr: 1e-05
2024-01-06 00:59:24 INFO     	 * (global step 24200: loss: 0.6663666367530823, lr: 1e-05
2024-01-06 00:59:31 INFO     	 * (global step 24250: loss: 0.7439642250537872, lr: 1e-05
2024-01-06 00:59:37 INFO     [epoch 5/15] average loss: 0.621, lr: 1e-05
2024-01-06 00:59:37 INFO     saving model related files
2024-01-06 00:59:37 INFO     saving model
2024-01-06 00:59:38 INFO     saving tokenizer
2024-01-06 00:59:38 INFO     saving optimizer
2024-01-06 00:59:39 INFO     remove old optimizer files
2024-01-06 00:59:41 INFO     	 * (global step 24300: loss: 0.5689461827278137, lr: 1e-05
2024-01-06 00:59:48 INFO     	 * (global step 24350: loss: 0.5649008452892303, lr: 1e-05
2024-01-06 00:59:55 INFO     	 * (global step 24400: loss: 0.5280446410179138, lr: 1e-05
2024-01-06 01:00:00 INFO     	 * (global step 24450: loss: 0.4932072162628174, lr: 1e-05
2024-01-06 01:00:06 INFO     	 * (global step 24500: loss: 0.7331530451774597, lr: 1e-05
2024-01-06 01:00:12 INFO     	 * (global step 24550: loss: 0.5525364279747009, lr: 1e-05
2024-01-06 01:00:19 INFO     	 * (global step 24600: loss: 0.5964732766151428, lr: 1e-05
2024-01-06 01:00:27 INFO     	 * (global step 24650: loss: 0.5981965661048889, lr: 1e-05
2024-01-06 01:00:34 INFO     	 * (global step 24700: loss: 0.45212429761886597, lr: 1e-05
2024-01-06 01:00:41 INFO     	 * (global step 24750: loss: 0.7378761172294617, lr: 1e-05
2024-01-06 01:00:49 INFO     	 * (global step 24800: loss: 0.7811472415924072, lr: 1e-05
2024-01-06 01:00:57 INFO     	 * (global step 24850: loss: 0.6082182228565216, lr: 1e-05
2024-01-06 01:01:04 INFO     	 * (global step 24900: loss: 0.7065928280353546, lr: 1e-05
2024-01-06 01:01:12 INFO     	 * (global step 24950: loss: 0.8029794096946716, lr: 1e-05
2024-01-06 01:01:20 INFO     	 * (global step 25000: loss: 0.502309575676918, lr: 1e-05
2024-01-06 01:01:27 INFO     	 * (global step 25050: loss: 0.758445680141449, lr: 1e-05
2024-01-06 01:01:34 INFO     	 * (global step 25100: loss: 0.5496307164430618, lr: 1e-05
2024-01-06 01:01:41 INFO     	 * (global step 25150: loss: 0.6510288715362549, lr: 1e-05
2024-01-06 01:01:49 INFO     	 * (global step 25200: loss: 0.8671011626720428, lr: 1e-05
2024-01-06 01:01:56 INFO     	 * (global step 25250: loss: 0.505620539188385, lr: 1e-05
2024-01-06 01:02:04 INFO     	 * (global step 25300: loss: 0.9376699924468994, lr: 1e-05
2024-01-06 01:02:11 INFO     	 * (global step 25350: loss: 0.5286658704280853, lr: 1e-05
2024-01-06 01:02:18 INFO     	 * (global step 25400: loss: 0.6628560721874237, lr: 1e-05
2024-01-06 01:02:26 INFO     	 * (global step 25450: loss: 0.8666255325078964, lr: 1e-05
2024-01-06 01:02:34 INFO     	 * (global step 25500: loss: 0.6960602104663849, lr: 1e-05
2024-01-06 01:02:39 INFO     	 * (global step 25550: loss: 0.48552095890045166, lr: 1e-05
2024-01-06 01:02:45 INFO     	 * (global step 25600: loss: 0.9926271438598633, lr: 1e-05
2024-01-06 01:02:51 INFO     	 * (global step 25650: loss: 0.7301577627658844, lr: 1e-05
2024-01-06 01:02:57 INFO     	 * (global step 25700: loss: 0.7146792709827423, lr: 1e-05
2024-01-06 01:03:04 INFO     	 * (global step 25750: loss: 0.35004131495952606, lr: 1e-05
2024-01-06 01:03:11 INFO     	 * (global step 25800: loss: 0.5701766908168793, lr: 1e-05
2024-01-06 01:03:19 INFO     	 * (global step 25850: loss: 0.5704925060272217, lr: 1e-05
2024-01-06 01:03:26 INFO     	 * (global step 25900: loss: 0.6447314918041229, lr: 1e-05
2024-01-06 01:03:34 INFO     	 * (global step 25950: loss: 0.5726860463619232, lr: 1e-05
2024-01-06 01:03:42 INFO     	 * (global step 26000: loss: 0.4495917856693268, lr: 1e-05
2024-01-06 01:03:49 INFO     	 * (global step 26050: loss: 0.51645328104496, lr: 1e-05
2024-01-06 01:03:56 INFO     	 * (global step 26100: loss: 0.5154041796922684, lr: 1e-05
2024-01-06 01:04:04 INFO     	 * (global step 26150: loss: 0.5670818388462067, lr: 1e-05
2024-01-06 01:04:11 INFO     	 * (global step 26200: loss: 0.7267169058322906, lr: 1e-05
2024-01-06 01:04:19 INFO     	 * (global step 26250: loss: 0.47318248450756073, lr: 1e-05
2024-01-06 01:04:26 INFO     	 * (global step 26300: loss: 0.6045156419277191, lr: 1e-05
2024-01-06 01:04:34 INFO     	 * (global step 26350: loss: 0.6000773906707764, lr: 1e-05
2024-01-06 01:04:41 INFO     	 * (global step 26400: loss: 0.5929756164550781, lr: 1e-05
2024-01-06 01:04:49 INFO     	 * (global step 26450: loss: 0.47760048508644104, lr: 1e-05
2024-01-06 01:04:56 INFO     	 * (global step 26500: loss: 0.9776754677295685, lr: 1e-05
2024-01-06 01:05:04 INFO     	 * (global step 26550: loss: 0.7020851075649261, lr: 1e-05
2024-01-06 01:05:11 INFO     	 * (global step 26600: loss: 0.7265531718730927, lr: 1e-05
2024-01-06 01:05:19 INFO     	 * (global step 26650: loss: 0.5306548476219177, lr: 1e-05
2024-01-06 01:05:25 INFO     	 * (global step 26700: loss: 0.7595654726028442, lr: 1e-05
2024-01-06 01:05:31 INFO     	 * (global step 26750: loss: 0.47163020074367523, lr: 1e-05
2024-01-06 01:05:37 INFO     	 * (global step 26800: loss: 0.752036452293396, lr: 1e-05
2024-01-06 01:05:43 INFO     	 * (global step 26850: loss: 0.8049919307231903, lr: 1e-05
2024-01-06 01:05:49 INFO     	 * (global step 26900: loss: 0.5136559754610062, lr: 1e-05
2024-01-06 01:05:57 INFO     	 * (global step 26950: loss: 0.5849839001893997, lr: 1e-05
2024-01-06 01:06:05 INFO     	 * (global step 27000: loss: 0.5881058275699615, lr: 1e-05
2024-01-06 01:06:12 INFO     	 * (global step 27050: loss: 0.46859273314476013, lr: 1e-05
2024-01-06 01:06:20 INFO     	 * (global step 27100: loss: 0.5009586811065674, lr: 1e-05
2024-01-06 01:06:27 INFO     	 * (global step 27150: loss: 0.6485614776611328, lr: 1e-05
2024-01-06 01:06:35 INFO     	 * (global step 27200: loss: 0.5077266246080399, lr: 1e-05
2024-01-06 01:06:42 INFO     	 * (global step 27250: loss: 0.4075150042772293, lr: 1e-05
2024-01-06 01:06:50 INFO     	 * (global step 27300: loss: 0.622249573469162, lr: 1e-05
2024-01-06 01:06:57 INFO     	 * (global step 27350: loss: 0.5227582007646561, lr: 1e-05
2024-01-06 01:07:05 INFO     	 * (global step 27400: loss: 0.6691242456436157, lr: 1e-05
2024-01-06 01:07:12 INFO     	 * (global step 27450: loss: 0.5181062072515488, lr: 1e-05
2024-01-06 01:07:19 INFO     	 * (global step 27500: loss: 0.5384785532951355, lr: 1e-05
2024-01-06 01:07:27 INFO     	 * (global step 27550: loss: 0.4523395299911499, lr: 1e-05
2024-01-06 01:07:35 INFO     	 * (global step 27600: loss: 0.4730985462665558, lr: 1e-05
2024-01-06 01:07:43 INFO     	 * (global step 27650: loss: 0.6112393587827682, lr: 1e-05
2024-01-06 01:07:50 INFO     	 * (global step 27700: loss: 0.6447961330413818, lr: 1e-05
2024-01-06 01:07:57 INFO     	 * (global step 27750: loss: 0.7906152307987213, lr: 1e-05
2024-01-06 01:08:05 INFO     	 * (global step 27800: loss: 0.6328801512718201, lr: 1e-05
2024-01-06 01:08:12 INFO     	 * (global step 27850: loss: 0.6994199156761169, lr: 1e-05
2024-01-06 01:08:20 INFO     	 * (global step 27900: loss: 0.6601169109344482, lr: 1e-05
2024-01-06 01:08:27 INFO     	 * (global step 27950: loss: 0.48309049010276794, lr: 1e-05
2024-01-06 01:08:32 INFO     	 * (global step 28000: loss: 0.5265750885009766, lr: 1e-05
2024-01-06 01:08:38 INFO     	 * (global step 28050: loss: 0.8668226301670074, lr: 1e-05
2024-01-06 01:08:44 INFO     	 * (global step 28100: loss: 0.7052171230316162, lr: 1e-05
2024-01-06 01:08:50 INFO     	 * (global step 28150: loss: 0.6346568167209625, lr: 1e-05
2024-01-06 01:08:58 INFO     	 * (global step 28200: loss: 0.7038286328315735, lr: 1e-05
2024-01-06 01:09:06 INFO     	 * (global step 28250: loss: 0.6507002115249634, lr: 1e-05
2024-01-06 01:09:13 INFO     	 * (global step 28300: loss: 0.7841033041477203, lr: 1e-05
2024-01-06 01:09:19 INFO     [epoch 6/15] average loss: 0.611, lr: 1e-05
2024-01-06 01:09:19 INFO     saving model related files
2024-01-06 01:09:19 INFO     saving model
2024-01-06 01:09:20 INFO     saving tokenizer
2024-01-06 01:09:20 INFO     saving optimizer
2024-01-06 01:09:22 INFO     remove old optimizer files
2024-01-06 01:09:24 INFO     	 * (global step 28350: loss: 0.7240523993968964, lr: 1e-05
2024-01-06 01:09:32 INFO     	 * (global step 28400: loss: 0.5277515947818756, lr: 1e-05
2024-01-06 01:09:40 INFO     	 * (global step 28450: loss: 0.6177780330181122, lr: 1e-05
2024-01-06 01:09:48 INFO     	 * (global step 28500: loss: 0.6755118668079376, lr: 1e-05
2024-01-06 01:09:55 INFO     	 * (global step 28550: loss: 0.6663705110549927, lr: 1e-05
2024-01-06 01:10:03 INFO     	 * (global step 28600: loss: 0.6840538084506989, lr: 1e-05
2024-01-06 01:10:11 INFO     	 * (global step 28650: loss: 0.7039567232131958, lr: 1e-05
2024-01-06 01:10:18 INFO     	 * (global step 28700: loss: 0.60279680788517, lr: 1e-05
2024-01-06 01:10:26 INFO     	 * (global step 28750: loss: 0.629299134016037, lr: 1e-05
2024-01-06 01:10:34 INFO     	 * (global step 28800: loss: 0.5107835680246353, lr: 1e-05
2024-01-06 01:10:41 INFO     	 * (global step 28850: loss: 0.6860672533512115, lr: 1e-05
2024-01-06 01:10:49 INFO     	 * (global step 28900: loss: 0.6624032557010651, lr: 1e-05
2024-01-06 01:10:57 INFO     	 * (global step 28950: loss: 0.5644247382879257, lr: 1e-05
2024-01-06 01:11:04 INFO     	 * (global step 29000: loss: 0.5663924068212509, lr: 1e-05
2024-01-06 01:11:12 INFO     	 * (global step 29050: loss: 0.4808274507522583, lr: 1e-05
2024-01-06 01:11:20 INFO     	 * (global step 29100: loss: 0.6452528834342957, lr: 1e-05
2024-01-06 01:11:28 INFO     	 * (global step 29150: loss: 0.7868667840957642, lr: 1e-05
2024-01-06 01:11:35 INFO     	 * (global step 29200: loss: 0.7190335988998413, lr: 1e-05
2024-01-06 01:11:41 INFO     	 * (global step 29250: loss: 0.5628443211317062, lr: 1e-05
2024-01-06 01:11:47 INFO     	 * (global step 29300: loss: 0.4842766970396042, lr: 1e-05
2024-01-06 01:11:53 INFO     	 * (global step 29350: loss: 0.636072039604187, lr: 1e-05
2024-01-06 01:11:59 INFO     	 * (global step 29400: loss: 0.4586554914712906, lr: 1e-05
2024-01-06 01:12:07 INFO     	 * (global step 29450: loss: 0.5936245322227478, lr: 1e-05
2024-01-06 01:12:14 INFO     	 * (global step 29500: loss: 0.40102051198482513, lr: 1e-05
2024-01-06 01:12:22 INFO     	 * (global step 29550: loss: 0.48402203619480133, lr: 1e-05
2024-01-06 01:12:29 INFO     	 * (global step 29600: loss: 0.4047689288854599, lr: 1e-05
2024-01-06 01:12:37 INFO     	 * (global step 29650: loss: 0.4733467847108841, lr: 1e-05
2024-01-06 01:12:44 INFO     	 * (global step 29700: loss: 0.5762364268302917, lr: 1e-05
2024-01-06 01:12:52 INFO     	 * (global step 29750: loss: 0.9010549187660217, lr: 1e-05
2024-01-06 01:12:59 INFO     	 * (global step 29800: loss: 0.6718919426202774, lr: 1e-05
2024-01-06 01:13:06 INFO     	 * (global step 29850: loss: 0.6107604652643204, lr: 1e-05
2024-01-06 01:13:14 INFO     	 * (global step 29900: loss: 0.531876266002655, lr: 1e-05
2024-01-06 01:13:21 INFO     	 * (global step 29950: loss: 0.6203000843524933, lr: 1e-05
2024-01-06 01:13:29 INFO     	 * (global step 30000: loss: 0.5956496894359589, lr: 1e-05
2024-01-06 01:13:36 INFO     	 * (global step 30050: loss: 0.6138758510351181, lr: 1e-05
2024-01-06 01:13:44 INFO     	 * (global step 30100: loss: 0.5898891389369965, lr: 1e-05
2024-01-06 01:13:52 INFO     	 * (global step 30150: loss: 0.5508165210485458, lr: 1e-05
2024-01-06 01:14:00 INFO     	 * (global step 30200: loss: 0.7379739284515381, lr: 1e-05
2024-01-06 01:14:08 INFO     	 * (global step 30250: loss: 0.6542397141456604, lr: 1e-05
2024-01-06 01:14:15 INFO     	 * (global step 30300: loss: 0.802196592092514, lr: 1e-05
2024-01-06 01:14:23 INFO     	 * (global step 30350: loss: 0.4171600490808487, lr: 1e-05
2024-01-06 01:14:29 INFO     	 * (global step 30400: loss: 0.6266160309314728, lr: 1e-05
2024-01-06 01:14:35 INFO     	 * (global step 30450: loss: 1.150699943304062, lr: 1e-05
2024-01-06 01:14:41 INFO     	 * (global step 30500: loss: 0.6221256703138351, lr: 1e-05
2024-01-06 01:14:47 INFO     	 * (global step 30550: loss: 0.38730984926223755, lr: 1e-05
2024-01-06 01:14:53 INFO     	 * (global step 30600: loss: 0.7217903137207031, lr: 1e-05
2024-01-06 01:15:01 INFO     	 * (global step 30650: loss: 0.8728549182415009, lr: 1e-05
2024-01-06 01:15:09 INFO     	 * (global step 30700: loss: 0.41664236783981323, lr: 1e-05
2024-01-06 01:15:16 INFO     	 * (global step 30750: loss: 0.5958223342895508, lr: 1e-05
2024-01-06 01:15:23 INFO     	 * (global step 30800: loss: 0.4840436279773712, lr: 1e-05
2024-01-06 01:15:30 INFO     	 * (global step 30850: loss: 0.4223839044570923, lr: 1e-05
2024-01-06 01:15:38 INFO     	 * (global step 30900: loss: 0.773284912109375, lr: 1e-05
2024-01-06 01:15:45 INFO     	 * (global step 30950: loss: 0.514409214258194, lr: 1e-05
2024-01-06 01:15:52 INFO     	 * (global step 31000: loss: 0.7553538084030151, lr: 1e-05
2024-01-06 01:15:59 INFO     	 * (global step 31050: loss: 0.4486533999443054, lr: 1e-05
2024-01-06 01:16:07 INFO     	 * (global step 31100: loss: 0.48529738187789917, lr: 1e-05
2024-01-06 01:16:14 INFO     	 * (global step 31150: loss: 0.5198331028223038, lr: 1e-05
2024-01-06 01:16:22 INFO     	 * (global step 31200: loss: 0.5655975490808487, lr: 1e-05
2024-01-06 01:16:29 INFO     	 * (global step 31250: loss: 0.5270327627658844, lr: 1e-05
2024-01-06 01:16:36 INFO     	 * (global step 31300: loss: 0.8658660352230072, lr: 1e-05
2024-01-06 01:16:44 INFO     	 * (global step 31350: loss: 0.6074073910713196, lr: 1e-05
2024-01-06 01:16:51 INFO     	 * (global step 31400: loss: 0.8345010280609131, lr: 1e-05
2024-01-06 01:16:58 INFO     	 * (global step 31450: loss: 0.6474476605653763, lr: 1e-05
2024-01-06 01:17:05 INFO     	 * (global step 31500: loss: 0.4506741166114807, lr: 1e-05
2024-01-06 01:17:13 INFO     	 * (global step 31550: loss: 0.47725388407707214, lr: 1e-05
2024-01-06 01:17:20 INFO     	 * (global step 31600: loss: 0.5854738503694534, lr: 1e-05
2024-01-06 01:17:28 INFO     	 * (global step 31650: loss: 0.6674003303050995, lr: 1e-05
2024-01-06 01:17:34 INFO     	 * (global step 31700: loss: 0.710050493478775, lr: 1e-05
2024-01-06 01:17:39 INFO     	 * (global step 31750: loss: 0.6421778798103333, lr: 1e-05
2024-01-06 01:17:45 INFO     	 * (global step 31800: loss: 0.4723975211381912, lr: 1e-05
2024-01-06 01:17:51 INFO     	 * (global step 31850: loss: 0.6522612869739532, lr: 1e-05
2024-01-06 01:17:58 INFO     	 * (global step 31900: loss: 0.5611065328121185, lr: 1e-05
2024-01-06 01:18:06 INFO     	 * (global step 31950: loss: 0.5395395457744598, lr: 1e-05
2024-01-06 01:18:13 INFO     	 * (global step 32000: loss: 0.6354320347309113, lr: 1e-05
2024-01-06 01:18:21 INFO     	 * (global step 32050: loss: 0.5441930890083313, lr: 1e-05
2024-01-06 01:18:29 INFO     	 * (global step 32100: loss: 0.6341200172901154, lr: 1e-05
2024-01-06 01:18:36 INFO     	 * (global step 32150: loss: 0.40668417513370514, lr: 1e-05
2024-01-06 01:18:44 INFO     	 * (global step 32200: loss: 0.54664745926857, lr: 1e-05
2024-01-06 01:18:52 INFO     	 * (global step 32250: loss: 0.43376462161540985, lr: 1e-05
2024-01-06 01:18:59 INFO     	 * (global step 32300: loss: 0.6137606650590897, lr: 1e-05
2024-01-06 01:19:07 INFO     	 * (global step 32350: loss: 0.48563219606876373, lr: 1e-05
2024-01-06 01:19:12 INFO     [epoch 7/15] average loss: 0.603, lr: 1e-05
2024-01-06 01:19:12 INFO     saving model related files
2024-01-06 01:19:12 INFO     saving model
2024-01-06 01:19:13 INFO     saving tokenizer
2024-01-06 01:19:13 INFO     saving optimizer
2024-01-06 01:19:15 INFO     remove old optimizer files
2024-01-06 01:19:17 INFO     	 * (global step 32400: loss: 0.7919161915779114, lr: 1e-05
2024-01-06 01:19:25 INFO     	 * (global step 32450: loss: 0.3752211183309555, lr: 1e-05
2024-01-06 01:19:32 INFO     	 * (global step 32500: loss: 0.5611127465963364, lr: 1e-05
2024-01-06 01:19:40 INFO     	 * (global step 32550: loss: 0.5079983621835709, lr: 1e-05
2024-01-06 01:19:47 INFO     	 * (global step 32600: loss: 0.5997724831104279, lr: 1e-05
2024-01-06 01:19:55 INFO     	 * (global step 32650: loss: 0.5652076303958893, lr: 1e-05
2024-01-06 01:20:03 INFO     	 * (global step 32700: loss: 0.4350191652774811, lr: 1e-05
2024-01-06 01:20:10 INFO     	 * (global step 32750: loss: 0.4871412217617035, lr: 1e-05
2024-01-06 01:20:18 INFO     	 * (global step 32800: loss: 0.6692680269479752, lr: 1e-05
2024-01-06 01:20:26 INFO     	 * (global step 32850: loss: 0.4559994637966156, lr: 1e-05
2024-01-06 01:20:33 INFO     	 * (global step 32900: loss: 0.5576919615268707, lr: 1e-05
2024-01-06 01:20:39 INFO     	 * (global step 32950: loss: 0.6434427201747894, lr: 1e-05
2024-01-06 01:20:45 INFO     	 * (global step 33000: loss: 0.34999023377895355, lr: 1e-05
2024-01-06 01:20:51 INFO     	 * (global step 33050: loss: 0.5734121799468994, lr: 1e-05
2024-01-06 01:20:57 INFO     	 * (global step 33100: loss: 0.6719244718551636, lr: 1e-05
2024-01-06 01:21:04 INFO     	 * (global step 33150: loss: 0.48458997905254364, lr: 1e-05
2024-01-06 01:21:12 INFO     	 * (global step 33200: loss: 0.5217371881008148, lr: 1e-05
2024-01-06 01:21:20 INFO     	 * (global step 33250: loss: 0.5484368950128555, lr: 1e-05
2024-01-06 01:21:27 INFO     	 * (global step 33300: loss: 0.2205621600151062, lr: 1e-05
2024-01-06 01:21:35 INFO     	 * (global step 33350: loss: 0.7174566835165024, lr: 1e-05
2024-01-06 01:21:42 INFO     	 * (global step 33400: loss: 0.5190673172473907, lr: 1e-05
2024-01-06 01:21:50 INFO     	 * (global step 33450: loss: 0.5565522313117981, lr: 1e-05
2024-01-06 01:21:57 INFO     	 * (global step 33500: loss: 0.6696303188800812, lr: 1e-05
2024-01-06 01:22:04 INFO     	 * (global step 33550: loss: 0.5231385976076126, lr: 1e-05
2024-01-06 01:22:12 INFO     	 * (global step 33600: loss: 0.628976434469223, lr: 1e-05
2024-01-06 01:22:20 INFO     	 * (global step 33650: loss: 0.736485093832016, lr: 1e-05
2024-01-06 01:22:28 INFO     	 * (global step 33700: loss: 0.42974264919757843, lr: 1e-05
2024-01-06 01:22:36 INFO     	 * (global step 33750: loss: 0.45419082045555115, lr: 1e-05
2024-01-06 01:22:43 INFO     	 * (global step 33800: loss: 0.6618258059024811, lr: 1e-05
2024-01-06 01:22:51 INFO     	 * (global step 33850: loss: 0.5555759817361832, lr: 1e-05
2024-01-06 01:22:59 INFO     	 * (global step 33900: loss: 0.5128871649503708, lr: 1e-05
2024-01-06 01:23:07 INFO     	 * (global step 33950: loss: 0.509115919470787, lr: 1e-05
2024-01-06 01:23:14 INFO     	 * (global step 34000: loss: 0.7486680448055267, lr: 1e-05
2024-01-06 01:23:23 INFO     	 * (global step 34050: loss: 1.0128744840621948, lr: 1e-05
2024-01-06 01:23:31 INFO     	 * (global step 34100: loss: 0.7093401253223419, lr: 1e-05
2024-01-06 01:23:38 INFO     	 * (global step 34150: loss: 0.48680005967617035, lr: 1e-05
2024-01-06 01:23:46 INFO     	 * (global step 34200: loss: 0.593016654253006, lr: 1e-05
2024-01-06 01:23:54 INFO     	 * (global step 34250: loss: 0.6285170614719391, lr: 1e-05
2024-01-06 01:24:02 INFO     	 * (global step 34300: loss: 0.38990119099617004, lr: 1e-05
2024-01-06 01:24:09 INFO     	 * (global step 34350: loss: 0.6032950282096863, lr: 1e-05
2024-01-06 01:24:15 INFO     	 * (global step 34400: loss: 0.3781422972679138, lr: 1e-05
2024-01-06 01:24:21 INFO     	 * (global step 34450: loss: 0.6518202424049377, lr: 1e-05
2024-01-06 01:24:27 INFO     	 * (global step 34500: loss: 0.3906812220811844, lr: 1e-05
2024-01-06 01:24:33 INFO     	 * (global step 34550: loss: 0.4635147452354431, lr: 1e-05
2024-01-06 01:24:40 INFO     	 * (global step 34600: loss: 0.7131564170122147, lr: 1e-05
2024-01-06 01:24:48 INFO     	 * (global step 34650: loss: 0.5102012753486633, lr: 1e-05
2024-01-06 01:24:56 INFO     	 * (global step 34700: loss: 0.7166100144386292, lr: 1e-05
2024-01-06 01:25:03 INFO     	 * (global step 34750: loss: 0.704509973526001, lr: 1e-05
2024-01-06 01:25:11 INFO     	 * (global step 34800: loss: 0.510026291012764, lr: 1e-05
2024-01-06 01:25:18 INFO     	 * (global step 34850: loss: 0.7345491647720337, lr: 1e-05
2024-01-06 01:25:26 INFO     	 * (global step 34900: loss: 0.5635958611965179, lr: 1e-05
2024-01-06 01:25:34 INFO     	 * (global step 34950: loss: 0.5461249947547913, lr: 1e-05
2024-01-06 01:25:41 INFO     	 * (global step 35000: loss: 0.5375817120075226, lr: 1e-05
2024-01-06 01:25:48 INFO     	 * (global step 35050: loss: 0.9388191103935242, lr: 1e-05
2024-01-06 01:25:56 INFO     	 * (global step 35100: loss: 0.6614268720149994, lr: 1e-05
2024-01-06 01:26:03 INFO     	 * (global step 35150: loss: 0.5810389220714569, lr: 1e-05
2024-01-06 01:26:11 INFO     	 * (global step 35200: loss: 0.6589166224002838, lr: 1e-05
2024-01-06 01:26:19 INFO     	 * (global step 35250: loss: 0.5622979998588562, lr: 1e-05
2024-01-06 01:26:26 INFO     	 * (global step 35300: loss: 0.5104857385158539, lr: 1e-05
2024-01-06 01:26:34 INFO     	 * (global step 35350: loss: 0.5036107152700424, lr: 1e-05
2024-01-06 01:26:41 INFO     	 * (global step 35400: loss: 0.5692682564258575, lr: 1e-05
2024-01-06 01:26:49 INFO     	 * (global step 35450: loss: 0.5499550104141235, lr: 1e-05
2024-01-06 01:26:56 INFO     	 * (global step 35500: loss: 0.5453769862651825, lr: 1e-05
2024-01-06 01:27:03 INFO     	 * (global step 35550: loss: 0.5729048252105713, lr: 1e-05
2024-01-06 01:27:09 INFO     	 * (global step 35600: loss: 0.5479055643081665, lr: 1e-05
2024-01-06 01:27:15 INFO     	 * (global step 35650: loss: 0.3819040507078171, lr: 1e-05
2024-01-06 01:27:21 INFO     	 * (global step 35700: loss: 0.7697196006774902, lr: 1e-05
2024-01-06 01:27:27 INFO     	 * (global step 35750: loss: 0.6131735891103745, lr: 1e-05
2024-01-06 01:27:34 INFO     	 * (global step 35800: loss: 0.5416712462902069, lr: 1e-05
2024-01-06 01:27:41 INFO     	 * (global step 35850: loss: 0.5876713469624519, lr: 1e-05
2024-01-06 01:27:49 INFO     	 * (global step 35900: loss: 0.8137209117412567, lr: 1e-05
2024-01-06 01:27:57 INFO     	 * (global step 35950: loss: 0.9350071847438812, lr: 1e-05
2024-01-06 01:28:05 INFO     	 * (global step 36000: loss: 0.6529146432876587, lr: 1e-05
2024-01-06 01:28:13 INFO     	 * (global step 36050: loss: 0.7735645771026611, lr: 1e-05
2024-01-06 01:28:21 INFO     	 * (global step 36100: loss: 0.4734172075986862, lr: 1e-05
2024-01-06 01:28:29 INFO     	 * (global step 36150: loss: 0.6098976731300354, lr: 1e-05
2024-01-06 01:28:36 INFO     	 * (global step 36200: loss: 0.5382237285375595, lr: 1e-05
2024-01-06 01:28:43 INFO     	 * (global step 36250: loss: 0.5259564816951752, lr: 1e-05
2024-01-06 01:28:51 INFO     	 * (global step 36300: loss: 0.46382761001586914, lr: 1e-05
2024-01-06 01:28:58 INFO     	 * (global step 36350: loss: 0.440392404794693, lr: 1e-05
2024-01-06 01:29:06 INFO     	 * (global step 36400: loss: 0.4789539724588394, lr: 1e-05
2024-01-06 01:29:11 INFO     [epoch 8/15] average loss: 0.595, lr: 1e-05
2024-01-06 01:29:11 INFO     saving model related files
2024-01-06 01:29:11 INFO     saving model
2024-01-06 01:29:12 INFO     saving tokenizer
2024-01-06 01:29:12 INFO     saving optimizer
2024-01-06 01:29:14 INFO     remove old optimizer files
2024-01-06 01:29:17 INFO     	 * (global step 36450: loss: 0.36758720874786377, lr: 1e-05
2024-01-06 01:29:25 INFO     	 * (global step 36500: loss: 0.6848690807819366, lr: 1e-05
2024-01-06 01:29:32 INFO     	 * (global step 36550: loss: 0.6473265886306763, lr: 1e-05
2024-01-06 01:29:39 INFO     	 * (global step 36600: loss: 0.44844384491443634, lr: 1e-05
2024-01-06 01:29:47 INFO     	 * (global step 36650: loss: 0.4787956476211548, lr: 1e-05
2024-01-06 01:29:54 INFO     	 * (global step 36700: loss: 0.6796190440654755, lr: 1e-05
2024-01-06 01:30:02 INFO     	 * (global step 36750: loss: 0.5919866561889648, lr: 1e-05
2024-01-06 01:30:09 INFO     	 * (global step 36800: loss: 0.7797320485115051, lr: 1e-05
2024-01-06 01:30:16 INFO     	 * (global step 36850: loss: 0.5771696865558624, lr: 1e-05
2024-01-06 01:30:22 INFO     	 * (global step 36900: loss: 0.5622853189706802, lr: 1e-05
2024-01-06 01:30:28 INFO     	 * (global step 36950: loss: 0.45018891990184784, lr: 1e-05
2024-01-06 01:30:34 INFO     	 * (global step 37000: loss: 0.733774870634079, lr: 1e-05
2024-01-06 01:30:40 INFO     	 * (global step 37050: loss: 0.5176222771406174, lr: 1e-05
2024-01-06 01:30:47 INFO     	 * (global step 37100: loss: 0.630479633808136, lr: 1e-05
2024-01-06 01:30:55 INFO     	 * (global step 37150: loss: 0.9732485115528107, lr: 1e-05
2024-01-06 01:31:02 INFO     	 * (global step 37200: loss: 0.7111556529998779, lr: 1e-05
2024-01-06 01:31:10 INFO     	 * (global step 37250: loss: 0.6967403590679169, lr: 1e-05
2024-01-06 01:31:17 INFO     	 * (global step 37300: loss: 0.49590666592121124, lr: 1e-05
2024-01-06 01:31:24 INFO     	 * (global step 37350: loss: 0.4193519949913025, lr: 1e-05
2024-01-06 01:31:31 INFO     	 * (global step 37400: loss: 0.8390300273895264, lr: 1e-05
2024-01-06 01:31:39 INFO     	 * (global step 37450: loss: 0.37464699149131775, lr: 1e-05
2024-01-06 01:31:46 INFO     	 * (global step 37500: loss: 0.8481850922107697, lr: 1e-05
2024-01-06 01:31:53 INFO     	 * (global step 37550: loss: 0.43693986535072327, lr: 1e-05
2024-01-06 01:32:01 INFO     	 * (global step 37600: loss: 0.6438153684139252, lr: 1e-05
2024-01-06 01:32:08 INFO     	 * (global step 37650: loss: 0.5695314407348633, lr: 1e-05
2024-01-06 01:32:15 INFO     	 * (global step 37700: loss: 0.5834369659423828, lr: 1e-05
2024-01-06 01:32:22 INFO     	 * (global step 37750: loss: 0.6359425187110901, lr: 1e-05
2024-01-06 01:32:29 INFO     	 * (global step 37800: loss: 0.5177274197340012, lr: 1e-05
2024-01-06 01:32:37 INFO     	 * (global step 37850: loss: 0.4606594443321228, lr: 1e-05
2024-01-06 01:32:44 INFO     	 * (global step 37900: loss: 0.4558347314596176, lr: 1e-05
2024-01-06 01:32:50 INFO     	 * (global step 37950: loss: 0.3727376163005829, lr: 1e-05
2024-01-06 01:32:56 INFO     	 * (global step 38000: loss: 0.6138138025999069, lr: 1e-05
2024-01-06 01:33:02 INFO     	 * (global step 38050: loss: 0.607703685760498, lr: 1e-05
2024-01-06 01:33:08 INFO     	 * (global step 38100: loss: 0.5156509727239609, lr: 1e-05
2024-01-06 01:33:14 INFO     	 * (global step 38150: loss: 0.623546838760376, lr: 1e-05
2024-01-06 01:33:22 INFO     	 * (global step 38200: loss: 0.5382312685251236, lr: 1e-05
2024-01-06 01:33:29 INFO     	 * (global step 38250: loss: 0.6414668709039688, lr: 1e-05
2024-01-06 01:33:37 INFO     	 * (global step 38300: loss: 0.6072471886873245, lr: 1e-05
2024-01-06 01:33:45 INFO     	 * (global step 38350: loss: 0.6851631104946136, lr: 1e-05
2024-01-06 01:33:53 INFO     	 * (global step 38400: loss: 0.8181673288345337, lr: 1e-05
2024-01-06 01:34:00 INFO     	 * (global step 38450: loss: 0.5651604235172272, lr: 1e-05
2024-01-06 01:34:08 INFO     	 * (global step 38500: loss: 0.5246692597866058, lr: 1e-05
2024-01-06 01:34:15 INFO     	 * (global step 38550: loss: 0.31662996858358383, lr: 1e-05
2024-01-06 01:34:23 INFO     	 * (global step 38600: loss: 0.33163776993751526, lr: 1e-05
2024-01-06 01:34:31 INFO     	 * (global step 38650: loss: 0.8493964672088623, lr: 1e-05
2024-01-06 01:34:38 INFO     	 * (global step 38700: loss: 0.5039090812206268, lr: 1e-05
2024-01-06 01:34:46 INFO     	 * (global step 38750: loss: 0.5488129109144211, lr: 1e-05
2024-01-06 01:34:53 INFO     	 * (global step 38800: loss: 0.32686948776245117, lr: 1e-05
2024-01-06 01:35:00 INFO     	 * (global step 38850: loss: 0.5438690930604935, lr: 1e-05
2024-01-06 01:35:08 INFO     	 * (global step 38900: loss: 0.4673031270503998, lr: 1e-05
2024-01-06 01:35:16 INFO     	 * (global step 38950: loss: 0.6968846619129181, lr: 1e-05
2024-01-06 01:35:23 INFO     	 * (global step 39000: loss: 0.489759236574173, lr: 1e-05
2024-01-06 01:35:30 INFO     	 * (global step 39050: loss: 0.5941114872694016, lr: 1e-05
2024-01-06 01:35:35 INFO     	 * (global step 39100: loss: 0.564902663230896, lr: 1e-05
2024-01-06 01:35:41 INFO     	 * (global step 39150: loss: 0.8254066705703735, lr: 1e-05
2024-01-06 01:35:47 INFO     	 * (global step 39200: loss: 0.4865853488445282, lr: 1e-05
2024-01-06 01:35:53 INFO     	 * (global step 39250: loss: 0.5817640423774719, lr: 1e-05
2024-01-06 01:36:00 INFO     	 * (global step 39300: loss: 0.528836727142334, lr: 1e-05
2024-01-06 01:36:08 INFO     	 * (global step 39350: loss: 0.510515570640564, lr: 1e-05
2024-01-06 01:36:15 INFO     	 * (global step 39400: loss: 0.48744721710681915, lr: 1e-05
2024-01-06 01:36:22 INFO     	 * (global step 39450: loss: 0.6087634265422821, lr: 1e-05
2024-01-06 01:36:29 INFO     	 * (global step 39500: loss: 0.7337682843208313, lr: 1e-05
2024-01-06 01:36:37 INFO     	 * (global step 39550: loss: 0.6905856132507324, lr: 1e-05
2024-01-06 01:36:44 INFO     	 * (global step 39600: loss: 0.8342635035514832, lr: 1e-05
2024-01-06 01:36:51 INFO     	 * (global step 39650: loss: 0.5755786001682281, lr: 1e-05
2024-01-06 01:36:58 INFO     	 * (global step 39700: loss: 0.5407524406909943, lr: 1e-05
2024-01-06 01:37:06 INFO     	 * (global step 39750: loss: 0.6762323677539825, lr: 1e-05
2024-01-06 01:37:13 INFO     	 * (global step 39800: loss: 1.0766067504882812, lr: 1e-05
2024-01-06 01:37:20 INFO     	 * (global step 39850: loss: 0.8779841363430023, lr: 1e-05
2024-01-06 01:37:28 INFO     	 * (global step 39900: loss: 0.4823095351457596, lr: 1e-05
2024-01-06 01:37:35 INFO     	 * (global step 39950: loss: 0.43377888202667236, lr: 1e-05
2024-01-06 01:37:42 INFO     	 * (global step 40000: loss: 0.8642740249633789, lr: 1e-05
2024-01-06 01:37:49 INFO     	 * (global step 40050: loss: 0.5641395747661591, lr: 1e-05
2024-01-06 01:37:56 INFO     	 * (global step 40100: loss: 0.6796468496322632, lr: 1e-05
2024-01-06 01:38:02 INFO     	 * (global step 40150: loss: 0.758635014295578, lr: 1e-05
2024-01-06 01:38:08 INFO     	 * (global step 40200: loss: 0.701718270778656, lr: 1e-05
2024-01-06 01:38:14 INFO     	 * (global step 40250: loss: 0.7785289585590363, lr: 1e-05
2024-01-06 01:38:20 INFO     	 * (global step 40300: loss: 0.4355941414833069, lr: 1e-05
2024-01-06 01:38:27 INFO     	 * (global step 40350: loss: 0.7141103446483612, lr: 1e-05
2024-01-06 01:38:35 INFO     	 * (global step 40400: loss: 0.4332519620656967, lr: 1e-05
2024-01-06 01:38:43 INFO     	 * (global step 40450: loss: 0.47257138788700104, lr: 1e-05
2024-01-06 01:38:47 INFO     [epoch 9/15] average loss: 0.588, lr: 1e-05
2024-01-06 01:38:47 INFO     saving model related files
2024-01-06 01:38:47 INFO     saving model
2024-01-06 01:38:48 INFO     saving tokenizer
2024-01-06 01:38:48 INFO     saving optimizer
2024-01-06 01:38:50 INFO     remove old optimizer files
2024-01-06 01:38:50 INFO     complete training: model ckpt was saved at small_recreated_ckpt/model_oprhlh
2024-01-06 01:38:51 INFO     ## 1st RUN: Configuration 10/12 ##
2024-01-06 01:38:51 INFO     initialize model trainer
2024-01-06 01:38:51 INFO     initialize checkpoint at small_recreated_ckpt/model_vhyoja
2024-01-06 01:38:51 INFO     hyperparameters
2024-01-06 01:38:51 INFO     	 * dataset_path: lmqg/qag_squad
2024-01-06 01:38:51 INFO     	 * dataset_name: default
2024-01-06 01:38:51 INFO     	 * input_types: ['paragraph']
2024-01-06 01:38:51 INFO     	 * output_types: ['questions_answers']
2024-01-06 01:38:51 INFO     	 * prefix_types: ['qag']
2024-01-06 01:38:51 INFO     	 * model: t5-small
2024-01-06 01:38:51 INFO     	 * max_length: 512
2024-01-06 01:38:51 INFO     	 * max_length_output: 256
2024-01-06 01:38:51 INFO     	 * epoch: 15
2024-01-06 01:38:51 INFO     	 * batch: 2
2024-01-06 01:38:51 INFO     	 * lr: 1e-05
2024-01-06 01:38:51 INFO     	 * fp16: False
2024-01-06 01:38:51 INFO     	 * random_seed: 1
2024-01-06 01:38:51 INFO     	 * gradient_accumulation_steps: 4
2024-01-06 01:38:51 INFO     	 * label_smoothing: 0.0
2024-01-06 01:38:51 INFO     initialize checkpoint with t5-small
2024-01-06 01:38:58 INFO     use spaCy answer extraction model: positionrank
2024-01-06 01:38:59 INFO     Model `t5-small`
2024-01-06 01:38:59 INFO     	 * Num of GPU in use: 1
2024-01-06 01:38:59 INFO     	 * Prefix: True
2024-01-06 01:38:59 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 01:38:59 INFO     dataset preprocessing
2024-01-06 01:39:00 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featurelmqg/qag_squad/t5-small.512.256.paragraph.questions_answers.train.qag.pkl
2024-01-06 01:39:04 INFO     start model training
2024-01-06 01:39:18 INFO     	 * (global step 50: loss: 4.917056858539581, lr: 1e-05
2024-01-06 01:39:32 INFO     	 * (global step 100: loss: 2.7308374643325806, lr: 1e-05
2024-01-06 01:39:47 INFO     	 * (global step 150: loss: 2.106336772441864, lr: 1e-05
2024-01-06 01:40:01 INFO     	 * (global step 200: loss: 1.856107771396637, lr: 1e-05
2024-01-06 01:40:15 INFO     	 * (global step 250: loss: 1.7061720788478851, lr: 1e-05
2024-01-06 01:40:30 INFO     	 * (global step 300: loss: 1.8264073729515076, lr: 1e-05
2024-01-06 01:40:44 INFO     	 * (global step 350: loss: 1.6261934638023376, lr: 1e-05
2024-01-06 01:40:56 INFO     	 * (global step 400: loss: 1.7227388322353363, lr: 1e-05
2024-01-06 01:41:07 INFO     	 * (global step 450: loss: 1.404605358839035, lr: 1e-05
2024-01-06 01:41:20 INFO     	 * (global step 500: loss: 1.3987509608268738, lr: 1e-05
2024-01-06 01:41:34 INFO     	 * (global step 550: loss: 1.3692759275436401, lr: 1e-05
2024-01-06 01:41:48 INFO     	 * (global step 600: loss: 1.4955163300037384, lr: 1e-05
2024-01-06 01:42:03 INFO     	 * (global step 650: loss: 1.2817923426628113, lr: 1e-05
2024-01-06 01:42:16 INFO     	 * (global step 700: loss: 1.0510715544223785, lr: 1e-05
2024-01-06 01:42:31 INFO     	 * (global step 750: loss: 1.1471454203128815, lr: 1e-05
2024-01-06 01:42:45 INFO     	 * (global step 800: loss: 0.8920959532260895, lr: 1e-05
2024-01-06 01:42:59 INFO     	 * (global step 850: loss: 1.0804439783096313, lr: 1e-05
2024-01-06 01:43:13 INFO     	 * (global step 900: loss: 1.141614705324173, lr: 1e-05
2024-01-06 01:43:27 INFO     	 * (global step 950: loss: 0.9120609313249588, lr: 1e-05
2024-01-06 01:43:38 INFO     	 * (global step 1000: loss: 1.5201641023159027, lr: 1e-05
2024-01-06 01:43:49 INFO     	 * (global step 1050: loss: 0.933428019285202, lr: 1e-05
2024-01-06 01:44:02 INFO     	 * (global step 1100: loss: 1.0352755784988403, lr: 1e-05
2024-01-06 01:44:16 INFO     	 * (global step 1150: loss: 1.443696141242981, lr: 1e-05
2024-01-06 01:44:30 INFO     	 * (global step 1200: loss: 0.8777892887592316, lr: 1e-05
2024-01-06 01:44:44 INFO     	 * (global step 1250: loss: 0.8706676214933395, lr: 1e-05
2024-01-06 01:44:57 INFO     	 * (global step 1300: loss: 0.7320647090673447, lr: 1e-05
2024-01-06 01:45:11 INFO     	 * (global step 1350: loss: 0.9739631712436676, lr: 1e-05
2024-01-06 01:45:25 INFO     	 * (global step 1400: loss: 0.9850045442581177, lr: 1e-05
2024-01-06 01:45:39 INFO     	 * (global step 1450: loss: 1.0478754043579102, lr: 1e-05
2024-01-06 01:45:52 INFO     	 * (global step 1500: loss: 0.8902233242988586, lr: 1e-05
2024-01-06 01:46:05 INFO     	 * (global step 1550: loss: 0.938806802034378, lr: 1e-05
2024-01-06 01:46:16 INFO     	 * (global step 1600: loss: 1.000888854265213, lr: 1e-05
2024-01-06 01:46:28 INFO     	 * (global step 1650: loss: 0.8252349942922592, lr: 1e-05
2024-01-06 01:46:42 INFO     	 * (global step 1700: loss: 0.9014475047588348, lr: 1e-05
2024-01-06 01:46:57 INFO     	 * (global step 1750: loss: 0.7600937187671661, lr: 1e-05
2024-01-06 01:47:11 INFO     	 * (global step 1800: loss: 0.775558814406395, lr: 1e-05
2024-01-06 01:47:25 INFO     	 * (global step 1850: loss: 0.5890153869986534, lr: 1e-05
2024-01-06 01:47:39 INFO     	 * (global step 1900: loss: 0.6798195242881775, lr: 1e-05
2024-01-06 01:47:53 INFO     	 * (global step 1950: loss: 0.9999025613069534, lr: 1e-05
2024-01-06 01:48:07 INFO     	 * (global step 2000: loss: 1.114661067724228, lr: 1e-05
2024-01-06 01:48:14 INFO     [epoch 0/15] average loss: 1.328, lr: 1e-05
2024-01-06 01:48:14 INFO     saving model related files
2024-01-06 01:48:14 INFO     saving model
2024-01-06 01:48:15 INFO     saving tokenizer
2024-01-06 01:48:15 INFO     saving optimizer
2024-01-06 01:48:16 INFO     remove old optimizer files
2024-01-06 01:48:24 INFO     	 * (global step 2050: loss: 0.6726418286561966, lr: 1e-05
2024-01-06 01:48:37 INFO     	 * (global step 2100: loss: 0.7638778537511826, lr: 1e-05
2024-01-06 01:48:48 INFO     	 * (global step 2150: loss: 0.9632978439331055, lr: 1e-05
2024-01-06 01:49:00 INFO     	 * (global step 2200: loss: 0.681011512875557, lr: 1e-05
2024-01-06 01:49:13 INFO     	 * (global step 2250: loss: 0.7123786360025406, lr: 1e-05
2024-01-06 01:49:28 INFO     	 * (global step 2300: loss: 0.8452218472957611, lr: 1e-05
2024-01-06 01:49:42 INFO     	 * (global step 2350: loss: 0.7948266267776489, lr: 1e-05
2024-01-06 01:49:55 INFO     	 * (global step 2400: loss: 0.7758551985025406, lr: 1e-05
2024-01-06 01:50:09 INFO     	 * (global step 2450: loss: 0.7368334829807281, lr: 1e-05
2024-01-06 01:50:23 INFO     	 * (global step 2500: loss: 0.8734922260046005, lr: 1e-05
2024-01-06 01:50:37 INFO     	 * (global step 2550: loss: 0.699472039937973, lr: 1e-05
2024-01-06 01:50:50 INFO     	 * (global step 2600: loss: 0.7330448180437088, lr: 1e-05
2024-01-06 01:51:04 INFO     	 * (global step 2650: loss: 0.8527432680130005, lr: 1e-05
2024-01-06 01:51:16 INFO     	 * (global step 2700: loss: 0.6770761460065842, lr: 1e-05
2024-01-06 01:51:27 INFO     	 * (global step 2750: loss: 0.7258691340684891, lr: 1e-05
2024-01-06 01:51:39 INFO     	 * (global step 2800: loss: 0.7496861219406128, lr: 1e-05
2024-01-06 01:51:53 INFO     	 * (global step 2850: loss: 0.85659608989954, lr: 1e-05
2024-01-06 01:52:07 INFO     	 * (global step 2900: loss: 0.9362447261810303, lr: 1e-05
2024-01-06 01:52:20 INFO     	 * (global step 2950: loss: 0.8823091983795166, lr: 1e-05
2024-01-06 01:52:34 INFO     	 * (global step 3000: loss: 0.7433388531208038, lr: 1e-05
2024-01-06 01:52:48 INFO     	 * (global step 3050: loss: 0.6827368438243866, lr: 1e-05
2024-01-06 01:53:01 INFO     	 * (global step 3100: loss: 0.67319405823946, lr: 1e-05
2024-01-06 01:53:15 INFO     	 * (global step 3150: loss: 0.8194966018199921, lr: 1e-05
2024-01-06 01:53:28 INFO     	 * (global step 3200: loss: 0.6534003838896751, lr: 1e-05
2024-01-06 01:53:42 INFO     	 * (global step 3250: loss: 0.8691475093364716, lr: 1e-05
2024-01-06 01:53:53 INFO     	 * (global step 3300: loss: 0.6192482113838196, lr: 1e-05
2024-01-06 01:54:04 INFO     	 * (global step 3350: loss: 0.759856715798378, lr: 1e-05
2024-01-06 01:54:18 INFO     	 * (global step 3400: loss: 0.8464123904705048, lr: 1e-05
2024-01-06 01:54:31 INFO     	 * (global step 3450: loss: 0.7899551093578339, lr: 1e-05
2024-01-06 01:54:45 INFO     	 * (global step 3500: loss: 0.74592025578022, lr: 1e-05
2024-01-06 01:54:59 INFO     	 * (global step 3550: loss: 0.6957978010177612, lr: 1e-05
2024-01-06 01:55:12 INFO     	 * (global step 3600: loss: 0.7696995735168457, lr: 1e-05
2024-01-06 01:55:26 INFO     	 * (global step 3650: loss: 0.8989294022321701, lr: 1e-05
2024-01-06 01:55:40 INFO     	 * (global step 3700: loss: 1.0762335062026978, lr: 1e-05
2024-01-06 01:55:54 INFO     	 * (global step 3750: loss: 0.7246150225400925, lr: 1e-05
2024-01-06 01:56:08 INFO     	 * (global step 3800: loss: 0.6502870097756386, lr: 1e-05
2024-01-06 01:56:20 INFO     	 * (global step 3850: loss: 1.0039863735437393, lr: 1e-05
2024-01-06 01:56:31 INFO     	 * (global step 3900: loss: 0.7163811177015305, lr: 1e-05
2024-01-06 01:56:43 INFO     	 * (global step 3950: loss: 0.5419599786400795, lr: 1e-05
2024-01-06 01:56:57 INFO     	 * (global step 4000: loss: 0.6173883900046349, lr: 1e-05
2024-01-06 01:57:10 INFO     [epoch 1/15] average loss: 0.775, lr: 1e-05
2024-01-06 01:57:10 INFO     saving model related files
2024-01-06 01:57:10 INFO     saving model
2024-01-06 01:57:11 INFO     saving tokenizer
2024-01-06 01:57:11 INFO     saving optimizer
2024-01-06 01:57:12 INFO     remove old optimizer files
2024-01-06 01:57:13 INFO     	 * (global step 4050: loss: 0.5648557469248772, lr: 1e-05
2024-01-06 01:57:27 INFO     	 * (global step 4100: loss: 0.6937594786286354, lr: 1e-05
2024-01-06 01:57:42 INFO     	 * (global step 4150: loss: 0.7252419143915176, lr: 1e-05
2024-01-06 01:57:56 INFO     	 * (global step 4200: loss: 0.9115630015730858, lr: 1e-05
2024-01-06 01:58:10 INFO     	 * (global step 4250: loss: 0.7358650714159012, lr: 1e-05
2024-01-06 01:58:24 INFO     	 * (global step 4300: loss: 0.5365257114171982, lr: 1e-05
2024-01-06 01:58:38 INFO     	 * (global step 4350: loss: 0.7823908627033234, lr: 1e-05
2024-01-06 01:58:51 INFO     	 * (global step 4400: loss: 0.6044096797704697, lr: 1e-05
2024-01-06 01:59:03 INFO     	 * (global step 4450: loss: 0.6188656911253929, lr: 1e-05
2024-01-06 01:59:14 INFO     	 * (global step 4500: loss: 0.6962577551603317, lr: 1e-05
2024-01-06 01:59:25 INFO     	 * (global step 4550: loss: 0.6810564994812012, lr: 1e-05
2024-01-06 01:59:37 INFO     	 * (global step 4600: loss: 0.6351006701588631, lr: 1e-05
2024-01-06 01:59:49 INFO     	 * (global step 4650: loss: 0.9325925558805466, lr: 1e-05
2024-01-06 02:00:00 INFO     	 * (global step 4700: loss: 0.7275092005729675, lr: 1e-05
2024-01-06 02:00:12 INFO     	 * (global step 4750: loss: 0.6485116258263588, lr: 1e-05
2024-01-06 02:00:23 INFO     	 * (global step 4800: loss: 0.7171027138829231, lr: 1e-05
2024-01-06 02:00:35 INFO     	 * (global step 4850: loss: 0.7244978919625282, lr: 1e-05
2024-01-06 02:00:47 INFO     	 * (global step 4900: loss: 0.6252811402082443, lr: 1e-05
2024-01-06 02:00:59 INFO     	 * (global step 4950: loss: 0.5599473789334297, lr: 1e-05
2024-01-06 02:01:10 INFO     	 * (global step 5000: loss: 0.6457046270370483, lr: 1e-05
2024-01-06 02:01:22 INFO     	 * (global step 5050: loss: 0.882525771856308, lr: 1e-05
2024-01-06 02:01:34 INFO     	 * (global step 5100: loss: 0.6508330851793289, lr: 1e-05
2024-01-06 02:01:46 INFO     	 * (global step 5150: loss: 0.7427068054676056, lr: 1e-05
2024-01-06 02:01:58 INFO     	 * (global step 5200: loss: 0.7183885276317596, lr: 1e-05
2024-01-06 02:02:09 INFO     	 * (global step 5250: loss: 0.6594568192958832, lr: 1e-05
2024-01-06 02:02:21 INFO     	 * (global step 5300: loss: 0.6329094246029854, lr: 1e-05
2024-01-06 02:02:33 INFO     	 * (global step 5350: loss: 0.7458727210760117, lr: 1e-05
2024-01-06 02:02:45 INFO     	 * (global step 5400: loss: 0.7246618717908859, lr: 1e-05
2024-01-06 02:02:57 INFO     	 * (global step 5450: loss: 0.6491982638835907, lr: 1e-05
2024-01-06 02:03:09 INFO     	 * (global step 5500: loss: 0.6265072748064995, lr: 1e-05
2024-01-06 02:03:20 INFO     	 * (global step 5550: loss: 0.7363491058349609, lr: 1e-05
2024-01-06 02:03:32 INFO     	 * (global step 5600: loss: 0.7672411650419235, lr: 1e-05
2024-01-06 02:03:44 INFO     	 * (global step 5650: loss: 0.6386456862092018, lr: 1e-05
2024-01-06 02:03:56 INFO     	 * (global step 5700: loss: 0.747886523604393, lr: 1e-05
2024-01-06 02:04:08 INFO     	 * (global step 5750: loss: 0.6988334208726883, lr: 1e-05
2024-01-06 02:04:19 INFO     	 * (global step 5800: loss: 0.7111552804708481, lr: 1e-05
2024-01-06 02:04:31 INFO     	 * (global step 5850: loss: 0.7872537672519684, lr: 1e-05
2024-01-06 02:04:43 INFO     	 * (global step 5900: loss: 0.7127583622932434, lr: 1e-05
2024-01-06 02:04:54 INFO     	 * (global step 5950: loss: 0.631389670073986, lr: 1e-05
2024-01-06 02:05:06 INFO     	 * (global step 6000: loss: 0.6011367961764336, lr: 1e-05
2024-01-06 02:05:18 INFO     	 * (global step 6050: loss: 0.8623236790299416, lr: 1e-05
2024-01-06 02:05:23 INFO     [epoch 2/15] average loss: 0.715, lr: 1e-05
2024-01-06 02:05:23 INFO     saving model related files
2024-01-06 02:05:23 INFO     saving model
2024-01-06 02:05:24 INFO     saving tokenizer
2024-01-06 02:05:24 INFO     saving optimizer
2024-01-06 02:05:25 INFO     remove old optimizer files
2024-01-06 02:05:31 INFO     	 * (global step 6100: loss: 0.735960528254509, lr: 1e-05
2024-01-06 02:05:43 INFO     	 * (global step 6150: loss: 0.7127228081226349, lr: 1e-05
2024-01-06 02:05:55 INFO     	 * (global step 6200: loss: 0.7244098037481308, lr: 1e-05
2024-01-06 02:06:06 INFO     	 * (global step 6250: loss: 0.7647132724523544, lr: 1e-05
2024-01-06 02:06:18 INFO     	 * (global step 6300: loss: 0.5888963341712952, lr: 1e-05
2024-01-06 02:06:29 INFO     	 * (global step 6350: loss: 0.9317487180233002, lr: 1e-05
2024-01-06 02:06:41 INFO     	 * (global step 6400: loss: 0.735124409198761, lr: 1e-05
2024-01-06 02:06:52 INFO     	 * (global step 6450: loss: 0.8297348767518997, lr: 1e-05
2024-01-06 02:07:04 INFO     	 * (global step 6500: loss: 0.7223718464374542, lr: 1e-05
2024-01-06 02:07:15 INFO     	 * (global step 6550: loss: 0.5765669718384743, lr: 1e-05
2024-01-06 02:07:27 INFO     	 * (global step 6600: loss: 0.5992936566472054, lr: 1e-05
2024-01-06 02:07:38 INFO     	 * (global step 6650: loss: 0.5552413165569305, lr: 1e-05
2024-01-06 02:07:50 INFO     	 * (global step 6700: loss: 0.63963283598423, lr: 1e-05
2024-01-06 02:08:01 INFO     	 * (global step 6750: loss: 0.6551429107785225, lr: 1e-05
2024-01-06 02:08:13 INFO     	 * (global step 6800: loss: 0.7615717574954033, lr: 1e-05
2024-01-06 02:08:24 INFO     	 * (global step 6850: loss: 0.6369536891579628, lr: 1e-05
2024-01-06 02:08:36 INFO     	 * (global step 6900: loss: 0.6057853251695633, lr: 1e-05
2024-01-06 02:08:47 INFO     	 * (global step 6950: loss: 0.6160698533058167, lr: 1e-05
2024-01-06 02:08:59 INFO     	 * (global step 7000: loss: 0.50897166877985, lr: 1e-05
2024-01-06 02:09:10 INFO     	 * (global step 7050: loss: 0.8022587150335312, lr: 1e-05
2024-01-06 02:09:22 INFO     	 * (global step 7100: loss: 0.7024240791797638, lr: 1e-05
2024-01-06 02:09:33 INFO     	 * (global step 7150: loss: 0.6125230714678764, lr: 1e-05
2024-01-06 02:09:45 INFO     	 * (global step 7200: loss: 0.6980392038822174, lr: 1e-05
2024-01-06 02:09:56 INFO     	 * (global step 7250: loss: 0.7074588388204575, lr: 1e-05
2024-01-06 02:10:08 INFO     	 * (global step 7300: loss: 0.5754370093345642, lr: 1e-05
2024-01-06 02:10:19 INFO     	 * (global step 7350: loss: 0.6828550845384598, lr: 1e-05
2024-01-06 02:10:31 INFO     	 * (global step 7400: loss: 0.6110261380672455, lr: 1e-05
2024-01-06 02:10:43 INFO     	 * (global step 7450: loss: 0.6022157296538353, lr: 1e-05
2024-01-06 02:10:54 INFO     	 * (global step 7500: loss: 0.7133044898509979, lr: 1e-05
2024-01-06 02:11:06 INFO     	 * (global step 7550: loss: 0.8386452198028564, lr: 1e-05
2024-01-06 02:11:17 INFO     	 * (global step 7600: loss: 0.5993479043245316, lr: 1e-05
2024-01-06 02:11:29 INFO     	 * (global step 7650: loss: 0.6471939533948898, lr: 1e-05
2024-01-06 02:11:41 INFO     	 * (global step 7700: loss: 0.5592780560255051, lr: 1e-05
2024-01-06 02:11:52 INFO     	 * (global step 7750: loss: 0.5581077411770821, lr: 1e-05
2024-01-06 02:12:04 INFO     	 * (global step 7800: loss: 0.6375931948423386, lr: 1e-05
2024-01-06 02:12:16 INFO     	 * (global step 7850: loss: 0.713587298989296, lr: 1e-05
2024-01-06 02:12:27 INFO     	 * (global step 7900: loss: 0.5942520648241043, lr: 1e-05
2024-01-06 02:12:39 INFO     	 * (global step 7950: loss: 0.7596396654844284, lr: 1e-05
2024-01-06 02:12:50 INFO     	 * (global step 8000: loss: 0.6840367615222931, lr: 1e-05
2024-01-06 02:13:02 INFO     	 * (global step 8050: loss: 0.7688783407211304, lr: 1e-05
2024-01-06 02:13:13 INFO     [epoch 3/15] average loss: 0.683, lr: 1e-05
2024-01-06 02:13:13 INFO     saving model related files
2024-01-06 02:13:13 INFO     saving model
2024-01-06 02:13:14 INFO     saving tokenizer
2024-01-06 02:13:14 INFO     saving optimizer
2024-01-06 02:13:15 INFO     remove old optimizer files
2024-01-06 02:13:16 INFO     	 * (global step 8100: loss: 0.7291631400585175, lr: 1e-05
2024-01-06 02:13:28 INFO     	 * (global step 8150: loss: 0.7110144346952438, lr: 1e-05
2024-01-06 02:13:39 INFO     	 * (global step 8200: loss: 0.5088981986045837, lr: 1e-05
2024-01-06 02:13:51 INFO     	 * (global step 8250: loss: 0.6202326491475105, lr: 1e-05
2024-01-06 02:14:03 INFO     	 * (global step 8300: loss: 0.5925181955099106, lr: 1e-05
2024-01-06 02:14:14 INFO     	 * (global step 8350: loss: 0.8191884458065033, lr: 1e-05
2024-01-06 02:14:26 INFO     	 * (global step 8400: loss: 0.7024769186973572, lr: 1e-05
2024-01-06 02:14:38 INFO     	 * (global step 8450: loss: 0.6171766519546509, lr: 1e-05
2024-01-06 02:14:50 INFO     	 * (global step 8500: loss: 0.7453739494085312, lr: 1e-05
2024-01-06 02:15:01 INFO     	 * (global step 8550: loss: 0.5140477269887924, lr: 1e-05
2024-01-06 02:15:13 INFO     	 * (global step 8600: loss: 0.481462724506855, lr: 1e-05
2024-01-06 02:15:25 INFO     	 * (global step 8650: loss: 0.6372040435671806, lr: 1e-05
2024-01-06 02:15:36 INFO     	 * (global step 8700: loss: 0.8323901146650314, lr: 1e-05
2024-01-06 02:15:48 INFO     	 * (global step 8750: loss: 0.5940345898270607, lr: 1e-05
2024-01-06 02:16:00 INFO     	 * (global step 8800: loss: 0.7248432040214539, lr: 1e-05
2024-01-06 02:16:11 INFO     	 * (global step 8850: loss: 0.7205767929553986, lr: 1e-05
2024-01-06 02:16:23 INFO     	 * (global step 8900: loss: 0.6436376571655273, lr: 1e-05
2024-01-06 02:16:34 INFO     	 * (global step 8950: loss: 0.7234655022621155, lr: 1e-05
2024-01-06 02:16:46 INFO     	 * (global step 9000: loss: 0.6368720382452011, lr: 1e-05
2024-01-06 02:16:57 INFO     	 * (global step 9050: loss: 0.6172456964850426, lr: 1e-05
2024-01-06 02:17:09 INFO     	 * (global step 9100: loss: 0.5773298293352127, lr: 1e-05
2024-01-06 02:17:21 INFO     	 * (global step 9150: loss: 0.3987183906137943, lr: 1e-05
2024-01-06 02:17:32 INFO     	 * (global step 9200: loss: 0.8227074891328812, lr: 1e-05
2024-01-06 02:17:44 INFO     	 * (global step 9250: loss: 0.5394781827926636, lr: 1e-05
2024-01-06 02:17:55 INFO     	 * (global step 9300: loss: 0.6383713185787201, lr: 1e-05
2024-01-06 02:18:07 INFO     	 * (global step 9350: loss: 0.6643656641244888, lr: 1e-05
2024-01-06 02:18:19 INFO     	 * (global step 9400: loss: 0.8370367586612701, lr: 1e-05
2024-01-06 02:18:30 INFO     	 * (global step 9450: loss: 0.662355899810791, lr: 1e-05
2024-01-06 02:18:42 INFO     	 * (global step 9500: loss: 0.6243843734264374, lr: 1e-05
2024-01-06 02:18:54 INFO     	 * (global step 9550: loss: 0.6363013088703156, lr: 1e-05
2024-01-06 02:19:05 INFO     	 * (global step 9600: loss: 0.534560352563858, lr: 1e-05
2024-01-06 02:19:17 INFO     	 * (global step 9650: loss: 0.5438896715641022, lr: 1e-05
2024-01-06 02:19:28 INFO     	 * (global step 9700: loss: 0.666669674217701, lr: 1e-05
2024-01-06 02:19:40 INFO     	 * (global step 9750: loss: 0.5776769369840622, lr: 1e-05
2024-01-06 02:19:51 INFO     	 * (global step 9800: loss: 0.6674968749284744, lr: 1e-05
2024-01-06 02:20:03 INFO     	 * (global step 9850: loss: 0.48477648198604584, lr: 1e-05
2024-01-06 02:20:15 INFO     	 * (global step 9900: loss: 0.6117071360349655, lr: 1e-05
2024-01-06 02:20:26 INFO     	 * (global step 9950: loss: 0.5901485085487366, lr: 1e-05
2024-01-06 02:20:38 INFO     	 * (global step 10000: loss: 0.5915016159415245, lr: 1e-05
2024-01-06 02:20:50 INFO     	 * (global step 10050: loss: 0.6286198496818542, lr: 1e-05
2024-01-06 02:21:02 INFO     	 * (global step 10100: loss: 0.5855629593133926, lr: 1e-05
2024-01-06 02:21:06 INFO     [epoch 4/15] average loss: 0.662, lr: 1e-05
2024-01-06 02:21:06 INFO     saving model related files
2024-01-06 02:21:06 INFO     saving model
2024-01-06 02:21:07 INFO     saving tokenizer
2024-01-06 02:21:07 INFO     saving optimizer
2024-01-06 02:21:08 INFO     remove old optimizer files
2024-01-06 02:21:15 INFO     	 * (global step 10150: loss: 0.5365713089704514, lr: 1e-05
2024-01-06 02:21:27 INFO     	 * (global step 10200: loss: 0.5981417670845985, lr: 1e-05
2024-01-06 02:21:39 INFO     	 * (global step 10250: loss: 0.5027329921722412, lr: 1e-05
2024-01-06 02:21:50 INFO     	 * (global step 10300: loss: 0.5603697448968887, lr: 1e-05
2024-01-06 02:22:02 INFO     	 * (global step 10350: loss: 0.7645251601934433, lr: 1e-05
2024-01-06 02:22:14 INFO     	 * (global step 10400: loss: 0.6583439782261848, lr: 1e-05
2024-01-06 02:22:26 INFO     	 * (global step 10450: loss: 0.5552060380578041, lr: 1e-05
2024-01-06 02:22:37 INFO     	 * (global step 10500: loss: 0.6247033551335335, lr: 1e-05
2024-01-06 02:22:49 INFO     	 * (global step 10550: loss: 0.598400354385376, lr: 1e-05
2024-01-06 02:23:01 INFO     	 * (global step 10600: loss: 0.6415719464421272, lr: 1e-05
2024-01-06 02:23:13 INFO     	 * (global step 10650: loss: 0.7197147905826569, lr: 1e-05
2024-01-06 02:23:24 INFO     	 * (global step 10700: loss: 0.616056889295578, lr: 1e-05
2024-01-06 02:23:36 INFO     	 * (global step 10750: loss: 0.47923510521650314, lr: 1e-05
2024-01-06 02:23:48 INFO     	 * (global step 10800: loss: 0.6526646018028259, lr: 1e-05
2024-01-06 02:23:59 INFO     	 * (global step 10850: loss: 0.6084596514701843, lr: 1e-05
2024-01-06 02:24:11 INFO     	 * (global step 10900: loss: 0.7620401084423065, lr: 1e-05
2024-01-06 02:24:23 INFO     	 * (global step 10950: loss: 0.6686878427863121, lr: 1e-05
2024-01-06 02:24:34 INFO     	 * (global step 11000: loss: 0.5575844347476959, lr: 1e-05
2024-01-06 02:24:46 INFO     	 * (global step 11050: loss: 0.608811043202877, lr: 1e-05
2024-01-06 02:24:58 INFO     	 * (global step 11100: loss: 0.5953408107161522, lr: 1e-05
2024-01-06 02:25:10 INFO     	 * (global step 11150: loss: 0.593337669968605, lr: 1e-05
2024-01-06 02:25:21 INFO     	 * (global step 11200: loss: 0.5952142030000687, lr: 1e-05
2024-01-06 02:25:33 INFO     	 * (global step 11250: loss: 0.7801658809185028, lr: 1e-05
2024-01-06 02:25:45 INFO     	 * (global step 11300: loss: 0.618744470179081, lr: 1e-05
2024-01-06 02:25:56 INFO     	 * (global step 11350: loss: 0.5740041732788086, lr: 1e-05
2024-01-06 02:26:07 INFO     	 * (global step 11400: loss: 0.6156342700123787, lr: 1e-05
2024-01-06 02:26:18 INFO     	 * (global step 11450: loss: 0.4763197898864746, lr: 1e-05
2024-01-06 02:26:29 INFO     	 * (global step 11500: loss: 0.6013584434986115, lr: 1e-05
2024-01-06 02:26:42 INFO     	 * (global step 11550: loss: 0.690862625837326, lr: 1e-05
2024-01-06 02:26:54 INFO     	 * (global step 11600: loss: 0.5763927325606346, lr: 1e-05
2024-01-06 02:27:07 INFO     	 * (global step 11650: loss: 0.3916347101330757, lr: 1e-05
2024-01-06 02:27:19 INFO     	 * (global step 11700: loss: 0.5848798528313637, lr: 1e-05
2024-01-06 02:27:31 INFO     	 * (global step 11750: loss: 0.6985104382038116, lr: 1e-05
2024-01-06 02:27:43 INFO     	 * (global step 11800: loss: 0.6747060120105743, lr: 1e-05
2024-01-06 02:27:56 INFO     	 * (global step 11850: loss: 0.6945604830980301, lr: 1e-05
2024-01-06 02:28:09 INFO     	 * (global step 11900: loss: 0.5199698507785797, lr: 1e-05
2024-01-06 02:28:22 INFO     	 * (global step 11950: loss: 0.5443515628576279, lr: 1e-05
2024-01-06 02:28:35 INFO     	 * (global step 12000: loss: 0.5287073254585266, lr: 1e-05
2024-01-06 02:28:48 INFO     	 * (global step 12050: loss: 0.5203713178634644, lr: 1e-05
2024-01-06 02:29:00 INFO     	 * (global step 12100: loss: 0.5918023511767387, lr: 1e-05
2024-01-06 02:29:11 INFO     [epoch 5/15] average loss: 0.648, lr: 1e-05
2024-01-06 02:29:11 INFO     saving model related files
2024-01-06 02:29:11 INFO     saving model
2024-01-06 02:29:12 INFO     saving tokenizer
2024-01-06 02:29:12 INFO     saving optimizer
2024-01-06 02:29:14 INFO     remove old optimizer files
2024-01-06 02:29:15 INFO     	 * (global step 12150: loss: 0.6178244575858116, lr: 1e-05
2024-01-06 02:29:28 INFO     	 * (global step 12200: loss: 0.6264094933867455, lr: 1e-05
2024-01-06 02:29:41 INFO     	 * (global step 12250: loss: 0.7758216261863708, lr: 1e-05
2024-01-06 02:29:53 INFO     	 * (global step 12300: loss: 0.7248823642730713, lr: 1e-05
2024-01-06 02:30:06 INFO     	 * (global step 12350: loss: 0.5544671639800072, lr: 1e-05
2024-01-06 02:30:19 INFO     	 * (global step 12400: loss: 0.6585743650794029, lr: 1e-05
2024-01-06 02:30:32 INFO     	 * (global step 12450: loss: 0.7839664965867996, lr: 1e-05
2024-01-06 02:30:45 INFO     	 * (global step 12500: loss: 0.5109015926718712, lr: 1e-05
2024-01-06 02:30:57 INFO     	 * (global step 12550: loss: 0.613072507083416, lr: 1e-05
2024-01-06 02:31:08 INFO     	 * (global step 12600: loss: 0.7702076882123947, lr: 1e-05
2024-01-06 02:31:20 INFO     	 * (global step 12650: loss: 0.7338564917445183, lr: 1e-05
2024-01-06 02:31:34 INFO     	 * (global step 12700: loss: 0.6640485674142838, lr: 1e-05
2024-01-06 02:31:50 INFO     	 * (global step 12750: loss: 0.644181452691555, lr: 1e-05
2024-01-06 02:32:05 INFO     	 * (global step 12800: loss: 0.954168513417244, lr: 1e-05
2024-01-06 02:32:20 INFO     	 * (global step 12850: loss: 0.6772991567850113, lr: 1e-05
2024-01-06 02:32:34 INFO     	 * (global step 12900: loss: 0.6498925313353539, lr: 1e-05
2024-01-06 02:32:49 INFO     	 * (global step 12950: loss: 0.6354267299175262, lr: 1e-05
2024-01-06 02:33:04 INFO     	 * (global step 13000: loss: 0.5521663650870323, lr: 1e-05
2024-01-06 02:33:19 INFO     	 * (global step 13050: loss: 0.6302634179592133, lr: 1e-05
2024-01-06 02:33:34 INFO     	 * (global step 13100: loss: 0.6571847349405289, lr: 1e-05
2024-01-06 02:33:49 INFO     	 * (global step 13150: loss: 0.703760102391243, lr: 1e-05
2024-01-06 02:34:04 INFO     	 * (global step 13200: loss: 0.5861966535449028, lr: 1e-05
2024-01-06 02:34:16 INFO     	 * (global step 13250: loss: 0.7165688052773476, lr: 1e-05
2024-01-06 02:34:27 INFO     	 * (global step 13300: loss: 0.6700188592076302, lr: 1e-05
2024-01-06 02:34:41 INFO     	 * (global step 13350: loss: 0.7928519546985626, lr: 1e-05
2024-01-06 02:34:55 INFO     	 * (global step 13400: loss: 0.7039620876312256, lr: 1e-05
2024-01-06 02:35:11 INFO     	 * (global step 13450: loss: 0.712130531668663, lr: 1e-05
2024-01-06 02:35:26 INFO     	 * (global step 13500: loss: 0.6716048866510391, lr: 1e-05
2024-01-06 02:35:41 INFO     	 * (global step 13550: loss: 0.5347146540880203, lr: 1e-05
2024-01-06 02:35:55 INFO     	 * (global step 13600: loss: 0.49653153866529465, lr: 1e-05
2024-01-06 02:36:11 INFO     	 * (global step 13650: loss: 0.5840642526745796, lr: 1e-05
2024-01-06 02:36:25 INFO     	 * (global step 13700: loss: 0.7237854897975922, lr: 1e-05
2024-01-06 02:36:40 INFO     	 * (global step 13750: loss: 0.6451178416609764, lr: 1e-05
2024-01-06 02:36:55 INFO     	 * (global step 13800: loss: 0.4966762810945511, lr: 1e-05
2024-01-06 02:37:10 INFO     	 * (global step 13850: loss: 0.6845104694366455, lr: 1e-05
2024-01-06 02:37:24 INFO     	 * (global step 13900: loss: 0.5661419481039047, lr: 1e-05
2024-01-06 02:37:39 INFO     	 * (global step 13950: loss: 0.6092072576284409, lr: 1e-05
2024-01-06 02:37:51 INFO     	 * (global step 14000: loss: 0.6665326058864594, lr: 1e-05
2024-01-06 02:38:02 INFO     	 * (global step 14050: loss: 0.7417503744363785, lr: 1e-05
2024-01-06 02:38:16 INFO     	 * (global step 14100: loss: 0.6379244327545166, lr: 1e-05
2024-01-06 02:38:31 INFO     	 * (global step 14150: loss: 0.6409206166863441, lr: 1e-05
2024-01-06 02:38:36 INFO     [epoch 6/15] average loss: 0.636, lr: 1e-05
2024-01-06 02:38:36 INFO     saving model related files
2024-01-06 02:38:36 INFO     saving model
2024-01-06 02:38:37 INFO     saving tokenizer
2024-01-06 02:38:37 INFO     saving optimizer
2024-01-06 02:38:39 INFO     remove old optimizer files
2024-01-06 02:38:49 INFO     	 * (global step 14200: loss: 0.6298418045043945, lr: 1e-05
2024-01-06 02:39:03 INFO     	 * (global step 14250: loss: 0.6586759313941002, lr: 1e-05
2024-01-06 02:39:19 INFO     	 * (global step 14300: loss: 0.6467831581830978, lr: 1e-05
2024-01-06 02:39:35 INFO     	 * (global step 14350: loss: 0.7422293424606323, lr: 1e-05
2024-01-06 02:39:50 INFO     	 * (global step 14400: loss: 0.5618665367364883, lr: 1e-05
2024-01-06 02:40:05 INFO     	 * (global step 14450: loss: 0.6666081100702286, lr: 1e-05
2024-01-06 02:40:20 INFO     	 * (global step 14500: loss: 0.6012236177921295, lr: 1e-05
2024-01-06 02:40:35 INFO     	 * (global step 14550: loss: 0.5577360764145851, lr: 1e-05
2024-01-06 02:40:50 INFO     	 * (global step 14600: loss: 0.5406804233789444, lr: 1e-05
2024-01-06 02:41:05 INFO     	 * (global step 14650: loss: 0.6037436723709106, lr: 1e-05
2024-01-06 02:41:17 INFO     	 * (global step 14700: loss: 0.588167130947113, lr: 1e-05
2024-01-06 02:41:28 INFO     	 * (global step 14750: loss: 0.45377787947654724, lr: 1e-05
2024-01-06 02:41:41 INFO     	 * (global step 14800: loss: 0.5996252298355103, lr: 1e-05
2024-01-06 02:41:56 INFO     	 * (global step 14850: loss: 0.5739760622382164, lr: 1e-05
2024-01-06 02:42:10 INFO     	 * (global step 14900: loss: 0.7428903952240944, lr: 1e-05
2024-01-06 02:42:26 INFO     	 * (global step 14950: loss: 0.6910968348383904, lr: 1e-05
2024-01-06 02:42:41 INFO     	 * (global step 15000: loss: 0.7003290951251984, lr: 1e-05
2024-01-06 02:42:56 INFO     	 * (global step 15050: loss: 0.5954434275627136, lr: 1e-05
2024-01-06 02:43:11 INFO     	 * (global step 15100: loss: 0.6867543160915375, lr: 1e-05
2024-01-06 02:43:26 INFO     	 * (global step 15150: loss: 0.9749933630228043, lr: 1e-05
2024-01-06 02:43:41 INFO     	 * (global step 15200: loss: 0.6235017627477646, lr: 1e-05
2024-01-06 02:43:56 INFO     	 * (global step 15250: loss: 0.5019140988588333, lr: 1e-05
2024-01-06 02:44:11 INFO     	 * (global step 15300: loss: 0.6534026935696602, lr: 1e-05
2024-01-06 02:44:26 INFO     	 * (global step 15350: loss: 0.6267775744199753, lr: 1e-05
2024-01-06 02:44:41 INFO     	 * (global step 15400: loss: 0.5535441115498543, lr: 1e-05
2024-01-06 02:44:52 INFO     	 * (global step 15450: loss: 0.7269446402788162, lr: 1e-05
2024-01-06 02:45:04 INFO     	 * (global step 15500: loss: 0.6575583890080452, lr: 1e-05
2024-01-06 02:45:17 INFO     	 * (global step 15550: loss: 0.5714728683233261, lr: 1e-05
2024-01-06 02:45:32 INFO     	 * (global step 15600: loss: 0.6308492124080658, lr: 1e-05
2024-01-06 02:45:47 INFO     	 * (global step 15650: loss: 0.7464992702007294, lr: 1e-05
2024-01-06 02:46:01 INFO     	 * (global step 15700: loss: 0.6934839338064194, lr: 1e-05
2024-01-06 02:46:16 INFO     	 * (global step 15750: loss: 0.5354353189468384, lr: 1e-05
2024-01-06 02:46:31 INFO     	 * (global step 15800: loss: 0.5715443715453148, lr: 1e-05
2024-01-06 02:46:47 INFO     	 * (global step 15850: loss: 0.7788465917110443, lr: 1e-05
2024-01-06 02:47:01 INFO     	 * (global step 15900: loss: 0.5203924775123596, lr: 1e-05
2024-01-06 02:47:16 INFO     	 * (global step 15950: loss: 0.5646328330039978, lr: 1e-05
2024-01-06 02:47:32 INFO     	 * (global step 16000: loss: 0.656474769115448, lr: 1e-05
2024-01-06 02:47:46 INFO     	 * (global step 16050: loss: 0.6464466005563736, lr: 1e-05
2024-01-06 02:48:02 INFO     	 * (global step 16100: loss: 0.6102245301008224, lr: 1e-05
2024-01-06 02:48:14 INFO     	 * (global step 16150: loss: 0.7492943927645683, lr: 1e-05
2024-01-06 02:48:24 INFO     [epoch 7/15] average loss: 0.626, lr: 1e-05
2024-01-06 02:48:24 INFO     saving model related files
2024-01-06 02:48:24 INFO     saving model
2024-01-06 02:48:24 INFO     saving tokenizer
2024-01-06 02:48:24 INFO     saving optimizer
2024-01-06 02:48:26 INFO     remove old optimizer files
2024-01-06 02:48:28 INFO     	 * (global step 16200: loss: 0.6617775335907936, lr: 1e-05
2024-01-06 02:48:41 INFO     	 * (global step 16250: loss: 0.5693766325712204, lr: 1e-05
2024-01-06 02:48:56 INFO     	 * (global step 16300: loss: 0.6218595132231712, lr: 1e-05
2024-01-06 02:49:11 INFO     	 * (global step 16350: loss: 0.5706480294466019, lr: 1e-05
2024-01-06 02:49:25 INFO     	 * (global step 16400: loss: 0.6647273525595665, lr: 1e-05
2024-01-06 02:49:39 INFO     	 * (global step 16450: loss: 0.7445200905203819, lr: 1e-05
2024-01-06 02:49:53 INFO     	 * (global step 16500: loss: 0.4803547337651253, lr: 1e-05
2024-01-06 02:50:08 INFO     	 * (global step 16550: loss: 0.71721450984478, lr: 1e-05
2024-01-06 02:50:23 INFO     	 * (global step 16600: loss: 0.6415634155273438, lr: 1e-05
2024-01-06 02:50:37 INFO     	 * (global step 16650: loss: 0.48508100025355816, lr: 1e-05
2024-01-06 02:50:53 INFO     	 * (global step 16700: loss: 0.6596399545669556, lr: 1e-05
2024-01-06 02:51:08 INFO     	 * (global step 16750: loss: 0.6975103467702866, lr: 1e-05
2024-01-06 02:51:23 INFO     	 * (global step 16800: loss: 0.8571493178606033, lr: 1e-05
2024-01-06 02:51:35 INFO     	 * (global step 16850: loss: 0.5162120014429092, lr: 1e-05
2024-01-06 02:51:47 INFO     	 * (global step 16900: loss: 0.6647935509681702, lr: 1e-05
2024-01-06 02:51:59 INFO     	 * (global step 16950: loss: 0.52748654037714, lr: 1e-05
2024-01-06 02:52:14 INFO     	 * (global step 17000: loss: 0.692290648818016, lr: 1e-05
2024-01-06 02:52:30 INFO     	 * (global step 17050: loss: 0.5990818440914154, lr: 1e-05
2024-01-06 02:52:46 INFO     	 * (global step 17100: loss: 0.5510732010006905, lr: 1e-05
2024-01-06 02:53:03 INFO     	 * (global step 17150: loss: 0.40353479236364365, lr: 1e-05
2024-01-06 02:53:20 INFO     	 * (global step 17200: loss: 0.39056895673274994, lr: 1e-05
2024-01-06 02:53:36 INFO     	 * (global step 17250: loss: 0.5359618365764618, lr: 1e-05
2024-01-06 02:53:52 INFO     	 * (global step 17300: loss: 0.6125803664326668, lr: 1e-05
2024-01-06 02:54:07 INFO     	 * (global step 17350: loss: 0.6262148171663284, lr: 1e-05
2024-01-06 02:54:22 INFO     	 * (global step 17400: loss: 0.45529337227344513, lr: 1e-05
2024-01-06 02:54:39 INFO     	 * (global step 17450: loss: 0.6287525370717049, lr: 1e-05
2024-01-06 02:54:54 INFO     	 * (global step 17500: loss: 0.6469086706638336, lr: 1e-05
2024-01-06 02:55:10 INFO     	 * (global step 17550: loss: 0.6269213184714317, lr: 1e-05
2024-01-06 02:55:26 INFO     	 * (global step 17600: loss: 0.6450587064027786, lr: 1e-05
2024-01-06 02:55:41 INFO     	 * (global step 17650: loss: 0.5624211058020592, lr: 1e-05
2024-01-06 02:55:56 INFO     	 * (global step 17700: loss: 0.48195964843034744, lr: 1e-05
2024-01-06 02:56:07 INFO     	 * (global step 17750: loss: 0.5552472472190857, lr: 1e-05
2024-01-06 02:56:19 INFO     	 * (global step 17800: loss: 0.5289672538638115, lr: 1e-05
2024-01-06 02:56:34 INFO     	 * (global step 17850: loss: 0.6765430271625519, lr: 1e-05
2024-01-06 02:56:50 INFO     	 * (global step 17900: loss: 0.576384924352169, lr: 1e-05
2024-01-06 02:57:06 INFO     	 * (global step 17950: loss: 0.661941684782505, lr: 1e-05
2024-01-06 02:57:22 INFO     	 * (global step 18000: loss: 0.5305807664990425, lr: 1e-05
2024-01-06 02:57:37 INFO     	 * (global step 18050: loss: 0.4542940929532051, lr: 1e-05
2024-01-06 02:57:53 INFO     	 * (global step 18100: loss: 0.6894938498735428, lr: 1e-05
2024-01-06 02:58:09 INFO     	 * (global step 18150: loss: 0.5056900009512901, lr: 1e-05
2024-01-06 02:58:24 INFO     	 * (global step 18200: loss: 0.6067919433116913, lr: 1e-05
2024-01-06 02:58:29 INFO     [epoch 8/15] average loss: 0.618, lr: 1e-05
2024-01-06 02:58:29 INFO     saving model related files
2024-01-06 02:58:29 INFO     saving model
2024-01-06 02:58:30 INFO     saving tokenizer
2024-01-06 02:58:30 INFO     saving optimizer
2024-01-06 02:58:32 INFO     remove old optimizer files
2024-01-06 02:58:43 INFO     	 * (global step 18250: loss: 0.6774251759052277, lr: 1e-05
2024-01-06 02:58:59 INFO     	 * (global step 18300: loss: 0.5895249545574188, lr: 1e-05
2024-01-06 02:59:14 INFO     	 * (global step 18350: loss: 0.6576378047466278, lr: 1e-05
2024-01-06 02:59:29 INFO     	 * (global step 18400: loss: 0.5794257372617722, lr: 1e-05
2024-01-06 02:59:44 INFO     	 * (global step 18450: loss: 0.7037744969129562, lr: 1e-05
2024-01-06 03:00:00 INFO     	 * (global step 18500: loss: 0.6018707007169724, lr: 1e-05
2024-01-06 03:00:12 INFO     	 * (global step 18550: loss: 0.5700216367840767, lr: 1e-05
2024-01-06 03:00:24 INFO     	 * (global step 18600: loss: 0.638168603181839, lr: 1e-05
2024-01-06 03:00:37 INFO     	 * (global step 18650: loss: 0.5606513917446136, lr: 1e-05
2024-01-06 03:00:52 INFO     	 * (global step 18700: loss: 0.8252592980861664, lr: 1e-05
2024-01-06 03:01:07 INFO     	 * (global step 18750: loss: 0.7865589261054993, lr: 1e-05
2024-01-06 03:01:22 INFO     	 * (global step 18800: loss: 0.6121331006288528, lr: 1e-05
2024-01-06 03:01:37 INFO     	 * (global step 18850: loss: 0.568048819899559, lr: 1e-05
2024-01-06 03:01:52 INFO     	 * (global step 18900: loss: 0.4858918711543083, lr: 1e-05
2024-01-06 03:02:07 INFO     	 * (global step 18950: loss: 0.4961651861667633, lr: 1e-05
2024-01-06 03:02:22 INFO     	 * (global step 19000: loss: 0.555709958076477, lr: 1e-05
2024-01-06 03:02:38 INFO     	 * (global step 19050: loss: 0.5746140256524086, lr: 1e-05
2024-01-06 03:02:53 INFO     	 * (global step 19100: loss: 0.6816328391432762, lr: 1e-05
2024-01-06 03:03:07 INFO     	 * (global step 19150: loss: 0.5698116049170494, lr: 1e-05
2024-01-06 03:03:23 INFO     	 * (global step 19200: loss: 0.7265195176005363, lr: 1e-05
2024-01-06 03:03:39 INFO     	 * (global step 19250: loss: 0.5277789309620857, lr: 1e-05
2024-01-06 03:03:52 INFO     	 * (global step 19300: loss: 0.46881040930747986, lr: 1e-05
2024-01-06 03:04:04 INFO     	 * (global step 19350: loss: 0.6235433667898178, lr: 1e-05
2024-01-06 03:04:15 INFO     	 * (global step 19400: loss: 0.5345550030469894, lr: 1e-05
2024-01-06 03:04:31 INFO     	 * (global step 19450: loss: 0.5177184045314789, lr: 1e-05
2024-01-06 03:04:46 INFO     	 * (global step 19500: loss: 0.536976121366024, lr: 1e-05
2024-01-06 03:05:02 INFO     	 * (global step 19550: loss: 0.5635085254907608, lr: 1e-05
2024-01-06 03:05:16 INFO     	 * (global step 19600: loss: 0.6060644313693047, lr: 1e-05
2024-01-06 03:05:32 INFO     	 * (global step 19650: loss: 0.5428312942385674, lr: 1e-05
2024-01-06 03:05:47 INFO     	 * (global step 19700: loss: 0.4856572151184082, lr: 1e-05
2024-01-06 03:06:03 INFO     	 * (global step 19750: loss: 0.7740907296538353, lr: 1e-05
2024-01-06 03:06:17 INFO     	 * (global step 19800: loss: 0.7885249257087708, lr: 1e-05
2024-01-06 03:06:32 INFO     	 * (global step 19850: loss: 0.6259486675262451, lr: 1e-05
2024-01-06 03:06:47 INFO     	 * (global step 19900: loss: 0.8278590962290764, lr: 1e-05
2024-01-06 03:07:02 INFO     	 * (global step 19950: loss: 0.5257948487997055, lr: 1e-05
2024-01-06 03:07:16 INFO     	 * (global step 20000: loss: 0.6993709281086922, lr: 1e-05
2024-01-06 03:07:30 INFO     	 * (global step 20050: loss: 0.6032468527555466, lr: 1e-05
2024-01-06 03:07:41 INFO     	 * (global step 20100: loss: 0.6182452961802483, lr: 1e-05
2024-01-06 03:07:53 INFO     	 * (global step 20150: loss: 0.470481738448143, lr: 1e-05
2024-01-06 03:08:07 INFO     	 * (global step 20200: loss: 0.5231761857867241, lr: 1e-05
2024-01-06 03:08:19 INFO     [epoch 9/15] average loss: 0.611, lr: 1e-05
2024-01-06 03:08:19 INFO     saving model related files
2024-01-06 03:08:19 INFO     saving model
2024-01-06 03:08:20 INFO     saving tokenizer
2024-01-06 03:08:20 INFO     saving optimizer
2024-01-06 03:08:22 INFO     remove old optimizer files
2024-01-06 03:08:22 INFO     complete training: model ckpt was saved at small_recreated_ckpt/model_vhyoja
2024-01-06 03:08:23 INFO     ## 1st RUN: Configuration 11/12 ##
2024-01-06 03:08:23 INFO     initialize model trainer
2024-01-06 03:08:23 INFO     initialize checkpoint at small_recreated_ckpt/model_nrudfu
2024-01-06 03:08:23 INFO     hyperparameters
2024-01-06 03:08:23 INFO     	 * dataset_path: lmqg/qag_squad
2024-01-06 03:08:23 INFO     	 * dataset_name: default
2024-01-06 03:08:23 INFO     	 * input_types: ['paragraph']
2024-01-06 03:08:23 INFO     	 * output_types: ['questions_answers']
2024-01-06 03:08:23 INFO     	 * prefix_types: ['qag']
2024-01-06 03:08:23 INFO     	 * model: t5-small
2024-01-06 03:08:23 INFO     	 * max_length: 512
2024-01-06 03:08:23 INFO     	 * max_length_output: 256
2024-01-06 03:08:23 INFO     	 * epoch: 15
2024-01-06 03:08:23 INFO     	 * batch: 2
2024-01-06 03:08:23 INFO     	 * lr: 1e-05
2024-01-06 03:08:23 INFO     	 * fp16: False
2024-01-06 03:08:23 INFO     	 * random_seed: 1
2024-01-06 03:08:23 INFO     	 * gradient_accumulation_steps: 2
2024-01-06 03:08:23 INFO     	 * label_smoothing: 0.0
2024-01-06 03:08:23 INFO     initialize checkpoint with t5-small
2024-01-06 03:08:32 INFO     use spaCy answer extraction model: positionrank
2024-01-06 03:08:36 INFO     Model `t5-small`
2024-01-06 03:08:36 INFO     	 * Num of GPU in use: 1
2024-01-06 03:08:36 INFO     	 * Prefix: True
2024-01-06 03:08:36 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 03:08:36 INFO     dataset preprocessing
2024-01-06 03:08:39 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featurelmqg/qag_squad/t5-small.512.256.paragraph.questions_answers.train.qag.pkl
2024-01-06 03:08:46 INFO     start model training
2024-01-06 03:08:54 INFO     	 * (global step 50: loss: 4.310180902481079, lr: 1e-05
2024-01-06 03:09:02 INFO     	 * (global step 100: loss: 3.2145384550094604, lr: 1e-05
2024-01-06 03:09:10 INFO     	 * (global step 150: loss: 1.8389996886253357, lr: 1e-05
2024-01-06 03:09:18 INFO     	 * (global step 200: loss: 2.1822739839553833, lr: 1e-05
2024-01-06 03:09:26 INFO     	 * (global step 250: loss: 1.8631829619407654, lr: 1e-05
2024-01-06 03:09:33 INFO     	 * (global step 300: loss: 1.454402506351471, lr: 1e-05
2024-01-06 03:09:41 INFO     	 * (global step 350: loss: 1.7291598916053772, lr: 1e-05
2024-01-06 03:09:49 INFO     	 * (global step 400: loss: 1.4717230796813965, lr: 1e-05
2024-01-06 03:09:56 INFO     	 * (global step 450: loss: 1.378155767917633, lr: 1e-05
2024-01-06 03:10:04 INFO     	 * (global step 500: loss: 1.2371894121170044, lr: 1e-05
2024-01-06 03:10:12 INFO     	 * (global step 550: loss: 1.3583593368530273, lr: 1e-05
2024-01-06 03:10:20 INFO     	 * (global step 600: loss: 1.4417025446891785, lr: 1e-05
2024-01-06 03:10:29 INFO     	 * (global step 650: loss: 1.272692084312439, lr: 1e-05
2024-01-06 03:10:37 INFO     	 * (global step 700: loss: 1.2847851514816284, lr: 1e-05
2024-01-06 03:10:44 INFO     	 * (global step 750: loss: 1.351819932460785, lr: 1e-05
2024-01-06 03:10:50 INFO     	 * (global step 800: loss: 1.5140576362609863, lr: 1e-05
2024-01-06 03:10:56 INFO     	 * (global step 850: loss: 1.2774102091789246, lr: 1e-05
2024-01-06 03:11:02 INFO     	 * (global step 900: loss: 1.0219069123268127, lr: 1e-05
2024-01-06 03:11:08 INFO     	 * (global step 950: loss: 0.9702576994895935, lr: 1e-05
2024-01-06 03:11:15 INFO     	 * (global step 1000: loss: 1.1033598482608795, lr: 1e-05
2024-01-06 03:11:23 INFO     	 * (global step 1050: loss: 1.0103358626365662, lr: 1e-05
2024-01-06 03:11:31 INFO     	 * (global step 1100: loss: 0.8842679560184479, lr: 1e-05
2024-01-06 03:11:39 INFO     	 * (global step 1150: loss: 1.0694872438907623, lr: 1e-05
2024-01-06 03:11:48 INFO     	 * (global step 1200: loss: 1.5346017479896545, lr: 1e-05
2024-01-06 03:11:56 INFO     	 * (global step 1250: loss: 1.0355868339538574, lr: 1e-05
2024-01-06 03:12:04 INFO     	 * (global step 1300: loss: 0.9869965016841888, lr: 1e-05
2024-01-06 03:12:12 INFO     	 * (global step 1350: loss: 1.2342310547828674, lr: 1e-05
2024-01-06 03:12:20 INFO     	 * (global step 1400: loss: 0.7541961669921875, lr: 1e-05
2024-01-06 03:12:28 INFO     	 * (global step 1450: loss: 0.8561093807220459, lr: 1e-05
2024-01-06 03:12:36 INFO     	 * (global step 1500: loss: 0.9722200334072113, lr: 1e-05
2024-01-06 03:12:44 INFO     	 * (global step 1550: loss: 1.1341052651405334, lr: 1e-05
2024-01-06 03:12:53 INFO     	 * (global step 1600: loss: 0.7686428725719452, lr: 1e-05
2024-01-06 03:13:00 INFO     	 * (global step 1650: loss: 0.7746158242225647, lr: 1e-05
2024-01-06 03:13:09 INFO     	 * (global step 1700: loss: 0.9820490479469299, lr: 1e-05
2024-01-06 03:13:16 INFO     	 * (global step 1750: loss: 1.0439279675483704, lr: 1e-05
2024-01-06 03:13:24 INFO     	 * (global step 1800: loss: 0.9279545545578003, lr: 1e-05
2024-01-06 03:13:32 INFO     	 * (global step 1850: loss: 0.9611206948757172, lr: 1e-05
2024-01-06 03:13:40 INFO     	 * (global step 1900: loss: 0.6188735514879227, lr: 1e-05
2024-01-06 03:13:47 INFO     	 * (global step 1950: loss: 0.8461036682128906, lr: 1e-05
2024-01-06 03:13:55 INFO     	 * (global step 2000: loss: 1.2859495282173157, lr: 1e-05
2024-01-06 03:14:03 INFO     	 * (global step 2050: loss: 0.8467083275318146, lr: 1e-05
2024-01-06 03:14:11 INFO     	 * (global step 2100: loss: 0.6921774446964264, lr: 1e-05
2024-01-06 03:14:19 INFO     	 * (global step 2150: loss: 0.7206156849861145, lr: 1e-05
2024-01-06 03:14:25 INFO     	 * (global step 2200: loss: 1.1742233037948608, lr: 1e-05
2024-01-06 03:14:31 INFO     	 * (global step 2250: loss: 0.7177814245223999, lr: 1e-05
2024-01-06 03:14:37 INFO     	 * (global step 2300: loss: 1.6168610155582428, lr: 1e-05
2024-01-06 03:14:43 INFO     	 * (global step 2350: loss: 0.5287576466798782, lr: 1e-05
2024-01-06 03:14:49 INFO     	 * (global step 2400: loss: 0.7405338287353516, lr: 1e-05
2024-01-06 03:14:56 INFO     	 * (global step 2450: loss: 0.8372392952442169, lr: 1e-05
2024-01-06 03:15:05 INFO     	 * (global step 2500: loss: 0.711274117231369, lr: 1e-05
2024-01-06 03:15:13 INFO     	 * (global step 2550: loss: 1.01499542593956, lr: 1e-05
2024-01-06 03:15:21 INFO     	 * (global step 2600: loss: 0.5622921735048294, lr: 1e-05
2024-01-06 03:15:29 INFO     	 * (global step 2650: loss: 0.6628347486257553, lr: 1e-05
2024-01-06 03:15:37 INFO     	 * (global step 2700: loss: 0.7817330956459045, lr: 1e-05
2024-01-06 03:15:45 INFO     	 * (global step 2750: loss: 0.5740059018135071, lr: 1e-05
2024-01-06 03:15:53 INFO     	 * (global step 2800: loss: 0.6399737298488617, lr: 1e-05
2024-01-06 03:16:01 INFO     	 * (global step 2850: loss: 0.7790201008319855, lr: 1e-05
2024-01-06 03:16:08 INFO     	 * (global step 2900: loss: 0.9098347425460815, lr: 1e-05
2024-01-06 03:16:16 INFO     	 * (global step 2950: loss: 0.40959616005420685, lr: 1e-05
2024-01-06 03:16:24 INFO     	 * (global step 3000: loss: 1.0068418979644775, lr: 1e-05
2024-01-06 03:16:32 INFO     	 * (global step 3050: loss: 0.8817207217216492, lr: 1e-05
2024-01-06 03:16:40 INFO     	 * (global step 3100: loss: 0.8750881850719452, lr: 1e-05
2024-01-06 03:16:48 INFO     	 * (global step 3150: loss: 0.6526415944099426, lr: 1e-05
2024-01-06 03:16:56 INFO     	 * (global step 3200: loss: 0.967170000076294, lr: 1e-05
2024-01-06 03:17:04 INFO     	 * (global step 3250: loss: 0.606879860162735, lr: 1e-05
2024-01-06 03:17:12 INFO     	 * (global step 3300: loss: 0.745899111032486, lr: 1e-05
2024-01-06 03:17:20 INFO     	 * (global step 3350: loss: 0.6714199483394623, lr: 1e-05
2024-01-06 03:17:28 INFO     	 * (global step 3400: loss: 0.7026410400867462, lr: 1e-05
2024-01-06 03:17:36 INFO     	 * (global step 3450: loss: 0.8316719830036163, lr: 1e-05
2024-01-06 03:17:43 INFO     	 * (global step 3500: loss: 0.7728663682937622, lr: 1e-05
2024-01-06 03:17:49 INFO     	 * (global step 3550: loss: 0.9823234379291534, lr: 1e-05
2024-01-06 03:17:55 INFO     	 * (global step 3600: loss: 0.7556896507740021, lr: 1e-05
2024-01-06 03:18:01 INFO     	 * (global step 3650: loss: 0.7879565954208374, lr: 1e-05
2024-01-06 03:18:07 INFO     	 * (global step 3700: loss: 0.4633239656686783, lr: 1e-05
2024-01-06 03:18:15 INFO     	 * (global step 3750: loss: 0.7349064350128174, lr: 1e-05
2024-01-06 03:18:23 INFO     	 * (global step 3800: loss: 0.5121905654668808, lr: 1e-05
2024-01-06 03:18:31 INFO     	 * (global step 3850: loss: 0.6761788427829742, lr: 1e-05
2024-01-06 03:18:39 INFO     	 * (global step 3900: loss: 0.9725692570209503, lr: 1e-05
2024-01-06 03:18:47 INFO     	 * (global step 3950: loss: 0.7598520219326019, lr: 1e-05
2024-01-06 03:18:55 INFO     	 * (global step 4000: loss: 0.645383283495903, lr: 1e-05
2024-01-06 03:19:02 INFO     [epoch 0/15] average loss: 1.111, lr: 1e-05
2024-01-06 03:19:02 INFO     saving model related files
2024-01-06 03:19:02 INFO     saving model
2024-01-06 03:19:03 INFO     saving tokenizer
2024-01-06 03:19:03 INFO     saving optimizer
2024-01-06 03:19:05 INFO     remove old optimizer files
2024-01-06 03:19:05 INFO     	 * (global step 4050: loss: 0.7376749217510223, lr: 1e-05
2024-01-06 03:19:13 INFO     	 * (global step 4100: loss: 0.646093562245369, lr: 1e-05
2024-01-06 03:19:21 INFO     	 * (global step 4150: loss: 0.7444116771221161, lr: 1e-05
2024-01-06 03:19:30 INFO     	 * (global step 4200: loss: 0.5807651877403259, lr: 1e-05
2024-01-06 03:19:38 INFO     	 * (global step 4250: loss: 0.9009491503238678, lr: 1e-05
2024-01-06 03:19:47 INFO     	 * (global step 4300: loss: 1.0807850360870361, lr: 1e-05
2024-01-06 03:19:54 INFO     	 * (global step 4350: loss: 0.5694548785686493, lr: 1e-05
2024-01-06 03:20:02 INFO     	 * (global step 4400: loss: 0.47207552194595337, lr: 1e-05
2024-01-06 03:20:10 INFO     	 * (global step 4450: loss: 0.9785670340061188, lr: 1e-05
2024-01-06 03:20:17 INFO     	 * (global step 4500: loss: 0.5401087701320648, lr: 1e-05
2024-01-06 03:20:25 INFO     	 * (global step 4550: loss: 0.6625156849622726, lr: 1e-05
2024-01-06 03:20:33 INFO     	 * (global step 4600: loss: 0.651030570268631, lr: 1e-05
2024-01-06 03:20:42 INFO     	 * (global step 4650: loss: 0.9020990133285522, lr: 1e-05
2024-01-06 03:20:50 INFO     	 * (global step 4700: loss: 0.9237690567970276, lr: 1e-05
2024-01-06 03:20:57 INFO     	 * (global step 4750: loss: 0.8861579298973083, lr: 1e-05
2024-01-06 03:21:05 INFO     	 * (global step 4800: loss: 0.6965555846691132, lr: 1e-05
2024-01-06 03:21:11 INFO     	 * (global step 4850: loss: 0.7968471646308899, lr: 1e-05
2024-01-06 03:21:17 INFO     	 * (global step 4900: loss: 0.7243223190307617, lr: 1e-05
2024-01-06 03:21:23 INFO     	 * (global step 4950: loss: 0.9789935797452927, lr: 1e-05
2024-01-06 03:21:29 INFO     	 * (global step 5000: loss: 0.855045884847641, lr: 1e-05
2024-01-06 03:21:36 INFO     	 * (global step 5050: loss: 0.7742142379283905, lr: 1e-05
2024-01-06 03:21:43 INFO     	 * (global step 5100: loss: 0.6519080996513367, lr: 1e-05
2024-01-06 03:21:52 INFO     	 * (global step 5150: loss: 0.7964014410972595, lr: 1e-05
2024-01-06 03:21:59 INFO     	 * (global step 5200: loss: 0.6107098162174225, lr: 1e-05
2024-01-06 03:22:08 INFO     	 * (global step 5250: loss: 0.6655118316411972, lr: 1e-05
2024-01-06 03:22:16 INFO     	 * (global step 5300: loss: 0.9067829251289368, lr: 1e-05
2024-01-06 03:22:24 INFO     	 * (global step 5350: loss: 0.7892976105213165, lr: 1e-05
2024-01-06 03:22:31 INFO     	 * (global step 5400: loss: 0.652528703212738, lr: 1e-05
2024-01-06 03:22:40 INFO     	 * (global step 5450: loss: 0.7123818099498749, lr: 1e-05
2024-01-06 03:22:48 INFO     	 * (global step 5500: loss: 0.5820002555847168, lr: 1e-05
2024-01-06 03:22:56 INFO     	 * (global step 5550: loss: 0.812355250120163, lr: 1e-05
2024-01-06 03:23:04 INFO     	 * (global step 5600: loss: 0.8395763039588928, lr: 1e-05
2024-01-06 03:23:12 INFO     	 * (global step 5650: loss: 0.7877157926559448, lr: 1e-05
2024-01-06 03:23:20 INFO     	 * (global step 5700: loss: 1.1168416142463684, lr: 1e-05
2024-01-06 03:23:28 INFO     	 * (global step 5750: loss: 0.5836213231086731, lr: 1e-05
2024-01-06 03:23:36 INFO     	 * (global step 5800: loss: 0.798801064491272, lr: 1e-05
2024-01-06 03:23:44 INFO     	 * (global step 5850: loss: 0.62031289935112, lr: 1e-05
2024-01-06 03:23:52 INFO     	 * (global step 5900: loss: 0.5430846065282822, lr: 1e-05
2024-01-06 03:23:59 INFO     	 * (global step 5950: loss: 0.8888377547264099, lr: 1e-05
2024-01-06 03:24:07 INFO     	 * (global step 6000: loss: 0.6431312263011932, lr: 1e-05
2024-01-06 03:24:15 INFO     	 * (global step 6050: loss: 0.5848158299922943, lr: 1e-05
2024-01-06 03:24:23 INFO     	 * (global step 6100: loss: 0.5732920169830322, lr: 1e-05
2024-01-06 03:24:30 INFO     	 * (global step 6150: loss: 0.679705411195755, lr: 1e-05
2024-01-06 03:24:36 INFO     	 * (global step 6200: loss: 0.7631078958511353, lr: 1e-05
2024-01-06 03:24:42 INFO     	 * (global step 6250: loss: 0.5849902033805847, lr: 1e-05
2024-01-06 03:24:48 INFO     	 * (global step 6300: loss: 0.6938280165195465, lr: 1e-05
2024-01-06 03:24:54 INFO     	 * (global step 6350: loss: 0.7628058791160583, lr: 1e-05
2024-01-06 03:25:01 INFO     	 * (global step 6400: loss: 0.6338317096233368, lr: 1e-05
2024-01-06 03:25:09 INFO     	 * (global step 6450: loss: 0.5529651492834091, lr: 1e-05
2024-01-06 03:25:17 INFO     	 * (global step 6500: loss: 0.7927877902984619, lr: 1e-05
2024-01-06 03:25:25 INFO     	 * (global step 6550: loss: 0.8906002342700958, lr: 1e-05
2024-01-06 03:25:33 INFO     	 * (global step 6600: loss: 0.759628027677536, lr: 1e-05
2024-01-06 03:25:40 INFO     	 * (global step 6650: loss: 0.6428033411502838, lr: 1e-05
2024-01-06 03:25:48 INFO     	 * (global step 6700: loss: 0.7737402021884918, lr: 1e-05
2024-01-06 03:25:55 INFO     	 * (global step 6750: loss: 0.918411374092102, lr: 1e-05
2024-01-06 03:26:04 INFO     	 * (global step 6800: loss: 0.8756024539470673, lr: 1e-05
2024-01-06 03:26:12 INFO     	 * (global step 6850: loss: 0.7042336463928223, lr: 1e-05
2024-01-06 03:26:20 INFO     	 * (global step 6900: loss: 0.7339164614677429, lr: 1e-05
2024-01-06 03:26:27 INFO     	 * (global step 6950: loss: 0.6339384019374847, lr: 1e-05
2024-01-06 03:26:35 INFO     	 * (global step 7000: loss: 0.628993034362793, lr: 1e-05
2024-01-06 03:26:44 INFO     	 * (global step 7050: loss: 0.7744928300380707, lr: 1e-05
2024-01-06 03:26:52 INFO     	 * (global step 7100: loss: 0.537755697965622, lr: 1e-05
2024-01-06 03:27:00 INFO     	 * (global step 7150: loss: 0.785569965839386, lr: 1e-05
2024-01-06 03:27:08 INFO     	 * (global step 7200: loss: 0.7077280580997467, lr: 1e-05
2024-01-06 03:27:15 INFO     	 * (global step 7250: loss: 0.8832387626171112, lr: 1e-05
2024-01-06 03:27:23 INFO     	 * (global step 7300: loss: 0.8710291683673859, lr: 1e-05
2024-01-06 03:27:31 INFO     	 * (global step 7350: loss: 0.7240933179855347, lr: 1e-05
2024-01-06 03:27:39 INFO     	 * (global step 7400: loss: 0.8454223275184631, lr: 1e-05
2024-01-06 03:27:45 INFO     	 * (global step 7450: loss: 0.6885892152786255, lr: 1e-05
2024-01-06 03:27:50 INFO     	 * (global step 7500: loss: 0.629964143037796, lr: 1e-05
2024-01-06 03:27:56 INFO     	 * (global step 7550: loss: 0.8561873137950897, lr: 1e-05
2024-01-06 03:28:03 INFO     	 * (global step 7600: loss: 0.6672124117612839, lr: 1e-05
2024-01-06 03:28:09 INFO     	 * (global step 7650: loss: 0.9518890380859375, lr: 1e-05
2024-01-06 03:28:15 INFO     	 * (global step 7700: loss: 1.0967938005924225, lr: 1e-05
2024-01-06 03:28:21 INFO     	 * (global step 7750: loss: 0.7242786884307861, lr: 1e-05
2024-01-06 03:28:27 INFO     	 * (global step 7800: loss: 0.7380533516407013, lr: 1e-05
2024-01-06 03:28:33 INFO     	 * (global step 7850: loss: 0.6404260247945786, lr: 1e-05
2024-01-06 03:28:39 INFO     	 * (global step 7900: loss: 0.568210557103157, lr: 1e-05
2024-01-06 03:28:45 INFO     	 * (global step 7950: loss: 0.5939228236675262, lr: 1e-05
2024-01-06 03:28:50 INFO     	 * (global step 8000: loss: 0.5719826519489288, lr: 1e-05
2024-01-06 03:28:56 INFO     	 * (global step 8050: loss: 0.9274095892906189, lr: 1e-05
2024-01-06 03:29:02 INFO     [epoch 1/15] average loss: 0.72, lr: 1e-05
2024-01-06 03:29:02 INFO     saving model related files
2024-01-06 03:29:02 INFO     saving model
2024-01-06 03:29:02 INFO     saving tokenizer
2024-01-06 03:29:02 INFO     saving optimizer
2024-01-06 03:29:03 INFO     remove old optimizer files
2024-01-06 03:29:04 INFO     	 * (global step 8100: loss: 0.6304499804973602, lr: 1e-05
2024-01-06 03:29:10 INFO     	 * (global step 8150: loss: 0.5321252346038818, lr: 1e-05
2024-01-06 03:29:16 INFO     	 * (global step 8200: loss: 0.573421061038971, lr: 1e-05
2024-01-06 03:29:21 INFO     	 * (global step 8250: loss: 0.39730481803417206, lr: 1e-05
2024-01-06 03:29:27 INFO     	 * (global step 8300: loss: 0.6931351125240326, lr: 1e-05
2024-01-06 03:29:33 INFO     	 * (global step 8350: loss: 0.9444292783737183, lr: 1e-05
2024-01-06 03:29:39 INFO     	 * (global step 8400: loss: 0.4980928301811218, lr: 1e-05
2024-01-06 03:29:44 INFO     	 * (global step 8450: loss: 0.5977313220500946, lr: 1e-05
2024-01-06 03:29:50 INFO     	 * (global step 8500: loss: 0.8338504731655121, lr: 1e-05
2024-01-06 03:29:56 INFO     	 * (global step 8550: loss: 0.7116146087646484, lr: 1e-05
2024-01-06 03:30:02 INFO     	 * (global step 8600: loss: 0.3732977658510208, lr: 1e-05
2024-01-06 03:30:07 INFO     	 * (global step 8650: loss: 0.49718308448791504, lr: 1e-05
2024-01-06 03:30:13 INFO     	 * (global step 8700: loss: 0.7726496160030365, lr: 1e-05
2024-01-06 03:30:19 INFO     	 * (global step 8750: loss: 0.6465017795562744, lr: 1e-05
2024-01-06 03:30:25 INFO     	 * (global step 8800: loss: 0.3646107465028763, lr: 1e-05
2024-01-06 03:30:30 INFO     	 * (global step 8850: loss: 0.6840067505836487, lr: 1e-05
2024-01-06 03:30:36 INFO     	 * (global step 8900: loss: 0.49875715374946594, lr: 1e-05
2024-01-06 03:30:42 INFO     	 * (global step 8950: loss: 0.4343561679124832, lr: 1e-05
2024-01-06 03:30:48 INFO     	 * (global step 9000: loss: 0.6386762857437134, lr: 1e-05
2024-01-06 03:30:53 INFO     	 * (global step 9050: loss: 0.7807322144508362, lr: 1e-05
2024-01-06 03:30:59 INFO     	 * (global step 9100: loss: 0.7267940044403076, lr: 1e-05
2024-01-06 03:31:05 INFO     	 * (global step 9150: loss: 0.7296931743621826, lr: 1e-05
2024-01-06 03:31:11 INFO     	 * (global step 9200: loss: 0.7230777740478516, lr: 1e-05
2024-01-06 03:31:16 INFO     	 * (global step 9250: loss: 0.5276426374912262, lr: 1e-05
2024-01-06 03:31:22 INFO     	 * (global step 9300: loss: 0.7850725948810577, lr: 1e-05
2024-01-06 03:31:28 INFO     	 * (global step 9350: loss: 0.45228660106658936, lr: 1e-05
2024-01-06 03:31:34 INFO     	 * (global step 9400: loss: 0.6749769449234009, lr: 1e-05
2024-01-06 03:31:39 INFO     	 * (global step 9450: loss: 0.5497872680425644, lr: 1e-05
2024-01-06 03:31:45 INFO     	 * (global step 9500: loss: 0.6223099827766418, lr: 1e-05
2024-01-06 03:31:51 INFO     	 * (global step 9550: loss: 0.7235552668571472, lr: 1e-05
2024-01-06 03:31:57 INFO     	 * (global step 9600: loss: 0.6475785374641418, lr: 1e-05
2024-01-06 03:32:02 INFO     	 * (global step 9650: loss: 0.8175826370716095, lr: 1e-05
2024-01-06 03:32:08 INFO     	 * (global step 9700: loss: 0.8065090477466583, lr: 1e-05
2024-01-06 03:32:14 INFO     	 * (global step 9750: loss: 0.8632806837558746, lr: 1e-05
2024-01-06 03:32:20 INFO     	 * (global step 9800: loss: 0.5577623248100281, lr: 1e-05
2024-01-06 03:32:25 INFO     	 * (global step 9850: loss: 0.8159051835536957, lr: 1e-05
2024-01-06 03:32:31 INFO     	 * (global step 9900: loss: 0.5260604918003082, lr: 1e-05
2024-01-06 03:32:37 INFO     	 * (global step 9950: loss: 0.5109604299068451, lr: 1e-05
2024-01-06 03:32:43 INFO     	 * (global step 10000: loss: 0.5645721554756165, lr: 1e-05
2024-01-06 03:32:49 INFO     	 * (global step 10050: loss: 0.6087363362312317, lr: 1e-05
2024-01-06 03:32:54 INFO     	 * (global step 10100: loss: 0.7942016422748566, lr: 1e-05
2024-01-06 03:33:00 INFO     	 * (global step 10150: loss: 0.45180028676986694, lr: 1e-05
2024-01-06 03:33:06 INFO     	 * (global step 10200: loss: 0.5720157325267792, lr: 1e-05
2024-01-06 03:33:12 INFO     	 * (global step 10250: loss: 0.6904003620147705, lr: 1e-05
2024-01-06 03:33:18 INFO     	 * (global step 10300: loss: 0.7409031689167023, lr: 1e-05
2024-01-06 03:33:23 INFO     	 * (global step 10350: loss: 0.41321877390146255, lr: 1e-05
2024-01-06 03:33:29 INFO     	 * (global step 10400: loss: 0.6042177081108093, lr: 1e-05
2024-01-06 03:33:35 INFO     	 * (global step 10450: loss: 0.7262648642063141, lr: 1e-05
2024-01-06 03:33:41 INFO     	 * (global step 10500: loss: 0.664669543504715, lr: 1e-05
2024-01-06 03:33:46 INFO     	 * (global step 10550: loss: 0.6373715996742249, lr: 1e-05
2024-01-06 03:33:52 INFO     	 * (global step 10600: loss: 0.351612389087677, lr: 1e-05
2024-01-06 03:33:58 INFO     	 * (global step 10650: loss: 0.5860619992017746, lr: 1e-05
2024-01-06 03:34:04 INFO     	 * (global step 10700: loss: 0.6485626399517059, lr: 1e-05
2024-01-06 03:34:09 INFO     	 * (global step 10750: loss: 0.6893987059593201, lr: 1e-05
2024-01-06 03:34:15 INFO     	 * (global step 10800: loss: 0.6429531127214432, lr: 1e-05
2024-01-06 03:34:21 INFO     	 * (global step 10850: loss: 0.5902554094791412, lr: 1e-05
2024-01-06 03:34:27 INFO     	 * (global step 10900: loss: 0.6134313642978668, lr: 1e-05
2024-01-06 03:34:33 INFO     	 * (global step 10950: loss: 0.5794327259063721, lr: 1e-05
2024-01-06 03:34:38 INFO     	 * (global step 11000: loss: 0.5143473148345947, lr: 1e-05
2024-01-06 03:34:44 INFO     	 * (global step 11050: loss: 0.7659961581230164, lr: 1e-05
2024-01-06 03:34:50 INFO     	 * (global step 11100: loss: 0.6084359288215637, lr: 1e-05
2024-01-06 03:34:56 INFO     	 * (global step 11150: loss: 0.6537002325057983, lr: 1e-05
2024-01-06 03:35:01 INFO     	 * (global step 11200: loss: 0.5541139543056488, lr: 1e-05
2024-01-06 03:35:07 INFO     	 * (global step 11250: loss: 0.7025776505470276, lr: 1e-05
2024-01-06 03:35:13 INFO     	 * (global step 11300: loss: 0.5006996840238571, lr: 1e-05
2024-01-06 03:35:19 INFO     	 * (global step 11350: loss: 0.7050058245658875, lr: 1e-05
2024-01-06 03:35:25 INFO     	 * (global step 11400: loss: 0.7989636957645416, lr: 1e-05
2024-01-06 03:35:30 INFO     	 * (global step 11450: loss: 0.942157506942749, lr: 1e-05
2024-01-06 03:35:36 INFO     	 * (global step 11500: loss: 0.6690593957901001, lr: 1e-05
2024-01-06 03:35:42 INFO     	 * (global step 11550: loss: 0.7732374966144562, lr: 1e-05
2024-01-06 03:35:48 INFO     	 * (global step 11600: loss: 0.6631940305233002, lr: 1e-05
2024-01-06 03:35:53 INFO     	 * (global step 11650: loss: 1.0071894824504852, lr: 1e-05
2024-01-06 03:35:59 INFO     	 * (global step 11700: loss: 0.684224545955658, lr: 1e-05
2024-01-06 03:36:05 INFO     	 * (global step 11750: loss: 0.7017152607440948, lr: 1e-05
2024-01-06 03:36:11 INFO     	 * (global step 11800: loss: 0.45590707659721375, lr: 1e-05
2024-01-06 03:36:17 INFO     	 * (global step 11850: loss: 0.6182111948728561, lr: 1e-05
2024-01-06 03:36:22 INFO     	 * (global step 11900: loss: 0.5167608112096786, lr: 1e-05
2024-01-06 03:36:28 INFO     	 * (global step 11950: loss: 0.4628141522407532, lr: 1e-05
2024-01-06 03:36:34 INFO     	 * (global step 12000: loss: 0.759223222732544, lr: 1e-05
2024-01-06 03:36:40 INFO     	 * (global step 12050: loss: 0.5629478394985199, lr: 1e-05
2024-01-06 03:36:45 INFO     	 * (global step 12100: loss: 0.7391658425331116, lr: 1e-05
2024-01-06 03:36:51 INFO     [epoch 2/15] average loss: 0.674, lr: 1e-05
2024-01-06 03:36:51 INFO     saving model related files
2024-01-06 03:36:51 INFO     saving model
2024-01-06 03:36:51 INFO     saving tokenizer
2024-01-06 03:36:51 INFO     saving optimizer
2024-01-06 03:36:53 INFO     remove old optimizer files
2024-01-06 03:36:53 INFO     	 * (global step 12150: loss: 0.5875865817070007, lr: 1e-05
2024-01-06 03:36:59 INFO     	 * (global step 12200: loss: 0.680733859539032, lr: 1e-05
2024-01-06 03:37:05 INFO     	 * (global step 12250: loss: 0.5352017730474472, lr: 1e-05
2024-01-06 03:37:11 INFO     	 * (global step 12300: loss: 0.6676132082939148, lr: 1e-05
2024-01-06 03:37:16 INFO     	 * (global step 12350: loss: 0.7762026786804199, lr: 1e-05
2024-01-06 03:37:22 INFO     	 * (global step 12400: loss: 0.772603303194046, lr: 1e-05
2024-01-06 03:37:28 INFO     	 * (global step 12450: loss: 0.46599043905735016, lr: 1e-05
2024-01-06 03:37:34 INFO     	 * (global step 12500: loss: 0.7661153823137283, lr: 1e-05
2024-01-06 03:37:40 INFO     	 * (global step 12550: loss: 0.5811251699924469, lr: 1e-05
2024-01-06 03:37:45 INFO     	 * (global step 12600: loss: 0.5328499227762222, lr: 1e-05
2024-01-06 03:37:51 INFO     	 * (global step 12650: loss: 0.6941891312599182, lr: 1e-05
2024-01-06 03:37:57 INFO     	 * (global step 12700: loss: 0.9347354471683502, lr: 1e-05
2024-01-06 03:38:03 INFO     	 * (global step 12750: loss: 0.8037879765033722, lr: 1e-05
2024-01-06 03:38:08 INFO     	 * (global step 12800: loss: 0.7995681464672089, lr: 1e-05
2024-01-06 03:38:14 INFO     	 * (global step 12850: loss: 0.834136962890625, lr: 1e-05
2024-01-06 03:38:20 INFO     	 * (global step 12900: loss: 0.7077276706695557, lr: 1e-05
2024-01-06 03:38:26 INFO     	 * (global step 12950: loss: 0.7112230062484741, lr: 1e-05
2024-01-06 03:38:31 INFO     	 * (global step 13000: loss: 0.8410547971725464, lr: 1e-05
2024-01-06 03:38:37 INFO     	 * (global step 13050: loss: 0.7076622545719147, lr: 1e-05
2024-01-06 03:38:43 INFO     	 * (global step 13100: loss: 0.5798610001802444, lr: 1e-05
2024-01-06 03:38:49 INFO     	 * (global step 13150: loss: 0.6620565354824066, lr: 1e-05
2024-01-06 03:38:55 INFO     	 * (global step 13200: loss: 0.6045610010623932, lr: 1e-05
2024-01-06 03:39:00 INFO     	 * (global step 13250: loss: 1.2096529006958008, lr: 1e-05
2024-01-06 03:39:06 INFO     	 * (global step 13300: loss: 0.4451695382595062, lr: 1e-05
2024-01-06 03:39:12 INFO     	 * (global step 13350: loss: 0.6256110668182373, lr: 1e-05
2024-01-06 03:39:18 INFO     	 * (global step 13400: loss: 0.7927678525447845, lr: 1e-05
2024-01-06 03:39:23 INFO     	 * (global step 13450: loss: 0.5888310372829437, lr: 1e-05
2024-01-06 03:39:29 INFO     	 * (global step 13500: loss: 0.7730801105499268, lr: 1e-05
2024-01-06 03:39:35 INFO     	 * (global step 13550: loss: 0.6784630119800568, lr: 1e-05
2024-01-06 03:39:41 INFO     	 * (global step 13600: loss: 0.8087619245052338, lr: 1e-05
2024-01-06 03:39:47 INFO     	 * (global step 13650: loss: 0.6416393667459488, lr: 1e-05
2024-01-06 03:39:53 INFO     	 * (global step 13700: loss: 0.6894341260194778, lr: 1e-05
2024-01-06 03:39:58 INFO     	 * (global step 13750: loss: 0.6331738531589508, lr: 1e-05
2024-01-06 03:40:04 INFO     	 * (global step 13800: loss: 0.6388566344976425, lr: 1e-05
2024-01-06 03:40:10 INFO     	 * (global step 13850: loss: 1.0027463734149933, lr: 1e-05
2024-01-06 03:40:16 INFO     	 * (global step 13900: loss: 0.42834529280662537, lr: 1e-05
2024-01-06 03:40:21 INFO     	 * (global step 13950: loss: 0.6627175807952881, lr: 1e-05
2024-01-06 03:40:27 INFO     	 * (global step 14000: loss: 0.5156283974647522, lr: 1e-05
2024-01-06 03:40:33 INFO     	 * (global step 14050: loss: 0.5304724276065826, lr: 1e-05
2024-01-06 03:40:39 INFO     	 * (global step 14100: loss: 0.7031361758708954, lr: 1e-05
2024-01-06 03:40:44 INFO     	 * (global step 14150: loss: 0.4677681624889374, lr: 1e-05
2024-01-06 03:40:50 INFO     	 * (global step 14200: loss: 0.66867995262146, lr: 1e-05
2024-01-06 03:40:56 INFO     	 * (global step 14250: loss: 0.7229800224304199, lr: 1e-05
2024-01-06 03:41:02 INFO     	 * (global step 14300: loss: 0.5278516262769699, lr: 1e-05
2024-01-06 03:41:08 INFO     	 * (global step 14350: loss: 1.206419587135315, lr: 1e-05
2024-01-06 03:41:13 INFO     	 * (global step 14400: loss: 0.728276401758194, lr: 1e-05
2024-01-06 03:41:19 INFO     	 * (global step 14450: loss: 0.45882923901081085, lr: 1e-05
2024-01-06 03:41:25 INFO     	 * (global step 14500: loss: 0.615014985203743, lr: 1e-05
2024-01-06 03:41:31 INFO     	 * (global step 14550: loss: 0.5631822645664215, lr: 1e-05
2024-01-06 03:41:37 INFO     	 * (global step 14600: loss: 0.6089268028736115, lr: 1e-05
2024-01-06 03:41:42 INFO     	 * (global step 14650: loss: 0.5715464055538177, lr: 1e-05
2024-01-06 03:41:48 INFO     	 * (global step 14700: loss: 0.484764039516449, lr: 1e-05
2024-01-06 03:41:54 INFO     	 * (global step 14750: loss: 0.6623222529888153, lr: 1e-05
2024-01-06 03:42:00 INFO     	 * (global step 14800: loss: 0.5960020124912262, lr: 1e-05
2024-01-06 03:42:05 INFO     	 * (global step 14850: loss: 0.5812821090221405, lr: 1e-05
2024-01-06 03:42:11 INFO     	 * (global step 14900: loss: 0.5230657458305359, lr: 1e-05
2024-01-06 03:42:17 INFO     	 * (global step 14950: loss: 0.5110732913017273, lr: 1e-05
2024-01-06 03:42:23 INFO     	 * (global step 15000: loss: 0.49538448452949524, lr: 1e-05
2024-01-06 03:42:28 INFO     	 * (global step 15050: loss: 0.7446277737617493, lr: 1e-05
2024-01-06 03:42:34 INFO     	 * (global step 15100: loss: 0.6064231097698212, lr: 1e-05
2024-01-06 03:42:40 INFO     	 * (global step 15150: loss: 0.6291553676128387, lr: 1e-05
2024-01-06 03:42:46 INFO     	 * (global step 15200: loss: 0.5818545818328857, lr: 1e-05
2024-01-06 03:42:51 INFO     	 * (global step 15250: loss: 0.48188385367393494, lr: 1e-05
2024-01-06 03:42:57 INFO     	 * (global step 15300: loss: 0.5868925452232361, lr: 1e-05
2024-01-06 03:43:03 INFO     	 * (global step 15350: loss: 0.8002466857433319, lr: 1e-05
2024-01-06 03:43:09 INFO     	 * (global step 15400: loss: 0.47754959762096405, lr: 1e-05
2024-01-06 03:43:14 INFO     	 * (global step 15450: loss: 0.4812116175889969, lr: 1e-05
2024-01-06 03:43:20 INFO     	 * (global step 15500: loss: 0.5175483822822571, lr: 1e-05
2024-01-06 03:43:26 INFO     	 * (global step 15550: loss: 0.7629557549953461, lr: 1e-05
2024-01-06 03:43:32 INFO     	 * (global step 15600: loss: 0.6695467233657837, lr: 1e-05
2024-01-06 03:43:37 INFO     	 * (global step 15650: loss: 0.7831536531448364, lr: 1e-05
2024-01-06 03:43:43 INFO     	 * (global step 15700: loss: 0.7426592111587524, lr: 1e-05
2024-01-06 03:43:49 INFO     	 * (global step 15750: loss: 0.625704288482666, lr: 1e-05
2024-01-06 03:43:55 INFO     	 * (global step 15800: loss: 0.3735455274581909, lr: 1e-05
2024-01-06 03:44:01 INFO     	 * (global step 15850: loss: 0.6159206032752991, lr: 1e-05
2024-01-06 03:44:07 INFO     	 * (global step 15900: loss: 0.746850460767746, lr: 1e-05
2024-01-06 03:44:12 INFO     	 * (global step 15950: loss: 0.8377982974052429, lr: 1e-05
2024-01-06 03:44:18 INFO     	 * (global step 16000: loss: 0.48671433329582214, lr: 1e-05
2024-01-06 03:44:24 INFO     	 * (global step 16050: loss: 0.4866003543138504, lr: 1e-05
2024-01-06 03:44:30 INFO     	 * (global step 16100: loss: 0.8584542274475098, lr: 1e-05
2024-01-06 03:44:36 INFO     	 * (global step 16150: loss: 0.550821840763092, lr: 1e-05
2024-01-06 03:44:40 INFO     [epoch 3/15] average loss: 0.65, lr: 1e-05
2024-01-06 03:44:40 INFO     saving model related files
2024-01-06 03:44:40 INFO     saving model
2024-01-06 03:44:41 INFO     saving tokenizer
2024-01-06 03:44:41 INFO     saving optimizer
2024-01-06 03:44:42 INFO     remove old optimizer files
2024-01-06 03:44:43 INFO     	 * (global step 16200: loss: 0.7858916521072388, lr: 1e-05
2024-01-06 03:44:49 INFO     	 * (global step 16250: loss: 0.6471613645553589, lr: 1e-05
2024-01-06 03:44:55 INFO     	 * (global step 16300: loss: 0.5389630049467087, lr: 1e-05
2024-01-06 03:45:00 INFO     	 * (global step 16350: loss: 0.5627269446849823, lr: 1e-05
2024-01-06 03:45:06 INFO     	 * (global step 16400: loss: 0.36811335384845734, lr: 1e-05
2024-01-06 03:45:12 INFO     	 * (global step 16450: loss: 0.48764684796333313, lr: 1e-05
2024-01-06 03:45:18 INFO     	 * (global step 16500: loss: 0.532894641160965, lr: 1e-05
2024-01-06 03:45:23 INFO     	 * (global step 16550: loss: 0.6911108195781708, lr: 1e-05
2024-01-06 03:45:29 INFO     	 * (global step 16600: loss: 0.5214059054851532, lr: 1e-05
2024-01-06 03:45:35 INFO     	 * (global step 16650: loss: 0.627654641866684, lr: 1e-05
2024-01-06 03:45:41 INFO     	 * (global step 16700: loss: 0.6471098065376282, lr: 1e-05
2024-01-06 03:45:47 INFO     	 * (global step 16750: loss: 0.5400592684745789, lr: 1e-05
2024-01-06 03:45:53 INFO     	 * (global step 16800: loss: 0.7593382894992828, lr: 1e-05
2024-01-06 03:45:59 INFO     	 * (global step 16850: loss: 0.4400273859500885, lr: 1e-05
2024-01-06 03:46:04 INFO     	 * (global step 16900: loss: 0.6745015382766724, lr: 1e-05
2024-01-06 03:46:10 INFO     	 * (global step 16950: loss: 0.6764092445373535, lr: 1e-05
2024-01-06 03:46:16 INFO     	 * (global step 17000: loss: 0.7318916618824005, lr: 1e-05
2024-01-06 03:46:22 INFO     	 * (global step 17050: loss: 0.6370302438735962, lr: 1e-05
2024-01-06 03:46:28 INFO     	 * (global step 17100: loss: 0.3094319850206375, lr: 1e-05
2024-01-06 03:46:34 INFO     	 * (global step 17150: loss: 0.6337432861328125, lr: 1e-05
2024-01-06 03:46:39 INFO     	 * (global step 17200: loss: 0.5307475924491882, lr: 1e-05
2024-01-06 03:46:45 INFO     	 * (global step 17250: loss: 0.43955494463443756, lr: 1e-05
2024-01-06 03:46:51 INFO     	 * (global step 17300: loss: 0.7595079243183136, lr: 1e-05
2024-01-06 03:46:57 INFO     	 * (global step 17350: loss: 0.7784493863582611, lr: 1e-05
2024-01-06 03:47:03 INFO     	 * (global step 17400: loss: 0.873094916343689, lr: 1e-05
2024-01-06 03:47:08 INFO     	 * (global step 17450: loss: 0.5529177486896515, lr: 1e-05
2024-01-06 03:47:14 INFO     	 * (global step 17500: loss: 0.5554143488407135, lr: 1e-05
2024-01-06 03:47:20 INFO     	 * (global step 17550: loss: 0.5525123178958893, lr: 1e-05
2024-01-06 03:47:26 INFO     	 * (global step 17600: loss: 0.7548589706420898, lr: 1e-05
2024-01-06 03:47:32 INFO     	 * (global step 17650: loss: 0.5127246379852295, lr: 1e-05
2024-01-06 03:47:37 INFO     	 * (global step 17700: loss: 0.48008161783218384, lr: 1e-05
2024-01-06 03:47:43 INFO     	 * (global step 17750: loss: 0.7428178489208221, lr: 1e-05
2024-01-06 03:47:49 INFO     	 * (global step 17800: loss: 0.8017680644989014, lr: 1e-05
2024-01-06 03:47:55 INFO     	 * (global step 17850: loss: 0.7969572246074677, lr: 1e-05
2024-01-06 03:48:01 INFO     	 * (global step 17900: loss: 0.5474146604537964, lr: 1e-05
2024-01-06 03:48:06 INFO     	 * (global step 17950: loss: 0.4984758645296097, lr: 1e-05
2024-01-06 03:48:12 INFO     	 * (global step 18000: loss: 0.6218039691448212, lr: 1e-05
2024-01-06 03:48:18 INFO     	 * (global step 18050: loss: 0.5903619080781937, lr: 1e-05
2024-01-06 03:48:24 INFO     	 * (global step 18100: loss: 0.670408695936203, lr: 1e-05
2024-01-06 03:48:30 INFO     	 * (global step 18150: loss: 0.6993608176708221, lr: 1e-05
2024-01-06 03:48:35 INFO     	 * (global step 18200: loss: 0.5831543207168579, lr: 1e-05
2024-01-06 03:48:41 INFO     	 * (global step 18250: loss: 0.5983110070228577, lr: 1e-05
2024-01-06 03:48:47 INFO     	 * (global step 18300: loss: 0.36463073641061783, lr: 1e-05
2024-01-06 03:48:53 INFO     	 * (global step 18350: loss: 0.9474826455116272, lr: 1e-05
2024-01-06 03:48:59 INFO     	 * (global step 18400: loss: 0.748723030090332, lr: 1e-05
2024-01-06 03:49:04 INFO     	 * (global step 18450: loss: 0.5990077555179596, lr: 1e-05
2024-01-06 03:49:10 INFO     	 * (global step 18500: loss: 0.5869184732437134, lr: 1e-05
2024-01-06 03:49:16 INFO     	 * (global step 18550: loss: 0.937500387430191, lr: 1e-05
2024-01-06 03:49:22 INFO     	 * (global step 18600: loss: 0.5397174060344696, lr: 1e-05
2024-01-06 03:49:28 INFO     	 * (global step 18650: loss: 0.834983617067337, lr: 1e-05
2024-01-06 03:49:33 INFO     	 * (global step 18700: loss: 0.719763845205307, lr: 1e-05
2024-01-06 03:49:39 INFO     	 * (global step 18750: loss: 0.5818381458520889, lr: 1e-05
2024-01-06 03:49:45 INFO     	 * (global step 18800: loss: 0.5732537508010864, lr: 1e-05
2024-01-06 03:49:51 INFO     	 * (global step 18850: loss: 0.6908743977546692, lr: 1e-05
2024-01-06 03:49:57 INFO     	 * (global step 18900: loss: 0.5928259044885635, lr: 1e-05
2024-01-06 03:50:03 INFO     	 * (global step 18950: loss: 0.5879128873348236, lr: 1e-05
2024-01-06 03:50:08 INFO     	 * (global step 19000: loss: 0.6485569477081299, lr: 1e-05
2024-01-06 03:50:14 INFO     	 * (global step 19050: loss: 0.8871256411075592, lr: 1e-05
2024-01-06 03:50:20 INFO     	 * (global step 19100: loss: 0.7511786818504333, lr: 1e-05
2024-01-06 03:50:26 INFO     	 * (global step 19150: loss: 0.6920840442180634, lr: 1e-05
2024-01-06 03:50:32 INFO     	 * (global step 19200: loss: 0.5490067005157471, lr: 1e-05
2024-01-06 03:50:37 INFO     	 * (global step 19250: loss: 0.680771678686142, lr: 1e-05
2024-01-06 03:50:43 INFO     	 * (global step 19300: loss: 0.49047665297985077, lr: 1e-05
2024-01-06 03:50:49 INFO     	 * (global step 19350: loss: 0.84290811419487, lr: 1e-05
2024-01-06 03:50:55 INFO     	 * (global step 19400: loss: 0.7483553290367126, lr: 1e-05
2024-01-06 03:51:01 INFO     	 * (global step 19450: loss: 0.5323151051998138, lr: 1e-05
2024-01-06 03:51:06 INFO     	 * (global step 19500: loss: 0.5477095693349838, lr: 1e-05
2024-01-06 03:51:12 INFO     	 * (global step 19550: loss: 0.3854716792702675, lr: 1e-05
2024-01-06 03:51:18 INFO     	 * (global step 19600: loss: 0.6322026252746582, lr: 1e-05
2024-01-06 03:51:24 INFO     	 * (global step 19650: loss: 0.5145425796508789, lr: 1e-05
2024-01-06 03:51:30 INFO     	 * (global step 19700: loss: 0.5438940078020096, lr: 1e-05
2024-01-06 03:51:36 INFO     	 * (global step 19750: loss: 0.4694926589727402, lr: 1e-05
2024-01-06 03:51:41 INFO     	 * (global step 19800: loss: 0.5560320913791656, lr: 1e-05
2024-01-06 03:51:47 INFO     	 * (global step 19850: loss: 0.3800538629293442, lr: 1e-05
2024-01-06 03:51:53 INFO     	 * (global step 19900: loss: 0.5639932453632355, lr: 1e-05
2024-01-06 03:51:59 INFO     	 * (global step 19950: loss: 0.5513802766799927, lr: 1e-05
2024-01-06 03:52:04 INFO     	 * (global step 20000: loss: 0.48593975603580475, lr: 1e-05
2024-01-06 03:52:10 INFO     	 * (global step 20050: loss: 0.7401508092880249, lr: 1e-05
2024-01-06 03:52:16 INFO     	 * (global step 20100: loss: 0.6516757309436798, lr: 1e-05
2024-01-06 03:52:22 INFO     	 * (global step 20150: loss: 0.5345345139503479, lr: 1e-05
2024-01-06 03:52:28 INFO     	 * (global step 20200: loss: 0.46652786433696747, lr: 1e-05
2024-01-06 03:52:33 INFO     [epoch 4/15] average loss: 0.634, lr: 1e-05
2024-01-06 03:52:33 INFO     saving model related files
2024-01-06 03:52:33 INFO     saving model
2024-01-06 03:52:33 INFO     saving tokenizer
2024-01-06 03:52:33 INFO     saving optimizer
2024-01-06 03:52:34 INFO     remove old optimizer files
2024-01-06 03:52:35 INFO     	 * (global step 20250: loss: 0.5476353019475937, lr: 1e-05
2024-01-06 03:52:41 INFO     	 * (global step 20300: loss: 0.5907951593399048, lr: 1e-05
2024-01-06 03:52:47 INFO     	 * (global step 20350: loss: 0.7823721170425415, lr: 1e-05
2024-01-06 03:52:53 INFO     	 * (global step 20400: loss: 0.544745609164238, lr: 1e-05
2024-01-06 03:52:59 INFO     	 * (global step 20450: loss: 0.4462967664003372, lr: 1e-05
2024-01-06 03:53:04 INFO     	 * (global step 20500: loss: 0.5048476010560989, lr: 1e-05
2024-01-06 03:53:10 INFO     	 * (global step 20550: loss: 0.6345032602548599, lr: 1e-05
2024-01-06 03:53:16 INFO     	 * (global step 20600: loss: 0.6477793455123901, lr: 1e-05
2024-01-06 03:53:22 INFO     	 * (global step 20650: loss: 0.8072548508644104, lr: 1e-05
2024-01-06 03:53:27 INFO     	 * (global step 20700: loss: 0.8955800831317902, lr: 1e-05
2024-01-06 03:53:33 INFO     	 * (global step 20750: loss: 0.42130161821842194, lr: 1e-05
2024-01-06 03:53:39 INFO     	 * (global step 20800: loss: 0.7326174080371857, lr: 1e-05
2024-01-06 03:53:45 INFO     	 * (global step 20850: loss: 0.6351051330566406, lr: 1e-05
2024-01-06 03:53:51 INFO     	 * (global step 20900: loss: 0.46602147817611694, lr: 1e-05
2024-01-06 03:53:56 INFO     	 * (global step 20950: loss: 0.7531323432922363, lr: 1e-05
2024-01-06 03:54:02 INFO     	 * (global step 21000: loss: 0.44050541520118713, lr: 1e-05
2024-01-06 03:54:08 INFO     	 * (global step 21050: loss: 0.7676005959510803, lr: 1e-05
2024-01-06 03:54:14 INFO     	 * (global step 21100: loss: 0.5929872393608093, lr: 1e-05
2024-01-06 03:54:20 INFO     	 * (global step 21150: loss: 0.5636177659034729, lr: 1e-05
2024-01-06 03:54:26 INFO     	 * (global step 21200: loss: 0.43471069633960724, lr: 1e-05
2024-01-06 03:54:31 INFO     	 * (global step 21250: loss: 0.4807368367910385, lr: 1e-05
2024-01-06 03:54:37 INFO     	 * (global step 21300: loss: 0.707612007856369, lr: 1e-05
2024-01-06 03:54:43 INFO     	 * (global step 21350: loss: 0.6344861537218094, lr: 1e-05
2024-01-06 03:54:49 INFO     	 * (global step 21400: loss: 0.6181194186210632, lr: 1e-05
2024-01-06 03:54:55 INFO     	 * (global step 21450: loss: 0.5303145349025726, lr: 1e-05
2024-01-06 03:55:00 INFO     	 * (global step 21500: loss: 0.4580710232257843, lr: 1e-05
2024-01-06 03:55:06 INFO     	 * (global step 21550: loss: 0.41372881829738617, lr: 1e-05
2024-01-06 03:55:12 INFO     	 * (global step 21600: loss: 0.5783174484968185, lr: 1e-05
2024-01-06 03:55:18 INFO     	 * (global step 21650: loss: 0.492117777466774, lr: 1e-05
2024-01-06 03:55:24 INFO     	 * (global step 21700: loss: 0.6149419695138931, lr: 1e-05
2024-01-06 03:55:30 INFO     	 * (global step 21750: loss: 0.6585688591003418, lr: 1e-05
2024-01-06 03:55:35 INFO     	 * (global step 21800: loss: 0.7419697940349579, lr: 1e-05
2024-01-06 03:55:41 INFO     	 * (global step 21850: loss: 0.8176344931125641, lr: 1e-05
2024-01-06 03:55:47 INFO     	 * (global step 21900: loss: 0.7662428915500641, lr: 1e-05
2024-01-06 03:55:53 INFO     	 * (global step 21950: loss: 0.7728271484375, lr: 1e-05
2024-01-06 03:55:59 INFO     	 * (global step 22000: loss: 0.6449792832136154, lr: 1e-05
2024-01-06 03:56:04 INFO     	 * (global step 22050: loss: 0.6120304763317108, lr: 1e-05
2024-01-06 03:56:10 INFO     	 * (global step 22100: loss: 0.6284297704696655, lr: 1e-05
2024-01-06 03:56:16 INFO     	 * (global step 22150: loss: 0.4924950450658798, lr: 1e-05
2024-01-06 03:56:22 INFO     	 * (global step 22200: loss: 0.666122704744339, lr: 1e-05
2024-01-06 03:56:28 INFO     	 * (global step 22250: loss: 0.7973608076572418, lr: 1e-05
2024-01-06 03:56:34 INFO     	 * (global step 22300: loss: 0.653135746717453, lr: 1e-05
2024-01-06 03:56:39 INFO     	 * (global step 22350: loss: 0.5968358814716339, lr: 1e-05
2024-01-06 03:56:45 INFO     	 * (global step 22400: loss: 0.6044777035713196, lr: 1e-05
2024-01-06 03:56:51 INFO     	 * (global step 22450: loss: 0.6008381843566895, lr: 1e-05
2024-01-06 03:56:57 INFO     	 * (global step 22500: loss: 0.7058533132076263, lr: 1e-05
2024-01-06 03:57:03 INFO     	 * (global step 22550: loss: 0.5971646904945374, lr: 1e-05
2024-01-06 03:57:09 INFO     	 * (global step 22600: loss: 0.6702375710010529, lr: 1e-05
2024-01-06 03:57:14 INFO     	 * (global step 22650: loss: 0.5158943831920624, lr: 1e-05
2024-01-06 03:57:20 INFO     	 * (global step 22700: loss: 0.48548319935798645, lr: 1e-05
2024-01-06 03:57:26 INFO     	 * (global step 22750: loss: 0.48988692462444305, lr: 1e-05
2024-01-06 03:57:32 INFO     	 * (global step 22800: loss: 0.5343603789806366, lr: 1e-05
2024-01-06 03:57:38 INFO     	 * (global step 22850: loss: 0.649834394454956, lr: 1e-05
2024-01-06 03:57:44 INFO     	 * (global step 22900: loss: 0.45323099195957184, lr: 1e-05
2024-01-06 03:57:49 INFO     	 * (global step 22950: loss: 0.6085619032382965, lr: 1e-05
2024-01-06 03:57:55 INFO     	 * (global step 23000: loss: 0.700621485710144, lr: 1e-05
2024-01-06 03:58:01 INFO     	 * (global step 23050: loss: 0.42957286536693573, lr: 1e-05
2024-01-06 03:58:07 INFO     	 * (global step 23100: loss: 0.6725853383541107, lr: 1e-05
2024-01-06 03:58:13 INFO     	 * (global step 23150: loss: 0.7200808823108673, lr: 1e-05
2024-01-06 03:58:19 INFO     	 * (global step 23200: loss: 0.5016544163227081, lr: 1e-05
2024-01-06 03:58:24 INFO     	 * (global step 23250: loss: 0.729867547750473, lr: 1e-05
2024-01-06 03:58:30 INFO     	 * (global step 23300: loss: 0.36245763301849365, lr: 1e-05
2024-01-06 03:58:36 INFO     	 * (global step 23350: loss: 0.5342501103878021, lr: 1e-05
2024-01-06 03:58:42 INFO     	 * (global step 23400: loss: 0.4733055979013443, lr: 1e-05
2024-01-06 03:58:48 INFO     	 * (global step 23450: loss: 0.5713726580142975, lr: 1e-05
2024-01-06 03:58:54 INFO     	 * (global step 23500: loss: 0.6007705330848694, lr: 1e-05
2024-01-06 03:58:59 INFO     	 * (global step 23550: loss: 0.5124195367097855, lr: 1e-05
2024-01-06 03:59:05 INFO     	 * (global step 23600: loss: 0.6997503936290741, lr: 1e-05
2024-01-06 03:59:11 INFO     	 * (global step 23650: loss: 0.3737707734107971, lr: 1e-05
2024-01-06 03:59:17 INFO     	 * (global step 23700: loss: 0.5732133835554123, lr: 1e-05
2024-01-06 03:59:23 INFO     	 * (global step 23750: loss: 0.7037204504013062, lr: 1e-05
2024-01-06 03:59:29 INFO     	 * (global step 23800: loss: 0.5841688811779022, lr: 1e-05
2024-01-06 03:59:34 INFO     	 * (global step 23850: loss: 0.5360314846038818, lr: 1e-05
2024-01-06 03:59:40 INFO     	 * (global step 23900: loss: 0.4809565246105194, lr: 1e-05
2024-01-06 03:59:46 INFO     	 * (global step 23950: loss: 1.0074420273303986, lr: 1e-05
2024-01-06 03:59:52 INFO     	 * (global step 24000: loss: 0.5058982521295547, lr: 1e-05
2024-01-06 03:59:58 INFO     	 * (global step 24050: loss: 0.8309171497821808, lr: 1e-05
2024-01-06 04:00:03 INFO     	 * (global step 24100: loss: 0.531779482960701, lr: 1e-05
2024-01-06 04:00:09 INFO     	 * (global step 24150: loss: 0.6292117089033127, lr: 1e-05
2024-01-06 04:00:15 INFO     	 * (global step 24200: loss: 0.6663666367530823, lr: 1e-05
2024-01-06 04:00:21 INFO     	 * (global step 24250: loss: 0.7439642250537872, lr: 1e-05
2024-01-06 04:00:25 INFO     [epoch 5/15] average loss: 0.621, lr: 1e-05
2024-01-06 04:00:25 INFO     saving model related files
2024-01-06 04:00:25 INFO     saving model
2024-01-06 04:00:26 INFO     saving tokenizer
2024-01-06 04:00:26 INFO     saving optimizer
2024-01-06 04:00:27 INFO     remove old optimizer files
2024-01-06 04:00:28 INFO     	 * (global step 24300: loss: 0.5689461827278137, lr: 1e-05
2024-01-06 04:00:34 INFO     	 * (global step 24350: loss: 0.5649008452892303, lr: 1e-05
2024-01-06 04:00:40 INFO     	 * (global step 24400: loss: 0.5280446410179138, lr: 1e-05
2024-01-06 04:00:46 INFO     	 * (global step 24450: loss: 0.4932072162628174, lr: 1e-05
2024-01-06 04:00:51 INFO     	 * (global step 24500: loss: 0.7331530451774597, lr: 1e-05
2024-01-06 04:00:57 INFO     	 * (global step 24550: loss: 0.5525364279747009, lr: 1e-05
2024-01-06 04:01:03 INFO     	 * (global step 24600: loss: 0.5964732766151428, lr: 1e-05
2024-01-06 04:01:09 INFO     	 * (global step 24650: loss: 0.5981965661048889, lr: 1e-05
2024-01-06 04:01:15 INFO     	 * (global step 24700: loss: 0.45212429761886597, lr: 1e-05
2024-01-06 04:01:21 INFO     	 * (global step 24750: loss: 0.7378761172294617, lr: 1e-05
2024-01-06 04:01:27 INFO     	 * (global step 24800: loss: 0.7811472415924072, lr: 1e-05
2024-01-06 04:01:32 INFO     	 * (global step 24850: loss: 0.6082182228565216, lr: 1e-05
2024-01-06 04:01:38 INFO     	 * (global step 24900: loss: 0.7065928280353546, lr: 1e-05
2024-01-06 04:01:44 INFO     	 * (global step 24950: loss: 0.8029794096946716, lr: 1e-05
2024-01-06 04:01:50 INFO     	 * (global step 25000: loss: 0.502309575676918, lr: 1e-05
2024-01-06 04:01:55 INFO     	 * (global step 25050: loss: 0.758445680141449, lr: 1e-05
2024-01-06 04:02:01 INFO     	 * (global step 25100: loss: 0.5496307164430618, lr: 1e-05
2024-01-06 04:02:07 INFO     	 * (global step 25150: loss: 0.6510288715362549, lr: 1e-05
2024-01-06 04:02:13 INFO     	 * (global step 25200: loss: 0.8671011626720428, lr: 1e-05
2024-01-06 04:02:19 INFO     	 * (global step 25250: loss: 0.505620539188385, lr: 1e-05
2024-01-06 04:02:24 INFO     	 * (global step 25300: loss: 0.9376699924468994, lr: 1e-05
2024-01-06 04:02:30 INFO     	 * (global step 25350: loss: 0.5286658704280853, lr: 1e-05
2024-01-06 04:02:36 INFO     	 * (global step 25400: loss: 0.6628560721874237, lr: 1e-05
2024-01-06 04:02:42 INFO     	 * (global step 25450: loss: 0.8666255325078964, lr: 1e-05
2024-01-06 04:02:48 INFO     	 * (global step 25500: loss: 0.6960602104663849, lr: 1e-05
2024-01-06 04:02:53 INFO     	 * (global step 25550: loss: 0.48552095890045166, lr: 1e-05
2024-01-06 04:02:59 INFO     	 * (global step 25600: loss: 0.9926271438598633, lr: 1e-05
2024-01-06 04:03:05 INFO     	 * (global step 25650: loss: 0.7301577627658844, lr: 1e-05
2024-01-06 04:03:11 INFO     	 * (global step 25700: loss: 0.7146792709827423, lr: 1e-05
2024-01-06 04:03:17 INFO     	 * (global step 25750: loss: 0.35004131495952606, lr: 1e-05
2024-01-06 04:03:22 INFO     	 * (global step 25800: loss: 0.5701766908168793, lr: 1e-05
2024-01-06 04:03:28 INFO     	 * (global step 25850: loss: 0.5704925060272217, lr: 1e-05
2024-01-06 04:03:34 INFO     	 * (global step 25900: loss: 0.6447314918041229, lr: 1e-05
2024-01-06 04:03:40 INFO     	 * (global step 25950: loss: 0.5726860463619232, lr: 1e-05
2024-01-06 04:03:45 INFO     	 * (global step 26000: loss: 0.4495917856693268, lr: 1e-05
2024-01-06 04:03:51 INFO     	 * (global step 26050: loss: 0.51645328104496, lr: 1e-05
2024-01-06 04:03:57 INFO     	 * (global step 26100: loss: 0.5154041796922684, lr: 1e-05
2024-01-06 04:04:03 INFO     	 * (global step 26150: loss: 0.5670818388462067, lr: 1e-05
2024-01-06 04:04:09 INFO     	 * (global step 26200: loss: 0.7267169058322906, lr: 1e-05
2024-01-06 04:04:15 INFO     	 * (global step 26250: loss: 0.47318248450756073, lr: 1e-05
2024-01-06 04:04:20 INFO     	 * (global step 26300: loss: 0.6045156419277191, lr: 1e-05
2024-01-06 04:04:26 INFO     	 * (global step 26350: loss: 0.6000773906707764, lr: 1e-05
2024-01-06 04:04:32 INFO     	 * (global step 26400: loss: 0.5929756164550781, lr: 1e-05
2024-01-06 04:04:38 INFO     	 * (global step 26450: loss: 0.47760048508644104, lr: 1e-05
2024-01-06 04:04:44 INFO     	 * (global step 26500: loss: 0.9776754677295685, lr: 1e-05
2024-01-06 04:04:50 INFO     	 * (global step 26550: loss: 0.7020851075649261, lr: 1e-05
2024-01-06 04:04:55 INFO     	 * (global step 26600: loss: 0.7265531718730927, lr: 1e-05
2024-01-06 04:05:01 INFO     	 * (global step 26650: loss: 0.5306548476219177, lr: 1e-05
2024-01-06 04:05:07 INFO     	 * (global step 26700: loss: 0.7595654726028442, lr: 1e-05
2024-01-06 04:05:13 INFO     	 * (global step 26750: loss: 0.47163020074367523, lr: 1e-05
2024-01-06 04:05:19 INFO     	 * (global step 26800: loss: 0.752036452293396, lr: 1e-05
2024-01-06 04:05:24 INFO     	 * (global step 26850: loss: 0.8049919307231903, lr: 1e-05
2024-01-06 04:05:30 INFO     	 * (global step 26900: loss: 0.5136559754610062, lr: 1e-05
2024-01-06 04:05:36 INFO     	 * (global step 26950: loss: 0.5849839001893997, lr: 1e-05
2024-01-06 04:05:42 INFO     	 * (global step 27000: loss: 0.5881058275699615, lr: 1e-05
2024-01-06 04:05:48 INFO     	 * (global step 27050: loss: 0.46859273314476013, lr: 1e-05
2024-01-06 04:05:53 INFO     	 * (global step 27100: loss: 0.5009586811065674, lr: 1e-05
2024-01-06 04:05:59 INFO     	 * (global step 27150: loss: 0.6485614776611328, lr: 1e-05
2024-01-06 04:06:05 INFO     	 * (global step 27200: loss: 0.5077266246080399, lr: 1e-05
2024-01-06 04:06:11 INFO     	 * (global step 27250: loss: 0.4075150042772293, lr: 1e-05
2024-01-06 04:06:16 INFO     	 * (global step 27300: loss: 0.622249573469162, lr: 1e-05
2024-01-06 04:06:22 INFO     	 * (global step 27350: loss: 0.5227582007646561, lr: 1e-05
2024-01-06 04:06:28 INFO     	 * (global step 27400: loss: 0.6691242456436157, lr: 1e-05
2024-01-06 04:06:34 INFO     	 * (global step 27450: loss: 0.5181062072515488, lr: 1e-05
2024-01-06 04:06:40 INFO     	 * (global step 27500: loss: 0.5384785532951355, lr: 1e-05
2024-01-06 04:06:46 INFO     	 * (global step 27550: loss: 0.4523395299911499, lr: 1e-05
2024-01-06 04:06:51 INFO     	 * (global step 27600: loss: 0.4730985462665558, lr: 1e-05
2024-01-06 04:06:57 INFO     	 * (global step 27650: loss: 0.6112393587827682, lr: 1e-05
2024-01-06 04:07:03 INFO     	 * (global step 27700: loss: 0.6447961330413818, lr: 1e-05
2024-01-06 04:07:09 INFO     	 * (global step 27750: loss: 0.7906152307987213, lr: 1e-05
2024-01-06 04:07:15 INFO     	 * (global step 27800: loss: 0.6328801512718201, lr: 1e-05
2024-01-06 04:07:20 INFO     	 * (global step 27850: loss: 0.6994199156761169, lr: 1e-05
2024-01-06 04:07:26 INFO     	 * (global step 27900: loss: 0.6601169109344482, lr: 1e-05
2024-01-06 04:07:32 INFO     	 * (global step 27950: loss: 0.48309049010276794, lr: 1e-05
2024-01-06 04:07:38 INFO     	 * (global step 28000: loss: 0.5265750885009766, lr: 1e-05
2024-01-06 04:07:44 INFO     	 * (global step 28050: loss: 0.8668226301670074, lr: 1e-05
2024-01-06 04:07:50 INFO     	 * (global step 28100: loss: 0.7052171230316162, lr: 1e-05
2024-01-06 04:07:55 INFO     	 * (global step 28150: loss: 0.6346568167209625, lr: 1e-05
2024-01-06 04:08:01 INFO     	 * (global step 28200: loss: 0.7038286328315735, lr: 1e-05
2024-01-06 04:08:07 INFO     	 * (global step 28250: loss: 0.6507002115249634, lr: 1e-05
2024-01-06 04:08:13 INFO     	 * (global step 28300: loss: 0.7841033041477203, lr: 1e-05
2024-01-06 04:08:17 INFO     [epoch 6/15] average loss: 0.611, lr: 1e-05
2024-01-06 04:08:17 INFO     saving model related files
2024-01-06 04:08:17 INFO     saving model
2024-01-06 04:08:18 INFO     saving tokenizer
2024-01-06 04:08:18 INFO     saving optimizer
2024-01-06 04:08:19 INFO     remove old optimizer files
2024-01-06 04:08:20 INFO     	 * (global step 28350: loss: 0.7240523993968964, lr: 1e-05
2024-01-06 04:08:26 INFO     	 * (global step 28400: loss: 0.5277515947818756, lr: 1e-05
2024-01-06 04:08:32 INFO     	 * (global step 28450: loss: 0.6177780330181122, lr: 1e-05
2024-01-06 04:08:38 INFO     	 * (global step 28500: loss: 0.6755118668079376, lr: 1e-05
2024-01-06 04:08:44 INFO     	 * (global step 28550: loss: 0.6663705110549927, lr: 1e-05
2024-01-06 04:08:49 INFO     	 * (global step 28600: loss: 0.6840538084506989, lr: 1e-05
2024-01-06 04:08:55 INFO     	 * (global step 28650: loss: 0.7039567232131958, lr: 1e-05
2024-01-06 04:09:01 INFO     	 * (global step 28700: loss: 0.60279680788517, lr: 1e-05
2024-01-06 04:09:07 INFO     	 * (global step 28750: loss: 0.629299134016037, lr: 1e-05
2024-01-06 04:09:12 INFO     	 * (global step 28800: loss: 0.5107835680246353, lr: 1e-05
2024-01-06 04:09:18 INFO     	 * (global step 28850: loss: 0.6860672533512115, lr: 1e-05
2024-01-06 04:09:24 INFO     	 * (global step 28900: loss: 0.6624032557010651, lr: 1e-05
2024-01-06 04:09:30 INFO     	 * (global step 28950: loss: 0.5644247382879257, lr: 1e-05
2024-01-06 04:09:36 INFO     	 * (global step 29000: loss: 0.5663924068212509, lr: 1e-05
2024-01-06 04:09:41 INFO     	 * (global step 29050: loss: 0.4808274507522583, lr: 1e-05
2024-01-06 04:09:47 INFO     	 * (global step 29100: loss: 0.6452528834342957, lr: 1e-05
2024-01-06 04:09:53 INFO     	 * (global step 29150: loss: 0.7868667840957642, lr: 1e-05
2024-01-06 04:09:59 INFO     	 * (global step 29200: loss: 0.7190335988998413, lr: 1e-05
2024-01-06 04:10:04 INFO     	 * (global step 29250: loss: 0.5628443211317062, lr: 1e-05
2024-01-06 04:10:10 INFO     	 * (global step 29300: loss: 0.4842766970396042, lr: 1e-05
2024-01-06 04:10:16 INFO     	 * (global step 29350: loss: 0.636072039604187, lr: 1e-05
2024-01-06 04:10:22 INFO     	 * (global step 29400: loss: 0.4586554914712906, lr: 1e-05
2024-01-06 04:10:27 INFO     	 * (global step 29450: loss: 0.5936245322227478, lr: 1e-05
2024-01-06 04:10:33 INFO     	 * (global step 29500: loss: 0.40102051198482513, lr: 1e-05
2024-01-06 04:10:39 INFO     	 * (global step 29550: loss: 0.48402203619480133, lr: 1e-05
2024-01-06 04:10:45 INFO     	 * (global step 29600: loss: 0.4047689288854599, lr: 1e-05
2024-01-06 04:10:50 INFO     	 * (global step 29650: loss: 0.4733467847108841, lr: 1e-05
2024-01-06 04:10:56 INFO     	 * (global step 29700: loss: 0.5762364268302917, lr: 1e-05
2024-01-06 04:11:02 INFO     	 * (global step 29750: loss: 0.9010549187660217, lr: 1e-05
2024-01-06 04:11:08 INFO     	 * (global step 29800: loss: 0.6718919426202774, lr: 1e-05
2024-01-06 04:11:14 INFO     	 * (global step 29850: loss: 0.6107604652643204, lr: 1e-05
2024-01-06 04:11:19 INFO     	 * (global step 29900: loss: 0.531876266002655, lr: 1e-05
2024-01-06 04:11:25 INFO     	 * (global step 29950: loss: 0.6203000843524933, lr: 1e-05
2024-01-06 04:11:31 INFO     	 * (global step 30000: loss: 0.5956496894359589, lr: 1e-05
2024-01-06 04:11:37 INFO     	 * (global step 30050: loss: 0.6138758510351181, lr: 1e-05
2024-01-06 04:11:43 INFO     	 * (global step 30100: loss: 0.5898891389369965, lr: 1e-05
2024-01-06 04:11:49 INFO     	 * (global step 30150: loss: 0.5508165210485458, lr: 1e-05
2024-01-06 04:11:54 INFO     	 * (global step 30200: loss: 0.7379739284515381, lr: 1e-05
2024-01-06 04:12:00 INFO     	 * (global step 30250: loss: 0.6542397141456604, lr: 1e-05
2024-01-06 04:12:06 INFO     	 * (global step 30300: loss: 0.802196592092514, lr: 1e-05
2024-01-06 04:12:12 INFO     	 * (global step 30350: loss: 0.4171600490808487, lr: 1e-05
2024-01-06 04:12:18 INFO     	 * (global step 30400: loss: 0.6266160309314728, lr: 1e-05
2024-01-06 04:12:24 INFO     	 * (global step 30450: loss: 1.150699943304062, lr: 1e-05
2024-01-06 04:12:29 INFO     	 * (global step 30500: loss: 0.6221256703138351, lr: 1e-05
2024-01-06 04:12:35 INFO     	 * (global step 30550: loss: 0.38730984926223755, lr: 1e-05
2024-01-06 04:12:41 INFO     	 * (global step 30600: loss: 0.7217903137207031, lr: 1e-05
2024-01-06 04:12:47 INFO     	 * (global step 30650: loss: 0.8728549182415009, lr: 1e-05
2024-01-06 04:12:53 INFO     	 * (global step 30700: loss: 0.41664236783981323, lr: 1e-05
2024-01-06 04:12:58 INFO     	 * (global step 30750: loss: 0.5958223342895508, lr: 1e-05
2024-01-06 04:13:04 INFO     	 * (global step 30800: loss: 0.4840436279773712, lr: 1e-05
2024-01-06 04:13:10 INFO     	 * (global step 30850: loss: 0.4223839044570923, lr: 1e-05
2024-01-06 04:13:16 INFO     	 * (global step 30900: loss: 0.773284912109375, lr: 1e-05
2024-01-06 04:13:22 INFO     	 * (global step 30950: loss: 0.514409214258194, lr: 1e-05
2024-01-06 04:13:28 INFO     	 * (global step 31000: loss: 0.7553538084030151, lr: 1e-05
2024-01-06 04:13:33 INFO     	 * (global step 31050: loss: 0.4486533999443054, lr: 1e-05
2024-01-06 04:13:39 INFO     	 * (global step 31100: loss: 0.48529738187789917, lr: 1e-05
2024-01-06 04:13:45 INFO     	 * (global step 31150: loss: 0.5198331028223038, lr: 1e-05
2024-01-06 04:13:51 INFO     	 * (global step 31200: loss: 0.5655975490808487, lr: 1e-05
2024-01-06 04:13:57 INFO     	 * (global step 31250: loss: 0.5270327627658844, lr: 1e-05
2024-01-06 04:14:02 INFO     	 * (global step 31300: loss: 0.8658660352230072, lr: 1e-05
2024-01-06 04:14:08 INFO     	 * (global step 31350: loss: 0.6074073910713196, lr: 1e-05
2024-01-06 04:14:14 INFO     	 * (global step 31400: loss: 0.8345010280609131, lr: 1e-05
2024-01-06 04:14:20 INFO     	 * (global step 31450: loss: 0.6474476605653763, lr: 1e-05
2024-01-06 04:14:26 INFO     	 * (global step 31500: loss: 0.4506741166114807, lr: 1e-05
2024-01-06 04:14:32 INFO     	 * (global step 31550: loss: 0.47725388407707214, lr: 1e-05
2024-01-06 04:14:37 INFO     	 * (global step 31600: loss: 0.5854738503694534, lr: 1e-05
2024-01-06 04:14:43 INFO     	 * (global step 31650: loss: 0.6674003303050995, lr: 1e-05
2024-01-06 04:14:49 INFO     	 * (global step 31700: loss: 0.710050493478775, lr: 1e-05
2024-01-06 04:14:55 INFO     	 * (global step 31750: loss: 0.6421778798103333, lr: 1e-05
2024-01-06 04:15:01 INFO     	 * (global step 31800: loss: 0.4723975211381912, lr: 1e-05
2024-01-06 04:15:07 INFO     	 * (global step 31850: loss: 0.6522612869739532, lr: 1e-05
2024-01-06 04:15:12 INFO     	 * (global step 31900: loss: 0.5611065328121185, lr: 1e-05
2024-01-06 04:15:18 INFO     	 * (global step 31950: loss: 0.5395395457744598, lr: 1e-05
2024-01-06 04:15:24 INFO     	 * (global step 32000: loss: 0.6354320347309113, lr: 1e-05
2024-01-06 04:15:30 INFO     	 * (global step 32050: loss: 0.5441930890083313, lr: 1e-05
2024-01-06 04:15:36 INFO     	 * (global step 32100: loss: 0.6341200172901154, lr: 1e-05
2024-01-06 04:15:41 INFO     	 * (global step 32150: loss: 0.40668417513370514, lr: 1e-05
2024-01-06 04:15:47 INFO     	 * (global step 32200: loss: 0.54664745926857, lr: 1e-05
2024-01-06 04:15:53 INFO     	 * (global step 32250: loss: 0.43376462161540985, lr: 1e-05
2024-01-06 04:15:59 INFO     	 * (global step 32300: loss: 0.6137606650590897, lr: 1e-05
2024-01-06 04:16:05 INFO     	 * (global step 32350: loss: 0.48563219606876373, lr: 1e-05
2024-01-06 04:16:09 INFO     [epoch 7/15] average loss: 0.603, lr: 1e-05
2024-01-06 04:16:09 INFO     saving model related files
2024-01-06 04:16:09 INFO     saving model
2024-01-06 04:16:09 INFO     saving tokenizer
2024-01-06 04:16:09 INFO     saving optimizer
2024-01-06 04:16:10 INFO     remove old optimizer files
2024-01-06 04:16:12 INFO     	 * (global step 32400: loss: 0.7919161915779114, lr: 1e-05
2024-01-06 04:16:18 INFO     	 * (global step 32450: loss: 0.3752211183309555, lr: 1e-05
2024-01-06 04:16:24 INFO     	 * (global step 32500: loss: 0.5611127465963364, lr: 1e-05
2024-01-06 04:16:30 INFO     	 * (global step 32550: loss: 0.5079983621835709, lr: 1e-05
2024-01-06 04:16:35 INFO     	 * (global step 32600: loss: 0.5997724831104279, lr: 1e-05
2024-01-06 04:16:41 INFO     	 * (global step 32650: loss: 0.5652076303958893, lr: 1e-05
2024-01-06 04:16:47 INFO     	 * (global step 32700: loss: 0.4350191652774811, lr: 1e-05
2024-01-06 04:16:53 INFO     	 * (global step 32750: loss: 0.4871412217617035, lr: 1e-05
2024-01-06 04:16:59 INFO     	 * (global step 32800: loss: 0.6692680269479752, lr: 1e-05
2024-01-06 04:17:04 INFO     	 * (global step 32850: loss: 0.4559994637966156, lr: 1e-05
2024-01-06 04:17:10 INFO     	 * (global step 32900: loss: 0.5576919615268707, lr: 1e-05
2024-01-06 04:17:16 INFO     	 * (global step 32950: loss: 0.6434427201747894, lr: 1e-05
2024-01-06 04:17:22 INFO     	 * (global step 33000: loss: 0.34999023377895355, lr: 1e-05
2024-01-06 04:17:28 INFO     	 * (global step 33050: loss: 0.5734121799468994, lr: 1e-05
2024-01-06 04:17:33 INFO     	 * (global step 33100: loss: 0.6719244718551636, lr: 1e-05
2024-01-06 04:17:39 INFO     	 * (global step 33150: loss: 0.48458997905254364, lr: 1e-05
2024-01-06 04:17:45 INFO     	 * (global step 33200: loss: 0.5217371881008148, lr: 1e-05
2024-01-06 04:17:51 INFO     	 * (global step 33250: loss: 0.5484368950128555, lr: 1e-05
2024-01-06 04:17:57 INFO     	 * (global step 33300: loss: 0.2205621600151062, lr: 1e-05
2024-01-06 04:18:02 INFO     	 * (global step 33350: loss: 0.7174566835165024, lr: 1e-05
2024-01-06 04:18:08 INFO     	 * (global step 33400: loss: 0.5190673172473907, lr: 1e-05
2024-01-06 04:18:14 INFO     	 * (global step 33450: loss: 0.5565522313117981, lr: 1e-05
2024-01-06 04:18:20 INFO     	 * (global step 33500: loss: 0.6696303188800812, lr: 1e-05
2024-01-06 04:18:26 INFO     	 * (global step 33550: loss: 0.5231385976076126, lr: 1e-05
2024-01-06 04:18:32 INFO     	 * (global step 33600: loss: 0.628976434469223, lr: 1e-05
2024-01-06 04:18:37 INFO     	 * (global step 33650: loss: 0.736485093832016, lr: 1e-05
2024-01-06 04:18:43 INFO     	 * (global step 33700: loss: 0.42974264919757843, lr: 1e-05
2024-01-06 04:18:49 INFO     	 * (global step 33750: loss: 0.45419082045555115, lr: 1e-05
2024-01-06 04:18:55 INFO     	 * (global step 33800: loss: 0.6618258059024811, lr: 1e-05
2024-01-06 04:19:01 INFO     	 * (global step 33850: loss: 0.5555759817361832, lr: 1e-05
2024-01-06 04:19:06 INFO     	 * (global step 33900: loss: 0.5128871649503708, lr: 1e-05
2024-01-06 04:19:12 INFO     	 * (global step 33950: loss: 0.509115919470787, lr: 1e-05
2024-01-06 04:19:18 INFO     	 * (global step 34000: loss: 0.7486680448055267, lr: 1e-05
2024-01-06 04:19:24 INFO     	 * (global step 34050: loss: 1.0128744840621948, lr: 1e-05
2024-01-06 04:19:29 INFO     	 * (global step 34100: loss: 0.7093401253223419, lr: 1e-05
2024-01-06 04:19:35 INFO     	 * (global step 34150: loss: 0.48680005967617035, lr: 1e-05
2024-01-06 04:19:41 INFO     	 * (global step 34200: loss: 0.593016654253006, lr: 1e-05
2024-01-06 04:19:47 INFO     	 * (global step 34250: loss: 0.6285170614719391, lr: 1e-05
2024-01-06 04:19:53 INFO     	 * (global step 34300: loss: 0.38990119099617004, lr: 1e-05
2024-01-06 04:19:59 INFO     	 * (global step 34350: loss: 0.6032950282096863, lr: 1e-05
2024-01-06 04:20:04 INFO     	 * (global step 34400: loss: 0.3781422972679138, lr: 1e-05
2024-01-06 04:20:10 INFO     	 * (global step 34450: loss: 0.6518202424049377, lr: 1e-05
2024-01-06 04:20:16 INFO     	 * (global step 34500: loss: 0.3906812220811844, lr: 1e-05
2024-01-06 04:20:22 INFO     	 * (global step 34550: loss: 0.4635147452354431, lr: 1e-05
2024-01-06 04:20:28 INFO     	 * (global step 34600: loss: 0.7131564170122147, lr: 1e-05
2024-01-06 04:20:34 INFO     	 * (global step 34650: loss: 0.5102012753486633, lr: 1e-05
2024-01-06 04:20:40 INFO     	 * (global step 34700: loss: 0.7166100144386292, lr: 1e-05
2024-01-06 04:20:45 INFO     	 * (global step 34750: loss: 0.704509973526001, lr: 1e-05
2024-01-06 04:20:51 INFO     	 * (global step 34800: loss: 0.510026291012764, lr: 1e-05
2024-01-06 04:20:57 INFO     	 * (global step 34850: loss: 0.7345491647720337, lr: 1e-05
2024-01-06 04:21:03 INFO     	 * (global step 34900: loss: 0.5635958611965179, lr: 1e-05
2024-01-06 04:21:09 INFO     	 * (global step 34950: loss: 0.5461249947547913, lr: 1e-05
2024-01-06 04:21:15 INFO     	 * (global step 35000: loss: 0.5375817120075226, lr: 1e-05
2024-01-06 04:21:20 INFO     	 * (global step 35050: loss: 0.9388191103935242, lr: 1e-05
2024-01-06 04:21:26 INFO     	 * (global step 35100: loss: 0.6614268720149994, lr: 1e-05
2024-01-06 04:21:32 INFO     	 * (global step 35150: loss: 0.5810389220714569, lr: 1e-05
2024-01-06 04:21:38 INFO     	 * (global step 35200: loss: 0.6589166224002838, lr: 1e-05
2024-01-06 04:21:44 INFO     	 * (global step 35250: loss: 0.5622979998588562, lr: 1e-05
2024-01-06 04:21:50 INFO     	 * (global step 35300: loss: 0.5104857385158539, lr: 1e-05
2024-01-06 04:21:55 INFO     	 * (global step 35350: loss: 0.5036107152700424, lr: 1e-05
2024-01-06 04:22:01 INFO     	 * (global step 35400: loss: 0.5692682564258575, lr: 1e-05
2024-01-06 04:22:07 INFO     	 * (global step 35450: loss: 0.5499550104141235, lr: 1e-05
2024-01-06 04:22:13 INFO     	 * (global step 35500: loss: 0.5453769862651825, lr: 1e-05
2024-01-06 04:22:19 INFO     	 * (global step 35550: loss: 0.5729048252105713, lr: 1e-05
2024-01-06 04:22:25 INFO     	 * (global step 35600: loss: 0.5479055643081665, lr: 1e-05
2024-01-06 04:22:30 INFO     	 * (global step 35650: loss: 0.3819040507078171, lr: 1e-05
2024-01-06 04:22:36 INFO     	 * (global step 35700: loss: 0.7697196006774902, lr: 1e-05
2024-01-06 04:22:42 INFO     	 * (global step 35750: loss: 0.6131735891103745, lr: 1e-05
2024-01-06 04:22:48 INFO     	 * (global step 35800: loss: 0.5416712462902069, lr: 1e-05
2024-01-06 04:22:54 INFO     	 * (global step 35850: loss: 0.5876713469624519, lr: 1e-05
2024-01-06 04:23:00 INFO     	 * (global step 35900: loss: 0.8137209117412567, lr: 1e-05
2024-01-06 04:23:05 INFO     	 * (global step 35950: loss: 0.9350071847438812, lr: 1e-05
2024-01-06 04:23:11 INFO     	 * (global step 36000: loss: 0.6529146432876587, lr: 1e-05
2024-01-06 04:23:17 INFO     	 * (global step 36050: loss: 0.7735645771026611, lr: 1e-05
2024-01-06 04:23:23 INFO     	 * (global step 36100: loss: 0.4734172075986862, lr: 1e-05
2024-01-06 04:23:29 INFO     	 * (global step 36150: loss: 0.6098976731300354, lr: 1e-05
2024-01-06 04:23:34 INFO     	 * (global step 36200: loss: 0.5382237285375595, lr: 1e-05
2024-01-06 04:23:40 INFO     	 * (global step 36250: loss: 0.5259564816951752, lr: 1e-05
2024-01-06 04:23:46 INFO     	 * (global step 36300: loss: 0.46382761001586914, lr: 1e-05
2024-01-06 04:23:52 INFO     	 * (global step 36350: loss: 0.440392404794693, lr: 1e-05
2024-01-06 04:23:58 INFO     	 * (global step 36400: loss: 0.4789539724588394, lr: 1e-05
2024-01-06 04:24:02 INFO     [epoch 8/15] average loss: 0.595, lr: 1e-05
2024-01-06 04:24:02 INFO     saving model related files
2024-01-06 04:24:02 INFO     saving model
2024-01-06 04:24:02 INFO     saving tokenizer
2024-01-06 04:24:02 INFO     saving optimizer
2024-01-06 04:24:03 INFO     remove old optimizer files
2024-01-06 04:24:05 INFO     	 * (global step 36450: loss: 0.36758720874786377, lr: 1e-05
2024-01-06 04:24:11 INFO     	 * (global step 36500: loss: 0.6848690807819366, lr: 1e-05
2024-01-06 04:24:17 INFO     	 * (global step 36550: loss: 0.6473265886306763, lr: 1e-05
2024-01-06 04:24:23 INFO     	 * (global step 36600: loss: 0.44844384491443634, lr: 1e-05
2024-01-06 04:24:28 INFO     	 * (global step 36650: loss: 0.4787956476211548, lr: 1e-05
2024-01-06 04:24:34 INFO     	 * (global step 36700: loss: 0.6796190440654755, lr: 1e-05
2024-01-06 04:24:40 INFO     	 * (global step 36750: loss: 0.5919866561889648, lr: 1e-05
2024-01-06 04:24:46 INFO     	 * (global step 36800: loss: 0.7797320485115051, lr: 1e-05
2024-01-06 04:24:52 INFO     	 * (global step 36850: loss: 0.5771696865558624, lr: 1e-05
2024-01-06 04:24:57 INFO     	 * (global step 36900: loss: 0.5622853189706802, lr: 1e-05
2024-01-06 04:25:03 INFO     	 * (global step 36950: loss: 0.45018891990184784, lr: 1e-05
2024-01-06 04:25:09 INFO     	 * (global step 37000: loss: 0.733774870634079, lr: 1e-05
2024-01-06 04:25:15 INFO     	 * (global step 37050: loss: 0.5176222771406174, lr: 1e-05
2024-01-06 04:25:21 INFO     	 * (global step 37100: loss: 0.630479633808136, lr: 1e-05
2024-01-06 04:25:27 INFO     	 * (global step 37150: loss: 0.9732485115528107, lr: 1e-05
2024-01-06 04:25:33 INFO     	 * (global step 37200: loss: 0.7111556529998779, lr: 1e-05
2024-01-06 04:25:38 INFO     	 * (global step 37250: loss: 0.6967403590679169, lr: 1e-05
2024-01-06 04:25:44 INFO     	 * (global step 37300: loss: 0.49590666592121124, lr: 1e-05
2024-01-06 04:25:50 INFO     	 * (global step 37350: loss: 0.4193519949913025, lr: 1e-05
2024-01-06 04:25:56 INFO     	 * (global step 37400: loss: 0.8390300273895264, lr: 1e-05
2024-01-06 04:26:02 INFO     	 * (global step 37450: loss: 0.37464699149131775, lr: 1e-05
2024-01-06 04:26:07 INFO     	 * (global step 37500: loss: 0.8481850922107697, lr: 1e-05
2024-01-06 04:26:13 INFO     	 * (global step 37550: loss: 0.43693986535072327, lr: 1e-05
2024-01-06 04:26:19 INFO     	 * (global step 37600: loss: 0.6438153684139252, lr: 1e-05
2024-01-06 04:26:25 INFO     	 * (global step 37650: loss: 0.5695314407348633, lr: 1e-05
2024-01-06 04:26:31 INFO     	 * (global step 37700: loss: 0.5834369659423828, lr: 1e-05
2024-01-06 04:26:36 INFO     	 * (global step 37750: loss: 0.6359425187110901, lr: 1e-05
2024-01-06 04:26:42 INFO     	 * (global step 37800: loss: 0.5177274197340012, lr: 1e-05
2024-01-06 04:26:48 INFO     	 * (global step 37850: loss: 0.4606594443321228, lr: 1e-05
2024-01-06 04:26:54 INFO     	 * (global step 37900: loss: 0.4558347314596176, lr: 1e-05
2024-01-06 04:27:00 INFO     	 * (global step 37950: loss: 0.3727376163005829, lr: 1e-05
2024-01-06 04:27:05 INFO     	 * (global step 38000: loss: 0.6138138025999069, lr: 1e-05
2024-01-06 04:27:11 INFO     	 * (global step 38050: loss: 0.607703685760498, lr: 1e-05
2024-01-06 04:27:17 INFO     	 * (global step 38100: loss: 0.5156509727239609, lr: 1e-05
2024-01-06 04:27:23 INFO     	 * (global step 38150: loss: 0.623546838760376, lr: 1e-05
2024-01-06 04:27:29 INFO     	 * (global step 38200: loss: 0.5382312685251236, lr: 1e-05
2024-01-06 04:27:34 INFO     	 * (global step 38250: loss: 0.6414668709039688, lr: 1e-05
2024-01-06 04:27:40 INFO     	 * (global step 38300: loss: 0.6072471886873245, lr: 1e-05
2024-01-06 04:27:46 INFO     	 * (global step 38350: loss: 0.6851631104946136, lr: 1e-05
2024-01-06 04:27:52 INFO     	 * (global step 38400: loss: 0.8181673288345337, lr: 1e-05
2024-01-06 04:27:57 INFO     	 * (global step 38450: loss: 0.5651604235172272, lr: 1e-05
2024-01-06 04:28:03 INFO     	 * (global step 38500: loss: 0.5246692597866058, lr: 1e-05
2024-01-06 04:28:09 INFO     	 * (global step 38550: loss: 0.31662996858358383, lr: 1e-05
2024-01-06 04:28:15 INFO     	 * (global step 38600: loss: 0.33163776993751526, lr: 1e-05
2024-01-06 04:28:21 INFO     	 * (global step 38650: loss: 0.8493964672088623, lr: 1e-05
2024-01-06 04:28:27 INFO     	 * (global step 38700: loss: 0.5039090812206268, lr: 1e-05
2024-01-06 04:28:32 INFO     	 * (global step 38750: loss: 0.5488129109144211, lr: 1e-05
2024-01-06 04:28:38 INFO     	 * (global step 38800: loss: 0.32686948776245117, lr: 1e-05
2024-01-06 04:28:44 INFO     	 * (global step 38850: loss: 0.5438690930604935, lr: 1e-05
2024-01-06 04:28:50 INFO     	 * (global step 38900: loss: 0.4673031270503998, lr: 1e-05
2024-01-06 04:28:56 INFO     	 * (global step 38950: loss: 0.6968846619129181, lr: 1e-05
2024-01-06 04:29:01 INFO     	 * (global step 39000: loss: 0.489759236574173, lr: 1e-05
2024-01-06 04:29:07 INFO     	 * (global step 39050: loss: 0.5941114872694016, lr: 1e-05
2024-01-06 04:29:13 INFO     	 * (global step 39100: loss: 0.564902663230896, lr: 1e-05
2024-01-06 04:29:19 INFO     	 * (global step 39150: loss: 0.8254066705703735, lr: 1e-05
2024-01-06 04:29:25 INFO     	 * (global step 39200: loss: 0.4865853488445282, lr: 1e-05
2024-01-06 04:29:31 INFO     	 * (global step 39250: loss: 0.5817640423774719, lr: 1e-05
2024-01-06 04:29:36 INFO     	 * (global step 39300: loss: 0.528836727142334, lr: 1e-05
2024-01-06 04:29:42 INFO     	 * (global step 39350: loss: 0.510515570640564, lr: 1e-05
2024-01-06 04:29:48 INFO     	 * (global step 39400: loss: 0.48744721710681915, lr: 1e-05
2024-01-06 04:29:54 INFO     	 * (global step 39450: loss: 0.6087634265422821, lr: 1e-05
2024-01-06 04:30:00 INFO     	 * (global step 39500: loss: 0.7337682843208313, lr: 1e-05
2024-01-06 04:30:06 INFO     	 * (global step 39550: loss: 0.6905856132507324, lr: 1e-05
2024-01-06 04:30:11 INFO     	 * (global step 39600: loss: 0.8342635035514832, lr: 1e-05
2024-01-06 04:30:17 INFO     	 * (global step 39650: loss: 0.5755786001682281, lr: 1e-05
2024-01-06 04:30:23 INFO     	 * (global step 39700: loss: 0.5407524406909943, lr: 1e-05
2024-01-06 04:30:29 INFO     	 * (global step 39750: loss: 0.6762323677539825, lr: 1e-05
2024-01-06 04:30:34 INFO     	 * (global step 39800: loss: 1.0766067504882812, lr: 1e-05
2024-01-06 04:30:40 INFO     	 * (global step 39850: loss: 0.8779841363430023, lr: 1e-05
2024-01-06 04:30:46 INFO     	 * (global step 39900: loss: 0.4823095351457596, lr: 1e-05
2024-01-06 04:30:52 INFO     	 * (global step 39950: loss: 0.43377888202667236, lr: 1e-05
2024-01-06 04:30:58 INFO     	 * (global step 40000: loss: 0.8642740249633789, lr: 1e-05
2024-01-06 04:31:04 INFO     	 * (global step 40050: loss: 0.5641395747661591, lr: 1e-05
2024-01-06 04:31:09 INFO     	 * (global step 40100: loss: 0.6796468496322632, lr: 1e-05
2024-01-06 04:31:15 INFO     	 * (global step 40150: loss: 0.758635014295578, lr: 1e-05
2024-01-06 04:31:21 INFO     	 * (global step 40200: loss: 0.701718270778656, lr: 1e-05
2024-01-06 04:31:27 INFO     	 * (global step 40250: loss: 0.7785289585590363, lr: 1e-05
2024-01-06 04:31:33 INFO     	 * (global step 40300: loss: 0.4355941414833069, lr: 1e-05
2024-01-06 04:31:38 INFO     	 * (global step 40350: loss: 0.7141103446483612, lr: 1e-05
2024-01-06 04:31:44 INFO     	 * (global step 40400: loss: 0.4332519620656967, lr: 1e-05
2024-01-06 04:31:50 INFO     	 * (global step 40450: loss: 0.47257138788700104, lr: 1e-05
2024-01-06 04:31:54 INFO     [epoch 9/15] average loss: 0.588, lr: 1e-05
2024-01-06 04:31:54 INFO     saving model related files
2024-01-06 04:31:54 INFO     saving model
2024-01-06 04:31:54 INFO     saving tokenizer
2024-01-06 04:31:54 INFO     saving optimizer
2024-01-06 04:31:55 INFO     remove old optimizer files
2024-01-06 04:31:55 INFO     complete training: model ckpt was saved at small_recreated_ckpt/model_nrudfu
2024-01-06 04:31:56 INFO     ## 1st RUN (EVAL): Configuration 0/12 ##
2024-01-06 04:32:02 INFO     use spaCy answer extraction model: positionrank
2024-01-06 04:32:03 INFO     Model `small_recreated_ckpt/model_efnljo/epoch_10`
2024-01-06 04:32:03 INFO     	 * Num of GPU in use: 1
2024-01-06 04:32:03 INFO     	 * Prefix: True
2024-01-06 04:32:03 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 04:32:04 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 04:39:35 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 04:46:10 INFO     	Bleu_1: 0.3652462365712084
2024-01-06 04:46:10 INFO     	Bleu_2: 0.22100678009594912
2024-01-06 04:46:10 INFO     	Bleu_3: 0.13856435660876815
2024-01-06 04:46:10 INFO     	Bleu_4: 0.09485688097694292
2024-01-06 04:46:10 INFO     	Bleu_1: 0.3297239254568611
2024-01-06 04:46:10 INFO     	Bleu_2: 0.1956679022762727
2024-01-06 04:46:10 INFO     	Bleu_3: 0.12024704183003149
2024-01-06 04:46:10 INFO     	Bleu_4: 0.08102672275100357
2024-01-06 04:46:11 INFO     ## 1st RUN (EVAL): Configuration 1/12 ##
2024-01-06 04:46:15 INFO     use spaCy answer extraction model: positionrank
2024-01-06 04:46:16 INFO     Model `small_recreated_ckpt/model_eszyci/epoch_10`
2024-01-06 04:46:16 INFO     	 * Num of GPU in use: 1
2024-01-06 04:46:16 INFO     	 * Prefix: True
2024-01-06 04:46:16 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 04:46:17 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 04:53:56 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 05:00:45 INFO     	Bleu_1: 0.3656599515812747
2024-01-06 05:00:45 INFO     	Bleu_2: 0.22170366533268474
2024-01-06 05:00:45 INFO     	Bleu_3: 0.13866323899749725
2024-01-06 05:00:45 INFO     	Bleu_4: 0.09466239682555477
2024-01-06 05:00:46 INFO     	Bleu_1: 0.3284857600500043
2024-01-06 05:00:46 INFO     	Bleu_2: 0.19532700895690952
2024-01-06 05:00:46 INFO     	Bleu_3: 0.12007665492115234
2024-01-06 05:00:46 INFO     	Bleu_4: 0.08094099899533204
2024-01-06 05:00:46 INFO     ## 1st RUN (EVAL): Configuration 2/12 ##
2024-01-06 05:00:52 INFO     use spaCy answer extraction model: positionrank
2024-01-06 05:00:52 INFO     Model `small_recreated_ckpt/model_dpyopu/epoch_10`
2024-01-06 05:00:52 INFO     	 * Num of GPU in use: 1
2024-01-06 05:00:52 INFO     	 * Prefix: True
2024-01-06 05:00:52 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 05:00:53 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 05:08:21 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 05:14:56 INFO     	Bleu_1: 0.3652462365712084
2024-01-06 05:14:56 INFO     	Bleu_2: 0.22100678009594912
2024-01-06 05:14:56 INFO     	Bleu_3: 0.13856435660876815
2024-01-06 05:14:56 INFO     	Bleu_4: 0.09485688097694292
2024-01-06 05:14:57 INFO     	Bleu_1: 0.3297239254568611
2024-01-06 05:14:57 INFO     	Bleu_2: 0.1956679022762727
2024-01-06 05:14:57 INFO     	Bleu_3: 0.12024704183003149
2024-01-06 05:14:57 INFO     	Bleu_4: 0.08102672275100357
2024-01-06 05:14:57 INFO     ## 1st RUN (EVAL): Configuration 3/12 ##
2024-01-06 05:15:02 INFO     use spaCy answer extraction model: positionrank
2024-01-06 05:15:03 INFO     Model `small_recreated_ckpt/model_mzgdpa/epoch_10`
2024-01-06 05:15:03 INFO     	 * Num of GPU in use: 1
2024-01-06 05:15:03 INFO     	 * Prefix: True
2024-01-06 05:15:03 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 05:15:04 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 05:22:41 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 05:29:28 INFO     	Bleu_1: 0.3656599515812747
2024-01-06 05:29:28 INFO     	Bleu_2: 0.22170366533268474
2024-01-06 05:29:28 INFO     	Bleu_3: 0.13866323899749725
2024-01-06 05:29:28 INFO     	Bleu_4: 0.09466239682555477
2024-01-06 05:29:29 INFO     	Bleu_1: 0.3284857600500043
2024-01-06 05:29:29 INFO     	Bleu_2: 0.19532700895690952
2024-01-06 05:29:29 INFO     	Bleu_3: 0.12007665492115234
2024-01-06 05:29:29 INFO     	Bleu_4: 0.08094099899533204
2024-01-06 05:29:29 INFO     ## 1st RUN (EVAL): Configuration 4/12 ##
2024-01-06 05:29:35 INFO     use spaCy answer extraction model: positionrank
2024-01-06 05:29:35 INFO     Model `small_recreated_ckpt/model_mntyya/epoch_10`
2024-01-06 05:29:35 INFO     	 * Num of GPU in use: 1
2024-01-06 05:29:35 INFO     	 * Prefix: True
2024-01-06 05:29:35 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 05:29:36 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 05:36:55 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 05:43:21 INFO     	Bleu_1: 0.36118446513533614
2024-01-06 05:43:21 INFO     	Bleu_2: 0.2175015057348699
2024-01-06 05:43:21 INFO     	Bleu_3: 0.1357454767021019
2024-01-06 05:43:21 INFO     	Bleu_4: 0.09272244472764085
2024-01-06 05:43:22 INFO     	Bleu_1: 0.32688014564168116
2024-01-06 05:43:22 INFO     	Bleu_2: 0.193862538013595
2024-01-06 05:43:22 INFO     	Bleu_3: 0.11934232757668609
2024-01-06 05:43:22 INFO     	Bleu_4: 0.08067450529654129
2024-01-06 05:43:22 INFO     ## 1st RUN (EVAL): Configuration 5/12 ##
2024-01-06 05:43:27 INFO     use spaCy answer extraction model: positionrank
2024-01-06 05:43:28 INFO     Model `small_recreated_ckpt/model_woixzh/epoch_10`
2024-01-06 05:43:28 INFO     	 * Num of GPU in use: 1
2024-01-06 05:43:28 INFO     	 * Prefix: True
2024-01-06 05:43:28 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 05:43:29 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 05:50:40 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 05:56:55 INFO     	Bleu_1: 0.3638110882039056
2024-01-06 05:56:55 INFO     	Bleu_2: 0.21998499414803557
2024-01-06 05:56:55 INFO     	Bleu_3: 0.13780389668379056
2024-01-06 05:56:55 INFO     	Bleu_4: 0.09445276580206216
2024-01-06 05:56:56 INFO     	Bleu_1: 0.3278093064738722
2024-01-06 05:56:56 INFO     	Bleu_2: 0.19461898458343158
2024-01-06 05:56:56 INFO     	Bleu_3: 0.11970092055324927
2024-01-06 05:56:56 INFO     	Bleu_4: 0.08070071062767081
2024-01-06 05:56:56 INFO     ## 1st RUN (EVAL): Configuration 6/12 ##
2024-01-06 05:57:01 INFO     use spaCy answer extraction model: positionrank
2024-01-06 05:57:02 INFO     Model `small_recreated_ckpt/model_sdkaaa/epoch_10`
2024-01-06 05:57:02 INFO     	 * Num of GPU in use: 1
2024-01-06 05:57:02 INFO     	 * Prefix: True
2024-01-06 05:57:02 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 05:57:03 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 06:03:54 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 06:09:48 INFO     	Bleu_1: 0.36118446513533614
2024-01-06 06:09:48 INFO     	Bleu_2: 0.2175015057348699
2024-01-06 06:09:48 INFO     	Bleu_3: 0.1357454767021019
2024-01-06 06:09:48 INFO     	Bleu_4: 0.09272244472764085
2024-01-06 06:09:48 INFO     	Bleu_1: 0.32688014564168116
2024-01-06 06:09:48 INFO     	Bleu_2: 0.193862538013595
2024-01-06 06:09:48 INFO     	Bleu_3: 0.11934232757668609
2024-01-06 06:09:48 INFO     	Bleu_4: 0.08067450529654129
2024-01-06 06:09:48 INFO     ## 1st RUN (EVAL): Configuration 7/12 ##
2024-01-06 06:09:53 INFO     use spaCy answer extraction model: positionrank
2024-01-06 06:09:54 INFO     Model `small_recreated_ckpt/model_uramvg/epoch_10`
2024-01-06 06:09:54 INFO     	 * Num of GPU in use: 1
2024-01-06 06:09:54 INFO     	 * Prefix: True
2024-01-06 06:09:54 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 06:09:55 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 06:16:46 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 06:22:48 INFO     	Bleu_1: 0.3638110882039056
2024-01-06 06:22:48 INFO     	Bleu_2: 0.21998499414803557
2024-01-06 06:22:48 INFO     	Bleu_3: 0.13780389668379056
2024-01-06 06:22:48 INFO     	Bleu_4: 0.09445276580206216
2024-01-06 06:22:49 INFO     	Bleu_1: 0.3278093064738722
2024-01-06 06:22:49 INFO     	Bleu_2: 0.19461898458343158
2024-01-06 06:22:49 INFO     	Bleu_3: 0.11970092055324927
2024-01-06 06:22:49 INFO     	Bleu_4: 0.08070071062767081
2024-01-06 06:22:49 INFO     ## 1st RUN (EVAL): Configuration 8/12 ##
2024-01-06 06:22:55 INFO     use spaCy answer extraction model: positionrank
2024-01-06 06:22:55 INFO     Model `small_recreated_ckpt/model_nxaqhy/epoch_10`
2024-01-06 06:22:55 INFO     	 * Num of GPU in use: 1
2024-01-06 06:22:55 INFO     	 * Prefix: True
2024-01-06 06:22:55 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 06:22:56 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 06:29:25 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 06:35:05 INFO     	Bleu_1: 0.34328607653296256
2024-01-06 06:35:05 INFO     	Bleu_2: 0.20273012651413985
2024-01-06 06:35:05 INFO     	Bleu_3: 0.12397426892733551
2024-01-06 06:35:05 INFO     	Bleu_4: 0.08383197685032773
2024-01-06 06:35:06 INFO     	Bleu_1: 0.30847729724478734
2024-01-06 06:35:06 INFO     	Bleu_2: 0.17760453336495843
2024-01-06 06:35:06 INFO     	Bleu_3: 0.10577491314355876
2024-01-06 06:35:06 INFO     	Bleu_4: 0.07047431832330289
2024-01-06 06:35:06 INFO     ## 1st RUN (EVAL): Configuration 9/12 ##
2024-01-06 06:35:12 INFO     use spaCy answer extraction model: positionrank
2024-01-06 06:35:12 INFO     Model `small_recreated_ckpt/model_oprhlh/epoch_10`
2024-01-06 06:35:12 INFO     	 * Num of GPU in use: 1
2024-01-06 06:35:12 INFO     	 * Prefix: True
2024-01-06 06:35:12 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 06:35:13 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 06:41:42 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 06:47:23 INFO     	Bleu_1: 0.34801259210571633
2024-01-06 06:47:23 INFO     	Bleu_2: 0.20609784153736702
2024-01-06 06:47:23 INFO     	Bleu_3: 0.12643896253114084
2024-01-06 06:47:23 INFO     	Bleu_4: 0.08550750582846844
2024-01-06 06:47:24 INFO     	Bleu_1: 0.3134731563331428
2024-01-06 06:47:24 INFO     	Bleu_2: 0.18261756154763556
2024-01-06 06:47:24 INFO     	Bleu_3: 0.11035749127256204
2024-01-06 06:47:24 INFO     	Bleu_4: 0.07421299981338561
2024-01-06 06:47:24 INFO     ## 1st RUN (EVAL): Configuration 10/12 ##
2024-01-06 06:47:29 INFO     use spaCy answer extraction model: positionrank
2024-01-06 06:47:30 INFO     Model `small_recreated_ckpt/model_vhyoja/epoch_10`
2024-01-06 06:47:30 INFO     	 * Num of GPU in use: 1
2024-01-06 06:47:30 INFO     	 * Prefix: True
2024-01-06 06:47:30 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 06:47:31 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 06:54:00 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 06:59:40 INFO     	Bleu_1: 0.34328607653296256
2024-01-06 06:59:40 INFO     	Bleu_2: 0.20273012651413985
2024-01-06 06:59:40 INFO     	Bleu_3: 0.12397426892733551
2024-01-06 06:59:40 INFO     	Bleu_4: 0.08383197685032773
2024-01-06 06:59:41 INFO     	Bleu_1: 0.30847729724478734
2024-01-06 06:59:41 INFO     	Bleu_2: 0.17760453336495843
2024-01-06 06:59:41 INFO     	Bleu_3: 0.10577491314355876
2024-01-06 06:59:41 INFO     	Bleu_4: 0.07047431832330289
2024-01-06 06:59:41 INFO     ## 1st RUN (EVAL): Configuration 11/12 ##
2024-01-06 06:59:47 INFO     use spaCy answer extraction model: positionrank
2024-01-06 06:59:47 INFO     Model `small_recreated_ckpt/model_nrudfu/epoch_10`
2024-01-06 06:59:47 INFO     	 * Num of GPU in use: 1
2024-01-06 06:59:47 INFO     	 * Prefix: True
2024-01-06 06:59:47 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 06:59:48 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 07:06:17 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 07:11:56 INFO     	Bleu_1: 0.34801259210571633
2024-01-06 07:11:56 INFO     	Bleu_2: 0.20609784153736702
2024-01-06 07:11:56 INFO     	Bleu_3: 0.12643896253114084
2024-01-06 07:11:56 INFO     	Bleu_4: 0.08550750582846844
2024-01-06 07:11:57 INFO     	Bleu_1: 0.3134731563331428
2024-01-06 07:11:57 INFO     	Bleu_2: 0.18261756154763556
2024-01-06 07:11:57 INFO     	Bleu_3: 0.11035749127256204
2024-01-06 07:11:57 INFO     	Bleu_4: 0.07421299981338561
2024-01-06 07:11:57 INFO     1st RUN RESULTS (validation/Bleu_4)
2024-01-06 07:11:57 INFO     	 * rank: 0 | metric: 0.095 | model: small_recreated_ckpt/model_efnljo/epoch_10 |
2024-01-06 07:11:57 INFO     	 * rank: 1 | metric: 0.095 | model: small_recreated_ckpt/model_dpyopu/epoch_10 |
2024-01-06 07:11:57 INFO     	 * rank: 2 | metric: 0.095 | model: small_recreated_ckpt/model_eszyci/epoch_10 |
2024-01-06 07:11:57 INFO     	 * rank: 3 | metric: 0.095 | model: small_recreated_ckpt/model_mzgdpa/epoch_10 |
2024-01-06 07:11:57 INFO     	 * rank: 4 | metric: 0.094 | model: small_recreated_ckpt/model_woixzh/epoch_10 |
2024-01-06 07:11:57 INFO     	 * rank: 5 | metric: 0.094 | model: small_recreated_ckpt/model_uramvg/epoch_10 |
2024-01-06 07:11:57 INFO     	 * rank: 6 | metric: 0.093 | model: small_recreated_ckpt/model_mntyya/epoch_10 |
2024-01-06 07:11:57 INFO     	 * rank: 7 | metric: 0.093 | model: small_recreated_ckpt/model_sdkaaa/epoch_10 |
2024-01-06 07:11:57 INFO     	 * rank: 8 | metric: 0.086 | model: small_recreated_ckpt/model_oprhlh/epoch_10 |
2024-01-06 07:11:57 INFO     	 * rank: 9 | metric: 0.086 | model: small_recreated_ckpt/model_nrudfu/epoch_10 |
2024-01-06 07:11:57 INFO     	 * rank: 10 | metric: 0.084 | model: small_recreated_ckpt/model_nxaqhy/epoch_10 |
2024-01-06 07:11:57 INFO     	 * rank: 11 | metric: 0.084 | model: small_recreated_ckpt/model_vhyoja/epoch_10 |
2024-01-06 07:11:57 INFO     ## 2nd RUN: Configuration 0/5: validation/Bleu_4 = 0.09485688097694292
2024-01-06 07:11:57 INFO     initialize model trainer
2024-01-06 07:11:57 INFO     load config from existing checkpoint at small_recreated_ckpt/model_efnljo
2024-01-06 07:11:57 INFO     hyperparameters
2024-01-06 07:11:57 INFO     	 * dataset_path: lmqg/qag_squad
2024-01-06 07:11:57 INFO     	 * dataset_name: default
2024-01-06 07:11:57 INFO     	 * input_types: ['paragraph']
2024-01-06 07:11:57 INFO     	 * output_types: ['questions_answers']
2024-01-06 07:11:57 INFO     	 * prefix_types: ['qag']
2024-01-06 07:11:57 INFO     	 * model: t5-small
2024-01-06 07:11:57 INFO     	 * max_length: 512
2024-01-06 07:11:57 INFO     	 * max_length_output: 256
2024-01-06 07:11:57 INFO     	 * epoch: 15
2024-01-06 07:11:57 INFO     	 * batch: 2
2024-01-06 07:11:57 INFO     	 * lr: 0.0001
2024-01-06 07:11:57 INFO     	 * fp16: False
2024-01-06 07:11:57 INFO     	 * random_seed: 1
2024-01-06 07:11:57 INFO     	 * gradient_accumulation_steps: 4
2024-01-06 07:11:57 INFO     	 * label_smoothing: 0.15
2024-01-06 07:11:57 INFO     load checkpoint from small_recreated_ckpt/model_efnljo/epoch_10
2024-01-06 07:11:58 INFO     use spaCy answer extraction model: positionrank
2024-01-06 07:11:58 INFO     Model `small_recreated_ckpt/model_efnljo/epoch_10`
2024-01-06 07:11:58 INFO     	 * Num of GPU in use: 1
2024-01-06 07:11:58 INFO     	 * Prefix: True
2024-01-06 07:11:58 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 07:11:58 INFO     load optimizer from small_recreated_ckpt/model_efnljo/optimizers/optimizer.10.pt
2024-01-06 07:11:58 INFO     optimizer is loading on cuda
2024-01-06 07:12:03 INFO     dataset preprocessing
2024-01-06 07:12:04 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featurelmqg/qag_squad/t5-small.512.256.paragraph.questions_answers.train.qag.pkl
2024-01-06 07:12:06 INFO     start model training
2024-01-06 07:12:17 INFO     	 * (global step 50: loss: 2.8514530658721924, lr: 0.0001
2024-01-06 07:12:29 INFO     	 * (global step 100: loss: 2.7319981455802917, lr: 0.0001
2024-01-06 07:12:40 INFO     	 * (global step 150: loss: 2.7267946600914, lr: 0.0001
2024-01-06 07:12:51 INFO     	 * (global step 200: loss: 2.757105529308319, lr: 0.0001
2024-01-06 07:13:02 INFO     	 * (global step 250: loss: 2.5893331170082092, lr: 0.0001
2024-01-06 07:13:13 INFO     	 * (global step 300: loss: 2.6291110515594482, lr: 0.0001
2024-01-06 07:13:24 INFO     	 * (global step 350: loss: 2.6275643706321716, lr: 0.0001
2024-01-06 07:13:36 INFO     	 * (global step 400: loss: 2.577884614467621, lr: 0.0001
2024-01-06 07:13:47 INFO     	 * (global step 450: loss: 2.6179383993148804, lr: 0.0001
2024-01-06 07:13:58 INFO     	 * (global step 500: loss: 2.5242316126823425, lr: 0.0001
2024-01-06 07:14:09 INFO     	 * (global step 550: loss: 2.6404945850372314, lr: 0.0001
2024-01-06 07:14:20 INFO     	 * (global step 600: loss: 2.5571064352989197, lr: 0.0001
2024-01-06 07:14:32 INFO     	 * (global step 650: loss: 2.582253634929657, lr: 0.0001
2024-01-06 07:14:43 INFO     	 * (global step 700: loss: 2.6016594767570496, lr: 0.0001
2024-01-06 07:14:54 INFO     	 * (global step 750: loss: 2.5424752235412598, lr: 0.0001
2024-01-06 07:15:05 INFO     	 * (global step 800: loss: 2.556935727596283, lr: 0.0001
2024-01-06 07:15:17 INFO     	 * (global step 850: loss: 2.5913631916046143, lr: 0.0001
2024-01-06 07:15:28 INFO     	 * (global step 900: loss: 2.513096511363983, lr: 0.0001
2024-01-06 07:15:39 INFO     	 * (global step 950: loss: 2.5536341667175293, lr: 0.0001
2024-01-06 07:15:50 INFO     	 * (global step 1000: loss: 2.624317467212677, lr: 0.0001
2024-01-06 07:16:01 INFO     	 * (global step 1050: loss: 2.5737319588661194, lr: 0.0001
2024-01-06 07:16:13 INFO     	 * (global step 1100: loss: 2.483689606189728, lr: 0.0001
2024-01-06 07:16:24 INFO     	 * (global step 1150: loss: 2.7524918913841248, lr: 0.0001
2024-01-06 07:16:35 INFO     	 * (global step 1200: loss: 2.414242208003998, lr: 0.0001
2024-01-06 07:16:46 INFO     	 * (global step 1250: loss: 2.4841914772987366, lr: 0.0001
2024-01-06 07:16:57 INFO     	 * (global step 1300: loss: 2.5421630144119263, lr: 0.0001
2024-01-06 07:17:09 INFO     	 * (global step 1350: loss: 2.5597140789031982, lr: 0.0001
2024-01-06 07:17:20 INFO     	 * (global step 1400: loss: 2.512950360774994, lr: 0.0001
2024-01-06 07:17:31 INFO     	 * (global step 1450: loss: 2.6334478855133057, lr: 0.0001
2024-01-06 07:17:42 INFO     	 * (global step 1500: loss: 2.5693905353546143, lr: 0.0001
2024-01-06 07:17:53 INFO     	 * (global step 1550: loss: 2.4166781902313232, lr: 0.0001
2024-01-06 07:18:05 INFO     	 * (global step 1600: loss: 2.6019437313079834, lr: 0.0001
2024-01-06 07:18:16 INFO     	 * (global step 1650: loss: 2.5035961270332336, lr: 0.0001
2024-01-06 07:18:27 INFO     	 * (global step 1700: loss: 2.58545845746994, lr: 0.0001
2024-01-06 07:18:38 INFO     	 * (global step 1750: loss: 2.5241222381591797, lr: 0.0001
2024-01-06 07:18:49 INFO     	 * (global step 1800: loss: 2.6611828804016113, lr: 0.0001
2024-01-06 07:19:01 INFO     	 * (global step 1850: loss: 2.4874521493911743, lr: 0.0001
2024-01-06 07:19:12 INFO     	 * (global step 1900: loss: 2.570656955242157, lr: 0.0001
2024-01-06 07:19:23 INFO     	 * (global step 1950: loss: 2.576488435268402, lr: 0.0001
2024-01-06 07:19:34 INFO     	 * (global step 2000: loss: 2.605347454547882, lr: 0.0001
2024-01-06 07:19:40 INFO     [epoch 10/15] average loss: 2.597, lr: 0.0001
2024-01-06 07:19:40 INFO     saving model related files
2024-01-06 07:19:40 INFO     saving model
2024-01-06 07:19:40 INFO     saving tokenizer
2024-01-06 07:19:40 INFO     saving optimizer
2024-01-06 07:19:41 INFO     remove old optimizer files
2024-01-06 07:19:47 INFO     	 * (global step 2050: loss: 2.5177807211875916, lr: 0.0001
2024-01-06 07:19:58 INFO     	 * (global step 2100: loss: 2.647402584552765, lr: 0.0001
2024-01-06 07:20:09 INFO     	 * (global step 2150: loss: 2.660293698310852, lr: 0.0001
2024-01-06 07:20:20 INFO     	 * (global step 2200: loss: 2.4673768877983093, lr: 0.0001
2024-01-06 07:20:32 INFO     	 * (global step 2250: loss: 2.485787570476532, lr: 0.0001
2024-01-06 07:20:43 INFO     	 * (global step 2300: loss: 2.498747944831848, lr: 0.0001
2024-01-06 07:20:54 INFO     	 * (global step 2350: loss: 2.370715916156769, lr: 0.0001
2024-01-06 07:21:05 INFO     	 * (global step 2400: loss: 2.5890870690345764, lr: 0.0001
2024-01-06 07:21:16 INFO     	 * (global step 2450: loss: 2.5330026149749756, lr: 0.0001
2024-01-06 07:21:28 INFO     	 * (global step 2500: loss: 2.465806782245636, lr: 0.0001
2024-01-06 07:21:39 INFO     	 * (global step 2550: loss: 2.4403860569000244, lr: 0.0001
2024-01-06 07:21:50 INFO     	 * (global step 2600: loss: 2.375626862049103, lr: 0.0001
2024-01-06 07:22:01 INFO     	 * (global step 2650: loss: 2.606117069721222, lr: 0.0001
2024-01-06 07:22:12 INFO     	 * (global step 2700: loss: 2.529555857181549, lr: 0.0001
2024-01-06 07:22:24 INFO     	 * (global step 2750: loss: 2.547475278377533, lr: 0.0001
2024-01-06 07:22:35 INFO     	 * (global step 2800: loss: 2.6673380732536316, lr: 0.0001
2024-01-06 07:22:46 INFO     	 * (global step 2850: loss: 2.622064530849457, lr: 0.0001
2024-01-06 07:22:57 INFO     	 * (global step 2900: loss: 2.498853623867035, lr: 0.0001
2024-01-06 07:23:08 INFO     	 * (global step 2950: loss: 2.5351304411888123, lr: 0.0001
2024-01-06 07:23:20 INFO     	 * (global step 3000: loss: 2.3724851608276367, lr: 0.0001
2024-01-06 07:23:31 INFO     	 * (global step 3050: loss: 2.388146162033081, lr: 0.0001
2024-01-06 07:23:42 INFO     	 * (global step 3100: loss: 2.463505983352661, lr: 0.0001
2024-01-06 07:23:53 INFO     	 * (global step 3150: loss: 2.562113106250763, lr: 0.0001
2024-01-06 07:24:04 INFO     	 * (global step 3200: loss: 2.5537323355674744, lr: 0.0001
2024-01-06 07:24:15 INFO     	 * (global step 3250: loss: 2.5489548444747925, lr: 0.0001
2024-01-06 07:24:27 INFO     	 * (global step 3300: loss: 2.538713216781616, lr: 0.0001
2024-01-06 07:24:38 INFO     	 * (global step 3350: loss: 2.428811550140381, lr: 0.0001
2024-01-06 07:24:49 INFO     	 * (global step 3400: loss: 2.4424774050712585, lr: 0.0001
2024-01-06 07:25:00 INFO     	 * (global step 3450: loss: 2.5314122438430786, lr: 0.0001
2024-01-06 07:25:11 INFO     	 * (global step 3500: loss: 2.838087558746338, lr: 0.0001
2024-01-06 07:25:23 INFO     	 * (global step 3550: loss: 2.6018437147140503, lr: 0.0001
2024-01-06 07:25:34 INFO     	 * (global step 3600: loss: 2.4402130246162415, lr: 0.0001
2024-01-06 07:25:45 INFO     	 * (global step 3650: loss: 2.473396122455597, lr: 0.0001
2024-01-06 07:25:56 INFO     	 * (global step 3700: loss: 2.454344928264618, lr: 0.0001
2024-01-06 07:26:08 INFO     	 * (global step 3750: loss: 2.4136118292808533, lr: 0.0001
2024-01-06 07:26:19 INFO     	 * (global step 3800: loss: 2.4854238629341125, lr: 0.0001
2024-01-06 07:26:30 INFO     	 * (global step 3850: loss: 2.434668719768524, lr: 0.0001
2024-01-06 07:26:41 INFO     	 * (global step 3900: loss: 2.526122570037842, lr: 0.0001
2024-01-06 07:26:52 INFO     	 * (global step 3950: loss: 2.4404144883155823, lr: 0.0001
2024-01-06 07:27:04 INFO     	 * (global step 4000: loss: 2.521625339984894, lr: 0.0001
2024-01-06 07:27:14 INFO     [epoch 11/15] average loss: 2.522, lr: 0.0001
2024-01-06 07:27:14 INFO     saving model related files
2024-01-06 07:27:14 INFO     saving model
2024-01-06 07:27:15 INFO     saving tokenizer
2024-01-06 07:27:15 INFO     saving optimizer
2024-01-06 07:27:16 INFO     remove old optimizer files
2024-01-06 07:27:16 INFO     	 * (global step 4050: loss: 2.4979236125946045, lr: 0.0001
2024-01-06 07:27:27 INFO     	 * (global step 4100: loss: 2.507648289203644, lr: 0.0001
2024-01-06 07:27:39 INFO     	 * (global step 4150: loss: 2.589064300060272, lr: 0.0001
2024-01-06 07:27:50 INFO     	 * (global step 4200: loss: 2.4646313786506653, lr: 0.0001
2024-01-06 07:28:01 INFO     	 * (global step 4250: loss: 2.430139482021332, lr: 0.0001
2024-01-06 07:28:12 INFO     	 * (global step 4300: loss: 2.497511863708496, lr: 0.0001
2024-01-06 07:28:23 INFO     	 * (global step 4350: loss: 2.469029188156128, lr: 0.0001
2024-01-06 07:28:35 INFO     	 * (global step 4400: loss: 2.5191030502319336, lr: 0.0001
2024-01-06 07:28:46 INFO     	 * (global step 4450: loss: 2.496702194213867, lr: 0.0001
2024-01-06 07:28:57 INFO     	 * (global step 4500: loss: 2.464249551296234, lr: 0.0001
2024-01-06 07:29:08 INFO     	 * (global step 4550: loss: 2.4201971888542175, lr: 0.0001
2024-01-06 07:29:19 INFO     	 * (global step 4600: loss: 2.379554808139801, lr: 0.0001
2024-01-06 07:29:31 INFO     	 * (global step 4650: loss: 2.361907422542572, lr: 0.0001
2024-01-06 07:29:42 INFO     	 * (global step 4700: loss: 2.5597644448280334, lr: 0.0001
2024-01-06 07:29:53 INFO     	 * (global step 4750: loss: 2.393459916114807, lr: 0.0001
2024-01-06 07:30:04 INFO     	 * (global step 4800: loss: 2.4008126854896545, lr: 0.0001
2024-01-06 07:30:15 INFO     	 * (global step 4850: loss: 2.411973476409912, lr: 0.0001
2024-01-06 07:30:27 INFO     	 * (global step 4900: loss: 2.6066134572029114, lr: 0.0001
2024-01-06 07:30:38 INFO     	 * (global step 4950: loss: 2.574750542640686, lr: 0.0001
2024-01-06 07:30:49 INFO     	 * (global step 5000: loss: 2.555602729320526, lr: 0.0001
2024-01-06 07:31:00 INFO     	 * (global step 5050: loss: 2.5309006571769714, lr: 0.0001
2024-01-06 07:31:11 INFO     	 * (global step 5100: loss: 2.4983401894569397, lr: 0.0001
2024-01-06 07:31:23 INFO     	 * (global step 5150: loss: 2.4594624042510986, lr: 0.0001
2024-01-06 07:31:34 INFO     	 * (global step 5200: loss: 2.551855444908142, lr: 0.0001
2024-01-06 07:31:45 INFO     	 * (global step 5250: loss: 2.524778127670288, lr: 0.0001
2024-01-06 07:31:56 INFO     	 * (global step 5300: loss: 2.4516363739967346, lr: 0.0001
2024-01-06 07:32:08 INFO     	 * (global step 5350: loss: 2.352746903896332, lr: 0.0001
2024-01-06 07:32:19 INFO     	 * (global step 5400: loss: 2.4236230850219727, lr: 0.0001
2024-01-06 07:32:30 INFO     	 * (global step 5450: loss: 2.4326928853988647, lr: 0.0001
2024-01-06 07:32:41 INFO     	 * (global step 5500: loss: 2.3920240998268127, lr: 0.0001
2024-01-06 07:32:53 INFO     	 * (global step 5550: loss: 2.5138378739356995, lr: 0.0001
2024-01-06 07:33:04 INFO     	 * (global step 5600: loss: 2.371652662754059, lr: 0.0001
2024-01-06 07:33:15 INFO     	 * (global step 5650: loss: 2.4483855962753296, lr: 0.0001
2024-01-06 07:33:26 INFO     	 * (global step 5700: loss: 2.4082234501838684, lr: 0.0001
2024-01-06 07:33:37 INFO     	 * (global step 5750: loss: 2.568796217441559, lr: 0.0001
2024-01-06 07:33:49 INFO     	 * (global step 5800: loss: 2.617469608783722, lr: 0.0001
2024-01-06 07:34:00 INFO     	 * (global step 5850: loss: 2.3919951915740967, lr: 0.0001
2024-01-06 07:34:11 INFO     	 * (global step 5900: loss: 2.5269925594329834, lr: 0.0001
2024-01-06 07:34:22 INFO     	 * (global step 5950: loss: 2.5905686616897583, lr: 0.0001
2024-01-06 07:34:34 INFO     	 * (global step 6000: loss: 2.428784728050232, lr: 0.0001
2024-01-06 07:34:45 INFO     	 * (global step 6050: loss: 2.5428532361984253, lr: 0.0001
2024-01-06 07:34:50 INFO     [epoch 12/15] average loss: 2.501, lr: 0.0001
2024-01-06 07:34:50 INFO     saving model related files
2024-01-06 07:34:50 INFO     saving model
2024-01-06 07:34:50 INFO     saving tokenizer
2024-01-06 07:34:50 INFO     saving optimizer
2024-01-06 07:34:51 INFO     remove old optimizer files
2024-01-06 07:34:58 INFO     	 * (global step 6100: loss: 2.476135551929474, lr: 0.0001
2024-01-06 07:35:09 INFO     	 * (global step 6150: loss: 2.4945993423461914, lr: 0.0001
2024-01-06 07:35:20 INFO     	 * (global step 6200: loss: 2.5277273058891296, lr: 0.0001
2024-01-06 07:35:31 INFO     	 * (global step 6250: loss: 2.5014467239379883, lr: 0.0001
2024-01-06 07:35:42 INFO     	 * (global step 6300: loss: 2.5589495301246643, lr: 0.0001
2024-01-06 07:35:54 INFO     	 * (global step 6350: loss: 2.5566030144691467, lr: 0.0001
2024-01-06 07:36:05 INFO     	 * (global step 6400: loss: 2.4089791774749756, lr: 0.0001
2024-01-06 07:36:16 INFO     	 * (global step 6450: loss: 2.551516056060791, lr: 0.0001
2024-01-06 07:36:27 INFO     	 * (global step 6500: loss: 2.4793670177459717, lr: 0.0001
2024-01-06 07:36:38 INFO     	 * (global step 6550: loss: 2.5460219979286194, lr: 0.0001
2024-01-06 07:36:49 INFO     	 * (global step 6600: loss: 2.4299078583717346, lr: 0.0001
2024-01-06 07:37:01 INFO     	 * (global step 6650: loss: 2.525968909263611, lr: 0.0001
2024-01-06 07:37:12 INFO     	 * (global step 6700: loss: 2.564257860183716, lr: 0.0001
2024-01-06 07:37:23 INFO     	 * (global step 6750: loss: 2.6751646995544434, lr: 0.0001
2024-01-06 07:37:34 INFO     	 * (global step 6800: loss: 2.5087878108024597, lr: 0.0001
2024-01-06 07:37:45 INFO     	 * (global step 6850: loss: 2.424136757850647, lr: 0.0001
2024-01-06 07:37:57 INFO     	 * (global step 6900: loss: 2.4961461424827576, lr: 0.0001
2024-01-06 07:38:08 INFO     	 * (global step 6950: loss: 2.5311238169670105, lr: 0.0001
2024-01-06 07:38:19 INFO     	 * (global step 7000: loss: 2.397995412349701, lr: 0.0001
2024-01-06 07:38:30 INFO     	 * (global step 7050: loss: 2.5900222659111023, lr: 0.0001
2024-01-06 07:38:41 INFO     	 * (global step 7100: loss: 2.4814088940620422, lr: 0.0001
2024-01-06 07:38:53 INFO     	 * (global step 7150: loss: 2.396975338459015, lr: 0.0001
2024-01-06 07:39:04 INFO     	 * (global step 7200: loss: 2.5668663382530212, lr: 0.0001
2024-01-06 07:39:15 INFO     	 * (global step 7250: loss: 2.4377811551094055, lr: 0.0001
2024-01-06 07:39:26 INFO     	 * (global step 7300: loss: 2.4676372408866882, lr: 0.0001
2024-01-06 07:39:37 INFO     	 * (global step 7350: loss: 2.482436418533325, lr: 0.0001
2024-01-06 07:39:49 INFO     	 * (global step 7400: loss: 2.5068607926368713, lr: 0.0001
2024-01-06 07:40:00 INFO     	 * (global step 7450: loss: 2.349426567554474, lr: 0.0001
2024-01-06 07:40:11 INFO     	 * (global step 7500: loss: 2.4085798263549805, lr: 0.0001
2024-01-06 07:40:22 INFO     	 * (global step 7550: loss: 2.502342462539673, lr: 0.0001
2024-01-06 07:40:33 INFO     	 * (global step 7600: loss: 2.4560717940330505, lr: 0.0001
2024-01-06 07:40:45 INFO     	 * (global step 7650: loss: 2.45952045917511, lr: 0.0001
2024-01-06 07:40:56 INFO     	 * (global step 7700: loss: 2.4853475689888, lr: 0.0001
2024-01-06 07:41:07 INFO     	 * (global step 7750: loss: 2.590970814228058, lr: 0.0001
2024-01-06 07:41:18 INFO     	 * (global step 7800: loss: 2.4289767146110535, lr: 0.0001
2024-01-06 07:41:29 INFO     	 * (global step 7850: loss: 2.461475193500519, lr: 0.0001
2024-01-06 07:41:41 INFO     	 * (global step 7900: loss: 2.4226640462875366, lr: 0.0001
2024-01-06 07:41:52 INFO     	 * (global step 7950: loss: 2.422691524028778, lr: 0.0001
2024-01-06 07:42:03 INFO     	 * (global step 8000: loss: 2.5042770504951477, lr: 0.0001
2024-01-06 07:42:14 INFO     	 * (global step 8050: loss: 2.558636963367462, lr: 0.0001
2024-01-06 07:42:25 INFO     [epoch 13/15] average loss: 2.487, lr: 0.0001
2024-01-06 07:42:25 INFO     saving model related files
2024-01-06 07:42:25 INFO     saving model
2024-01-06 07:42:25 INFO     saving tokenizer
2024-01-06 07:42:25 INFO     saving optimizer
2024-01-06 07:42:26 INFO     remove old optimizer files
2024-01-06 07:42:27 INFO     	 * (global step 8100: loss: 2.506839096546173, lr: 0.0001
2024-01-06 07:42:38 INFO     	 * (global step 8150: loss: 2.5616507530212402, lr: 0.0001
2024-01-06 07:42:49 INFO     	 * (global step 8200: loss: 2.4757601618766785, lr: 0.0001
2024-01-06 07:43:00 INFO     	 * (global step 8250: loss: 2.455132305622101, lr: 0.0001
2024-01-06 07:43:12 INFO     	 * (global step 8300: loss: 2.3828251361846924, lr: 0.0001
2024-01-06 07:43:23 INFO     	 * (global step 8350: loss: 2.445104956626892, lr: 0.0001
2024-01-06 07:43:34 INFO     	 * (global step 8400: loss: 2.515433132648468, lr: 0.0001
2024-01-06 07:43:45 INFO     	 * (global step 8450: loss: 2.3714725375175476, lr: 0.0001
2024-01-06 07:43:56 INFO     	 * (global step 8500: loss: 2.3774827122688293, lr: 0.0001
2024-01-06 07:44:08 INFO     	 * (global step 8550: loss: 2.417056679725647, lr: 0.0001
2024-01-06 07:44:19 INFO     	 * (global step 8600: loss: 2.5045947432518005, lr: 0.0001
2024-01-06 07:44:30 INFO     	 * (global step 8650: loss: 2.346933603286743, lr: 0.0001
2024-01-06 07:44:41 INFO     	 * (global step 8700: loss: 2.479716181755066, lr: 0.0001
2024-01-06 07:44:52 INFO     	 * (global step 8750: loss: 2.5023255348205566, lr: 0.0001
2024-01-06 07:45:04 INFO     	 * (global step 8800: loss: 2.488091289997101, lr: 0.0001
2024-01-06 07:45:15 INFO     	 * (global step 8850: loss: 2.4051475524902344, lr: 0.0001
2024-01-06 07:45:26 INFO     	 * (global step 8900: loss: 2.3564628958702087, lr: 0.0001
2024-01-06 07:45:37 INFO     	 * (global step 8950: loss: 2.4299190044403076, lr: 0.0001
2024-01-06 07:45:48 INFO     	 * (global step 9000: loss: 2.3663262724876404, lr: 0.0001
2024-01-06 07:46:00 INFO     	 * (global step 9050: loss: 2.423416554927826, lr: 0.0001
2024-01-06 07:46:11 INFO     	 * (global step 9100: loss: 2.4033257961273193, lr: 0.0001
2024-01-06 07:46:22 INFO     	 * (global step 9150: loss: 2.4671930074691772, lr: 0.0001
2024-01-06 07:46:33 INFO     	 * (global step 9200: loss: 2.6241867542266846, lr: 0.0001
2024-01-06 07:46:44 INFO     	 * (global step 9250: loss: 2.3977376222610474, lr: 0.0001
2024-01-06 07:46:56 INFO     	 * (global step 9300: loss: 2.5176310539245605, lr: 0.0001
2024-01-06 07:47:07 INFO     	 * (global step 9350: loss: 2.397358536720276, lr: 0.0001
2024-01-06 07:47:18 INFO     	 * (global step 9400: loss: 2.462570071220398, lr: 0.0001
2024-01-06 07:47:29 INFO     	 * (global step 9450: loss: 2.524387776851654, lr: 0.0001
2024-01-06 07:47:40 INFO     	 * (global step 9500: loss: 2.458914279937744, lr: 0.0001
2024-01-06 07:47:52 INFO     	 * (global step 9550: loss: 2.4747357964515686, lr: 0.0001
2024-01-06 07:48:03 INFO     	 * (global step 9600: loss: 2.58253675699234, lr: 0.0001
2024-01-06 07:48:14 INFO     	 * (global step 9650: loss: 2.517294466495514, lr: 0.0001
2024-01-06 07:48:25 INFO     	 * (global step 9700: loss: 2.4999396204948425, lr: 0.0001
2024-01-06 07:48:36 INFO     	 * (global step 9750: loss: 2.3963034749031067, lr: 0.0001
2024-01-06 07:48:48 INFO     	 * (global step 9800: loss: 2.3673941493034363, lr: 0.0001
2024-01-06 07:48:59 INFO     	 * (global step 9850: loss: 2.5272459387779236, lr: 0.0001
2024-01-06 07:49:10 INFO     	 * (global step 9900: loss: 2.4982340931892395, lr: 0.0001
2024-01-06 07:49:21 INFO     	 * (global step 9950: loss: 2.444689631462097, lr: 0.0001
2024-01-06 07:49:32 INFO     	 * (global step 10000: loss: 2.5234203338623047, lr: 0.0001
2024-01-06 07:49:44 INFO     	 * (global step 10050: loss: 2.3419398069381714, lr: 0.0001
2024-01-06 07:49:55 INFO     	 * (global step 10100: loss: 2.414349675178528, lr: 0.0001
2024-01-06 07:49:59 INFO     [epoch 14/15] average loss: 2.475, lr: 0.0001
2024-01-06 07:49:59 INFO     saving model related files
2024-01-06 07:49:59 INFO     saving model
2024-01-06 07:50:00 INFO     saving tokenizer
2024-01-06 07:50:00 INFO     saving optimizer
2024-01-06 07:50:01 INFO     remove old optimizer files
2024-01-06 07:50:01 INFO     complete training: model ckpt was saved at small_recreated_ckpt/model_efnljo
2024-01-06 07:50:01 INFO     ## 2nd RUN: Configuration 1/5: validation/Bleu_4 = 0.09485688097694292
2024-01-06 07:50:01 INFO     initialize model trainer
2024-01-06 07:50:01 INFO     load config from existing checkpoint at small_recreated_ckpt/model_dpyopu
2024-01-06 07:50:01 INFO     hyperparameters
2024-01-06 07:50:01 INFO     	 * dataset_path: lmqg/qag_squad
2024-01-06 07:50:01 INFO     	 * dataset_name: default
2024-01-06 07:50:01 INFO     	 * input_types: ['paragraph']
2024-01-06 07:50:01 INFO     	 * output_types: ['questions_answers']
2024-01-06 07:50:01 INFO     	 * prefix_types: ['qag']
2024-01-06 07:50:01 INFO     	 * model: t5-small
2024-01-06 07:50:01 INFO     	 * max_length: 512
2024-01-06 07:50:01 INFO     	 * max_length_output: 256
2024-01-06 07:50:01 INFO     	 * epoch: 15
2024-01-06 07:50:01 INFO     	 * batch: 2
2024-01-06 07:50:01 INFO     	 * lr: 0.0001
2024-01-06 07:50:01 INFO     	 * fp16: False
2024-01-06 07:50:01 INFO     	 * random_seed: 1
2024-01-06 07:50:01 INFO     	 * gradient_accumulation_steps: 4
2024-01-06 07:50:01 INFO     	 * label_smoothing: 0.0
2024-01-06 07:50:01 INFO     load checkpoint from small_recreated_ckpt/model_dpyopu/epoch_10
2024-01-06 07:50:02 INFO     use spaCy answer extraction model: positionrank
2024-01-06 07:50:02 INFO     Model `small_recreated_ckpt/model_dpyopu/epoch_10`
2024-01-06 07:50:02 INFO     	 * Num of GPU in use: 1
2024-01-06 07:50:02 INFO     	 * Prefix: True
2024-01-06 07:50:02 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 07:50:02 INFO     load optimizer from small_recreated_ckpt/model_dpyopu/optimizers/optimizer.10.pt
2024-01-06 07:50:02 INFO     optimizer is loading on cuda
2024-01-06 07:50:08 INFO     dataset preprocessing
2024-01-06 07:50:09 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featurelmqg/qag_squad/t5-small.512.256.paragraph.questions_answers.train.qag.pkl
2024-01-06 07:50:10 INFO     start model training
2024-01-06 07:50:21 INFO     	 * (global step 50: loss: 0.5154661387205124, lr: 0.0001
2024-01-06 07:50:32 INFO     	 * (global step 100: loss: 0.4834935963153839, lr: 0.0001
2024-01-06 07:50:43 INFO     	 * (global step 150: loss: 0.5349985435605049, lr: 0.0001
2024-01-06 07:50:54 INFO     	 * (global step 200: loss: 0.5756383985280991, lr: 0.0001
2024-01-06 07:51:05 INFO     	 * (global step 250: loss: 0.4149413928389549, lr: 0.0001
2024-01-06 07:51:16 INFO     	 * (global step 300: loss: 0.4676159918308258, lr: 0.0001
2024-01-06 07:51:27 INFO     	 * (global step 350: loss: 0.485037624835968, lr: 0.0001
2024-01-06 07:51:38 INFO     	 * (global step 400: loss: 0.4392630383372307, lr: 0.0001
2024-01-06 07:51:49 INFO     	 * (global step 450: loss: 0.46785296499729156, lr: 0.0001
2024-01-06 07:52:00 INFO     	 * (global step 500: loss: 0.39136718213558197, lr: 0.0001
2024-01-06 07:52:11 INFO     	 * (global step 550: loss: 0.5161837562918663, lr: 0.0001
2024-01-06 07:52:22 INFO     	 * (global step 600: loss: 0.43386975675821304, lr: 0.0001
2024-01-06 07:52:32 INFO     	 * (global step 650: loss: 0.4753086566925049, lr: 0.0001
2024-01-06 07:52:43 INFO     	 * (global step 700: loss: 0.4958351030945778, lr: 0.0001
2024-01-06 07:52:54 INFO     	 * (global step 750: loss: 0.43780671060085297, lr: 0.0001
2024-01-06 07:53:05 INFO     	 * (global step 800: loss: 0.45578034222126007, lr: 0.0001
2024-01-06 07:53:16 INFO     	 * (global step 850: loss: 0.5075488314032555, lr: 0.0001
2024-01-06 07:53:27 INFO     	 * (global step 900: loss: 0.4154026061296463, lr: 0.0001
2024-01-06 07:53:38 INFO     	 * (global step 950: loss: 0.46703825145959854, lr: 0.0001
2024-01-06 07:53:49 INFO     	 * (global step 1000: loss: 0.5089117661118507, lr: 0.0001
2024-01-06 07:54:00 INFO     	 * (global step 1050: loss: 0.4967322051525116, lr: 0.0001
2024-01-06 07:54:11 INFO     	 * (global step 1100: loss: 0.4074774235486984, lr: 0.0001
2024-01-06 07:54:22 INFO     	 * (global step 1150: loss: 0.6901633851230145, lr: 0.0001
2024-01-06 07:54:33 INFO     	 * (global step 1200: loss: 0.323746494948864, lr: 0.0001
2024-01-06 07:54:44 INFO     	 * (global step 1250: loss: 0.4058334305882454, lr: 0.0001
2024-01-06 07:54:55 INFO     	 * (global step 1300: loss: 0.4681014120578766, lr: 0.0001
2024-01-06 07:55:06 INFO     	 * (global step 1350: loss: 0.47973383218050003, lr: 0.0001
2024-01-06 07:55:17 INFO     	 * (global step 1400: loss: 0.4378293454647064, lr: 0.0001
2024-01-06 07:55:28 INFO     	 * (global step 1450: loss: 0.5571260675787926, lr: 0.0001
2024-01-06 07:55:39 INFO     	 * (global step 1500: loss: 0.5091037303209305, lr: 0.0001
2024-01-06 07:55:50 INFO     	 * (global step 1550: loss: 0.3426768556237221, lr: 0.0001
2024-01-06 07:56:01 INFO     	 * (global step 1600: loss: 0.5287599489092827, lr: 0.0001
2024-01-06 07:56:12 INFO     	 * (global step 1650: loss: 0.4225146025419235, lr: 0.0001
2024-01-06 07:56:23 INFO     	 * (global step 1700: loss: 0.5190333724021912, lr: 0.0001
2024-01-06 07:56:34 INFO     	 * (global step 1750: loss: 0.4597969800233841, lr: 0.0001
2024-01-06 07:56:45 INFO     	 * (global step 1800: loss: 0.6011900790035725, lr: 0.0001
2024-01-06 07:56:56 INFO     	 * (global step 1850: loss: 0.4160810187458992, lr: 0.0001
2024-01-06 07:57:07 INFO     	 * (global step 1900: loss: 0.4900656118988991, lr: 0.0001
2024-01-06 07:57:18 INFO     	 * (global step 1950: loss: 0.511576347053051, lr: 0.0001
2024-01-06 07:57:29 INFO     	 * (global step 2000: loss: 0.5366161242127419, lr: 0.0001
2024-01-06 07:57:34 INFO     [epoch 10/15] average loss: 0.473, lr: 0.0001
2024-01-06 07:57:34 INFO     saving model related files
2024-01-06 07:57:34 INFO     saving model
2024-01-06 07:57:34 INFO     saving tokenizer
2024-01-06 07:57:34 INFO     saving optimizer
2024-01-06 07:57:35 INFO     remove old optimizer files
2024-01-06 07:57:41 INFO     	 * (global step 2050: loss: 0.44805264472961426, lr: 0.0001
2024-01-06 07:57:52 INFO     	 * (global step 2100: loss: 0.5747921168804169, lr: 0.0001
2024-01-06 07:58:03 INFO     	 * (global step 2150: loss: 0.609537661075592, lr: 0.0001
2024-01-06 07:58:14 INFO     	 * (global step 2200: loss: 0.399726927280426, lr: 0.0001
2024-01-06 07:58:25 INFO     	 * (global step 2250: loss: 0.4181106872856617, lr: 0.0001
2024-01-06 07:58:36 INFO     	 * (global step 2300: loss: 0.43358784168958664, lr: 0.0001
2024-01-06 07:58:47 INFO     	 * (global step 2350: loss: 0.31639641895890236, lr: 0.0001
2024-01-06 07:58:58 INFO     	 * (global step 2400: loss: 0.5441119968891144, lr: 0.0001
2024-01-06 07:59:09 INFO     	 * (global step 2450: loss: 0.4729269817471504, lr: 0.0001
2024-01-06 07:59:20 INFO     	 * (global step 2500: loss: 0.4139852561056614, lr: 0.0001
2024-01-06 07:59:31 INFO     	 * (global step 2550: loss: 0.3847532868385315, lr: 0.0001
2024-01-06 07:59:42 INFO     	 * (global step 2600: loss: 0.3032752051949501, lr: 0.0001
2024-01-06 07:59:53 INFO     	 * (global step 2650: loss: 0.567439004778862, lr: 0.0001
2024-01-06 08:00:04 INFO     	 * (global step 2700: loss: 0.46750665456056595, lr: 0.0001
2024-01-06 08:00:15 INFO     	 * (global step 2750: loss: 0.5035890936851501, lr: 0.0001
2024-01-06 08:00:26 INFO     	 * (global step 2800: loss: 0.633237212896347, lr: 0.0001
2024-01-06 08:00:37 INFO     	 * (global step 2850: loss: 0.5846840739250183, lr: 0.0001
2024-01-06 08:00:48 INFO     	 * (global step 2900: loss: 0.4525918513536453, lr: 0.0001
2024-01-06 08:00:59 INFO     	 * (global step 2950: loss: 0.47643353044986725, lr: 0.0001
2024-01-06 08:01:10 INFO     	 * (global step 3000: loss: 0.317286878824234, lr: 0.0001
2024-01-06 08:01:21 INFO     	 * (global step 3050: loss: 0.3198810815811157, lr: 0.0001
2024-01-06 08:01:32 INFO     	 * (global step 3100: loss: 0.4147501736879349, lr: 0.0001
2024-01-06 08:01:43 INFO     	 * (global step 3150: loss: 0.5267402529716492, lr: 0.0001
2024-01-06 08:01:54 INFO     	 * (global step 3200: loss: 0.5027939677238464, lr: 0.0001
2024-01-06 08:02:05 INFO     	 * (global step 3250: loss: 0.47077763080596924, lr: 0.0001
2024-01-06 08:02:16 INFO     	 * (global step 3300: loss: 0.48028533160686493, lr: 0.0001
2024-01-06 08:02:26 INFO     	 * (global step 3350: loss: 0.3628265857696533, lr: 0.0001
2024-01-06 08:02:37 INFO     	 * (global step 3400: loss: 0.3788015618920326, lr: 0.0001
2024-01-06 08:02:48 INFO     	 * (global step 3450: loss: 0.47316624969244003, lr: 0.0001
2024-01-06 08:02:59 INFO     	 * (global step 3500: loss: 0.7817237973213196, lr: 0.0001
2024-01-06 08:03:10 INFO     	 * (global step 3550: loss: 0.5725388303399086, lr: 0.0001
2024-01-06 08:03:21 INFO     	 * (global step 3600: loss: 0.39132507890462875, lr: 0.0001
2024-01-06 08:03:32 INFO     	 * (global step 3650: loss: 0.4126828908920288, lr: 0.0001
2024-01-06 08:03:43 INFO     	 * (global step 3700: loss: 0.4110577628016472, lr: 0.0001
2024-01-06 08:03:54 INFO     	 * (global step 3750: loss: 0.3711257055401802, lr: 0.0001
2024-01-06 08:04:05 INFO     	 * (global step 3800: loss: 0.4394349157810211, lr: 0.0001
2024-01-06 08:04:16 INFO     	 * (global step 3850: loss: 0.3887215591967106, lr: 0.0001
2024-01-06 08:04:27 INFO     	 * (global step 3900: loss: 0.46162109076976776, lr: 0.0001
2024-01-06 08:04:38 INFO     	 * (global step 3950: loss: 0.40963414311408997, lr: 0.0001
2024-01-06 08:04:49 INFO     	 * (global step 4000: loss: 0.4755835235118866, lr: 0.0001
2024-01-06 08:05:00 INFO     [epoch 11/15] average loss: 0.466, lr: 0.0001
2024-01-06 08:05:00 INFO     saving model related files
2024-01-06 08:05:00 INFO     saving model
2024-01-06 08:05:00 INFO     saving tokenizer
2024-01-06 08:05:00 INFO     saving optimizer
2024-01-06 08:05:01 INFO     remove old optimizer files
2024-01-06 08:05:01 INFO     	 * (global step 4050: loss: 0.44682707637548447, lr: 0.0001
2024-01-06 08:05:12 INFO     	 * (global step 4100: loss: 0.45991354435682297, lr: 0.0001
2024-01-06 08:05:23 INFO     	 * (global step 4150: loss: 0.5318102687597275, lr: 0.0001
2024-01-06 08:05:34 INFO     	 * (global step 4200: loss: 0.4135589748620987, lr: 0.0001
2024-01-06 08:05:45 INFO     	 * (global step 4250: loss: 0.38749826326966286, lr: 0.0001
2024-01-06 08:05:56 INFO     	 * (global step 4300: loss: 0.4756690561771393, lr: 0.0001
2024-01-06 08:06:07 INFO     	 * (global step 4350: loss: 0.40705734118819237, lr: 0.0001
2024-01-06 08:06:18 INFO     	 * (global step 4400: loss: 0.4613136649131775, lr: 0.0001
2024-01-06 08:06:29 INFO     	 * (global step 4450: loss: 0.4389337860047817, lr: 0.0001
2024-01-06 08:06:40 INFO     	 * (global step 4500: loss: 0.4005821794271469, lr: 0.0001
2024-01-06 08:06:51 INFO     	 * (global step 4550: loss: 0.3749993145465851, lr: 0.0001
2024-01-06 08:07:02 INFO     	 * (global step 4600: loss: 0.3201708048582077, lr: 0.0001
2024-01-06 08:07:13 INFO     	 * (global step 4650: loss: 0.31876977905631065, lr: 0.0001
2024-01-06 08:07:24 INFO     	 * (global step 4700: loss: 0.5229043141007423, lr: 0.0001
2024-01-06 08:07:35 INFO     	 * (global step 4750: loss: 0.3324325904250145, lr: 0.0001
2024-01-06 08:07:46 INFO     	 * (global step 4800: loss: 0.3416622281074524, lr: 0.0001
2024-01-06 08:07:57 INFO     	 * (global step 4850: loss: 0.35528069734573364, lr: 0.0001
2024-01-06 08:08:08 INFO     	 * (global step 4900: loss: 0.5506978556513786, lr: 0.0001
2024-01-06 08:08:19 INFO     	 * (global step 4950: loss: 0.5358722433447838, lr: 0.0001
2024-01-06 08:08:30 INFO     	 * (global step 5000: loss: 0.5325499176979065, lr: 0.0001
2024-01-06 08:08:41 INFO     	 * (global step 5050: loss: 0.4759926348924637, lr: 0.0001
2024-01-06 08:08:52 INFO     	 * (global step 5100: loss: 0.44226229190826416, lr: 0.0001
2024-01-06 08:09:02 INFO     	 * (global step 5150: loss: 0.4094848111271858, lr: 0.0001
2024-01-06 08:09:13 INFO     	 * (global step 5200: loss: 0.49150507152080536, lr: 0.0001
2024-01-06 08:09:24 INFO     	 * (global step 5250: loss: 0.49853114038705826, lr: 0.0001
2024-01-06 08:09:35 INFO     	 * (global step 5300: loss: 0.3946891725063324, lr: 0.0001
2024-01-06 08:09:46 INFO     	 * (global step 5350: loss: 0.3017633631825447, lr: 0.0001
2024-01-06 08:09:57 INFO     	 * (global step 5400: loss: 0.3741593509912491, lr: 0.0001
2024-01-06 08:10:08 INFO     	 * (global step 5450: loss: 0.38176949322223663, lr: 0.0001
2024-01-06 08:10:19 INFO     	 * (global step 5500: loss: 0.34073053672909737, lr: 0.0001
2024-01-06 08:10:30 INFO     	 * (global step 5550: loss: 0.48851922154426575, lr: 0.0001
2024-01-06 08:10:41 INFO     	 * (global step 5600: loss: 0.32925115153193474, lr: 0.0001
2024-01-06 08:10:52 INFO     	 * (global step 5650: loss: 0.400566466152668, lr: 0.0001
2024-01-06 08:11:03 INFO     	 * (global step 5700: loss: 0.3661814257502556, lr: 0.0001
2024-01-06 08:11:14 INFO     	 * (global step 5750: loss: 0.5494658574461937, lr: 0.0001
2024-01-06 08:11:25 INFO     	 * (global step 5800: loss: 0.5978123918175697, lr: 0.0001
2024-01-06 08:11:36 INFO     	 * (global step 5850: loss: 0.36044688522815704, lr: 0.0001
2024-01-06 08:11:47 INFO     	 * (global step 5900: loss: 0.4893879145383835, lr: 0.0001
2024-01-06 08:11:58 INFO     	 * (global step 5950: loss: 0.5691727623343468, lr: 0.0001
2024-01-06 08:12:09 INFO     	 * (global step 6000: loss: 0.3854648992419243, lr: 0.0001
2024-01-06 08:12:20 INFO     	 * (global step 6050: loss: 0.4972551763057709, lr: 0.0001
2024-01-06 08:12:25 INFO     [epoch 12/15] average loss: 0.458, lr: 0.0001
2024-01-06 08:12:25 INFO     saving model related files
2024-01-06 08:12:25 INFO     saving model
2024-01-06 08:12:25 INFO     saving tokenizer
2024-01-06 08:12:25 INFO     saving optimizer
2024-01-06 08:12:26 INFO     remove old optimizer files
2024-01-06 08:12:32 INFO     	 * (global step 6100: loss: 0.4416332393884659, lr: 0.0001
2024-01-06 08:12:43 INFO     	 * (global step 6150: loss: 0.45693766325712204, lr: 0.0001
2024-01-06 08:12:54 INFO     	 * (global step 6200: loss: 0.49705833941698074, lr: 0.0001
2024-01-06 08:13:05 INFO     	 * (global step 6250: loss: 0.46639588475227356, lr: 0.0001
2024-01-06 08:13:16 INFO     	 * (global step 6300: loss: 0.5210953801870346, lr: 0.0001
2024-01-06 08:13:27 INFO     	 * (global step 6350: loss: 0.5341161862015724, lr: 0.0001
2024-01-06 08:13:38 INFO     	 * (global step 6400: loss: 0.35713842138648033, lr: 0.0001
2024-01-06 08:13:49 INFO     	 * (global step 6450: loss: 0.5180501639842987, lr: 0.0001
2024-01-06 08:14:00 INFO     	 * (global step 6500: loss: 0.4433652386069298, lr: 0.0001
2024-01-06 08:14:11 INFO     	 * (global step 6550: loss: 0.5150926783680916, lr: 0.0001
2024-01-06 08:14:22 INFO     	 * (global step 6600: loss: 0.3719778433442116, lr: 0.0001
2024-01-06 08:14:33 INFO     	 * (global step 6650: loss: 0.502804696559906, lr: 0.0001
2024-01-06 08:14:44 INFO     	 * (global step 6700: loss: 0.5350778922438622, lr: 0.0001
2024-01-06 08:14:54 INFO     	 * (global step 6750: loss: 0.6486500352621078, lr: 0.0001
2024-01-06 08:15:05 INFO     	 * (global step 6800: loss: 0.47699176520109177, lr: 0.0001
2024-01-06 08:15:16 INFO     	 * (global step 6850: loss: 0.37789174169301987, lr: 0.0001
2024-01-06 08:15:27 INFO     	 * (global step 6900: loss: 0.46116042882204056, lr: 0.0001
2024-01-06 08:15:38 INFO     	 * (global step 6950: loss: 0.49997588247060776, lr: 0.0001
2024-01-06 08:15:49 INFO     	 * (global step 7000: loss: 0.3447108380496502, lr: 0.0001
2024-01-06 08:16:00 INFO     	 * (global step 7050: loss: 0.5641297847032547, lr: 0.0001
2024-01-06 08:16:11 INFO     	 * (global step 7100: loss: 0.4395659416913986, lr: 0.0001
2024-01-06 08:16:22 INFO     	 * (global step 7150: loss: 0.356738667935133, lr: 0.0001
2024-01-06 08:16:33 INFO     	 * (global step 7200: loss: 0.5724253281950951, lr: 0.0001
2024-01-06 08:16:44 INFO     	 * (global step 7250: loss: 0.402406707406044, lr: 0.0001
2024-01-06 08:16:55 INFO     	 * (global step 7300: loss: 0.4153628945350647, lr: 0.0001
2024-01-06 08:17:06 INFO     	 * (global step 7350: loss: 0.4336647093296051, lr: 0.0001
2024-01-06 08:17:17 INFO     	 * (global step 7400: loss: 0.4715195521712303, lr: 0.0001
2024-01-06 08:17:28 INFO     	 * (global step 7450: loss: 0.2908865325152874, lr: 0.0001
2024-01-06 08:17:39 INFO     	 * (global step 7500: loss: 0.38983154296875, lr: 0.0001
2024-01-06 08:17:50 INFO     	 * (global step 7550: loss: 0.4777875691652298, lr: 0.0001
2024-01-06 08:18:01 INFO     	 * (global step 7600: loss: 0.4239749535918236, lr: 0.0001
2024-01-06 08:18:12 INFO     	 * (global step 7650: loss: 0.4149816557765007, lr: 0.0001
2024-01-06 08:18:23 INFO     	 * (global step 7700: loss: 0.4580783024430275, lr: 0.0001
2024-01-06 08:18:34 INFO     	 * (global step 7750: loss: 0.5491511970758438, lr: 0.0001
2024-01-06 08:18:45 INFO     	 * (global step 7800: loss: 0.3922969326376915, lr: 0.0001
2024-01-06 08:18:56 INFO     	 * (global step 7850: loss: 0.4118793159723282, lr: 0.0001
2024-01-06 08:19:07 INFO     	 * (global step 7900: loss: 0.382059209048748, lr: 0.0001
2024-01-06 08:19:17 INFO     	 * (global step 7950: loss: 0.38760970532894135, lr: 0.0001
2024-01-06 08:19:28 INFO     	 * (global step 8000: loss: 0.4601821042597294, lr: 0.0001
2024-01-06 08:19:39 INFO     	 * (global step 8050: loss: 0.5179019421339035, lr: 0.0001
2024-01-06 08:19:50 INFO     [epoch 13/15] average loss: 0.451, lr: 0.0001
2024-01-06 08:19:50 INFO     saving model related files
2024-01-06 08:19:50 INFO     saving model
2024-01-06 08:19:50 INFO     saving tokenizer
2024-01-06 08:19:50 INFO     saving optimizer
2024-01-06 08:19:51 INFO     remove old optimizer files
2024-01-06 08:19:52 INFO     	 * (global step 8100: loss: 0.48578084260225296, lr: 0.0001
2024-01-06 08:20:03 INFO     	 * (global step 8150: loss: 0.5228050947189331, lr: 0.0001
2024-01-06 08:20:14 INFO     	 * (global step 8200: loss: 0.43109290301799774, lr: 0.0001
2024-01-06 08:20:25 INFO     	 * (global step 8250: loss: 0.4181623309850693, lr: 0.0001
2024-01-06 08:20:36 INFO     	 * (global step 8300: loss: 0.3569980189204216, lr: 0.0001
2024-01-06 08:20:47 INFO     	 * (global step 8350: loss: 0.41199465095996857, lr: 0.0001
2024-01-06 08:20:58 INFO     	 * (global step 8400: loss: 0.48199786990880966, lr: 0.0001
2024-01-06 08:21:08 INFO     	 * (global step 8450: loss: 0.32741858810186386, lr: 0.0001
2024-01-06 08:21:19 INFO     	 * (global step 8500: loss: 0.3410547450184822, lr: 0.0001
2024-01-06 08:21:30 INFO     	 * (global step 8550: loss: 0.3672148883342743, lr: 0.0001
2024-01-06 08:21:41 INFO     	 * (global step 8600: loss: 0.48068922758102417, lr: 0.0001
2024-01-06 08:21:52 INFO     	 * (global step 8650: loss: 0.31596339493989944, lr: 0.0001
2024-01-06 08:22:03 INFO     	 * (global step 8700: loss: 0.4203275702893734, lr: 0.0001
2024-01-06 08:22:14 INFO     	 * (global step 8750: loss: 0.47577349096536636, lr: 0.0001
2024-01-06 08:22:25 INFO     	 * (global step 8800: loss: 0.44529228657484055, lr: 0.0001
2024-01-06 08:22:36 INFO     	 * (global step 8850: loss: 0.36724430322647095, lr: 0.0001
2024-01-06 08:22:47 INFO     	 * (global step 8900: loss: 0.3271089866757393, lr: 0.0001
2024-01-06 08:22:58 INFO     	 * (global step 8950: loss: 0.41042204201221466, lr: 0.0001
2024-01-06 08:23:09 INFO     	 * (global step 9000: loss: 0.3166544586420059, lr: 0.0001
2024-01-06 08:23:20 INFO     	 * (global step 9050: loss: 0.3969855532050133, lr: 0.0001
2024-01-06 08:23:31 INFO     	 * (global step 9100: loss: 0.3604556508362293, lr: 0.0001
2024-01-06 08:23:42 INFO     	 * (global step 9150: loss: 0.4419739469885826, lr: 0.0001
2024-01-06 08:23:53 INFO     	 * (global step 9200: loss: 0.6013711914420128, lr: 0.0001
2024-01-06 08:24:04 INFO     	 * (global step 9250: loss: 0.36544742435216904, lr: 0.0001
2024-01-06 08:24:15 INFO     	 * (global step 9300: loss: 0.49620433896780014, lr: 0.0001
2024-01-06 08:24:26 INFO     	 * (global step 9350: loss: 0.36193402856588364, lr: 0.0001
2024-01-06 08:24:37 INFO     	 * (global step 9400: loss: 0.43469924479722977, lr: 0.0001
2024-01-06 08:24:48 INFO     	 * (global step 9450: loss: 0.5076335147023201, lr: 0.0001
2024-01-06 08:24:59 INFO     	 * (global step 9500: loss: 0.42082205414772034, lr: 0.0001
2024-01-06 08:25:10 INFO     	 * (global step 9550: loss: 0.4443839266896248, lr: 0.0001
2024-01-06 08:25:21 INFO     	 * (global step 9600: loss: 0.5563115999102592, lr: 0.0001
2024-01-06 08:25:31 INFO     	 * (global step 9650: loss: 0.49081284552812576, lr: 0.0001
2024-01-06 08:25:42 INFO     	 * (global step 9700: loss: 0.4794975817203522, lr: 0.0001
2024-01-06 08:25:53 INFO     	 * (global step 9750: loss: 0.3545354902744293, lr: 0.0001
2024-01-06 08:26:04 INFO     	 * (global step 9800: loss: 0.33021125942468643, lr: 0.0001
2024-01-06 08:26:15 INFO     	 * (global step 9850: loss: 0.5073384717106819, lr: 0.0001
2024-01-06 08:26:26 INFO     	 * (global step 9900: loss: 0.46865882724523544, lr: 0.0001
2024-01-06 08:26:37 INFO     	 * (global step 9950: loss: 0.41004127636551857, lr: 0.0001
2024-01-06 08:26:48 INFO     	 * (global step 10000: loss: 0.5003172680735588, lr: 0.0001
2024-01-06 08:26:59 INFO     	 * (global step 10050: loss: 0.3152688406407833, lr: 0.0001
2024-01-06 08:27:10 INFO     	 * (global step 10100: loss: 0.3699435517191887, lr: 0.0001
2024-01-06 08:27:15 INFO     [epoch 14/15] average loss: 0.445, lr: 0.0001
2024-01-06 08:27:15 INFO     saving model related files
2024-01-06 08:27:15 INFO     saving model
2024-01-06 08:27:15 INFO     saving tokenizer
2024-01-06 08:27:15 INFO     saving optimizer
2024-01-06 08:27:16 INFO     remove old optimizer files
2024-01-06 08:27:16 INFO     complete training: model ckpt was saved at small_recreated_ckpt/model_dpyopu
2024-01-06 08:27:16 INFO     ## 2nd RUN: Configuration 2/5: validation/Bleu_4 = 0.09466239682555477
2024-01-06 08:27:16 INFO     initialize model trainer
2024-01-06 08:27:16 INFO     load config from existing checkpoint at small_recreated_ckpt/model_eszyci
2024-01-06 08:27:16 INFO     hyperparameters
2024-01-06 08:27:16 INFO     	 * dataset_path: lmqg/qag_squad
2024-01-06 08:27:16 INFO     	 * dataset_name: default
2024-01-06 08:27:16 INFO     	 * input_types: ['paragraph']
2024-01-06 08:27:16 INFO     	 * output_types: ['questions_answers']
2024-01-06 08:27:16 INFO     	 * prefix_types: ['qag']
2024-01-06 08:27:16 INFO     	 * model: t5-small
2024-01-06 08:27:16 INFO     	 * max_length: 512
2024-01-06 08:27:16 INFO     	 * max_length_output: 256
2024-01-06 08:27:16 INFO     	 * epoch: 15
2024-01-06 08:27:16 INFO     	 * batch: 2
2024-01-06 08:27:16 INFO     	 * lr: 0.0001
2024-01-06 08:27:16 INFO     	 * fp16: False
2024-01-06 08:27:16 INFO     	 * random_seed: 1
2024-01-06 08:27:16 INFO     	 * gradient_accumulation_steps: 2
2024-01-06 08:27:16 INFO     	 * label_smoothing: 0.15
2024-01-06 08:27:16 INFO     load checkpoint from small_recreated_ckpt/model_eszyci/epoch_10
2024-01-06 08:27:17 INFO     use spaCy answer extraction model: positionrank
2024-01-06 08:27:17 INFO     Model `small_recreated_ckpt/model_eszyci/epoch_10`
2024-01-06 08:27:17 INFO     	 * Num of GPU in use: 1
2024-01-06 08:27:17 INFO     	 * Prefix: True
2024-01-06 08:27:17 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 08:27:17 INFO     load optimizer from small_recreated_ckpt/model_eszyci/optimizers/optimizer.10.pt
2024-01-06 08:27:17 INFO     optimizer is loading on cuda
2024-01-06 08:27:21 INFO     dataset preprocessing
2024-01-06 08:27:24 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featurelmqg/qag_squad/t5-small.512.256.paragraph.questions_answers.train.qag.pkl
2024-01-06 08:27:25 INFO     start model training
2024-01-06 08:27:31 INFO     	 * (global step 50: loss: 2.786098837852478, lr: 0.0001
2024-01-06 08:27:37 INFO     	 * (global step 100: loss: 2.820736050605774, lr: 0.0001
2024-01-06 08:27:43 INFO     	 * (global step 150: loss: 2.7039053440093994, lr: 0.0001
2024-01-06 08:27:49 INFO     	 * (global step 200: loss: 2.710424304008484, lr: 0.0001
2024-01-06 08:27:55 INFO     	 * (global step 250: loss: 2.6302860975265503, lr: 0.0001
2024-01-06 08:28:01 INFO     	 * (global step 300: loss: 2.808215379714966, lr: 0.0001
2024-01-06 08:28:06 INFO     	 * (global step 350: loss: 2.6265488862991333, lr: 0.0001
2024-01-06 08:28:12 INFO     	 * (global step 400: loss: 2.6932356357574463, lr: 0.0001
2024-01-06 08:28:18 INFO     	 * (global step 450: loss: 2.5189385414123535, lr: 0.0001
2024-01-06 08:28:24 INFO     	 * (global step 500: loss: 2.4820762872695923, lr: 0.0001
2024-01-06 08:28:30 INFO     	 * (global step 550: loss: 2.6932854652404785, lr: 0.0001
2024-01-06 08:28:36 INFO     	 * (global step 600: loss: 2.6036765575408936, lr: 0.0001
2024-01-06 08:28:42 INFO     	 * (global step 650: loss: 2.563656806945801, lr: 0.0001
2024-01-06 08:28:48 INFO     	 * (global step 700: loss: 2.5489695072174072, lr: 0.0001
2024-01-06 08:28:54 INFO     	 * (global step 750: loss: 2.577238917350769, lr: 0.0001
2024-01-06 08:29:00 INFO     	 * (global step 800: loss: 2.598181962966919, lr: 0.0001
2024-01-06 08:29:06 INFO     	 * (global step 850: loss: 2.4963878393173218, lr: 0.0001
2024-01-06 08:29:12 INFO     	 * (global step 900: loss: 2.5448700189590454, lr: 0.0001
2024-01-06 08:29:18 INFO     	 * (global step 950: loss: 2.5490399599075317, lr: 0.0001
2024-01-06 08:29:24 INFO     	 * (global step 1000: loss: 2.393248200416565, lr: 0.0001
2024-01-06 08:29:30 INFO     	 * (global step 1050: loss: 2.4695481061935425, lr: 0.0001
2024-01-06 08:29:35 INFO     	 * (global step 1100: loss: 2.6187227964401245, lr: 0.0001
2024-01-06 08:29:41 INFO     	 * (global step 1150: loss: 2.4608964920043945, lr: 0.0001
2024-01-06 08:29:47 INFO     	 * (global step 1200: loss: 2.4052324295043945, lr: 0.0001
2024-01-06 08:29:53 INFO     	 * (global step 1250: loss: 2.3914560079574585, lr: 0.0001
2024-01-06 08:29:59 INFO     	 * (global step 1300: loss: 2.602564811706543, lr: 0.0001
2024-01-06 08:30:05 INFO     	 * (global step 1350: loss: 2.507179021835327, lr: 0.0001
2024-01-06 08:30:11 INFO     	 * (global step 1400: loss: 2.5460039377212524, lr: 0.0001
2024-01-06 08:30:17 INFO     	 * (global step 1450: loss: 2.6009994745254517, lr: 0.0001
2024-01-06 08:30:23 INFO     	 * (global step 1500: loss: 2.5532689094543457, lr: 0.0001
2024-01-06 08:30:29 INFO     	 * (global step 1550: loss: 2.558396339416504, lr: 0.0001
2024-01-06 08:30:35 INFO     	 * (global step 1600: loss: 2.581818103790283, lr: 0.0001
2024-01-06 08:30:41 INFO     	 * (global step 1650: loss: 2.7153979539871216, lr: 0.0001
2024-01-06 08:30:47 INFO     	 * (global step 1700: loss: 2.6716097593307495, lr: 0.0001
2024-01-06 08:30:53 INFO     	 * (global step 1750: loss: 2.3513200283050537, lr: 0.0001
2024-01-06 08:30:59 INFO     	 * (global step 1800: loss: 2.4155741930007935, lr: 0.0001
2024-01-06 08:31:05 INFO     	 * (global step 1850: loss: 2.4763587713241577, lr: 0.0001
2024-01-06 08:31:11 INFO     	 * (global step 1900: loss: 2.419769048690796, lr: 0.0001
2024-01-06 08:31:16 INFO     	 * (global step 1950: loss: 2.4887725114822388, lr: 0.0001
2024-01-06 08:31:22 INFO     	 * (global step 2000: loss: 2.5738818645477295, lr: 0.0001
2024-01-06 08:31:28 INFO     	 * (global step 2050: loss: 2.4565372467041016, lr: 0.0001
2024-01-06 08:31:34 INFO     	 * (global step 2100: loss: 2.612742781639099, lr: 0.0001
2024-01-06 08:31:40 INFO     	 * (global step 2150: loss: 2.586975336074829, lr: 0.0001
2024-01-06 08:31:46 INFO     	 * (global step 2200: loss: 2.5109739303588867, lr: 0.0001
2024-01-06 08:31:52 INFO     	 * (global step 2250: loss: 2.4540648460388184, lr: 0.0001
2024-01-06 08:31:58 INFO     	 * (global step 2300: loss: 2.466596245765686, lr: 0.0001
2024-01-06 08:32:04 INFO     	 * (global step 2350: loss: 2.468398690223694, lr: 0.0001
2024-01-06 08:32:10 INFO     	 * (global step 2400: loss: 2.303318738937378, lr: 0.0001
2024-01-06 08:32:16 INFO     	 * (global step 2450: loss: 2.466267704963684, lr: 0.0001
2024-01-06 08:32:22 INFO     	 * (global step 2500: loss: 2.5375267267227173, lr: 0.0001
2024-01-06 08:32:28 INFO     	 * (global step 2550: loss: 2.59181547164917, lr: 0.0001
2024-01-06 08:32:34 INFO     	 * (global step 2600: loss: 2.434224247932434, lr: 0.0001
2024-01-06 08:32:40 INFO     	 * (global step 2650: loss: 2.5252604484558105, lr: 0.0001
2024-01-06 08:32:46 INFO     	 * (global step 2700: loss: 2.608107089996338, lr: 0.0001
2024-01-06 08:32:52 INFO     	 * (global step 2750: loss: 2.595083713531494, lr: 0.0001
2024-01-06 08:32:58 INFO     	 * (global step 2800: loss: 2.419779896736145, lr: 0.0001
2024-01-06 08:33:04 INFO     	 * (global step 2850: loss: 2.6015011072158813, lr: 0.0001
2024-01-06 08:33:09 INFO     	 * (global step 2900: loss: 2.621009945869446, lr: 0.0001
2024-01-06 08:33:15 INFO     	 * (global step 2950: loss: 2.403498411178589, lr: 0.0001
2024-01-06 08:33:21 INFO     	 * (global step 3000: loss: 2.5645216703414917, lr: 0.0001
2024-01-06 08:33:27 INFO     	 * (global step 3050: loss: 2.4735831022262573, lr: 0.0001
2024-01-06 08:33:33 INFO     	 * (global step 3100: loss: 2.4320768117904663, lr: 0.0001
2024-01-06 08:33:39 INFO     	 * (global step 3150: loss: 2.5637099742889404, lr: 0.0001
2024-01-06 08:33:45 INFO     	 * (global step 3200: loss: 2.428112506866455, lr: 0.0001
2024-01-06 08:33:51 INFO     	 * (global step 3250: loss: 2.34795343875885, lr: 0.0001
2024-01-06 08:33:57 INFO     	 * (global step 3300: loss: 2.4341870546340942, lr: 0.0001
2024-01-06 08:34:03 INFO     	 * (global step 3350: loss: 2.5178593397140503, lr: 0.0001
2024-01-06 08:34:09 INFO     	 * (global step 3400: loss: 2.537540078163147, lr: 0.0001
2024-01-06 08:34:15 INFO     	 * (global step 3450: loss: 2.360182285308838, lr: 0.0001
2024-01-06 08:34:21 INFO     	 * (global step 3500: loss: 2.5013495683670044, lr: 0.0001
2024-01-06 08:34:27 INFO     	 * (global step 3550: loss: 2.6344451904296875, lr: 0.0001
2024-01-06 08:34:33 INFO     	 * (global step 3600: loss: 2.369047999382019, lr: 0.0001
2024-01-06 08:34:39 INFO     	 * (global step 3650: loss: 2.4870041608810425, lr: 0.0001
2024-01-06 08:34:45 INFO     	 * (global step 3700: loss: 2.4176526069641113, lr: 0.0001
2024-01-06 08:34:51 INFO     	 * (global step 3750: loss: 2.4822797775268555, lr: 0.0001
2024-01-06 08:34:56 INFO     	 * (global step 3800: loss: 2.5378276109695435, lr: 0.0001
2024-01-06 08:35:02 INFO     	 * (global step 3850: loss: 2.6479179859161377, lr: 0.0001
2024-01-06 08:35:08 INFO     	 * (global step 3900: loss: 2.5753254890441895, lr: 0.0001
2024-01-06 08:35:14 INFO     	 * (global step 3950: loss: 2.488982677459717, lr: 0.0001
2024-01-06 08:35:20 INFO     	 * (global step 4000: loss: 2.642406940460205, lr: 0.0001
2024-01-06 08:35:26 INFO     [epoch 10/15] average loss: 2.555, lr: 0.0001
2024-01-06 08:35:26 INFO     saving model related files
2024-01-06 08:35:26 INFO     saving model
2024-01-06 08:35:26 INFO     saving tokenizer
2024-01-06 08:35:27 INFO     saving optimizer
2024-01-06 08:35:27 INFO     remove old optimizer files
2024-01-06 08:35:28 INFO     	 * (global step 4050: loss: 2.5298949480056763, lr: 0.0001
2024-01-06 08:35:34 INFO     	 * (global step 4100: loss: 2.5782068967819214, lr: 0.0001
2024-01-06 08:35:40 INFO     	 * (global step 4150: loss: 2.4576821327209473, lr: 0.0001
2024-01-06 08:35:46 INFO     	 * (global step 4200: loss: 2.458488345146179, lr: 0.0001
2024-01-06 08:35:51 INFO     	 * (global step 4250: loss: 2.5058504343032837, lr: 0.0001
2024-01-06 08:35:57 INFO     	 * (global step 4300: loss: 2.5743801593780518, lr: 0.0001
2024-01-06 08:36:03 INFO     	 * (global step 4350: loss: 2.459690809249878, lr: 0.0001
2024-01-06 08:36:09 INFO     	 * (global step 4400: loss: 2.387988805770874, lr: 0.0001
2024-01-06 08:36:15 INFO     	 * (global step 4450: loss: 2.6322076320648193, lr: 0.0001
2024-01-06 08:36:21 INFO     	 * (global step 4500: loss: 2.355843663215637, lr: 0.0001
2024-01-06 08:36:27 INFO     	 * (global step 4550: loss: 2.6851603984832764, lr: 0.0001
2024-01-06 08:36:33 INFO     	 * (global step 4600: loss: 2.546483635902405, lr: 0.0001
2024-01-06 08:36:39 INFO     	 * (global step 4650: loss: 2.592968702316284, lr: 0.0001
2024-01-06 08:36:45 INFO     	 * (global step 4700: loss: 2.3885481357574463, lr: 0.0001
2024-01-06 08:36:51 INFO     	 * (global step 4750: loss: 2.464749574661255, lr: 0.0001
2024-01-06 08:36:57 INFO     	 * (global step 4800: loss: 2.4233028888702393, lr: 0.0001
2024-01-06 08:37:03 INFO     	 * (global step 4850: loss: 2.4315379858016968, lr: 0.0001
2024-01-06 08:37:09 INFO     	 * (global step 4900: loss: 2.537723422050476, lr: 0.0001
2024-01-06 08:37:15 INFO     	 * (global step 4950: loss: 2.509831428527832, lr: 0.0001
2024-01-06 08:37:21 INFO     	 * (global step 5000: loss: 2.532625198364258, lr: 0.0001
2024-01-06 08:37:27 INFO     	 * (global step 5050: loss: 2.401503562927246, lr: 0.0001
2024-01-06 08:37:32 INFO     	 * (global step 5100: loss: 2.3443992137908936, lr: 0.0001
2024-01-06 08:37:38 INFO     	 * (global step 5150: loss: 2.4627023935317993, lr: 0.0001
2024-01-06 08:37:44 INFO     	 * (global step 5200: loss: 2.3367592096328735, lr: 0.0001
2024-01-06 08:37:50 INFO     	 * (global step 5250: loss: 2.489262819290161, lr: 0.0001
2024-01-06 08:37:56 INFO     	 * (global step 5300: loss: 2.5715785026550293, lr: 0.0001
2024-01-06 08:38:02 INFO     	 * (global step 5350: loss: 2.5471291542053223, lr: 0.0001
2024-01-06 08:38:08 INFO     	 * (global step 5400: loss: 2.518751382827759, lr: 0.0001
2024-01-06 08:38:14 INFO     	 * (global step 5450: loss: 2.563952684402466, lr: 0.0001
2024-01-06 08:38:20 INFO     	 * (global step 5500: loss: 2.5169436931610107, lr: 0.0001
2024-01-06 08:38:26 INFO     	 * (global step 5550: loss: 2.4460690021514893, lr: 0.0001
2024-01-06 08:38:32 INFO     	 * (global step 5600: loss: 2.6996268033981323, lr: 0.0001
2024-01-06 08:38:38 INFO     	 * (global step 5650: loss: 2.5353983640670776, lr: 0.0001
2024-01-06 08:38:44 INFO     	 * (global step 5700: loss: 2.635100245475769, lr: 0.0001
2024-01-06 08:38:50 INFO     	 * (global step 5750: loss: 2.5667823553085327, lr: 0.0001
2024-01-06 08:38:56 INFO     	 * (global step 5800: loss: 2.379964232444763, lr: 0.0001
2024-01-06 08:39:02 INFO     	 * (global step 5850: loss: 2.3929054737091064, lr: 0.0001
2024-01-06 08:39:08 INFO     	 * (global step 5900: loss: 2.368784785270691, lr: 0.0001
2024-01-06 08:39:13 INFO     	 * (global step 5950: loss: 2.3375006914138794, lr: 0.0001
2024-01-06 08:39:19 INFO     	 * (global step 6000: loss: 2.2627246379852295, lr: 0.0001
2024-01-06 08:39:25 INFO     	 * (global step 6050: loss: 2.4715007543563843, lr: 0.0001
2024-01-06 08:39:31 INFO     	 * (global step 6100: loss: 2.361114978790283, lr: 0.0001
2024-01-06 08:39:37 INFO     	 * (global step 6150: loss: 2.6650558710098267, lr: 0.0001
2024-01-06 08:39:43 INFO     	 * (global step 6200: loss: 2.5152512788772583, lr: 0.0001
2024-01-06 08:39:49 INFO     	 * (global step 6250: loss: 2.3721790313720703, lr: 0.0001
2024-01-06 08:39:55 INFO     	 * (global step 6300: loss: 2.544465661048889, lr: 0.0001
2024-01-06 08:40:01 INFO     	 * (global step 6350: loss: 2.4141379594802856, lr: 0.0001
2024-01-06 08:40:07 INFO     	 * (global step 6400: loss: 2.441865563392639, lr: 0.0001
2024-01-06 08:40:13 INFO     	 * (global step 6450: loss: 2.483220338821411, lr: 0.0001
2024-01-06 08:40:19 INFO     	 * (global step 6500: loss: 2.5435760021209717, lr: 0.0001
2024-01-06 08:40:25 INFO     	 * (global step 6550: loss: 2.346730947494507, lr: 0.0001
2024-01-06 08:40:31 INFO     	 * (global step 6600: loss: 2.473832130432129, lr: 0.0001
2024-01-06 08:40:37 INFO     	 * (global step 6650: loss: 2.445106625556946, lr: 0.0001
2024-01-06 08:40:43 INFO     	 * (global step 6700: loss: 2.390544891357422, lr: 0.0001
2024-01-06 08:40:49 INFO     	 * (global step 6750: loss: 2.5923115015029907, lr: 0.0001
2024-01-06 08:40:55 INFO     	 * (global step 6800: loss: 2.307507038116455, lr: 0.0001
2024-01-06 08:41:01 INFO     	 * (global step 6850: loss: 2.5007495880126953, lr: 0.0001
2024-01-06 08:41:06 INFO     	 * (global step 6900: loss: 2.496136784553528, lr: 0.0001
2024-01-06 08:41:12 INFO     	 * (global step 6950: loss: 2.3733898401260376, lr: 0.0001
2024-01-06 08:41:18 INFO     	 * (global step 7000: loss: 2.8615227937698364, lr: 0.0001
2024-01-06 08:41:24 INFO     	 * (global step 7050: loss: 2.4001476764678955, lr: 0.0001
2024-01-06 08:41:30 INFO     	 * (global step 7100: loss: 2.7602344751358032, lr: 0.0001
2024-01-06 08:41:36 INFO     	 * (global step 7150: loss: 2.4809707403182983, lr: 0.0001
2024-01-06 08:41:42 INFO     	 * (global step 7200: loss: 2.406942367553711, lr: 0.0001
2024-01-06 08:41:48 INFO     	 * (global step 7250: loss: 2.4707462787628174, lr: 0.0001
2024-01-06 08:41:54 INFO     	 * (global step 7300: loss: 2.514459729194641, lr: 0.0001
2024-01-06 08:42:00 INFO     	 * (global step 7350: loss: 2.434078574180603, lr: 0.0001
2024-01-06 08:42:06 INFO     	 * (global step 7400: loss: 2.484686255455017, lr: 0.0001
2024-01-06 08:42:12 INFO     	 * (global step 7450: loss: 2.5586891174316406, lr: 0.0001
2024-01-06 08:42:18 INFO     	 * (global step 7500: loss: 2.3944236040115356, lr: 0.0001
2024-01-06 08:42:24 INFO     	 * (global step 7550: loss: 2.4069924354553223, lr: 0.0001
2024-01-06 08:42:30 INFO     	 * (global step 7600: loss: 2.492335081100464, lr: 0.0001
2024-01-06 08:42:36 INFO     	 * (global step 7650: loss: 2.582612156867981, lr: 0.0001
2024-01-06 08:42:42 INFO     	 * (global step 7700: loss: 2.4434326887130737, lr: 0.0001
2024-01-06 08:42:48 INFO     	 * (global step 7750: loss: 2.58635675907135, lr: 0.0001
2024-01-06 08:42:54 INFO     	 * (global step 7800: loss: 2.5363547801971436, lr: 0.0001
2024-01-06 08:43:00 INFO     	 * (global step 7850: loss: 2.3284919261932373, lr: 0.0001
2024-01-06 08:43:05 INFO     	 * (global step 7900: loss: 2.4291796684265137, lr: 0.0001
2024-01-06 08:43:11 INFO     	 * (global step 7950: loss: 2.3665082454681396, lr: 0.0001
2024-01-06 08:43:17 INFO     	 * (global step 8000: loss: 2.4328893423080444, lr: 0.0001
2024-01-06 08:43:23 INFO     	 * (global step 8050: loss: 2.3932379484176636, lr: 0.0001
2024-01-06 08:43:29 INFO     [epoch 11/15] average loss: 2.488, lr: 0.0001
2024-01-06 08:43:29 INFO     saving model related files
2024-01-06 08:43:29 INFO     saving model
2024-01-06 08:43:29 INFO     saving tokenizer
2024-01-06 08:43:29 INFO     saving optimizer
2024-01-06 08:43:30 INFO     remove old optimizer files
2024-01-06 08:43:31 INFO     	 * (global step 8100: loss: 2.4556649923324585, lr: 0.0001
2024-01-06 08:43:37 INFO     	 * (global step 8150: loss: 2.391392230987549, lr: 0.0001
2024-01-06 08:43:43 INFO     	 * (global step 8200: loss: 2.423954129219055, lr: 0.0001
2024-01-06 08:43:48 INFO     	 * (global step 8250: loss: 2.44935941696167, lr: 0.0001
2024-01-06 08:43:54 INFO     	 * (global step 8300: loss: 2.4289742708206177, lr: 0.0001
2024-01-06 08:44:00 INFO     	 * (global step 8350: loss: 2.472073793411255, lr: 0.0001
2024-01-06 08:44:06 INFO     	 * (global step 8400: loss: 2.3971869945526123, lr: 0.0001
2024-01-06 08:44:12 INFO     	 * (global step 8450: loss: 2.3850985765457153, lr: 0.0001
2024-01-06 08:44:18 INFO     	 * (global step 8500: loss: 2.4596974849700928, lr: 0.0001
2024-01-06 08:44:24 INFO     	 * (global step 8550: loss: 2.3689364194869995, lr: 0.0001
2024-01-06 08:44:30 INFO     	 * (global step 8600: loss: 2.4283053874969482, lr: 0.0001
2024-01-06 08:44:36 INFO     	 * (global step 8650: loss: 2.4125475883483887, lr: 0.0001
2024-01-06 08:44:42 INFO     	 * (global step 8700: loss: 2.409613013267517, lr: 0.0001
2024-01-06 08:44:48 INFO     	 * (global step 8750: loss: 2.3925528526306152, lr: 0.0001
2024-01-06 08:44:54 INFO     	 * (global step 8800: loss: 2.50915265083313, lr: 0.0001
2024-01-06 08:45:00 INFO     	 * (global step 8850: loss: 2.4884400367736816, lr: 0.0001
2024-01-06 08:45:06 INFO     	 * (global step 8900: loss: 2.4044214487075806, lr: 0.0001
2024-01-06 08:45:12 INFO     	 * (global step 8950: loss: 2.45407497882843, lr: 0.0001
2024-01-06 08:45:18 INFO     	 * (global step 9000: loss: 2.3319567441940308, lr: 0.0001
2024-01-06 08:45:24 INFO     	 * (global step 9050: loss: 2.333568572998047, lr: 0.0001
2024-01-06 08:45:30 INFO     	 * (global step 9100: loss: 2.4347307682037354, lr: 0.0001
2024-01-06 08:45:35 INFO     	 * (global step 9150: loss: 2.444732904434204, lr: 0.0001
2024-01-06 08:45:41 INFO     	 * (global step 9200: loss: 2.416961669921875, lr: 0.0001
2024-01-06 08:45:47 INFO     	 * (global step 9250: loss: 2.458429455757141, lr: 0.0001
2024-01-06 08:45:53 INFO     	 * (global step 9300: loss: 2.3285067081451416, lr: 0.0001
2024-01-06 08:45:59 INFO     	 * (global step 9350: loss: 2.310678482055664, lr: 0.0001
2024-01-06 08:46:05 INFO     	 * (global step 9400: loss: 2.499638795852661, lr: 0.0001
2024-01-06 08:46:11 INFO     	 * (global step 9450: loss: 2.2739731073379517, lr: 0.0001
2024-01-06 08:46:17 INFO     	 * (global step 9500: loss: 2.390053629875183, lr: 0.0001
2024-01-06 08:46:23 INFO     	 * (global step 9550: loss: 2.4668731689453125, lr: 0.0001
2024-01-06 08:46:29 INFO     	 * (global step 9600: loss: 2.318023204803467, lr: 0.0001
2024-01-06 08:46:35 INFO     	 * (global step 9650: loss: 2.502129912376404, lr: 0.0001
2024-01-06 08:46:41 INFO     	 * (global step 9700: loss: 2.34841525554657, lr: 0.0001
2024-01-06 08:46:47 INFO     	 * (global step 9750: loss: 2.3426495790481567, lr: 0.0001
2024-01-06 08:46:53 INFO     	 * (global step 9800: loss: 2.517469882965088, lr: 0.0001
2024-01-06 08:46:59 INFO     	 * (global step 9850: loss: 2.3357231616973877, lr: 0.0001
2024-01-06 08:47:05 INFO     	 * (global step 9900: loss: 2.53154456615448, lr: 0.0001
2024-01-06 08:47:11 INFO     	 * (global step 9950: loss: 2.4931827783584595, lr: 0.0001
2024-01-06 08:47:16 INFO     	 * (global step 10000: loss: 2.526698350906372, lr: 0.0001
2024-01-06 08:47:22 INFO     	 * (global step 10050: loss: 2.4720568656921387, lr: 0.0001
2024-01-06 08:47:28 INFO     	 * (global step 10100: loss: 2.4944915771484375, lr: 0.0001
2024-01-06 08:47:34 INFO     	 * (global step 10150: loss: 2.4991101026535034, lr: 0.0001
2024-01-06 08:47:40 INFO     	 * (global step 10200: loss: 2.4619979858398438, lr: 0.0001
2024-01-06 08:47:46 INFO     	 * (global step 10250: loss: 2.4631248712539673, lr: 0.0001
2024-01-06 08:47:52 INFO     	 * (global step 10300: loss: 2.5215035676956177, lr: 0.0001
2024-01-06 08:47:58 INFO     	 * (global step 10350: loss: 2.3837112188339233, lr: 0.0001
2024-01-06 08:48:04 INFO     	 * (global step 10400: loss: 2.4483771324157715, lr: 0.0001
2024-01-06 08:48:10 INFO     	 * (global step 10450: loss: 2.3899272680282593, lr: 0.0001
2024-01-06 08:48:16 INFO     	 * (global step 10500: loss: 2.50276780128479, lr: 0.0001
2024-01-06 08:48:22 INFO     	 * (global step 10550: loss: 2.4119791984558105, lr: 0.0001
2024-01-06 08:48:28 INFO     	 * (global step 10600: loss: 2.4050750732421875, lr: 0.0001
2024-01-06 08:48:34 INFO     	 * (global step 10650: loss: 2.4624581336975098, lr: 0.0001
2024-01-06 08:48:40 INFO     	 * (global step 10700: loss: 2.3305559158325195, lr: 0.0001
2024-01-06 08:48:46 INFO     	 * (global step 10750: loss: 2.406020998954773, lr: 0.0001
2024-01-06 08:48:52 INFO     	 * (global step 10800: loss: 2.355554938316345, lr: 0.0001
2024-01-06 08:48:58 INFO     	 * (global step 10850: loss: 2.5324639081954956, lr: 0.0001
2024-01-06 08:49:03 INFO     	 * (global step 10900: loss: 2.3986411094665527, lr: 0.0001
2024-01-06 08:49:09 INFO     	 * (global step 10950: loss: 2.314608335494995, lr: 0.0001
2024-01-06 08:49:15 INFO     	 * (global step 11000: loss: 2.3647032976150513, lr: 0.0001
2024-01-06 08:49:21 INFO     	 * (global step 11050: loss: 2.4583417177200317, lr: 0.0001
2024-01-06 08:49:27 INFO     	 * (global step 11100: loss: 2.4770712852478027, lr: 0.0001
2024-01-06 08:49:33 INFO     	 * (global step 11150: loss: 2.571860194206238, lr: 0.0001
2024-01-06 08:49:39 INFO     	 * (global step 11200: loss: 2.371238589286804, lr: 0.0001
2024-01-06 08:49:45 INFO     	 * (global step 11250: loss: 2.3291473388671875, lr: 0.0001
2024-01-06 08:49:51 INFO     	 * (global step 11300: loss: 2.4142855405807495, lr: 0.0001
2024-01-06 08:49:57 INFO     	 * (global step 11350: loss: 2.4208714962005615, lr: 0.0001
2024-01-06 08:50:03 INFO     	 * (global step 11400: loss: 2.4052937030792236, lr: 0.0001
2024-01-06 08:50:09 INFO     	 * (global step 11450: loss: 2.5385793447494507, lr: 0.0001
2024-01-06 08:50:15 INFO     	 * (global step 11500: loss: 2.5349507331848145, lr: 0.0001
2024-01-06 08:50:21 INFO     	 * (global step 11550: loss: 2.4212392568588257, lr: 0.0001
2024-01-06 08:50:27 INFO     	 * (global step 11600: loss: 2.6625630855560303, lr: 0.0001
2024-01-06 08:50:33 INFO     	 * (global step 11650: loss: 2.381411910057068, lr: 0.0001
2024-01-06 08:50:39 INFO     	 * (global step 11700: loss: 2.416045665740967, lr: 0.0001
2024-01-06 08:50:45 INFO     	 * (global step 11750: loss: 2.4374815225601196, lr: 0.0001
2024-01-06 08:50:50 INFO     	 * (global step 11800: loss: 2.4499109983444214, lr: 0.0001
2024-01-06 08:50:56 INFO     	 * (global step 11850: loss: 2.584381341934204, lr: 0.0001
2024-01-06 08:51:02 INFO     	 * (global step 11900: loss: 2.5003243684768677, lr: 0.0001
2024-01-06 08:51:08 INFO     	 * (global step 11950: loss: 2.3026766777038574, lr: 0.0001
2024-01-06 08:51:14 INFO     	 * (global step 12000: loss: 2.3564401865005493, lr: 0.0001
2024-01-06 08:51:20 INFO     	 * (global step 12050: loss: 2.4335272312164307, lr: 0.0001
2024-01-06 08:51:26 INFO     	 * (global step 12100: loss: 2.543345093727112, lr: 0.0001
2024-01-06 08:51:31 INFO     [epoch 12/15] average loss: 2.468, lr: 0.0001
2024-01-06 08:51:31 INFO     saving model related files
2024-01-06 08:51:31 INFO     saving model
2024-01-06 08:51:32 INFO     saving tokenizer
2024-01-06 08:51:32 INFO     saving optimizer
2024-01-06 08:51:33 INFO     remove old optimizer files
2024-01-06 08:51:34 INFO     	 * (global step 12150: loss: 2.535353183746338, lr: 0.0001
2024-01-06 08:51:39 INFO     	 * (global step 12200: loss: 2.419254779815674, lr: 0.0001
2024-01-06 08:51:45 INFO     	 * (global step 12250: loss: 2.4016324281692505, lr: 0.0001
2024-01-06 08:51:51 INFO     	 * (global step 12300: loss: 2.424157977104187, lr: 0.0001
2024-01-06 08:51:57 INFO     	 * (global step 12350: loss: 2.5077379941940308, lr: 0.0001
2024-01-06 08:52:03 INFO     	 * (global step 12400: loss: 2.5663751363754272, lr: 0.0001
2024-01-06 08:52:09 INFO     	 * (global step 12450: loss: 2.4682514667510986, lr: 0.0001
2024-01-06 08:52:15 INFO     	 * (global step 12500: loss: 2.4556294679641724, lr: 0.0001
2024-01-06 08:52:21 INFO     	 * (global step 12550: loss: 2.535730481147766, lr: 0.0001
2024-01-06 08:52:27 INFO     	 * (global step 12600: loss: 2.5184690952301025, lr: 0.0001
2024-01-06 08:52:33 INFO     	 * (global step 12650: loss: 2.4142647981643677, lr: 0.0001
2024-01-06 08:52:39 INFO     	 * (global step 12700: loss: 2.4567980766296387, lr: 0.0001
2024-01-06 08:52:45 INFO     	 * (global step 12750: loss: 2.733714461326599, lr: 0.0001
2024-01-06 08:52:51 INFO     	 * (global step 12800: loss: 2.4375892877578735, lr: 0.0001
2024-01-06 08:52:57 INFO     	 * (global step 12850: loss: 2.58856999874115, lr: 0.0001
2024-01-06 08:53:03 INFO     	 * (global step 12900: loss: 2.374610185623169, lr: 0.0001
2024-01-06 08:53:09 INFO     	 * (global step 12950: loss: 2.41275954246521, lr: 0.0001
2024-01-06 08:53:15 INFO     	 * (global step 13000: loss: 2.4836655855178833, lr: 0.0001
2024-01-06 08:53:20 INFO     	 * (global step 13050: loss: 2.4650447368621826, lr: 0.0001
2024-01-06 08:53:26 INFO     	 * (global step 13100: loss: 2.540769577026367, lr: 0.0001
2024-01-06 08:53:32 INFO     	 * (global step 13150: loss: 2.552732467651367, lr: 0.0001
2024-01-06 08:53:38 INFO     	 * (global step 13200: loss: 2.3867225646972656, lr: 0.0001
2024-01-06 08:53:44 INFO     	 * (global step 13250: loss: 2.387334108352661, lr: 0.0001
2024-01-06 08:53:50 INFO     	 * (global step 13300: loss: 2.377473473548889, lr: 0.0001
2024-01-06 08:53:56 INFO     	 * (global step 13350: loss: 2.3686734437942505, lr: 0.0001
2024-01-06 08:54:02 INFO     	 * (global step 13400: loss: 2.521290421485901, lr: 0.0001
2024-01-06 08:54:08 INFO     	 * (global step 13450: loss: 2.8127031326293945, lr: 0.0001
2024-01-06 08:54:14 INFO     	 * (global step 13500: loss: 2.7703707218170166, lr: 0.0001
2024-01-06 08:54:20 INFO     	 * (global step 13550: loss: 2.5825313329696655, lr: 0.0001
2024-01-06 08:54:26 INFO     	 * (global step 13600: loss: 2.468949794769287, lr: 0.0001
2024-01-06 08:54:32 INFO     	 * (global step 13650: loss: 2.4659630060195923, lr: 0.0001
2024-01-06 08:54:38 INFO     	 * (global step 13700: loss: 2.3889832496643066, lr: 0.0001
2024-01-06 08:54:44 INFO     	 * (global step 13750: loss: 2.4276981353759766, lr: 0.0001
2024-01-06 08:54:50 INFO     	 * (global step 13800: loss: 2.455899477005005, lr: 0.0001
2024-01-06 08:54:56 INFO     	 * (global step 13850: loss: 2.3762848377227783, lr: 0.0001
2024-01-06 08:55:02 INFO     	 * (global step 13900: loss: 2.3862603902816772, lr: 0.0001
2024-01-06 08:55:07 INFO     	 * (global step 13950: loss: 2.471230983734131, lr: 0.0001
2024-01-06 08:55:13 INFO     	 * (global step 14000: loss: 2.447161555290222, lr: 0.0001
2024-01-06 08:55:19 INFO     	 * (global step 14050: loss: 2.6455708742141724, lr: 0.0001
2024-01-06 08:55:25 INFO     	 * (global step 14100: loss: 2.4723713397979736, lr: 0.0001
2024-01-06 08:55:31 INFO     	 * (global step 14150: loss: 2.509101629257202, lr: 0.0001
2024-01-06 08:55:37 INFO     	 * (global step 14200: loss: 2.490978479385376, lr: 0.0001
2024-01-06 08:55:43 INFO     	 * (global step 14250: loss: 2.435380697250366, lr: 0.0001
2024-01-06 08:55:49 INFO     	 * (global step 14300: loss: 2.3943560123443604, lr: 0.0001
2024-01-06 08:55:55 INFO     	 * (global step 14350: loss: 2.5381431579589844, lr: 0.0001
2024-01-06 08:56:01 INFO     	 * (global step 14400: loss: 2.639536738395691, lr: 0.0001
2024-01-06 08:56:07 INFO     	 * (global step 14450: loss: 2.4784666299819946, lr: 0.0001
2024-01-06 08:56:13 INFO     	 * (global step 14500: loss: 2.409778118133545, lr: 0.0001
2024-01-06 08:56:19 INFO     	 * (global step 14550: loss: 2.5056705474853516, lr: 0.0001
2024-01-06 08:56:25 INFO     	 * (global step 14600: loss: 2.512457251548767, lr: 0.0001
2024-01-06 08:56:31 INFO     	 * (global step 14650: loss: 2.3803577423095703, lr: 0.0001
2024-01-06 08:56:37 INFO     	 * (global step 14700: loss: 2.3650962114334106, lr: 0.0001
2024-01-06 08:56:43 INFO     	 * (global step 14750: loss: 2.6473259925842285, lr: 0.0001
2024-01-06 08:56:49 INFO     	 * (global step 14800: loss: 2.4464162588119507, lr: 0.0001
2024-01-06 08:56:54 INFO     	 * (global step 14850: loss: 2.5355422496795654, lr: 0.0001
2024-01-06 08:57:00 INFO     	 * (global step 14900: loss: 2.367567300796509, lr: 0.0001
2024-01-06 08:57:06 INFO     	 * (global step 14950: loss: 2.4561363458633423, lr: 0.0001
2024-01-06 08:57:12 INFO     	 * (global step 15000: loss: 2.3838995695114136, lr: 0.0001
2024-01-06 08:57:18 INFO     	 * (global step 15050: loss: 2.3767038583755493, lr: 0.0001
2024-01-06 08:57:24 INFO     	 * (global step 15100: loss: 2.4685118198394775, lr: 0.0001
2024-01-06 08:57:30 INFO     	 * (global step 15150: loss: 2.3660067319869995, lr: 0.0001
2024-01-06 08:57:36 INFO     	 * (global step 15200: loss: 2.3862463235855103, lr: 0.0001
2024-01-06 08:57:42 INFO     	 * (global step 15250: loss: 2.3665941953659058, lr: 0.0001
2024-01-06 08:57:48 INFO     	 * (global step 15300: loss: 2.4664496183395386, lr: 0.0001
2024-01-06 08:57:54 INFO     	 * (global step 15350: loss: 2.3772685527801514, lr: 0.0001
2024-01-06 08:58:00 INFO     	 * (global step 15400: loss: 2.4312877655029297, lr: 0.0001
2024-01-06 08:58:06 INFO     	 * (global step 15450: loss: 2.4237993955612183, lr: 0.0001
2024-01-06 08:58:12 INFO     	 * (global step 15500: loss: 2.6035823822021484, lr: 0.0001
2024-01-06 08:58:18 INFO     	 * (global step 15550: loss: 2.4458940029144287, lr: 0.0001
2024-01-06 08:58:24 INFO     	 * (global step 15600: loss: 2.3741610050201416, lr: 0.0001
2024-01-06 08:58:30 INFO     	 * (global step 15650: loss: 2.4048935174942017, lr: 0.0001
2024-01-06 08:58:36 INFO     	 * (global step 15700: loss: 2.313681483268738, lr: 0.0001
2024-01-06 08:58:41 INFO     	 * (global step 15750: loss: 2.519890785217285, lr: 0.0001
2024-01-06 08:58:47 INFO     	 * (global step 15800: loss: 2.46199369430542, lr: 0.0001
2024-01-06 08:58:53 INFO     	 * (global step 15850: loss: 2.521513342857361, lr: 0.0001
2024-01-06 08:58:59 INFO     	 * (global step 15900: loss: 2.359334945678711, lr: 0.0001
2024-01-06 08:59:05 INFO     	 * (global step 15950: loss: 2.4858713150024414, lr: 0.0001
2024-01-06 08:59:11 INFO     	 * (global step 16000: loss: 2.5764559507369995, lr: 0.0001
2024-01-06 08:59:17 INFO     	 * (global step 16050: loss: 2.540342330932617, lr: 0.0001
2024-01-06 08:59:23 INFO     	 * (global step 16100: loss: 2.5081288814544678, lr: 0.0001
2024-01-06 08:59:29 INFO     	 * (global step 16150: loss: 2.447848916053772, lr: 0.0001
2024-01-06 08:59:34 INFO     [epoch 13/15] average loss: 2.453, lr: 0.0001
2024-01-06 08:59:34 INFO     saving model related files
2024-01-06 08:59:34 INFO     saving model
2024-01-06 08:59:34 INFO     saving tokenizer
2024-01-06 08:59:35 INFO     saving optimizer
2024-01-06 08:59:35 INFO     remove old optimizer files
2024-01-06 08:59:36 INFO     	 * (global step 16200: loss: 2.4240195751190186, lr: 0.0001
2024-01-06 08:59:42 INFO     	 * (global step 16250: loss: 2.4651951789855957, lr: 0.0001
2024-01-06 08:59:48 INFO     	 * (global step 16300: loss: 2.397167682647705, lr: 0.0001
2024-01-06 08:59:54 INFO     	 * (global step 16350: loss: 2.4653358459472656, lr: 0.0001
2024-01-06 09:00:00 INFO     	 * (global step 16400: loss: 2.4576207399368286, lr: 0.0001
2024-01-06 09:00:06 INFO     	 * (global step 16450: loss: 2.3014715909957886, lr: 0.0001
2024-01-06 09:00:12 INFO     	 * (global step 16500: loss: 2.4448500871658325, lr: 0.0001
2024-01-06 09:00:18 INFO     	 * (global step 16550: loss: 2.4478466510772705, lr: 0.0001
2024-01-06 09:00:24 INFO     	 * (global step 16600: loss: 2.37973952293396, lr: 0.0001
2024-01-06 09:00:30 INFO     	 * (global step 16650: loss: 2.4415758848190308, lr: 0.0001
2024-01-06 09:00:36 INFO     	 * (global step 16700: loss: 2.45808207988739, lr: 0.0001
2024-01-06 09:00:42 INFO     	 * (global step 16750: loss: 2.4128516912460327, lr: 0.0001
2024-01-06 09:00:48 INFO     	 * (global step 16800: loss: 2.4819672107696533, lr: 0.0001
2024-01-06 09:00:54 INFO     	 * (global step 16850: loss: 2.3536776304244995, lr: 0.0001
2024-01-06 09:01:00 INFO     	 * (global step 16900: loss: 2.2970880270004272, lr: 0.0001
2024-01-06 09:01:06 INFO     	 * (global step 16950: loss: 2.4801615476608276, lr: 0.0001
2024-01-06 09:01:11 INFO     	 * (global step 17000: loss: 2.4131332635879517, lr: 0.0001
2024-01-06 09:01:17 INFO     	 * (global step 17050: loss: 2.467766046524048, lr: 0.0001
2024-01-06 09:01:23 INFO     	 * (global step 17100: loss: 2.33870005607605, lr: 0.0001
2024-01-06 09:01:29 INFO     	 * (global step 17150: loss: 2.4501620531082153, lr: 0.0001
2024-01-06 09:01:35 INFO     	 * (global step 17200: loss: 2.438989520072937, lr: 0.0001
2024-01-06 09:01:41 INFO     	 * (global step 17250: loss: 2.4720702171325684, lr: 0.0001
2024-01-06 09:01:47 INFO     	 * (global step 17300: loss: 2.2224881649017334, lr: 0.0001
2024-01-06 09:01:53 INFO     	 * (global step 17350: loss: 2.4964786767959595, lr: 0.0001
2024-01-06 09:01:59 INFO     	 * (global step 17400: loss: 2.5649739503860474, lr: 0.0001
2024-01-06 09:02:05 INFO     	 * (global step 17450: loss: 2.3711317777633667, lr: 0.0001
2024-01-06 09:02:11 INFO     	 * (global step 17500: loss: 2.5499507188796997, lr: 0.0001
2024-01-06 09:02:17 INFO     	 * (global step 17550: loss: 2.3629425764083862, lr: 0.0001
2024-01-06 09:02:23 INFO     	 * (global step 17600: loss: 2.34658682346344, lr: 0.0001
2024-01-06 09:02:29 INFO     	 * (global step 17650: loss: 2.8221054077148438, lr: 0.0001
2024-01-06 09:02:35 INFO     	 * (global step 17700: loss: 2.347815752029419, lr: 0.0001
2024-01-06 09:02:41 INFO     	 * (global step 17750: loss: 2.5693894624710083, lr: 0.0001
2024-01-06 09:02:47 INFO     	 * (global step 17800: loss: 2.260887622833252, lr: 0.0001
2024-01-06 09:02:52 INFO     	 * (global step 17850: loss: 2.4051705598831177, lr: 0.0001
2024-01-06 09:02:58 INFO     	 * (global step 17900: loss: 2.4529056549072266, lr: 0.0001
2024-01-06 09:03:04 INFO     	 * (global step 17950: loss: 2.3699995279312134, lr: 0.0001
2024-01-06 09:03:10 INFO     	 * (global step 18000: loss: 2.218135714530945, lr: 0.0001
2024-01-06 09:03:16 INFO     	 * (global step 18050: loss: 2.446373701095581, lr: 0.0001
2024-01-06 09:03:22 INFO     	 * (global step 18100: loss: 2.4008326530456543, lr: 0.0001
2024-01-06 09:03:28 INFO     	 * (global step 18150: loss: 2.466370701789856, lr: 0.0001
2024-01-06 09:03:34 INFO     	 * (global step 18200: loss: 2.4304826259613037, lr: 0.0001
2024-01-06 09:03:40 INFO     	 * (global step 18250: loss: 2.399153232574463, lr: 0.0001
2024-01-06 09:03:46 INFO     	 * (global step 18300: loss: 2.344789505004883, lr: 0.0001
2024-01-06 09:03:52 INFO     	 * (global step 18350: loss: 2.2952160835266113, lr: 0.0001
2024-01-06 09:03:58 INFO     	 * (global step 18400: loss: 2.669654965400696, lr: 0.0001
2024-01-06 09:04:04 INFO     	 * (global step 18450: loss: 2.276075005531311, lr: 0.0001
2024-01-06 09:04:10 INFO     	 * (global step 18500: loss: 2.458378791809082, lr: 0.0001
2024-01-06 09:04:16 INFO     	 * (global step 18550: loss: 2.429175019264221, lr: 0.0001
2024-01-06 09:04:22 INFO     	 * (global step 18600: loss: 2.499254822731018, lr: 0.0001
2024-01-06 09:04:28 INFO     	 * (global step 18650: loss: 2.4895408153533936, lr: 0.0001
2024-01-06 09:04:34 INFO     	 * (global step 18700: loss: 2.385911226272583, lr: 0.0001
2024-01-06 09:04:39 INFO     	 * (global step 18750: loss: 2.5404086112976074, lr: 0.0001
2024-01-06 09:04:45 INFO     	 * (global step 18800: loss: 2.5009926557540894, lr: 0.0001
2024-01-06 09:04:51 INFO     	 * (global step 18850: loss: 2.6397722959518433, lr: 0.0001
2024-01-06 09:04:57 INFO     	 * (global step 18900: loss: 2.3709291219711304, lr: 0.0001
2024-01-06 09:05:03 INFO     	 * (global step 18950: loss: 2.4111270904541016, lr: 0.0001
2024-01-06 09:05:09 INFO     	 * (global step 19000: loss: 2.471344232559204, lr: 0.0001
2024-01-06 09:05:15 INFO     	 * (global step 19050: loss: 2.5975236892700195, lr: 0.0001
2024-01-06 09:05:21 INFO     	 * (global step 19100: loss: 2.421904444694519, lr: 0.0001
2024-01-06 09:05:27 INFO     	 * (global step 19150: loss: 2.489232301712036, lr: 0.0001
2024-01-06 09:05:33 INFO     	 * (global step 19200: loss: 2.6510924100875854, lr: 0.0001
2024-01-06 09:05:39 INFO     	 * (global step 19250: loss: 2.5089513063430786, lr: 0.0001
2024-01-06 09:05:45 INFO     	 * (global step 19300: loss: 2.4646692276000977, lr: 0.0001
2024-01-06 09:05:51 INFO     	 * (global step 19350: loss: 2.4465821981430054, lr: 0.0001
2024-01-06 09:05:57 INFO     	 * (global step 19400: loss: 2.3621866703033447, lr: 0.0001
2024-01-06 09:06:03 INFO     	 * (global step 19450: loss: 2.5513139963150024, lr: 0.0001
2024-01-06 09:06:09 INFO     	 * (global step 19500: loss: 2.3849196434020996, lr: 0.0001
2024-01-06 09:06:15 INFO     	 * (global step 19550: loss: 2.41512393951416, lr: 0.0001
2024-01-06 09:06:21 INFO     	 * (global step 19600: loss: 2.3229912519454956, lr: 0.0001
2024-01-06 09:06:27 INFO     	 * (global step 19650: loss: 2.3303087949752808, lr: 0.0001
2024-01-06 09:06:33 INFO     	 * (global step 19700: loss: 2.438284993171692, lr: 0.0001
2024-01-06 09:06:39 INFO     	 * (global step 19750: loss: 2.5091238021850586, lr: 0.0001
2024-01-06 09:06:45 INFO     	 * (global step 19800: loss: 2.4841864109039307, lr: 0.0001
2024-01-06 09:06:51 INFO     	 * (global step 19850: loss: 2.4199037551879883, lr: 0.0001
2024-01-06 09:06:57 INFO     	 * (global step 19900: loss: 2.456545352935791, lr: 0.0001
2024-01-06 09:07:03 INFO     	 * (global step 19950: loss: 2.50352144241333, lr: 0.0001
2024-01-06 09:07:09 INFO     	 * (global step 20000: loss: 2.545270323753357, lr: 0.0001
2024-01-06 09:07:15 INFO     	 * (global step 20050: loss: 2.612017869949341, lr: 0.0001
2024-01-06 09:07:21 INFO     	 * (global step 20100: loss: 2.3174283504486084, lr: 0.0001
2024-01-06 09:07:27 INFO     	 * (global step 20150: loss: 2.4385173320770264, lr: 0.0001
2024-01-06 09:07:33 INFO     	 * (global step 20200: loss: 2.4665892124176025, lr: 0.0001
2024-01-06 09:07:38 INFO     [epoch 14/15] average loss: 2.442, lr: 0.0001
2024-01-06 09:07:38 INFO     saving model related files
2024-01-06 09:07:38 INFO     saving model
2024-01-06 09:07:38 INFO     saving tokenizer
2024-01-06 09:07:38 INFO     saving optimizer
2024-01-06 09:07:39 INFO     remove old optimizer files
2024-01-06 09:07:39 INFO     complete training: model ckpt was saved at small_recreated_ckpt/model_eszyci
2024-01-06 09:07:39 INFO     ## 2nd RUN: Configuration 3/5: validation/Bleu_4 = 0.09466239682555477
2024-01-06 09:07:39 INFO     initialize model trainer
2024-01-06 09:07:39 INFO     load config from existing checkpoint at small_recreated_ckpt/model_mzgdpa
2024-01-06 09:07:39 INFO     hyperparameters
2024-01-06 09:07:39 INFO     	 * dataset_path: lmqg/qag_squad
2024-01-06 09:07:39 INFO     	 * dataset_name: default
2024-01-06 09:07:39 INFO     	 * input_types: ['paragraph']
2024-01-06 09:07:39 INFO     	 * output_types: ['questions_answers']
2024-01-06 09:07:39 INFO     	 * prefix_types: ['qag']
2024-01-06 09:07:39 INFO     	 * model: t5-small
2024-01-06 09:07:39 INFO     	 * max_length: 512
2024-01-06 09:07:39 INFO     	 * max_length_output: 256
2024-01-06 09:07:39 INFO     	 * epoch: 15
2024-01-06 09:07:39 INFO     	 * batch: 2
2024-01-06 09:07:39 INFO     	 * lr: 0.0001
2024-01-06 09:07:39 INFO     	 * fp16: False
2024-01-06 09:07:39 INFO     	 * random_seed: 1
2024-01-06 09:07:39 INFO     	 * gradient_accumulation_steps: 2
2024-01-06 09:07:39 INFO     	 * label_smoothing: 0.0
2024-01-06 09:07:39 INFO     load checkpoint from small_recreated_ckpt/model_mzgdpa/epoch_10
2024-01-06 09:07:40 INFO     use spaCy answer extraction model: positionrank
2024-01-06 09:07:40 INFO     Model `small_recreated_ckpt/model_mzgdpa/epoch_10`
2024-01-06 09:07:40 INFO     	 * Num of GPU in use: 1
2024-01-06 09:07:40 INFO     	 * Prefix: True
2024-01-06 09:07:40 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 09:07:40 INFO     load optimizer from small_recreated_ckpt/model_mzgdpa/optimizers/optimizer.10.pt
2024-01-06 09:07:40 INFO     optimizer is loading on cuda
2024-01-06 09:07:44 INFO     dataset preprocessing
2024-01-06 09:07:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featurelmqg/qag_squad/t5-small.512.256.paragraph.questions_answers.train.qag.pkl
2024-01-06 09:07:46 INFO     start model training
2024-01-06 09:07:52 INFO     	 * (global step 50: loss: 0.3709024637937546, lr: 0.0001
2024-01-06 09:07:58 INFO     	 * (global step 100: loss: 0.5384281128644943, lr: 0.0001
2024-01-06 09:08:04 INFO     	 * (global step 150: loss: 0.4841551035642624, lr: 0.0001
2024-01-06 09:08:10 INFO     	 * (global step 200: loss: 0.49568426609039307, lr: 0.0001
2024-01-06 09:08:15 INFO     	 * (global step 250: loss: 0.4249236285686493, lr: 0.0001
2024-01-06 09:08:21 INFO     	 * (global step 300: loss: 0.6255639493465424, lr: 0.0001
2024-01-06 09:08:27 INFO     	 * (global step 350: loss: 0.4427247643470764, lr: 0.0001
2024-01-06 09:08:33 INFO     	 * (global step 400: loss: 0.531762570142746, lr: 0.0001
2024-01-06 09:08:39 INFO     	 * (global step 450: loss: 0.38069289922714233, lr: 0.0001
2024-01-06 09:08:44 INFO     	 * (global step 500: loss: 0.3200952857732773, lr: 0.0001
2024-01-06 09:08:50 INFO     	 * (global step 550: loss: 0.5733359754085541, lr: 0.0001
2024-01-06 09:08:56 INFO     	 * (global step 600: loss: 0.4624617248773575, lr: 0.0001
2024-01-06 09:09:02 INFO     	 * (global step 650: loss: 0.4230726584792137, lr: 0.0001
2024-01-06 09:09:08 INFO     	 * (global step 700: loss: 0.3891144245862961, lr: 0.0001
2024-01-06 09:09:13 INFO     	 * (global step 750: loss: 0.4470379799604416, lr: 0.0001
2024-01-06 09:09:19 INFO     	 * (global step 800: loss: 0.4628361463546753, lr: 0.0001
2024-01-06 09:09:25 INFO     	 * (global step 850: loss: 0.40430112183094025, lr: 0.0001
2024-01-06 09:09:31 INFO     	 * (global step 900: loss: 0.4249839633703232, lr: 0.0001
2024-01-06 09:09:37 INFO     	 * (global step 950: loss: 0.4427582025527954, lr: 0.0001
2024-01-06 09:09:42 INFO     	 * (global step 1000: loss: 0.2766039967536926, lr: 0.0001
2024-01-06 09:09:48 INFO     	 * (global step 1050: loss: 0.37335319072008133, lr: 0.0001
2024-01-06 09:09:54 INFO     	 * (global step 1100: loss: 0.5052480250597, lr: 0.0001
2024-01-06 09:10:00 INFO     	 * (global step 1150: loss: 0.37869633734226227, lr: 0.0001
2024-01-06 09:10:06 INFO     	 * (global step 1200: loss: 0.280693843960762, lr: 0.0001
2024-01-06 09:10:11 INFO     	 * (global step 1250: loss: 0.2781822830438614, lr: 0.0001
2024-01-06 09:10:17 INFO     	 * (global step 1300: loss: 0.5239287763834, lr: 0.0001
2024-01-06 09:10:23 INFO     	 * (global step 1350: loss: 0.41089965403079987, lr: 0.0001
2024-01-06 09:10:29 INFO     	 * (global step 1400: loss: 0.4680100828409195, lr: 0.0001
2024-01-06 09:10:35 INFO     	 * (global step 1450: loss: 0.5034524202346802, lr: 0.0001
2024-01-06 09:10:40 INFO     	 * (global step 1500: loss: 0.4450625330209732, lr: 0.0001
2024-01-06 09:10:46 INFO     	 * (global step 1550: loss: 0.4691256210207939, lr: 0.0001
2024-01-06 09:10:52 INFO     	 * (global step 1600: loss: 0.4887496083974838, lr: 0.0001
2024-01-06 09:10:58 INFO     	 * (global step 1650: loss: 0.6352104544639587, lr: 0.0001
2024-01-06 09:11:04 INFO     	 * (global step 1700: loss: 0.6154255867004395, lr: 0.0001
2024-01-06 09:11:10 INFO     	 * (global step 1750: loss: 0.2530341148376465, lr: 0.0001
2024-01-06 09:11:15 INFO     	 * (global step 1800: loss: 0.33125069737434387, lr: 0.0001
2024-01-06 09:11:21 INFO     	 * (global step 1850: loss: 0.3812513053417206, lr: 0.0001
2024-01-06 09:11:27 INFO     	 * (global step 1900: loss: 0.3325086683034897, lr: 0.0001
2024-01-06 09:11:33 INFO     	 * (global step 1950: loss: 0.4066277742385864, lr: 0.0001
2024-01-06 09:11:39 INFO     	 * (global step 2000: loss: 0.45235034823417664, lr: 0.0001
2024-01-06 09:11:44 INFO     	 * (global step 2050: loss: 0.3558327257633209, lr: 0.0001
2024-01-06 09:11:50 INFO     	 * (global step 2100: loss: 0.5561952441930771, lr: 0.0001
2024-01-06 09:11:56 INFO     	 * (global step 2150: loss: 0.49933508038520813, lr: 0.0001
2024-01-06 09:12:02 INFO     	 * (global step 2200: loss: 0.4439239203929901, lr: 0.0001
2024-01-06 09:12:08 INFO     	 * (global step 2250: loss: 0.3722532391548157, lr: 0.0001
2024-01-06 09:12:13 INFO     	 * (global step 2300: loss: 0.39118725806474686, lr: 0.0001
2024-01-06 09:12:19 INFO     	 * (global step 2350: loss: 0.39422938227653503, lr: 0.0001
2024-01-06 09:12:25 INFO     	 * (global step 2400: loss: 0.21288928389549255, lr: 0.0001
2024-01-06 09:12:31 INFO     	 * (global step 2450: loss: 0.394950732588768, lr: 0.0001
2024-01-06 09:12:37 INFO     	 * (global step 2500: loss: 0.47377072274684906, lr: 0.0001
2024-01-06 09:12:43 INFO     	 * (global step 2550: loss: 0.5452911406755447, lr: 0.0001
2024-01-06 09:12:48 INFO     	 * (global step 2600: loss: 0.3638736605644226, lr: 0.0001
2024-01-06 09:12:54 INFO     	 * (global step 2650: loss: 0.45757000148296356, lr: 0.0001
2024-01-06 09:13:00 INFO     	 * (global step 2700: loss: 0.5540862083435059, lr: 0.0001
2024-01-06 09:13:06 INFO     	 * (global step 2750: loss: 0.5251709520816803, lr: 0.0001
2024-01-06 09:13:12 INFO     	 * (global step 2800: loss: 0.34147775173187256, lr: 0.0001
2024-01-06 09:13:17 INFO     	 * (global step 2850: loss: 0.5766953080892563, lr: 0.0001
2024-01-06 09:13:23 INFO     	 * (global step 2900: loss: 0.572785809636116, lr: 0.0001
2024-01-06 09:13:29 INFO     	 * (global step 2950: loss: 0.2892707884311676, lr: 0.0001
2024-01-06 09:13:35 INFO     	 * (global step 3000: loss: 0.5167276561260223, lr: 0.0001
2024-01-06 09:13:41 INFO     	 * (global step 3050: loss: 0.39797961711883545, lr: 0.0001
2024-01-06 09:13:47 INFO     	 * (global step 3100: loss: 0.3427803963422775, lr: 0.0001
2024-01-06 09:13:52 INFO     	 * (global step 3150: loss: 0.49725526571273804, lr: 0.0001
2024-01-06 09:13:58 INFO     	 * (global step 3200: loss: 0.3412524610757828, lr: 0.0001
2024-01-06 09:14:04 INFO     	 * (global step 3250: loss: 0.2917891964316368, lr: 0.0001
2024-01-06 09:14:10 INFO     	 * (global step 3300: loss: 0.37038055062294006, lr: 0.0001
2024-01-06 09:14:16 INFO     	 * (global step 3350: loss: 0.4650953710079193, lr: 0.0001
2024-01-06 09:14:21 INFO     	 * (global step 3400: loss: 0.4919023960828781, lr: 0.0001
2024-01-06 09:14:27 INFO     	 * (global step 3450: loss: 0.28804393857717514, lr: 0.0001
2024-01-06 09:14:33 INFO     	 * (global step 3500: loss: 0.46568408608436584, lr: 0.0001
2024-01-06 09:14:39 INFO     	 * (global step 3550: loss: 0.5888561308383942, lr: 0.0001
2024-01-06 09:14:45 INFO     	 * (global step 3600: loss: 0.29792875051498413, lr: 0.0001
2024-01-06 09:14:50 INFO     	 * (global step 3650: loss: 0.4271742254495621, lr: 0.0001
2024-01-06 09:14:56 INFO     	 * (global step 3700: loss: 0.3589368760585785, lr: 0.0001
2024-01-06 09:15:02 INFO     	 * (global step 3750: loss: 0.43854792416095734, lr: 0.0001
2024-01-06 09:15:08 INFO     	 * (global step 3800: loss: 0.49414481222629547, lr: 0.0001
2024-01-06 09:15:14 INFO     	 * (global step 3850: loss: 0.614526242017746, lr: 0.0001
2024-01-06 09:15:20 INFO     	 * (global step 3900: loss: 0.5234956741333008, lr: 0.0001
2024-01-06 09:15:25 INFO     	 * (global step 3950: loss: 0.418916255235672, lr: 0.0001
2024-01-06 09:15:31 INFO     	 * (global step 4000: loss: 0.5776218771934509, lr: 0.0001
2024-01-06 09:15:37 INFO     [epoch 10/15] average loss: 0.448, lr: 0.0001
2024-01-06 09:15:37 INFO     saving model related files
2024-01-06 09:15:37 INFO     saving model
2024-01-06 09:15:37 INFO     saving tokenizer
2024-01-06 09:15:37 INFO     saving optimizer
2024-01-06 09:15:38 INFO     remove old optimizer files
2024-01-06 09:15:39 INFO     	 * (global step 4050: loss: 0.4658668786287308, lr: 0.0001
2024-01-06 09:15:44 INFO     	 * (global step 4100: loss: 0.519343301653862, lr: 0.0001
2024-01-06 09:15:50 INFO     	 * (global step 4150: loss: 0.4038146585226059, lr: 0.0001
2024-01-06 09:15:56 INFO     	 * (global step 4200: loss: 0.37129800021648407, lr: 0.0001
2024-01-06 09:16:02 INFO     	 * (global step 4250: loss: 0.44851377606391907, lr: 0.0001
2024-01-06 09:16:08 INFO     	 * (global step 4300: loss: 0.5272222459316254, lr: 0.0001
2024-01-06 09:16:13 INFO     	 * (global step 4350: loss: 0.3817591518163681, lr: 0.0001
2024-01-06 09:16:19 INFO     	 * (global step 4400: loss: 0.32604745030403137, lr: 0.0001
2024-01-06 09:16:25 INFO     	 * (global step 4450: loss: 0.6008288562297821, lr: 0.0001
2024-01-06 09:16:31 INFO     	 * (global step 4500: loss: 0.28490663319826126, lr: 0.0001
2024-01-06 09:16:37 INFO     	 * (global step 4550: loss: 0.6677285581827164, lr: 0.0001
2024-01-06 09:16:42 INFO     	 * (global step 4600: loss: 0.5080436617136002, lr: 0.0001
2024-01-06 09:16:48 INFO     	 * (global step 4650: loss: 0.5595780611038208, lr: 0.0001
2024-01-06 09:16:54 INFO     	 * (global step 4700: loss: 0.35297152400016785, lr: 0.0001
2024-01-06 09:17:00 INFO     	 * (global step 4750: loss: 0.39137497544288635, lr: 0.0001
2024-01-06 09:17:06 INFO     	 * (global step 4800: loss: 0.3764440417289734, lr: 0.0001
2024-01-06 09:17:12 INFO     	 * (global step 4850: loss: 0.37254762649536133, lr: 0.0001
2024-01-06 09:17:17 INFO     	 * (global step 4900: loss: 0.4987860769033432, lr: 0.0001
2024-01-06 09:17:23 INFO     	 * (global step 4950: loss: 0.4629773199558258, lr: 0.0001
2024-01-06 09:17:29 INFO     	 * (global step 5000: loss: 0.5080225318670273, lr: 0.0001
2024-01-06 09:17:35 INFO     	 * (global step 5050: loss: 0.35535623133182526, lr: 0.0001
2024-01-06 09:17:41 INFO     	 * (global step 5100: loss: 0.28283558785915375, lr: 0.0001
2024-01-06 09:17:46 INFO     	 * (global step 5150: loss: 0.3910454958677292, lr: 0.0001
2024-01-06 09:17:52 INFO     	 * (global step 5200: loss: 0.2730994299054146, lr: 0.0001
2024-01-06 09:17:58 INFO     	 * (global step 5250: loss: 0.4285562187433243, lr: 0.0001
2024-01-06 09:18:04 INFO     	 * (global step 5300: loss: 0.5302615165710449, lr: 0.0001
2024-01-06 09:18:10 INFO     	 * (global step 5350: loss: 0.49555331468582153, lr: 0.0001
2024-01-06 09:18:15 INFO     	 * (global step 5400: loss: 0.46731822192668915, lr: 0.0001
2024-01-06 09:18:21 INFO     	 * (global step 5450: loss: 0.512864351272583, lr: 0.0001
2024-01-06 09:18:27 INFO     	 * (global step 5500: loss: 0.4683355987071991, lr: 0.0001
2024-01-06 09:18:33 INFO     	 * (global step 5550: loss: 0.41284923255443573, lr: 0.0001
2024-01-06 09:18:39 INFO     	 * (global step 5600: loss: 0.6921710968017578, lr: 0.0001
2024-01-06 09:18:44 INFO     	 * (global step 5650: loss: 0.4949851334095001, lr: 0.0001
2024-01-06 09:18:50 INFO     	 * (global step 5700: loss: 0.6076050102710724, lr: 0.0001
2024-01-06 09:18:56 INFO     	 * (global step 5750: loss: 0.5349401086568832, lr: 0.0001
2024-01-06 09:19:02 INFO     	 * (global step 5800: loss: 0.3352358192205429, lr: 0.0001
2024-01-06 09:19:08 INFO     	 * (global step 5850: loss: 0.3283326327800751, lr: 0.0001
2024-01-06 09:19:14 INFO     	 * (global step 5900: loss: 0.304796002805233, lr: 0.0001
2024-01-06 09:19:19 INFO     	 * (global step 5950: loss: 0.2878125309944153, lr: 0.0001
2024-01-06 09:19:25 INFO     	 * (global step 6000: loss: 0.2027551308274269, lr: 0.0001
2024-01-06 09:19:31 INFO     	 * (global step 6050: loss: 0.43562711775302887, lr: 0.0001
2024-01-06 09:19:37 INFO     	 * (global step 6100: loss: 0.2982371971011162, lr: 0.0001
2024-01-06 09:19:43 INFO     	 * (global step 6150: loss: 0.6199644804000854, lr: 0.0001
2024-01-06 09:19:48 INFO     	 * (global step 6200: loss: 0.4683120399713516, lr: 0.0001
2024-01-06 09:19:54 INFO     	 * (global step 6250: loss: 0.29227787256240845, lr: 0.0001
2024-01-06 09:20:00 INFO     	 * (global step 6300: loss: 0.5080796629190445, lr: 0.0001
2024-01-06 09:20:06 INFO     	 * (global step 6350: loss: 0.35634662210941315, lr: 0.0001
2024-01-06 09:20:12 INFO     	 * (global step 6400: loss: 0.38919076323509216, lr: 0.0001
2024-01-06 09:20:17 INFO     	 * (global step 6450: loss: 0.4362212121486664, lr: 0.0001
2024-01-06 09:20:23 INFO     	 * (global step 6500: loss: 0.4763725697994232, lr: 0.0001
2024-01-06 09:20:29 INFO     	 * (global step 6550: loss: 0.2812209129333496, lr: 0.0001
2024-01-06 09:20:35 INFO     	 * (global step 6600: loss: 0.40310366451740265, lr: 0.0001
2024-01-06 09:20:41 INFO     	 * (global step 6650: loss: 0.3999970406293869, lr: 0.0001
2024-01-06 09:20:46 INFO     	 * (global step 6700: loss: 0.3298283517360687, lr: 0.0001
2024-01-06 09:20:52 INFO     	 * (global step 6750: loss: 0.5695742815732956, lr: 0.0001
2024-01-06 09:20:58 INFO     	 * (global step 6800: loss: 0.25395169109106064, lr: 0.0001
2024-01-06 09:21:04 INFO     	 * (global step 6850: loss: 0.47934454679489136, lr: 0.0001
2024-01-06 09:21:10 INFO     	 * (global step 6900: loss: 0.4501053988933563, lr: 0.0001
2024-01-06 09:21:16 INFO     	 * (global step 6950: loss: 0.32989178597927094, lr: 0.0001
2024-01-06 09:21:21 INFO     	 * (global step 7000: loss: 0.8073389828205109, lr: 0.0001
2024-01-06 09:21:27 INFO     	 * (global step 7050: loss: 0.3358142375946045, lr: 0.0001
2024-01-06 09:21:33 INFO     	 * (global step 7100: loss: 0.7553067952394485, lr: 0.0001
2024-01-06 09:21:39 INFO     	 * (global step 7150: loss: 0.42801134288311005, lr: 0.0001
2024-01-06 09:21:45 INFO     	 * (global step 7200: loss: 0.36200472712516785, lr: 0.0001
2024-01-06 09:21:50 INFO     	 * (global step 7250: loss: 0.4239262789487839, lr: 0.0001
2024-01-06 09:21:56 INFO     	 * (global step 7300: loss: 0.4694444537162781, lr: 0.0001
2024-01-06 09:22:02 INFO     	 * (global step 7350: loss: 0.40782634913921356, lr: 0.0001
2024-01-06 09:22:08 INFO     	 * (global step 7400: loss: 0.44311095774173737, lr: 0.0001
2024-01-06 09:22:14 INFO     	 * (global step 7450: loss: 0.5346572399139404, lr: 0.0001
2024-01-06 09:22:19 INFO     	 * (global step 7500: loss: 0.3614695072174072, lr: 0.0001
2024-01-06 09:22:25 INFO     	 * (global step 7550: loss: 0.34951600432395935, lr: 0.0001
2024-01-06 09:22:31 INFO     	 * (global step 7600: loss: 0.451994851231575, lr: 0.0001
2024-01-06 09:22:37 INFO     	 * (global step 7650: loss: 0.5542940199375153, lr: 0.0001
2024-01-06 09:22:43 INFO     	 * (global step 7700: loss: 0.40129440277814865, lr: 0.0001
2024-01-06 09:22:49 INFO     	 * (global step 7750: loss: 0.5382452756166458, lr: 0.0001
2024-01-06 09:22:54 INFO     	 * (global step 7800: loss: 0.4542447179555893, lr: 0.0001
2024-01-06 09:23:00 INFO     	 * (global step 7850: loss: 0.2830161303281784, lr: 0.0001
2024-01-06 09:23:06 INFO     	 * (global step 7900: loss: 0.4073302894830704, lr: 0.0001
2024-01-06 09:23:12 INFO     	 * (global step 7950: loss: 0.31850259006023407, lr: 0.0001
2024-01-06 09:23:18 INFO     	 * (global step 8000: loss: 0.38255317509174347, lr: 0.0001
2024-01-06 09:23:23 INFO     	 * (global step 8050: loss: 0.34738563001155853, lr: 0.0001
2024-01-06 09:23:29 INFO     [epoch 11/15] average loss: 0.439, lr: 0.0001
2024-01-06 09:23:29 INFO     saving model related files
2024-01-06 09:23:29 INFO     saving model
2024-01-06 09:23:29 INFO     saving tokenizer
2024-01-06 09:23:29 INFO     saving optimizer
2024-01-06 09:23:30 INFO     remove old optimizer files
2024-01-06 09:23:31 INFO     	 * (global step 8100: loss: 0.4005533903837204, lr: 0.0001
2024-01-06 09:23:36 INFO     	 * (global step 8150: loss: 0.33122240006923676, lr: 0.0001
2024-01-06 09:23:42 INFO     	 * (global step 8200: loss: 0.3729870468378067, lr: 0.0001
2024-01-06 09:23:48 INFO     	 * (global step 8250: loss: 0.4175034165382385, lr: 0.0001
2024-01-06 09:23:54 INFO     	 * (global step 8300: loss: 0.37588414549827576, lr: 0.0001
2024-01-06 09:24:00 INFO     	 * (global step 8350: loss: 0.4388752579689026, lr: 0.0001
2024-01-06 09:24:06 INFO     	 * (global step 8400: loss: 0.3552459180355072, lr: 0.0001
2024-01-06 09:24:11 INFO     	 * (global step 8450: loss: 0.3534094989299774, lr: 0.0001
2024-01-06 09:24:17 INFO     	 * (global step 8500: loss: 0.4253499507904053, lr: 0.0001
2024-01-06 09:24:23 INFO     	 * (global step 8550: loss: 0.3099699169397354, lr: 0.0001
2024-01-06 09:24:29 INFO     	 * (global step 8600: loss: 0.4212922900915146, lr: 0.0001
2024-01-06 09:24:35 INFO     	 * (global step 8650: loss: 0.3651082217693329, lr: 0.0001
2024-01-06 09:24:40 INFO     	 * (global step 8700: loss: 0.35176698863506317, lr: 0.0001
2024-01-06 09:24:46 INFO     	 * (global step 8750: loss: 0.35193681716918945, lr: 0.0001
2024-01-06 09:24:52 INFO     	 * (global step 8800: loss: 0.45964911580085754, lr: 0.0001
2024-01-06 09:24:58 INFO     	 * (global step 8850: loss: 0.4317670166492462, lr: 0.0001
2024-01-06 09:25:04 INFO     	 * (global step 8900: loss: 0.3451298549771309, lr: 0.0001
2024-01-06 09:25:09 INFO     	 * (global step 8950: loss: 0.4272160083055496, lr: 0.0001
2024-01-06 09:25:15 INFO     	 * (global step 9000: loss: 0.2609034329652786, lr: 0.0001
2024-01-06 09:25:21 INFO     	 * (global step 9050: loss: 0.2498990222811699, lr: 0.0001
2024-01-06 09:25:27 INFO     	 * (global step 9100: loss: 0.39444203674793243, lr: 0.0001
2024-01-06 09:25:33 INFO     	 * (global step 9150: loss: 0.3985796868801117, lr: 0.0001
2024-01-06 09:25:38 INFO     	 * (global step 9200: loss: 0.3765387237071991, lr: 0.0001
2024-01-06 09:25:44 INFO     	 * (global step 9250: loss: 0.39648687839508057, lr: 0.0001
2024-01-06 09:25:50 INFO     	 * (global step 9300: loss: 0.288875013589859, lr: 0.0001
2024-01-06 09:25:56 INFO     	 * (global step 9350: loss: 0.24784738570451736, lr: 0.0001
2024-01-06 09:26:02 INFO     	 * (global step 9400: loss: 0.4597546011209488, lr: 0.0001
2024-01-06 09:26:07 INFO     	 * (global step 9450: loss: 0.23590782284736633, lr: 0.0001
2024-01-06 09:26:13 INFO     	 * (global step 9500: loss: 0.34349052608013153, lr: 0.0001
2024-01-06 09:26:19 INFO     	 * (global step 9550: loss: 0.4273066967725754, lr: 0.0001
2024-01-06 09:26:25 INFO     	 * (global step 9600: loss: 0.2581067532300949, lr: 0.0001
2024-01-06 09:26:31 INFO     	 * (global step 9650: loss: 0.4644564986228943, lr: 0.0001
2024-01-06 09:26:37 INFO     	 * (global step 9700: loss: 0.3010026663541794, lr: 0.0001
2024-01-06 09:26:42 INFO     	 * (global step 9750: loss: 0.2733387276530266, lr: 0.0001
2024-01-06 09:26:48 INFO     	 * (global step 9800: loss: 0.4835476875305176, lr: 0.0001
2024-01-06 09:26:54 INFO     	 * (global step 9850: loss: 0.2860056757926941, lr: 0.0001
2024-01-06 09:27:00 INFO     	 * (global step 9900: loss: 0.5053586065769196, lr: 0.0001
2024-01-06 09:27:06 INFO     	 * (global step 9950: loss: 0.4349644184112549, lr: 0.0001
2024-01-06 09:27:11 INFO     	 * (global step 10000: loss: 0.5272804796695709, lr: 0.0001
2024-01-06 09:27:17 INFO     	 * (global step 10050: loss: 0.42338478565216064, lr: 0.0001
2024-01-06 09:27:23 INFO     	 * (global step 10100: loss: 0.43424609303474426, lr: 0.0001
2024-01-06 09:27:29 INFO     	 * (global step 10150: loss: 0.4632524698972702, lr: 0.0001
2024-01-06 09:27:35 INFO     	 * (global step 10200: loss: 0.4204622507095337, lr: 0.0001
2024-01-06 09:27:41 INFO     	 * (global step 10250: loss: 0.41085660457611084, lr: 0.0001
2024-01-06 09:27:47 INFO     	 * (global step 10300: loss: 0.49665120244026184, lr: 0.0001
2024-01-06 09:27:52 INFO     	 * (global step 10350: loss: 0.3453109413385391, lr: 0.0001
2024-01-06 09:27:58 INFO     	 * (global step 10400: loss: 0.3784194886684418, lr: 0.0001
2024-01-06 09:28:04 INFO     	 * (global step 10450: loss: 0.35662662982940674, lr: 0.0001
2024-01-06 09:28:10 INFO     	 * (global step 10500: loss: 0.4607580155134201, lr: 0.0001
2024-01-06 09:28:16 INFO     	 * (global step 10550: loss: 0.3845798820257187, lr: 0.0001
2024-01-06 09:28:22 INFO     	 * (global step 10600: loss: 0.3606715649366379, lr: 0.0001
2024-01-06 09:28:27 INFO     	 * (global step 10650: loss: 0.4461812227964401, lr: 0.0001
2024-01-06 09:28:33 INFO     	 * (global step 10700: loss: 0.2793922871351242, lr: 0.0001
2024-01-06 09:28:39 INFO     	 * (global step 10750: loss: 0.35003991425037384, lr: 0.0001
2024-01-06 09:28:45 INFO     	 * (global step 10800: loss: 0.3088909536600113, lr: 0.0001
2024-01-06 09:28:51 INFO     	 * (global step 10850: loss: 0.4955708533525467, lr: 0.0001
2024-01-06 09:28:56 INFO     	 * (global step 10900: loss: 0.35043400526046753, lr: 0.0001
2024-01-06 09:29:02 INFO     	 * (global step 10950: loss: 0.27453137934207916, lr: 0.0001
2024-01-06 09:29:08 INFO     	 * (global step 11000: loss: 0.3218613415956497, lr: 0.0001
2024-01-06 09:29:14 INFO     	 * (global step 11050: loss: 0.4032042473554611, lr: 0.0001
2024-01-06 09:29:20 INFO     	 * (global step 11100: loss: 0.4477520138025284, lr: 0.0001
2024-01-06 09:29:25 INFO     	 * (global step 11150: loss: 0.5286982506513596, lr: 0.0001
2024-01-06 09:29:31 INFO     	 * (global step 11200: loss: 0.3406525105237961, lr: 0.0001
2024-01-06 09:29:37 INFO     	 * (global step 11250: loss: 0.2880495488643646, lr: 0.0001
2024-01-06 09:29:43 INFO     	 * (global step 11300: loss: 0.3578381985425949, lr: 0.0001
2024-01-06 09:29:49 INFO     	 * (global step 11350: loss: 0.37299518287181854, lr: 0.0001
2024-01-06 09:29:55 INFO     	 * (global step 11400: loss: 0.3805685341358185, lr: 0.0001
2024-01-06 09:30:00 INFO     	 * (global step 11450: loss: 0.5129882097244263, lr: 0.0001
2024-01-06 09:30:06 INFO     	 * (global step 11500: loss: 0.516774982213974, lr: 0.0001
2024-01-06 09:30:12 INFO     	 * (global step 11550: loss: 0.4078768789768219, lr: 0.0001
2024-01-06 09:30:18 INFO     	 * (global step 11600: loss: 0.6419453024864197, lr: 0.0001
2024-01-06 09:30:24 INFO     	 * (global step 11650: loss: 0.35151535272598267, lr: 0.0001
2024-01-06 09:30:29 INFO     	 * (global step 11700: loss: 0.3987162932753563, lr: 0.0001
2024-01-06 09:30:35 INFO     	 * (global step 11750: loss: 0.39810654520988464, lr: 0.0001
2024-01-06 09:30:41 INFO     	 * (global step 11800: loss: 0.4215310961008072, lr: 0.0001
2024-01-06 09:30:47 INFO     	 * (global step 11850: loss: 0.567682296037674, lr: 0.0001
2024-01-06 09:30:53 INFO     	 * (global step 11900: loss: 0.4789670407772064, lr: 0.0001
2024-01-06 09:30:59 INFO     	 * (global step 11950: loss: 0.2808419466018677, lr: 0.0001
2024-01-06 09:31:04 INFO     	 * (global step 12000: loss: 0.3032934367656708, lr: 0.0001
2024-01-06 09:31:10 INFO     	 * (global step 12050: loss: 0.40669260919094086, lr: 0.0001
2024-01-06 09:31:16 INFO     	 * (global step 12100: loss: 0.48235342651605606, lr: 0.0001
2024-01-06 09:31:21 INFO     [epoch 12/15] average loss: 0.431, lr: 0.0001
2024-01-06 09:31:21 INFO     saving model related files
2024-01-06 09:31:21 INFO     saving model
2024-01-06 09:31:22 INFO     saving tokenizer
2024-01-06 09:31:22 INFO     saving optimizer
2024-01-06 09:31:23 INFO     remove old optimizer files
2024-01-06 09:31:23 INFO     	 * (global step 12150: loss: 0.4614643454551697, lr: 0.0001
2024-01-06 09:31:29 INFO     	 * (global step 12200: loss: 0.38942134380340576, lr: 0.0001
2024-01-06 09:31:35 INFO     	 * (global step 12250: loss: 0.3524494022130966, lr: 0.0001
2024-01-06 09:31:41 INFO     	 * (global step 12300: loss: 0.4061161130666733, lr: 0.0001
2024-01-06 09:31:46 INFO     	 * (global step 12350: loss: 0.45807141065597534, lr: 0.0001
2024-01-06 09:31:52 INFO     	 * (global step 12400: loss: 0.516750156879425, lr: 0.0001
2024-01-06 09:31:58 INFO     	 * (global step 12450: loss: 0.44168271124362946, lr: 0.0001
2024-01-06 09:32:04 INFO     	 * (global step 12500: loss: 0.4458676874637604, lr: 0.0001
2024-01-06 09:32:10 INFO     	 * (global step 12550: loss: 0.500534176826477, lr: 0.0001
2024-01-06 09:32:16 INFO     	 * (global step 12600: loss: 0.4793378859758377, lr: 0.0001
2024-01-06 09:32:21 INFO     	 * (global step 12650: loss: 0.36733250319957733, lr: 0.0001
2024-01-06 09:32:27 INFO     	 * (global step 12700: loss: 0.42403289675712585, lr: 0.0001
2024-01-06 09:32:33 INFO     	 * (global step 12750: loss: 0.7139603197574615, lr: 0.0001
2024-01-06 09:32:39 INFO     	 * (global step 12800: loss: 0.39881031960248947, lr: 0.0001
2024-01-06 09:32:45 INFO     	 * (global step 12850: loss: 0.5337329059839249, lr: 0.0001
2024-01-06 09:32:50 INFO     	 * (global step 12900: loss: 0.3452869951725006, lr: 0.0001
2024-01-06 09:32:56 INFO     	 * (global step 12950: loss: 0.3946651667356491, lr: 0.0001
2024-01-06 09:33:02 INFO     	 * (global step 13000: loss: 0.4646628201007843, lr: 0.0001
2024-01-06 09:33:08 INFO     	 * (global step 13050: loss: 0.42748627066612244, lr: 0.0001
2024-01-06 09:33:14 INFO     	 * (global step 13100: loss: 0.49568919837474823, lr: 0.0001
2024-01-06 09:33:19 INFO     	 * (global step 13150: loss: 0.5419517755508423, lr: 0.0001
2024-01-06 09:33:25 INFO     	 * (global step 13200: loss: 0.3296288251876831, lr: 0.0001
2024-01-06 09:33:31 INFO     	 * (global step 13250: loss: 0.35744771361351013, lr: 0.0001
2024-01-06 09:33:37 INFO     	 * (global step 13300: loss: 0.3444063067436218, lr: 0.0001
2024-01-06 09:33:43 INFO     	 * (global step 13350: loss: 0.3292561024427414, lr: 0.0001
2024-01-06 09:33:48 INFO     	 * (global step 13400: loss: 0.49117177724838257, lr: 0.0001
2024-01-06 09:33:54 INFO     	 * (global step 13450: loss: 0.812120795249939, lr: 0.0001
2024-01-06 09:34:00 INFO     	 * (global step 13500: loss: 0.7498399317264557, lr: 0.0001
2024-01-06 09:34:06 INFO     	 * (global step 13550: loss: 0.5822480618953705, lr: 0.0001
2024-01-06 09:34:12 INFO     	 * (global step 13600: loss: 0.4318067580461502, lr: 0.0001
2024-01-06 09:34:17 INFO     	 * (global step 13650: loss: 0.45945924520492554, lr: 0.0001
2024-01-06 09:34:23 INFO     	 * (global step 13700: loss: 0.34896156191825867, lr: 0.0001
2024-01-06 09:34:29 INFO     	 * (global step 13750: loss: 0.39113324880599976, lr: 0.0001
2024-01-06 09:34:35 INFO     	 * (global step 13800: loss: 0.4217383563518524, lr: 0.0001
2024-01-06 09:34:41 INFO     	 * (global step 13850: loss: 0.34228411316871643, lr: 0.0001
2024-01-06 09:34:47 INFO     	 * (global step 13900: loss: 0.3614267334342003, lr: 0.0001
2024-01-06 09:34:52 INFO     	 * (global step 13950: loss: 0.4780890643596649, lr: 0.0001
2024-01-06 09:34:58 INFO     	 * (global step 14000: loss: 0.3909837007522583, lr: 0.0001
2024-01-06 09:35:04 INFO     	 * (global step 14050: loss: 0.6272953450679779, lr: 0.0001
2024-01-06 09:35:10 INFO     	 * (global step 14100: loss: 0.4500682055950165, lr: 0.0001
2024-01-06 09:35:16 INFO     	 * (global step 14150: loss: 0.4933115243911743, lr: 0.0001
2024-01-06 09:35:21 INFO     	 * (global step 14200: loss: 0.45429806411266327, lr: 0.0001
2024-01-06 09:35:27 INFO     	 * (global step 14250: loss: 0.3960021883249283, lr: 0.0001
2024-01-06 09:35:33 INFO     	 * (global step 14300: loss: 0.3630901426076889, lr: 0.0001
2024-01-06 09:35:39 INFO     	 * (global step 14350: loss: 0.512344166636467, lr: 0.0001
2024-01-06 09:35:45 INFO     	 * (global step 14400: loss: 0.6472345292568207, lr: 0.0001
2024-01-06 09:35:50 INFO     	 * (global step 14450: loss: 0.4238193482160568, lr: 0.0001
2024-01-06 09:35:56 INFO     	 * (global step 14500: loss: 0.38402704894542694, lr: 0.0001
2024-01-06 09:36:02 INFO     	 * (global step 14550: loss: 0.48759396374225616, lr: 0.0001
2024-01-06 09:36:08 INFO     	 * (global step 14600: loss: 0.4806647151708603, lr: 0.0001
2024-01-06 09:36:14 INFO     	 * (global step 14650: loss: 0.3258408308029175, lr: 0.0001
2024-01-06 09:36:19 INFO     	 * (global step 14700: loss: 0.3156875967979431, lr: 0.0001
2024-01-06 09:36:25 INFO     	 * (global step 14750: loss: 0.633755087852478, lr: 0.0001
2024-01-06 09:36:31 INFO     	 * (global step 14800: loss: 0.42030490934848785, lr: 0.0001
2024-01-06 09:36:37 INFO     	 * (global step 14850: loss: 0.5249210596084595, lr: 0.0001
2024-01-06 09:36:43 INFO     	 * (global step 14900: loss: 0.31930121779441833, lr: 0.0001
2024-01-06 09:36:49 INFO     	 * (global step 14950: loss: 0.44654230773448944, lr: 0.0001
2024-01-06 09:36:54 INFO     	 * (global step 15000: loss: 0.37012695521116257, lr: 0.0001
2024-01-06 09:37:00 INFO     	 * (global step 15050: loss: 0.3472418636083603, lr: 0.0001
2024-01-06 09:37:06 INFO     	 * (global step 15100: loss: 0.4519077241420746, lr: 0.0001
2024-01-06 09:37:12 INFO     	 * (global step 15150: loss: 0.33327144384384155, lr: 0.0001
2024-01-06 09:37:18 INFO     	 * (global step 15200: loss: 0.349313423037529, lr: 0.0001
2024-01-06 09:37:23 INFO     	 * (global step 15250: loss: 0.336882084608078, lr: 0.0001
2024-01-06 09:37:29 INFO     	 * (global step 15300: loss: 0.44145649671554565, lr: 0.0001
2024-01-06 09:37:35 INFO     	 * (global step 15350: loss: 0.34994669258594513, lr: 0.0001
2024-01-06 09:37:41 INFO     	 * (global step 15400: loss: 0.401989609003067, lr: 0.0001
2024-01-06 09:37:47 INFO     	 * (global step 15450: loss: 0.3974592834711075, lr: 0.0001
2024-01-06 09:37:52 INFO     	 * (global step 15500: loss: 0.5822801887989044, lr: 0.0001
2024-01-06 09:37:58 INFO     	 * (global step 15550: loss: 0.4053622931241989, lr: 0.0001
2024-01-06 09:38:04 INFO     	 * (global step 15600: loss: 0.3473486751317978, lr: 0.0001
2024-01-06 09:38:10 INFO     	 * (global step 15650: loss: 0.3773050904273987, lr: 0.0001
2024-01-06 09:38:16 INFO     	 * (global step 15700: loss: 0.24481622874736786, lr: 0.0001
2024-01-06 09:38:21 INFO     	 * (global step 15750: loss: 0.5091095715761185, lr: 0.0001
2024-01-06 09:38:27 INFO     	 * (global step 15800: loss: 0.42487606406211853, lr: 0.0001
2024-01-06 09:38:33 INFO     	 * (global step 15850: loss: 0.5105255395174026, lr: 0.0001
2024-01-06 09:38:39 INFO     	 * (global step 15900: loss: 0.32098500430583954, lr: 0.0001
2024-01-06 09:38:45 INFO     	 * (global step 15950: loss: 0.4598042219877243, lr: 0.0001
2024-01-06 09:38:51 INFO     	 * (global step 16000: loss: 0.5601904988288879, lr: 0.0001
2024-01-06 09:38:56 INFO     	 * (global step 16050: loss: 0.5219043344259262, lr: 0.0001
2024-01-06 09:39:02 INFO     	 * (global step 16100: loss: 0.4805375188589096, lr: 0.0001
2024-01-06 09:39:08 INFO     	 * (global step 16150: loss: 0.3907894641160965, lr: 0.0001
2024-01-06 09:39:13 INFO     [epoch 13/15] average loss: 0.424, lr: 0.0001
2024-01-06 09:39:13 INFO     saving model related files
2024-01-06 09:39:13 INFO     saving model
2024-01-06 09:39:13 INFO     saving tokenizer
2024-01-06 09:39:13 INFO     saving optimizer
2024-01-06 09:39:14 INFO     remove old optimizer files
2024-01-06 09:39:15 INFO     	 * (global step 16200: loss: 0.389129176735878, lr: 0.0001
2024-01-06 09:39:21 INFO     	 * (global step 16250: loss: 0.4090789258480072, lr: 0.0001
2024-01-06 09:39:27 INFO     	 * (global step 16300: loss: 0.3407905027270317, lr: 0.0001
2024-01-06 09:39:33 INFO     	 * (global step 16350: loss: 0.4370981603860855, lr: 0.0001
2024-01-06 09:39:38 INFO     	 * (global step 16400: loss: 0.43061065673828125, lr: 0.0001
2024-01-06 09:39:44 INFO     	 * (global step 16450: loss: 0.25504229962825775, lr: 0.0001
2024-01-06 09:39:50 INFO     	 * (global step 16500: loss: 0.4390058219432831, lr: 0.0001
2024-01-06 09:39:56 INFO     	 * (global step 16550: loss: 0.44224637746810913, lr: 0.0001
2024-01-06 09:40:02 INFO     	 * (global step 16600: loss: 0.36667462438344955, lr: 0.0001
2024-01-06 09:40:08 INFO     	 * (global step 16650: loss: 0.4130421578884125, lr: 0.0001
2024-01-06 09:40:13 INFO     	 * (global step 16700: loss: 0.4432213008403778, lr: 0.0001
2024-01-06 09:40:19 INFO     	 * (global step 16750: loss: 0.3969338685274124, lr: 0.0001
2024-01-06 09:40:25 INFO     	 * (global step 16800: loss: 0.43765318393707275, lr: 0.0001
2024-01-06 09:40:31 INFO     	 * (global step 16850: loss: 0.32799138128757477, lr: 0.0001
2024-01-06 09:40:37 INFO     	 * (global step 16900: loss: 0.25799109786748886, lr: 0.0001
2024-01-06 09:40:42 INFO     	 * (global step 16950: loss: 0.44973720610141754, lr: 0.0001
2024-01-06 09:40:48 INFO     	 * (global step 17000: loss: 0.38794247806072235, lr: 0.0001
2024-01-06 09:40:54 INFO     	 * (global step 17050: loss: 0.4695623815059662, lr: 0.0001
2024-01-06 09:41:00 INFO     	 * (global step 17100: loss: 0.2842273861169815, lr: 0.0001
2024-01-06 09:41:06 INFO     	 * (global step 17150: loss: 0.42828963696956635, lr: 0.0001
2024-01-06 09:41:11 INFO     	 * (global step 17200: loss: 0.41909871995449066, lr: 0.0001
2024-01-06 09:41:17 INFO     	 * (global step 17250: loss: 0.4461522847414017, lr: 0.0001
2024-01-06 09:41:23 INFO     	 * (global step 17300: loss: 0.19701507315039635, lr: 0.0001
2024-01-06 09:41:29 INFO     	 * (global step 17350: loss: 0.47744952142238617, lr: 0.0001
2024-01-06 09:41:35 INFO     	 * (global step 17400: loss: 0.5198596268892288, lr: 0.0001
2024-01-06 09:41:40 INFO     	 * (global step 17450: loss: 0.32499927282333374, lr: 0.0001
2024-01-06 09:41:46 INFO     	 * (global step 17500: loss: 0.5348013043403625, lr: 0.0001
2024-01-06 09:41:52 INFO     	 * (global step 17550: loss: 0.34740713238716125, lr: 0.0001
2024-01-06 09:41:58 INFO     	 * (global step 17600: loss: 0.3090177997946739, lr: 0.0001
2024-01-06 09:42:04 INFO     	 * (global step 17650: loss: 0.8429425060749054, lr: 0.0001
2024-01-06 09:42:10 INFO     	 * (global step 17700: loss: 0.3194531500339508, lr: 0.0001
2024-01-06 09:42:15 INFO     	 * (global step 17750: loss: 0.5449540019035339, lr: 0.0001
2024-01-06 09:42:21 INFO     	 * (global step 17800: loss: 0.24181420356035233, lr: 0.0001
2024-01-06 09:42:27 INFO     	 * (global step 17850: loss: 0.3954850733280182, lr: 0.0001
2024-01-06 09:42:33 INFO     	 * (global step 17900: loss: 0.4447575509548187, lr: 0.0001
2024-01-06 09:42:39 INFO     	 * (global step 17950: loss: 0.32733161747455597, lr: 0.0001
2024-01-06 09:42:44 INFO     	 * (global step 18000: loss: 0.15459661185741425, lr: 0.0001
2024-01-06 09:42:50 INFO     	 * (global step 18050: loss: 0.39422607421875, lr: 0.0001
2024-01-06 09:42:56 INFO     	 * (global step 18100: loss: 0.37514179944992065, lr: 0.0001
2024-01-06 09:43:02 INFO     	 * (global step 18150: loss: 0.43054378032684326, lr: 0.0001
2024-01-06 09:43:08 INFO     	 * (global step 18200: loss: 0.3905705064535141, lr: 0.0001
2024-01-06 09:43:13 INFO     	 * (global step 18250: loss: 0.36428965628147125, lr: 0.0001
2024-01-06 09:43:19 INFO     	 * (global step 18300: loss: 0.32061026990413666, lr: 0.0001
2024-01-06 09:43:25 INFO     	 * (global step 18350: loss: 0.26343008875846863, lr: 0.0001
2024-01-06 09:43:31 INFO     	 * (global step 18400: loss: 0.6926971673965454, lr: 0.0001
2024-01-06 09:43:37 INFO     	 * (global step 18450: loss: 0.22721827030181885, lr: 0.0001
2024-01-06 09:43:42 INFO     	 * (global step 18500: loss: 0.44812019169330597, lr: 0.0001
2024-01-06 09:43:48 INFO     	 * (global step 18550: loss: 0.3933116942644119, lr: 0.0001
2024-01-06 09:43:54 INFO     	 * (global step 18600: loss: 0.49190689623355865, lr: 0.0001
2024-01-06 09:44:00 INFO     	 * (global step 18650: loss: 0.46414531767368317, lr: 0.0001
2024-01-06 09:44:06 INFO     	 * (global step 18700: loss: 0.34932326525449753, lr: 0.0001
2024-01-06 09:44:12 INFO     	 * (global step 18750: loss: 0.5075078010559082, lr: 0.0001
2024-01-06 09:44:17 INFO     	 * (global step 18800: loss: 0.4778638780117035, lr: 0.0001
2024-01-06 09:44:23 INFO     	 * (global step 18850: loss: 0.6409583985805511, lr: 0.0001
2024-01-06 09:44:29 INFO     	 * (global step 18900: loss: 0.34115901589393616, lr: 0.0001
2024-01-06 09:44:35 INFO     	 * (global step 18950: loss: 0.3821875900030136, lr: 0.0001
2024-01-06 09:44:41 INFO     	 * (global step 19000: loss: 0.4359150677919388, lr: 0.0001
2024-01-06 09:44:46 INFO     	 * (global step 19050: loss: 0.6200463175773621, lr: 0.0001
2024-01-06 09:44:52 INFO     	 * (global step 19100: loss: 0.4065070152282715, lr: 0.0001
2024-01-06 09:44:58 INFO     	 * (global step 19150: loss: 0.4538847357034683, lr: 0.0001
2024-01-06 09:45:04 INFO     	 * (global step 19200: loss: 0.6615964770317078, lr: 0.0001
2024-01-06 09:45:10 INFO     	 * (global step 19250: loss: 0.5037870109081268, lr: 0.0001
2024-01-06 09:45:15 INFO     	 * (global step 19300: loss: 0.4468958228826523, lr: 0.0001
2024-01-06 09:45:21 INFO     	 * (global step 19350: loss: 0.4393911063671112, lr: 0.0001
2024-01-06 09:45:27 INFO     	 * (global step 19400: loss: 0.3318622559309006, lr: 0.0001
2024-01-06 09:45:33 INFO     	 * (global step 19450: loss: 0.5362043082714081, lr: 0.0001
2024-01-06 09:45:39 INFO     	 * (global step 19500: loss: 0.3613930940628052, lr: 0.0001
2024-01-06 09:45:44 INFO     	 * (global step 19550: loss: 0.38053956627845764, lr: 0.0001
2024-01-06 09:45:50 INFO     	 * (global step 19600: loss: 0.28658372163772583, lr: 0.0001
2024-01-06 09:45:56 INFO     	 * (global step 19650: loss: 0.27612246572971344, lr: 0.0001
2024-01-06 09:46:02 INFO     	 * (global step 19700: loss: 0.41774024069309235, lr: 0.0001
2024-01-06 09:46:08 INFO     	 * (global step 19750: loss: 0.4616669565439224, lr: 0.0001
2024-01-06 09:46:14 INFO     	 * (global step 19800: loss: 0.4593040943145752, lr: 0.0001
2024-01-06 09:46:19 INFO     	 * (global step 19850: loss: 0.40752172470092773, lr: 0.0001
2024-01-06 09:46:25 INFO     	 * (global step 19900: loss: 0.4537633806467056, lr: 0.0001
2024-01-06 09:46:31 INFO     	 * (global step 19950: loss: 0.4612233638763428, lr: 0.0001
2024-01-06 09:46:37 INFO     	 * (global step 20000: loss: 0.5389394313097, lr: 0.0001
2024-01-06 09:46:43 INFO     	 * (global step 20050: loss: 0.6367686688899994, lr: 0.0001
2024-01-06 09:46:48 INFO     	 * (global step 20100: loss: 0.29625194519758224, lr: 0.0001
2024-01-06 09:46:54 INFO     	 * (global step 20150: loss: 0.4225965291261673, lr: 0.0001
2024-01-06 09:47:00 INFO     	 * (global step 20200: loss: 0.4423573315143585, lr: 0.0001
2024-01-06 09:47:05 INFO     [epoch 14/15] average loss: 0.416, lr: 0.0001
2024-01-06 09:47:05 INFO     saving model related files
2024-01-06 09:47:05 INFO     saving model
2024-01-06 09:47:05 INFO     saving tokenizer
2024-01-06 09:47:05 INFO     saving optimizer
2024-01-06 09:47:06 INFO     remove old optimizer files
2024-01-06 09:47:06 INFO     complete training: model ckpt was saved at small_recreated_ckpt/model_mzgdpa
2024-01-06 09:47:07 INFO     ## 2nd RUN: Configuration 4/5: validation/Bleu_4 = 0.09445276580206216
2024-01-06 09:47:07 INFO     initialize model trainer
2024-01-06 09:47:07 INFO     load config from existing checkpoint at small_recreated_ckpt/model_woixzh
2024-01-06 09:47:07 INFO     hyperparameters
2024-01-06 09:47:07 INFO     	 * dataset_path: lmqg/qag_squad
2024-01-06 09:47:07 INFO     	 * dataset_name: default
2024-01-06 09:47:07 INFO     	 * input_types: ['paragraph']
2024-01-06 09:47:07 INFO     	 * output_types: ['questions_answers']
2024-01-06 09:47:07 INFO     	 * prefix_types: ['qag']
2024-01-06 09:47:07 INFO     	 * model: t5-small
2024-01-06 09:47:07 INFO     	 * max_length: 512
2024-01-06 09:47:07 INFO     	 * max_length_output: 256
2024-01-06 09:47:07 INFO     	 * epoch: 15
2024-01-06 09:47:07 INFO     	 * batch: 2
2024-01-06 09:47:07 INFO     	 * lr: 5e-05
2024-01-06 09:47:07 INFO     	 * fp16: False
2024-01-06 09:47:07 INFO     	 * random_seed: 1
2024-01-06 09:47:07 INFO     	 * gradient_accumulation_steps: 2
2024-01-06 09:47:07 INFO     	 * label_smoothing: 0.15
2024-01-06 09:47:07 INFO     load checkpoint from small_recreated_ckpt/model_woixzh/epoch_10
2024-01-06 09:47:07 INFO     use spaCy answer extraction model: positionrank
2024-01-06 09:47:07 INFO     Model `small_recreated_ckpt/model_woixzh/epoch_10`
2024-01-06 09:47:07 INFO     	 * Num of GPU in use: 1
2024-01-06 09:47:07 INFO     	 * Prefix: True
2024-01-06 09:47:07 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 09:47:07 INFO     load optimizer from small_recreated_ckpt/model_woixzh/optimizers/optimizer.10.pt
2024-01-06 09:47:07 INFO     optimizer is loading on cuda
2024-01-06 09:47:12 INFO     dataset preprocessing
2024-01-06 09:47:13 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featurelmqg/qag_squad/t5-small.512.256.paragraph.questions_answers.train.qag.pkl
2024-01-06 09:47:15 INFO     start model training
2024-01-06 09:47:20 INFO     	 * (global step 50: loss: 2.8989970684051514, lr: 5e-05
2024-01-06 09:47:26 INFO     	 * (global step 100: loss: 2.915064215660095, lr: 5e-05
2024-01-06 09:47:32 INFO     	 * (global step 150: loss: 2.793176531791687, lr: 5e-05
2024-01-06 09:47:38 INFO     	 * (global step 200: loss: 2.80209743976593, lr: 5e-05
2024-01-06 09:47:44 INFO     	 * (global step 250: loss: 2.7205737829208374, lr: 5e-05
2024-01-06 09:47:50 INFO     	 * (global step 300: loss: 2.8953423500061035, lr: 5e-05
2024-01-06 09:47:56 INFO     	 * (global step 350: loss: 2.7027132511138916, lr: 5e-05
2024-01-06 09:48:02 INFO     	 * (global step 400: loss: 2.77635657787323, lr: 5e-05
2024-01-06 09:48:08 INFO     	 * (global step 450: loss: 2.591630458831787, lr: 5e-05
2024-01-06 09:48:14 INFO     	 * (global step 500: loss: 2.554831027984619, lr: 5e-05
2024-01-06 09:48:20 INFO     	 * (global step 550: loss: 2.7775840759277344, lr: 5e-05
2024-01-06 09:48:26 INFO     	 * (global step 600: loss: 2.6662791967391968, lr: 5e-05
2024-01-06 09:48:32 INFO     	 * (global step 650: loss: 2.636486530303955, lr: 5e-05
2024-01-06 09:48:38 INFO     	 * (global step 700: loss: 2.613488793373108, lr: 5e-05
2024-01-06 09:48:44 INFO     	 * (global step 750: loss: 2.6503289937973022, lr: 5e-05
2024-01-06 09:48:49 INFO     	 * (global step 800: loss: 2.6676377058029175, lr: 5e-05
2024-01-06 09:48:55 INFO     	 * (global step 850: loss: 2.5527454614639282, lr: 5e-05
2024-01-06 09:49:01 INFO     	 * (global step 900: loss: 2.596148371696472, lr: 5e-05
2024-01-06 09:49:07 INFO     	 * (global step 950: loss: 2.6198811531066895, lr: 5e-05
2024-01-06 09:49:13 INFO     	 * (global step 1000: loss: 2.450458526611328, lr: 5e-05
2024-01-06 09:49:19 INFO     	 * (global step 1050: loss: 2.516185760498047, lr: 5e-05
2024-01-06 09:49:25 INFO     	 * (global step 1100: loss: 2.691863179206848, lr: 5e-05
2024-01-06 09:49:31 INFO     	 * (global step 1150: loss: 2.5155482292175293, lr: 5e-05
2024-01-06 09:49:37 INFO     	 * (global step 1200: loss: 2.469037413597107, lr: 5e-05
2024-01-06 09:49:43 INFO     	 * (global step 1250: loss: 2.450108766555786, lr: 5e-05
2024-01-06 09:49:49 INFO     	 * (global step 1300: loss: 2.659128427505493, lr: 5e-05
2024-01-06 09:49:55 INFO     	 * (global step 1350: loss: 2.573217272758484, lr: 5e-05
2024-01-06 09:50:01 INFO     	 * (global step 1400: loss: 2.597959518432617, lr: 5e-05
2024-01-06 09:50:07 INFO     	 * (global step 1450: loss: 2.6683343648910522, lr: 5e-05
2024-01-06 09:50:13 INFO     	 * (global step 1500: loss: 2.5992977619171143, lr: 5e-05
2024-01-06 09:50:19 INFO     	 * (global step 1550: loss: 2.6001118421554565, lr: 5e-05
2024-01-06 09:50:24 INFO     	 * (global step 1600: loss: 2.6520761251449585, lr: 5e-05
2024-01-06 09:50:30 INFO     	 * (global step 1650: loss: 2.7842345237731934, lr: 5e-05
2024-01-06 09:50:36 INFO     	 * (global step 1700: loss: 2.7275878190994263, lr: 5e-05
2024-01-06 09:50:42 INFO     	 * (global step 1750: loss: 2.404081106185913, lr: 5e-05
2024-01-06 09:50:48 INFO     	 * (global step 1800: loss: 2.4637606143951416, lr: 5e-05
2024-01-06 09:50:54 INFO     	 * (global step 1850: loss: 2.53342342376709, lr: 5e-05
2024-01-06 09:51:00 INFO     	 * (global step 1900: loss: 2.486371874809265, lr: 5e-05
2024-01-06 09:51:06 INFO     	 * (global step 1950: loss: 2.5500311851501465, lr: 5e-05
2024-01-06 09:51:12 INFO     	 * (global step 2000: loss: 2.639691472053528, lr: 5e-05
2024-01-06 09:51:18 INFO     	 * (global step 2050: loss: 2.5039539337158203, lr: 5e-05
2024-01-06 09:51:24 INFO     	 * (global step 2100: loss: 2.6700323820114136, lr: 5e-05
2024-01-06 09:51:30 INFO     	 * (global step 2150: loss: 2.6571723222732544, lr: 5e-05
2024-01-06 09:51:36 INFO     	 * (global step 2200: loss: 2.560715675354004, lr: 5e-05
2024-01-06 09:51:42 INFO     	 * (global step 2250: loss: 2.5101969242095947, lr: 5e-05
2024-01-06 09:51:48 INFO     	 * (global step 2300: loss: 2.5100855827331543, lr: 5e-05
2024-01-06 09:51:54 INFO     	 * (global step 2350: loss: 2.525611400604248, lr: 5e-05
2024-01-06 09:52:00 INFO     	 * (global step 2400: loss: 2.342398166656494, lr: 5e-05
2024-01-06 09:52:05 INFO     	 * (global step 2450: loss: 2.5181089639663696, lr: 5e-05
2024-01-06 09:52:11 INFO     	 * (global step 2500: loss: 2.5969326496124268, lr: 5e-05
2024-01-06 09:52:17 INFO     	 * (global step 2550: loss: 2.658410668373108, lr: 5e-05
2024-01-06 09:52:23 INFO     	 * (global step 2600: loss: 2.494089722633362, lr: 5e-05
2024-01-06 09:52:29 INFO     	 * (global step 2650: loss: 2.56251060962677, lr: 5e-05
2024-01-06 09:52:35 INFO     	 * (global step 2700: loss: 2.666217088699341, lr: 5e-05
2024-01-06 09:52:41 INFO     	 * (global step 2750: loss: 2.6397719383239746, lr: 5e-05
2024-01-06 09:52:47 INFO     	 * (global step 2800: loss: 2.4699395895004272, lr: 5e-05
2024-01-06 09:52:53 INFO     	 * (global step 2850: loss: 2.6528549194335938, lr: 5e-05
2024-01-06 09:52:59 INFO     	 * (global step 2900: loss: 2.675177216529846, lr: 5e-05
2024-01-06 09:53:05 INFO     	 * (global step 2950: loss: 2.451790928840637, lr: 5e-05
2024-01-06 09:53:11 INFO     	 * (global step 3000: loss: 2.616899847984314, lr: 5e-05
2024-01-06 09:53:17 INFO     	 * (global step 3050: loss: 2.5218225717544556, lr: 5e-05
2024-01-06 09:53:23 INFO     	 * (global step 3100: loss: 2.4779038429260254, lr: 5e-05
2024-01-06 09:53:29 INFO     	 * (global step 3150: loss: 2.617260694503784, lr: 5e-05
2024-01-06 09:53:35 INFO     	 * (global step 3200: loss: 2.468796730041504, lr: 5e-05
2024-01-06 09:53:41 INFO     	 * (global step 3250: loss: 2.3872697353363037, lr: 5e-05
2024-01-06 09:53:46 INFO     	 * (global step 3300: loss: 2.4812716245651245, lr: 5e-05
2024-01-06 09:53:52 INFO     	 * (global step 3350: loss: 2.564756155014038, lr: 5e-05
2024-01-06 09:53:58 INFO     	 * (global step 3400: loss: 2.5757734775543213, lr: 5e-05
2024-01-06 09:54:04 INFO     	 * (global step 3450: loss: 2.4013302326202393, lr: 5e-05
2024-01-06 09:54:10 INFO     	 * (global step 3500: loss: 2.5523828268051147, lr: 5e-05
2024-01-06 09:54:16 INFO     	 * (global step 3550: loss: 2.6907739639282227, lr: 5e-05
2024-01-06 09:54:22 INFO     	 * (global step 3600: loss: 2.4082634449005127, lr: 5e-05
2024-01-06 09:54:28 INFO     	 * (global step 3650: loss: 2.529575824737549, lr: 5e-05
2024-01-06 09:54:34 INFO     	 * (global step 3700: loss: 2.456887125968933, lr: 5e-05
2024-01-06 09:54:40 INFO     	 * (global step 3750: loss: 2.5290515422821045, lr: 5e-05
2024-01-06 09:54:46 INFO     	 * (global step 3800: loss: 2.6027302742004395, lr: 5e-05
2024-01-06 09:54:52 INFO     	 * (global step 3850: loss: 2.7101621627807617, lr: 5e-05
2024-01-06 09:54:58 INFO     	 * (global step 3900: loss: 2.636154532432556, lr: 5e-05
2024-01-06 09:55:04 INFO     	 * (global step 3950: loss: 2.543652653694153, lr: 5e-05
2024-01-06 09:55:10 INFO     	 * (global step 4000: loss: 2.711796998977661, lr: 5e-05
2024-01-06 09:55:15 INFO     [epoch 10/15] average loss: 2.615, lr: 5e-05
2024-01-06 09:55:15 INFO     saving model related files
2024-01-06 09:55:15 INFO     saving model
2024-01-06 09:55:16 INFO     saving tokenizer
2024-01-06 09:55:16 INFO     saving optimizer
2024-01-06 09:55:17 INFO     remove old optimizer files
2024-01-06 09:55:17 INFO     	 * (global step 4050: loss: 2.5816729068756104, lr: 5e-05
2024-01-06 09:55:23 INFO     	 * (global step 4100: loss: 2.641666531562805, lr: 5e-05
2024-01-06 09:55:29 INFO     	 * (global step 4150: loss: 2.500888466835022, lr: 5e-05
2024-01-06 09:55:35 INFO     	 * (global step 4200: loss: 2.501716136932373, lr: 5e-05
2024-01-06 09:55:41 INFO     	 * (global step 4250: loss: 2.5659384727478027, lr: 5e-05
2024-01-06 09:55:47 INFO     	 * (global step 4300: loss: 2.618730306625366, lr: 5e-05
2024-01-06 09:55:53 INFO     	 * (global step 4350: loss: 2.5145117044448853, lr: 5e-05
2024-01-06 09:55:59 INFO     	 * (global step 4400: loss: 2.4335063695907593, lr: 5e-05
2024-01-06 09:56:05 INFO     	 * (global step 4450: loss: 2.700271248817444, lr: 5e-05
2024-01-06 09:56:11 INFO     	 * (global step 4500: loss: 2.3888306617736816, lr: 5e-05
2024-01-06 09:56:16 INFO     	 * (global step 4550: loss: 2.749549627304077, lr: 5e-05
2024-01-06 09:56:22 INFO     	 * (global step 4600: loss: 2.6194329261779785, lr: 5e-05
2024-01-06 09:56:28 INFO     	 * (global step 4650: loss: 2.6504095792770386, lr: 5e-05
2024-01-06 09:56:34 INFO     	 * (global step 4700: loss: 2.4222254753112793, lr: 5e-05
2024-01-06 09:56:40 INFO     	 * (global step 4750: loss: 2.5195024013519287, lr: 5e-05
2024-01-06 09:56:46 INFO     	 * (global step 4800: loss: 2.4657325744628906, lr: 5e-05
2024-01-06 09:56:52 INFO     	 * (global step 4850: loss: 2.474552035331726, lr: 5e-05
2024-01-06 09:56:58 INFO     	 * (global step 4900: loss: 2.597678065299988, lr: 5e-05
2024-01-06 09:57:04 INFO     	 * (global step 4950: loss: 2.5542339086532593, lr: 5e-05
2024-01-06 09:57:10 INFO     	 * (global step 5000: loss: 2.580836296081543, lr: 5e-05
2024-01-06 09:57:16 INFO     	 * (global step 5050: loss: 2.438234806060791, lr: 5e-05
2024-01-06 09:57:22 INFO     	 * (global step 5100: loss: 2.3907240629196167, lr: 5e-05
2024-01-06 09:57:28 INFO     	 * (global step 5150: loss: 2.5133641958236694, lr: 5e-05
2024-01-06 09:57:34 INFO     	 * (global step 5200: loss: 2.3745709657669067, lr: 5e-05
2024-01-06 09:57:39 INFO     	 * (global step 5250: loss: 2.53658926486969, lr: 5e-05
2024-01-06 09:57:45 INFO     	 * (global step 5300: loss: 2.627825975418091, lr: 5e-05
2024-01-06 09:57:51 INFO     	 * (global step 5350: loss: 2.611832618713379, lr: 5e-05
2024-01-06 09:57:57 INFO     	 * (global step 5400: loss: 2.5655624866485596, lr: 5e-05
2024-01-06 09:58:03 INFO     	 * (global step 5450: loss: 2.617499589920044, lr: 5e-05
2024-01-06 09:58:09 INFO     	 * (global step 5500: loss: 2.5750991106033325, lr: 5e-05
2024-01-06 09:58:15 INFO     	 * (global step 5550: loss: 2.4889097213745117, lr: 5e-05
2024-01-06 09:58:21 INFO     	 * (global step 5600: loss: 2.758557677268982, lr: 5e-05
2024-01-06 09:58:27 INFO     	 * (global step 5650: loss: 2.5977622270584106, lr: 5e-05
2024-01-06 09:58:33 INFO     	 * (global step 5700: loss: 2.68430233001709, lr: 5e-05
2024-01-06 09:58:39 INFO     	 * (global step 5750: loss: 2.6292176246643066, lr: 5e-05
2024-01-06 09:58:45 INFO     	 * (global step 5800: loss: 2.424187183380127, lr: 5e-05
2024-01-06 09:58:51 INFO     	 * (global step 5850: loss: 2.4420701265335083, lr: 5e-05
2024-01-06 09:58:57 INFO     	 * (global step 5900: loss: 2.4056838750839233, lr: 5e-05
2024-01-06 09:59:02 INFO     	 * (global step 5950: loss: 2.3784828186035156, lr: 5e-05
2024-01-06 09:59:08 INFO     	 * (global step 6000: loss: 2.286978006362915, lr: 5e-05
2024-01-06 09:59:14 INFO     	 * (global step 6050: loss: 2.5198835134506226, lr: 5e-05
2024-01-06 09:59:20 INFO     	 * (global step 6100: loss: 2.404572606086731, lr: 5e-05
2024-01-06 09:59:26 INFO     	 * (global step 6150: loss: 2.733457088470459, lr: 5e-05
2024-01-06 09:59:32 INFO     	 * (global step 6200: loss: 2.5800944566726685, lr: 5e-05
2024-01-06 09:59:38 INFO     	 * (global step 6250: loss: 2.4134961366653442, lr: 5e-05
2024-01-06 09:59:44 INFO     	 * (global step 6300: loss: 2.59862220287323, lr: 5e-05
2024-01-06 09:59:50 INFO     	 * (global step 6350: loss: 2.4641913175582886, lr: 5e-05
2024-01-06 09:59:56 INFO     	 * (global step 6400: loss: 2.4805784225463867, lr: 5e-05
2024-01-06 10:00:02 INFO     	 * (global step 6450: loss: 2.5455795526504517, lr: 5e-05
2024-01-06 10:00:08 INFO     	 * (global step 6500: loss: 2.602060556411743, lr: 5e-05
2024-01-06 10:00:14 INFO     	 * (global step 6550: loss: 2.3909268379211426, lr: 5e-05
2024-01-06 10:00:20 INFO     	 * (global step 6600: loss: 2.5143370628356934, lr: 5e-05
2024-01-06 10:00:25 INFO     	 * (global step 6650: loss: 2.4830803871154785, lr: 5e-05
2024-01-06 10:00:31 INFO     	 * (global step 6700: loss: 2.4433867931365967, lr: 5e-05
2024-01-06 10:00:37 INFO     	 * (global step 6750: loss: 2.6459726095199585, lr: 5e-05
2024-01-06 10:00:43 INFO     	 * (global step 6800: loss: 2.3366416692733765, lr: 5e-05
2024-01-06 10:00:49 INFO     	 * (global step 6850: loss: 2.5375194549560547, lr: 5e-05
2024-01-06 10:00:55 INFO     	 * (global step 6900: loss: 2.5477336645126343, lr: 5e-05
2024-01-06 10:01:01 INFO     	 * (global step 6950: loss: 2.4257208108901978, lr: 5e-05
2024-01-06 10:01:07 INFO     	 * (global step 7000: loss: 2.960586428642273, lr: 5e-05
2024-01-06 10:01:13 INFO     	 * (global step 7050: loss: 2.4483895301818848, lr: 5e-05
2024-01-06 10:01:19 INFO     	 * (global step 7100: loss: 2.8220819234848022, lr: 5e-05
2024-01-06 10:01:25 INFO     	 * (global step 7150: loss: 2.5225859880447388, lr: 5e-05
2024-01-06 10:01:31 INFO     	 * (global step 7200: loss: 2.460793972015381, lr: 5e-05
2024-01-06 10:01:37 INFO     	 * (global step 7250: loss: 2.5266963243484497, lr: 5e-05
2024-01-06 10:01:42 INFO     	 * (global step 7300: loss: 2.5627856254577637, lr: 5e-05
2024-01-06 10:01:48 INFO     	 * (global step 7350: loss: 2.4752570390701294, lr: 5e-05
2024-01-06 10:01:54 INFO     	 * (global step 7400: loss: 2.5438623428344727, lr: 5e-05
2024-01-06 10:02:00 INFO     	 * (global step 7450: loss: 2.610872983932495, lr: 5e-05
2024-01-06 10:02:06 INFO     	 * (global step 7500: loss: 2.4339137077331543, lr: 5e-05
2024-01-06 10:02:12 INFO     	 * (global step 7550: loss: 2.4484633207321167, lr: 5e-05
2024-01-06 10:02:18 INFO     	 * (global step 7600: loss: 2.5394036769866943, lr: 5e-05
2024-01-06 10:02:24 INFO     	 * (global step 7650: loss: 2.628051280975342, lr: 5e-05
2024-01-06 10:02:30 INFO     	 * (global step 7700: loss: 2.5081056356430054, lr: 5e-05
2024-01-06 10:02:36 INFO     	 * (global step 7750: loss: 2.625279426574707, lr: 5e-05
2024-01-06 10:02:42 INFO     	 * (global step 7800: loss: 2.594385027885437, lr: 5e-05
2024-01-06 10:02:48 INFO     	 * (global step 7850: loss: 2.3640717267990112, lr: 5e-05
2024-01-06 10:02:54 INFO     	 * (global step 7900: loss: 2.4755685329437256, lr: 5e-05
2024-01-06 10:03:00 INFO     	 * (global step 7950: loss: 2.3959206342697144, lr: 5e-05
2024-01-06 10:03:05 INFO     	 * (global step 8000: loss: 2.4673008918762207, lr: 5e-05
2024-01-06 10:03:11 INFO     	 * (global step 8050: loss: 2.433089852333069, lr: 5e-05
2024-01-06 10:03:17 INFO     [epoch 11/15] average loss: 2.538, lr: 5e-05
2024-01-06 10:03:17 INFO     saving model related files
2024-01-06 10:03:17 INFO     saving model
2024-01-06 10:03:17 INFO     saving tokenizer
2024-01-06 10:03:17 INFO     saving optimizer
2024-01-06 10:03:18 INFO     remove old optimizer files
2024-01-06 10:03:19 INFO     	 * (global step 8100: loss: 2.498411178588867, lr: 5e-05
2024-01-06 10:03:25 INFO     	 * (global step 8150: loss: 2.449506163597107, lr: 5e-05
2024-01-06 10:03:31 INFO     	 * (global step 8200: loss: 2.4758232831954956, lr: 5e-05
2024-01-06 10:03:37 INFO     	 * (global step 8250: loss: 2.500661849975586, lr: 5e-05
2024-01-06 10:03:43 INFO     	 * (global step 8300: loss: 2.49141526222229, lr: 5e-05
2024-01-06 10:03:49 INFO     	 * (global step 8350: loss: 2.519834876060486, lr: 5e-05
2024-01-06 10:03:54 INFO     	 * (global step 8400: loss: 2.4365986585617065, lr: 5e-05
2024-01-06 10:04:00 INFO     	 * (global step 8450: loss: 2.443466901779175, lr: 5e-05
2024-01-06 10:04:06 INFO     	 * (global step 8500: loss: 2.511393189430237, lr: 5e-05
2024-01-06 10:04:12 INFO     	 * (global step 8550: loss: 2.418436646461487, lr: 5e-05
2024-01-06 10:04:18 INFO     	 * (global step 8600: loss: 2.481000304222107, lr: 5e-05
2024-01-06 10:04:24 INFO     	 * (global step 8650: loss: 2.4476332664489746, lr: 5e-05
2024-01-06 10:04:30 INFO     	 * (global step 8700: loss: 2.453518033027649, lr: 5e-05
2024-01-06 10:04:36 INFO     	 * (global step 8750: loss: 2.4308853149414062, lr: 5e-05
2024-01-06 10:04:42 INFO     	 * (global step 8800: loss: 2.5429102182388306, lr: 5e-05
2024-01-06 10:04:48 INFO     	 * (global step 8850: loss: 2.535914659500122, lr: 5e-05
2024-01-06 10:04:54 INFO     	 * (global step 8900: loss: 2.458507776260376, lr: 5e-05
2024-01-06 10:05:00 INFO     	 * (global step 8950: loss: 2.495432138442993, lr: 5e-05
2024-01-06 10:05:06 INFO     	 * (global step 9000: loss: 2.384645462036133, lr: 5e-05
2024-01-06 10:05:12 INFO     	 * (global step 9050: loss: 2.3805519342422485, lr: 5e-05
2024-01-06 10:05:18 INFO     	 * (global step 9100: loss: 2.472513198852539, lr: 5e-05
2024-01-06 10:05:24 INFO     	 * (global step 9150: loss: 2.495843768119812, lr: 5e-05
2024-01-06 10:05:30 INFO     	 * (global step 9200: loss: 2.4588887691497803, lr: 5e-05
2024-01-06 10:05:35 INFO     	 * (global step 9250: loss: 2.5139002799987793, lr: 5e-05
2024-01-06 10:05:41 INFO     	 * (global step 9300: loss: 2.3724054098129272, lr: 5e-05
2024-01-06 10:05:47 INFO     	 * (global step 9350: loss: 2.339797854423523, lr: 5e-05
2024-01-06 10:05:53 INFO     	 * (global step 9400: loss: 2.554918885231018, lr: 5e-05
2024-01-06 10:05:59 INFO     	 * (global step 9450: loss: 2.2997138500213623, lr: 5e-05
2024-01-06 10:06:05 INFO     	 * (global step 9500: loss: 2.4423528909683228, lr: 5e-05
2024-01-06 10:06:11 INFO     	 * (global step 9550: loss: 2.5056331157684326, lr: 5e-05
2024-01-06 10:06:17 INFO     	 * (global step 9600: loss: 2.3542126417160034, lr: 5e-05
2024-01-06 10:06:23 INFO     	 * (global step 9650: loss: 2.5557680130004883, lr: 5e-05
2024-01-06 10:06:29 INFO     	 * (global step 9700: loss: 2.3730655908584595, lr: 5e-05
2024-01-06 10:06:35 INFO     	 * (global step 9750: loss: 2.4020004272460938, lr: 5e-05
2024-01-06 10:06:41 INFO     	 * (global step 9800: loss: 2.580325484275818, lr: 5e-05
2024-01-06 10:06:47 INFO     	 * (global step 9850: loss: 2.3853174448013306, lr: 5e-05
2024-01-06 10:06:53 INFO     	 * (global step 9900: loss: 2.5721397399902344, lr: 5e-05
2024-01-06 10:06:59 INFO     	 * (global step 9950: loss: 2.5531023740768433, lr: 5e-05
2024-01-06 10:07:05 INFO     	 * (global step 10000: loss: 2.587560772895813, lr: 5e-05
2024-01-06 10:07:10 INFO     	 * (global step 10050: loss: 2.5357147455215454, lr: 5e-05
2024-01-06 10:07:16 INFO     	 * (global step 10100: loss: 2.5481451749801636, lr: 5e-05
2024-01-06 10:07:22 INFO     	 * (global step 10150: loss: 2.5477548837661743, lr: 5e-05
2024-01-06 10:07:28 INFO     	 * (global step 10200: loss: 2.5190439224243164, lr: 5e-05
2024-01-06 10:07:34 INFO     	 * (global step 10250: loss: 2.515947103500366, lr: 5e-05
2024-01-06 10:07:40 INFO     	 * (global step 10300: loss: 2.5755664110183716, lr: 5e-05
2024-01-06 10:07:46 INFO     	 * (global step 10350: loss: 2.4203684329986572, lr: 5e-05
2024-01-06 10:07:52 INFO     	 * (global step 10400: loss: 2.4903407096862793, lr: 5e-05
2024-01-06 10:07:58 INFO     	 * (global step 10450: loss: 2.4381263256073, lr: 5e-05
2024-01-06 10:08:04 INFO     	 * (global step 10500: loss: 2.557543158531189, lr: 5e-05
2024-01-06 10:08:10 INFO     	 * (global step 10550: loss: 2.4500850439071655, lr: 5e-05
2024-01-06 10:08:16 INFO     	 * (global step 10600: loss: 2.4487624168395996, lr: 5e-05
2024-01-06 10:08:22 INFO     	 * (global step 10650: loss: 2.511134386062622, lr: 5e-05
2024-01-06 10:08:28 INFO     	 * (global step 10700: loss: 2.372806668281555, lr: 5e-05
2024-01-06 10:08:34 INFO     	 * (global step 10750: loss: 2.4355406761169434, lr: 5e-05
2024-01-06 10:08:40 INFO     	 * (global step 10800: loss: 2.398651957511902, lr: 5e-05
2024-01-06 10:08:46 INFO     	 * (global step 10850: loss: 2.5889564752578735, lr: 5e-05
2024-01-06 10:08:51 INFO     	 * (global step 10900: loss: 2.4416459798812866, lr: 5e-05
2024-01-06 10:08:57 INFO     	 * (global step 10950: loss: 2.366219639778137, lr: 5e-05
2024-01-06 10:09:03 INFO     	 * (global step 11000: loss: 2.3917012214660645, lr: 5e-05
2024-01-06 10:09:09 INFO     	 * (global step 11050: loss: 2.501924753189087, lr: 5e-05
2024-01-06 10:09:15 INFO     	 * (global step 11100: loss: 2.5328896045684814, lr: 5e-05
2024-01-06 10:09:21 INFO     	 * (global step 11150: loss: 2.631110429763794, lr: 5e-05
2024-01-06 10:09:27 INFO     	 * (global step 11200: loss: 2.4051209688186646, lr: 5e-05
2024-01-06 10:09:33 INFO     	 * (global step 11250: loss: 2.360060930252075, lr: 5e-05
2024-01-06 10:09:39 INFO     	 * (global step 11300: loss: 2.464243769645691, lr: 5e-05
2024-01-06 10:09:45 INFO     	 * (global step 11350: loss: 2.479844570159912, lr: 5e-05
2024-01-06 10:09:51 INFO     	 * (global step 11400: loss: 2.437603712081909, lr: 5e-05
2024-01-06 10:09:57 INFO     	 * (global step 11450: loss: 2.583595633506775, lr: 5e-05
2024-01-06 10:10:03 INFO     	 * (global step 11500: loss: 2.5984877347946167, lr: 5e-05
2024-01-06 10:10:09 INFO     	 * (global step 11550: loss: 2.459060788154602, lr: 5e-05
2024-01-06 10:10:15 INFO     	 * (global step 11600: loss: 2.727787494659424, lr: 5e-05
2024-01-06 10:10:21 INFO     	 * (global step 11650: loss: 2.4374401569366455, lr: 5e-05
2024-01-06 10:10:27 INFO     	 * (global step 11700: loss: 2.454722762107849, lr: 5e-05
2024-01-06 10:10:33 INFO     	 * (global step 11750: loss: 2.4844579696655273, lr: 5e-05
2024-01-06 10:10:39 INFO     	 * (global step 11800: loss: 2.504245400428772, lr: 5e-05
2024-01-06 10:10:45 INFO     	 * (global step 11850: loss: 2.6594778299331665, lr: 5e-05
2024-01-06 10:10:51 INFO     	 * (global step 11900: loss: 2.551611542701721, lr: 5e-05
2024-01-06 10:10:57 INFO     	 * (global step 11950: loss: 2.338099718093872, lr: 5e-05
2024-01-06 10:11:02 INFO     	 * (global step 12000: loss: 2.398895025253296, lr: 5e-05
2024-01-06 10:11:08 INFO     	 * (global step 12050: loss: 2.481045365333557, lr: 5e-05
2024-01-06 10:11:14 INFO     	 * (global step 12100: loss: 2.609248399734497, lr: 5e-05
2024-01-06 10:11:20 INFO     [epoch 12/15] average loss: 2.518, lr: 5e-05
2024-01-06 10:11:20 INFO     saving model related files
2024-01-06 10:11:20 INFO     saving model
2024-01-06 10:11:20 INFO     saving tokenizer
2024-01-06 10:11:20 INFO     saving optimizer
2024-01-06 10:11:21 INFO     remove old optimizer files
2024-01-06 10:11:22 INFO     	 * (global step 12150: loss: 2.592225432395935, lr: 5e-05
2024-01-06 10:11:28 INFO     	 * (global step 12200: loss: 2.469393730163574, lr: 5e-05
2024-01-06 10:11:34 INFO     	 * (global step 12250: loss: 2.455398201942444, lr: 5e-05
2024-01-06 10:11:40 INFO     	 * (global step 12300: loss: 2.4837886095046997, lr: 5e-05
2024-01-06 10:11:46 INFO     	 * (global step 12350: loss: 2.5652889013290405, lr: 5e-05
2024-01-06 10:11:52 INFO     	 * (global step 12400: loss: 2.6242425441741943, lr: 5e-05
2024-01-06 10:11:58 INFO     	 * (global step 12450: loss: 2.5145201683044434, lr: 5e-05
2024-01-06 10:12:04 INFO     	 * (global step 12500: loss: 2.505666732788086, lr: 5e-05
2024-01-06 10:12:10 INFO     	 * (global step 12550: loss: 2.5977582931518555, lr: 5e-05
2024-01-06 10:12:16 INFO     	 * (global step 12600: loss: 2.5629732608795166, lr: 5e-05
2024-01-06 10:12:22 INFO     	 * (global step 12650: loss: 2.4686572551727295, lr: 5e-05
2024-01-06 10:12:28 INFO     	 * (global step 12700: loss: 2.5096142292022705, lr: 5e-05
2024-01-06 10:12:34 INFO     	 * (global step 12750: loss: 2.7906384468078613, lr: 5e-05
2024-01-06 10:12:39 INFO     	 * (global step 12800: loss: 2.4983094930648804, lr: 5e-05
2024-01-06 10:12:45 INFO     	 * (global step 12850: loss: 2.648431897163391, lr: 5e-05
2024-01-06 10:12:51 INFO     	 * (global step 12900: loss: 2.4310044050216675, lr: 5e-05
2024-01-06 10:12:57 INFO     	 * (global step 12950: loss: 2.459401488304138, lr: 5e-05
2024-01-06 10:13:03 INFO     	 * (global step 13000: loss: 2.5372270345687866, lr: 5e-05
2024-01-06 10:13:09 INFO     	 * (global step 13050: loss: 2.5121874809265137, lr: 5e-05
2024-01-06 10:13:15 INFO     	 * (global step 13100: loss: 2.6138991117477417, lr: 5e-05
2024-01-06 10:13:21 INFO     	 * (global step 13150: loss: 2.6189786195755005, lr: 5e-05
2024-01-06 10:13:27 INFO     	 * (global step 13200: loss: 2.432147741317749, lr: 5e-05
2024-01-06 10:13:33 INFO     	 * (global step 13250: loss: 2.433921694755554, lr: 5e-05
2024-01-06 10:13:39 INFO     	 * (global step 13300: loss: 2.416404962539673, lr: 5e-05
2024-01-06 10:13:45 INFO     	 * (global step 13350: loss: 2.418138265609741, lr: 5e-05
2024-01-06 10:13:51 INFO     	 * (global step 13400: loss: 2.5766602754592896, lr: 5e-05
2024-01-06 10:13:57 INFO     	 * (global step 13450: loss: 2.909149408340454, lr: 5e-05
2024-01-06 10:14:03 INFO     	 * (global step 13500: loss: 2.8654956817626953, lr: 5e-05
2024-01-06 10:14:09 INFO     	 * (global step 13550: loss: 2.6529682874679565, lr: 5e-05
2024-01-06 10:14:15 INFO     	 * (global step 13600: loss: 2.5193341970443726, lr: 5e-05
2024-01-06 10:14:21 INFO     	 * (global step 13650: loss: 2.5165040493011475, lr: 5e-05
2024-01-06 10:14:26 INFO     	 * (global step 13700: loss: 2.43870747089386, lr: 5e-05
2024-01-06 10:14:32 INFO     	 * (global step 13750: loss: 2.4792587757110596, lr: 5e-05
2024-01-06 10:14:38 INFO     	 * (global step 13800: loss: 2.5195465087890625, lr: 5e-05
2024-01-06 10:14:44 INFO     	 * (global step 13850: loss: 2.4276379346847534, lr: 5e-05
2024-01-06 10:14:50 INFO     	 * (global step 13900: loss: 2.4349995851516724, lr: 5e-05
2024-01-06 10:14:56 INFO     	 * (global step 13950: loss: 2.5093973875045776, lr: 5e-05
2024-01-06 10:15:02 INFO     	 * (global step 14000: loss: 2.4966100454330444, lr: 5e-05
2024-01-06 10:15:08 INFO     	 * (global step 14050: loss: 2.7148772478103638, lr: 5e-05
2024-01-06 10:15:14 INFO     	 * (global step 14100: loss: 2.52879536151886, lr: 5e-05
2024-01-06 10:15:20 INFO     	 * (global step 14150: loss: 2.5619858503341675, lr: 5e-05
2024-01-06 10:15:26 INFO     	 * (global step 14200: loss: 2.544605016708374, lr: 5e-05
2024-01-06 10:15:32 INFO     	 * (global step 14250: loss: 2.486351490020752, lr: 5e-05
2024-01-06 10:15:38 INFO     	 * (global step 14300: loss: 2.440325140953064, lr: 5e-05
2024-01-06 10:15:44 INFO     	 * (global step 14350: loss: 2.595549464225769, lr: 5e-05
2024-01-06 10:15:50 INFO     	 * (global step 14400: loss: 2.7101882696151733, lr: 5e-05
2024-01-06 10:15:56 INFO     	 * (global step 14450: loss: 2.5495619773864746, lr: 5e-05
2024-01-06 10:16:02 INFO     	 * (global step 14500: loss: 2.4700385332107544, lr: 5e-05
2024-01-06 10:16:08 INFO     	 * (global step 14550: loss: 2.575946569442749, lr: 5e-05
2024-01-06 10:16:13 INFO     	 * (global step 14600: loss: 2.5740134716033936, lr: 5e-05
2024-01-06 10:16:19 INFO     	 * (global step 14650: loss: 2.4324194192886353, lr: 5e-05
2024-01-06 10:16:25 INFO     	 * (global step 14700: loss: 2.4114906787872314, lr: 5e-05
2024-01-06 10:16:31 INFO     	 * (global step 14750: loss: 2.7093496322631836, lr: 5e-05
2024-01-06 10:16:37 INFO     	 * (global step 14800: loss: 2.5109554529190063, lr: 5e-05
2024-01-06 10:16:43 INFO     	 * (global step 14850: loss: 2.5992977619171143, lr: 5e-05
2024-01-06 10:16:49 INFO     	 * (global step 14900: loss: 2.424904704093933, lr: 5e-05
2024-01-06 10:16:55 INFO     	 * (global step 14950: loss: 2.5068552494049072, lr: 5e-05
2024-01-06 10:17:01 INFO     	 * (global step 15000: loss: 2.42996346950531, lr: 5e-05
2024-01-06 10:17:07 INFO     	 * (global step 15050: loss: 2.424024224281311, lr: 5e-05
2024-01-06 10:17:13 INFO     	 * (global step 15100: loss: 2.5246602296829224, lr: 5e-05
2024-01-06 10:17:19 INFO     	 * (global step 15150: loss: 2.4092321395874023, lr: 5e-05
2024-01-06 10:17:25 INFO     	 * (global step 15200: loss: 2.4356894493103027, lr: 5e-05
2024-01-06 10:17:31 INFO     	 * (global step 15250: loss: 2.4056262969970703, lr: 5e-05
2024-01-06 10:17:37 INFO     	 * (global step 15300: loss: 2.525250196456909, lr: 5e-05
2024-01-06 10:17:43 INFO     	 * (global step 15350: loss: 2.424439549446106, lr: 5e-05
2024-01-06 10:17:49 INFO     	 * (global step 15400: loss: 2.4789751768112183, lr: 5e-05
2024-01-06 10:17:55 INFO     	 * (global step 15450: loss: 2.478501796722412, lr: 5e-05
2024-01-06 10:18:01 INFO     	 * (global step 15500: loss: 2.6686519384384155, lr: 5e-05
2024-01-06 10:18:06 INFO     	 * (global step 15550: loss: 2.489334464073181, lr: 5e-05
2024-01-06 10:18:12 INFO     	 * (global step 15600: loss: 2.4159421920776367, lr: 5e-05
2024-01-06 10:18:18 INFO     	 * (global step 15650: loss: 2.4420207738876343, lr: 5e-05
2024-01-06 10:18:24 INFO     	 * (global step 15700: loss: 2.3442708253860474, lr: 5e-05
2024-01-06 10:18:30 INFO     	 * (global step 15750: loss: 2.577897310256958, lr: 5e-05
2024-01-06 10:18:36 INFO     	 * (global step 15800: loss: 2.5101646184921265, lr: 5e-05
2024-01-06 10:18:42 INFO     	 * (global step 15850: loss: 2.573529601097107, lr: 5e-05
2024-01-06 10:18:48 INFO     	 * (global step 15900: loss: 2.402652382850647, lr: 5e-05
2024-01-06 10:18:54 INFO     	 * (global step 15950: loss: 2.5390636920928955, lr: 5e-05
2024-01-06 10:19:00 INFO     	 * (global step 16000: loss: 2.628287196159363, lr: 5e-05
2024-01-06 10:19:06 INFO     	 * (global step 16050: loss: 2.571247935295105, lr: 5e-05
2024-01-06 10:19:12 INFO     	 * (global step 16100: loss: 2.548483967781067, lr: 5e-05
2024-01-06 10:19:18 INFO     	 * (global step 16150: loss: 2.507106900215149, lr: 5e-05
2024-01-06 10:19:23 INFO     [epoch 13/15] average loss: 2.505, lr: 5e-05
2024-01-06 10:19:23 INFO     saving model related files
2024-01-06 10:19:23 INFO     saving model
2024-01-06 10:19:23 INFO     saving tokenizer
2024-01-06 10:19:23 INFO     saving optimizer
2024-01-06 10:19:24 INFO     remove old optimizer files
2024-01-06 10:19:25 INFO     	 * (global step 16200: loss: 2.4853261709213257, lr: 5e-05
2024-01-06 10:19:31 INFO     	 * (global step 16250: loss: 2.520176649093628, lr: 5e-05
2024-01-06 10:19:37 INFO     	 * (global step 16300: loss: 2.4626513719558716, lr: 5e-05
2024-01-06 10:19:43 INFO     	 * (global step 16350: loss: 2.522567868232727, lr: 5e-05
2024-01-06 10:19:49 INFO     	 * (global step 16400: loss: 2.5125362873077393, lr: 5e-05
2024-01-06 10:19:55 INFO     	 * (global step 16450: loss: 2.345301389694214, lr: 5e-05
2024-01-06 10:20:01 INFO     	 * (global step 16500: loss: 2.5123674869537354, lr: 5e-05
2024-01-06 10:20:07 INFO     	 * (global step 16550: loss: 2.5144670009613037, lr: 5e-05
2024-01-06 10:20:13 INFO     	 * (global step 16600: loss: 2.4323008060455322, lr: 5e-05
2024-01-06 10:20:19 INFO     	 * (global step 16650: loss: 2.508947491645813, lr: 5e-05
2024-01-06 10:20:25 INFO     	 * (global step 16700: loss: 2.512274384498596, lr: 5e-05
2024-01-06 10:20:31 INFO     	 * (global step 16750: loss: 2.472021222114563, lr: 5e-05
2024-01-06 10:20:37 INFO     	 * (global step 16800: loss: 2.53641939163208, lr: 5e-05
2024-01-06 10:20:43 INFO     	 * (global step 16850: loss: 2.405025005340576, lr: 5e-05
2024-01-06 10:20:48 INFO     	 * (global step 16900: loss: 2.3437607288360596, lr: 5e-05
2024-01-06 10:20:54 INFO     	 * (global step 16950: loss: 2.553847312927246, lr: 5e-05
2024-01-06 10:21:00 INFO     	 * (global step 17000: loss: 2.4618773460388184, lr: 5e-05
2024-01-06 10:21:06 INFO     	 * (global step 17050: loss: 2.5340105295181274, lr: 5e-05
2024-01-06 10:21:12 INFO     	 * (global step 17100: loss: 2.3787553310394287, lr: 5e-05
2024-01-06 10:21:18 INFO     	 * (global step 17150: loss: 2.504126191139221, lr: 5e-05
2024-01-06 10:21:24 INFO     	 * (global step 17200: loss: 2.5010414123535156, lr: 5e-05
2024-01-06 10:21:30 INFO     	 * (global step 17250: loss: 2.530879497528076, lr: 5e-05
2024-01-06 10:21:36 INFO     	 * (global step 17300: loss: 2.2506033182144165, lr: 5e-05
2024-01-06 10:21:42 INFO     	 * (global step 17350: loss: 2.551278233528137, lr: 5e-05
2024-01-06 10:21:48 INFO     	 * (global step 17400: loss: 2.6231237649917603, lr: 5e-05
2024-01-06 10:21:54 INFO     	 * (global step 17450: loss: 2.4198153018951416, lr: 5e-05
2024-01-06 10:22:00 INFO     	 * (global step 17500: loss: 2.613454580307007, lr: 5e-05
2024-01-06 10:22:06 INFO     	 * (global step 17550: loss: 2.3973135948181152, lr: 5e-05
2024-01-06 10:22:12 INFO     	 * (global step 17600: loss: 2.3762784004211426, lr: 5e-05
2024-01-06 10:22:18 INFO     	 * (global step 17650: loss: 2.8881213665008545, lr: 5e-05
2024-01-06 10:22:24 INFO     	 * (global step 17700: loss: 2.395844340324402, lr: 5e-05
2024-01-06 10:22:29 INFO     	 * (global step 17750: loss: 2.6293739080429077, lr: 5e-05
2024-01-06 10:22:35 INFO     	 * (global step 17800: loss: 2.294809937477112, lr: 5e-05
2024-01-06 10:22:41 INFO     	 * (global step 17850: loss: 2.4443092346191406, lr: 5e-05
2024-01-06 10:22:47 INFO     	 * (global step 17900: loss: 2.496846318244934, lr: 5e-05
2024-01-06 10:22:53 INFO     	 * (global step 17950: loss: 2.4015976190567017, lr: 5e-05
2024-01-06 10:22:59 INFO     	 * (global step 18000: loss: 2.2456493377685547, lr: 5e-05
2024-01-06 10:23:05 INFO     	 * (global step 18050: loss: 2.5014668703079224, lr: 5e-05
2024-01-06 10:23:11 INFO     	 * (global step 18100: loss: 2.446500062942505, lr: 5e-05
2024-01-06 10:23:17 INFO     	 * (global step 18150: loss: 2.5241689682006836, lr: 5e-05
2024-01-06 10:23:23 INFO     	 * (global step 18200: loss: 2.4830271005630493, lr: 5e-05
2024-01-06 10:23:29 INFO     	 * (global step 18250: loss: 2.442001223564148, lr: 5e-05
2024-01-06 10:23:35 INFO     	 * (global step 18300: loss: 2.3840330839157104, lr: 5e-05
2024-01-06 10:23:41 INFO     	 * (global step 18350: loss: 2.324397921562195, lr: 5e-05
2024-01-06 10:23:47 INFO     	 * (global step 18400: loss: 2.7490437030792236, lr: 5e-05
2024-01-06 10:23:53 INFO     	 * (global step 18450: loss: 2.3111006021499634, lr: 5e-05
2024-01-06 10:23:59 INFO     	 * (global step 18500: loss: 2.5074559450149536, lr: 5e-05
2024-01-06 10:24:05 INFO     	 * (global step 18550: loss: 2.4906723499298096, lr: 5e-05
2024-01-06 10:24:10 INFO     	 * (global step 18600: loss: 2.5656630992889404, lr: 5e-05
2024-01-06 10:24:16 INFO     	 * (global step 18650: loss: 2.5435631275177, lr: 5e-05
2024-01-06 10:24:22 INFO     	 * (global step 18700: loss: 2.4361287355422974, lr: 5e-05
2024-01-06 10:24:28 INFO     	 * (global step 18750: loss: 2.600249767303467, lr: 5e-05
2024-01-06 10:24:34 INFO     	 * (global step 18800: loss: 2.5550529956817627, lr: 5e-05
2024-01-06 10:24:40 INFO     	 * (global step 18850: loss: 2.698702573776245, lr: 5e-05
2024-01-06 10:24:46 INFO     	 * (global step 18900: loss: 2.408126950263977, lr: 5e-05
2024-01-06 10:24:52 INFO     	 * (global step 18950: loss: 2.4493027925491333, lr: 5e-05
2024-01-06 10:24:58 INFO     	 * (global step 19000: loss: 2.537976622581482, lr: 5e-05
2024-01-06 10:25:04 INFO     	 * (global step 19050: loss: 2.653747320175171, lr: 5e-05
2024-01-06 10:25:10 INFO     	 * (global step 19100: loss: 2.4738720655441284, lr: 5e-05
2024-01-06 10:25:16 INFO     	 * (global step 19150: loss: 2.5545397996902466, lr: 5e-05
2024-01-06 10:25:22 INFO     	 * (global step 19200: loss: 2.7190096378326416, lr: 5e-05
2024-01-06 10:25:28 INFO     	 * (global step 19250: loss: 2.5727139711380005, lr: 5e-05
2024-01-06 10:25:34 INFO     	 * (global step 19300: loss: 2.5122607946395874, lr: 5e-05
2024-01-06 10:25:40 INFO     	 * (global step 19350: loss: 2.501797914505005, lr: 5e-05
2024-01-06 10:25:46 INFO     	 * (global step 19400: loss: 2.40145742893219, lr: 5e-05
2024-01-06 10:25:52 INFO     	 * (global step 19450: loss: 2.6016005277633667, lr: 5e-05
2024-01-06 10:25:57 INFO     	 * (global step 19500: loss: 2.431045174598694, lr: 5e-05
2024-01-06 10:26:03 INFO     	 * (global step 19550: loss: 2.458694815635681, lr: 5e-05
2024-01-06 10:26:09 INFO     	 * (global step 19600: loss: 2.361963987350464, lr: 5e-05
2024-01-06 10:26:15 INFO     	 * (global step 19650: loss: 2.37372362613678, lr: 5e-05
2024-01-06 10:26:21 INFO     	 * (global step 19700: loss: 2.4810609817504883, lr: 5e-05
2024-01-06 10:26:27 INFO     	 * (global step 19750: loss: 2.560879349708557, lr: 5e-05
2024-01-06 10:26:33 INFO     	 * (global step 19800: loss: 2.533523678779602, lr: 5e-05
2024-01-06 10:26:39 INFO     	 * (global step 19850: loss: 2.4773846864700317, lr: 5e-05
2024-01-06 10:26:45 INFO     	 * (global step 19900: loss: 2.5130585432052612, lr: 5e-05
2024-01-06 10:26:51 INFO     	 * (global step 19950: loss: 2.5662620067596436, lr: 5e-05
2024-01-06 10:26:57 INFO     	 * (global step 20000: loss: 2.6148749589920044, lr: 5e-05
2024-01-06 10:27:03 INFO     	 * (global step 20050: loss: 2.666980743408203, lr: 5e-05
2024-01-06 10:27:09 INFO     	 * (global step 20100: loss: 2.3585708141326904, lr: 5e-05
2024-01-06 10:27:15 INFO     	 * (global step 20150: loss: 2.4713505506515503, lr: 5e-05
2024-01-06 10:27:21 INFO     	 * (global step 20200: loss: 2.513723850250244, lr: 5e-05
2024-01-06 10:27:25 INFO     [epoch 14/15] average loss: 2.494, lr: 5e-05
2024-01-06 10:27:25 INFO     saving model related files
2024-01-06 10:27:25 INFO     saving model
2024-01-06 10:27:26 INFO     saving tokenizer
2024-01-06 10:27:26 INFO     saving optimizer
2024-01-06 10:27:27 INFO     remove old optimizer files
2024-01-06 10:27:27 INFO     complete training: model ckpt was saved at small_recreated_ckpt/model_woixzh
2024-01-06 10:27:27 INFO     ## 2nd RUN (EVAL): Configuration 0/5 ##
2024-01-06 10:27:40 INFO     use spaCy answer extraction model: positionrank
2024-01-06 10:27:40 INFO     Model `small_recreated_ckpt/model_efnljo/epoch_1`
2024-01-06 10:27:40 INFO     	 * Num of GPU in use: 1
2024-01-06 10:27:40 INFO     	 * Prefix: True
2024-01-06 10:27:40 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 10:27:41 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 10:34:29 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 10:40:28 INFO     	Bleu_1: 0.3283614461851593
2024-01-06 10:40:28 INFO     	Bleu_2: 0.1909401109223588
2024-01-06 10:40:28 INFO     	Bleu_3: 0.1142975034196719
2024-01-06 10:40:28 INFO     	Bleu_4: 0.07616116620154732
2024-01-06 10:40:29 INFO     	Bleu_1: 0.2964283623734157
2024-01-06 10:40:29 INFO     	Bleu_2: 0.16953085464574244
2024-01-06 10:40:29 INFO     	Bleu_3: 0.10032514974533265
2024-01-06 10:40:29 INFO     	Bleu_4: 0.0664950537948636
2024-01-06 10:40:42 INFO     use spaCy answer extraction model: positionrank
2024-01-06 10:40:42 INFO     Model `small_recreated_ckpt/model_efnljo/epoch_11`
2024-01-06 10:40:42 INFO     	 * Num of GPU in use: 1
2024-01-06 10:40:42 INFO     	 * Prefix: True
2024-01-06 10:40:42 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 10:40:43 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 10:47:46 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 10:53:54 INFO     	Bleu_1: 0.3626231426626363
2024-01-06 10:53:54 INFO     	Bleu_2: 0.21874598836950093
2024-01-06 10:53:54 INFO     	Bleu_3: 0.1363444009653362
2024-01-06 10:53:54 INFO     	Bleu_4: 0.09299202692407522
2024-01-06 10:53:55 INFO     	Bleu_1: 0.3240806346988657
2024-01-06 10:53:55 INFO     	Bleu_2: 0.19117022169564965
2024-01-06 10:53:55 INFO     	Bleu_3: 0.11631349500934739
2024-01-06 10:53:55 INFO     	Bleu_4: 0.077737763608114
2024-01-06 10:54:09 INFO     use spaCy answer extraction model: positionrank
2024-01-06 10:54:09 INFO     Model `small_recreated_ckpt/model_efnljo/epoch_12`
2024-01-06 10:54:09 INFO     	 * Num of GPU in use: 1
2024-01-06 10:54:09 INFO     	 * Prefix: True
2024-01-06 10:54:09 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 10:54:10 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 11:00:57 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 11:07:01 INFO     	Bleu_1: 0.3670859091878044
2024-01-06 11:07:01 INFO     	Bleu_2: 0.2230302067481359
2024-01-06 11:07:01 INFO     	Bleu_3: 0.14025993552170105
2024-01-06 11:07:01 INFO     	Bleu_4: 0.09605839390423555
2024-01-06 11:07:01 INFO     	Bleu_1: 0.3335884512162561
2024-01-06 11:07:01 INFO     	Bleu_2: 0.19955040869013504
2024-01-06 11:07:01 INFO     	Bleu_3: 0.12346957522410981
2024-01-06 11:07:01 INFO     	Bleu_4: 0.0833773076311025
2024-01-06 11:07:15 INFO     use spaCy answer extraction model: positionrank
2024-01-06 11:07:16 INFO     Model `small_recreated_ckpt/model_efnljo/epoch_13`
2024-01-06 11:07:16 INFO     	 * Num of GPU in use: 1
2024-01-06 11:07:16 INFO     	 * Prefix: True
2024-01-06 11:07:16 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 11:07:17 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 11:13:55 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 11:19:55 INFO     	Bleu_1: 0.372340303637924
2024-01-06 11:19:55 INFO     	Bleu_2: 0.22662736603271838
2024-01-06 11:19:55 INFO     	Bleu_3: 0.14272077573629063
2024-01-06 11:19:55 INFO     	Bleu_4: 0.09802578050801573
2024-01-06 11:19:56 INFO     	Bleu_1: 0.34025001553534634
2024-01-06 11:19:56 INFO     	Bleu_2: 0.2033035900011024
2024-01-06 11:19:56 INFO     	Bleu_3: 0.12570056543347236
2024-01-06 11:19:56 INFO     	Bleu_4: 0.08494420529462905
2024-01-06 11:20:08 INFO     use spaCy answer extraction model: positionrank
2024-01-06 11:20:08 INFO     Model `small_recreated_ckpt/model_efnljo/epoch_14`
2024-01-06 11:20:08 INFO     	 * Num of GPU in use: 1
2024-01-06 11:20:08 INFO     	 * Prefix: True
2024-01-06 11:20:08 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 11:20:09 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 11:26:51 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 11:32:47 INFO     	Bleu_1: 0.3721146601520562
2024-01-06 11:32:47 INFO     	Bleu_2: 0.22608808078096598
2024-01-06 11:32:47 INFO     	Bleu_3: 0.1419844077091157
2024-01-06 11:32:47 INFO     	Bleu_4: 0.09719964539327132
2024-01-06 11:32:48 INFO     	Bleu_1: 0.3367583810782251
2024-01-06 11:32:48 INFO     	Bleu_2: 0.20072021274683768
2024-01-06 11:32:48 INFO     	Bleu_3: 0.12350010161972463
2024-01-06 11:32:48 INFO     	Bleu_4: 0.08288736484091218
2024-01-06 11:33:01 INFO     use spaCy answer extraction model: positionrank
2024-01-06 11:33:01 INFO     Model `small_recreated_ckpt/model_efnljo/epoch_15`
2024-01-06 11:33:01 INFO     	 * Num of GPU in use: 1
2024-01-06 11:33:01 INFO     	 * Prefix: True
2024-01-06 11:33:01 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 11:33:02 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 11:43:01 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 11:50:15 INFO     	Bleu_1: 0.3699954452288752
2024-01-06 11:50:15 INFO     	Bleu_2: 0.22500331205226737
2024-01-06 11:50:15 INFO     	Bleu_3: 0.14172187524004096
2024-01-06 11:50:15 INFO     	Bleu_4: 0.09742177211005702
2024-01-06 11:50:16 INFO     	Bleu_1: 0.3366521468574967
2024-01-06 11:50:16 INFO     	Bleu_2: 0.2001314327033647
2024-01-06 11:50:16 INFO     	Bleu_3: 0.12336932057301592
2024-01-06 11:50:16 INFO     	Bleu_4: 0.08314670795785009
2024-01-06 11:50:31 INFO     use spaCy answer extraction model: positionrank
2024-01-06 11:50:35 INFO     Model `small_recreated_ckpt/model_efnljo/epoch_2`
2024-01-06 11:50:35 INFO     	 * Num of GPU in use: 1
2024-01-06 11:50:35 INFO     	 * Prefix: True
2024-01-06 11:50:35 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 11:50:36 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 11:58:19 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 12:04:44 INFO     	Bleu_1: 0.33864400373388426
2024-01-06 12:04:44 INFO     	Bleu_2: 0.19828378877593814
2024-01-06 12:04:44 INFO     	Bleu_3: 0.11961489315135988
2024-01-06 12:04:44 INFO     	Bleu_4: 0.08025792928090675
2024-01-06 12:04:46 INFO     	Bleu_1: 0.30475903160117507
2024-01-06 12:04:46 INFO     	Bleu_2: 0.17592516950648812
2024-01-06 12:04:46 INFO     	Bleu_3: 0.1046405189126726
2024-01-06 12:04:46 INFO     	Bleu_4: 0.06937277143941264
2024-01-06 12:05:07 INFO     use spaCy answer extraction model: positionrank
2024-01-06 12:05:12 INFO     Model `small_recreated_ckpt/model_efnljo/epoch_3`
2024-01-06 12:05:12 INFO     	 * Num of GPU in use: 1
2024-01-06 12:05:12 INFO     	 * Prefix: True
2024-01-06 12:05:12 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 12:05:13 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 12:13:06 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 12:19:22 INFO     	Bleu_1: 0.3486864705054183
2024-01-06 12:19:22 INFO     	Bleu_2: 0.2083029433548779
2024-01-06 12:19:22 INFO     	Bleu_3: 0.12960428916878372
2024-01-06 12:19:22 INFO     	Bleu_4: 0.08874169940651182
2024-01-06 12:19:23 INFO     	Bleu_1: 0.31906642437286015
2024-01-06 12:19:23 INFO     	Bleu_2: 0.18737570370415765
2024-01-06 12:19:23 INFO     	Bleu_3: 0.11479496923278946
2024-01-06 12:19:23 INFO     	Bleu_4: 0.07768657216267257
2024-01-06 12:19:59 INFO     use spaCy answer extraction model: positionrank
2024-01-06 12:20:05 INFO     Model `small_recreated_ckpt/model_efnljo/epoch_4`
2024-01-06 12:20:05 INFO     	 * Num of GPU in use: 1
2024-01-06 12:20:05 INFO     	 * Prefix: True
2024-01-06 12:20:05 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 12:20:06 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 12:26:46 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 12:32:53 INFO     	Bleu_1: 0.35678998028528147
2024-01-06 12:32:53 INFO     	Bleu_2: 0.2136528532089758
2024-01-06 12:32:53 INFO     	Bleu_3: 0.13236130255601938
2024-01-06 12:32:53 INFO     	Bleu_4: 0.08994229896408151
2024-01-06 12:32:54 INFO     	Bleu_1: 0.32142080013262847
2024-01-06 12:32:54 INFO     	Bleu_2: 0.1885709877436482
2024-01-06 12:32:54 INFO     	Bleu_3: 0.11491569048488044
2024-01-06 12:32:54 INFO     	Bleu_4: 0.07723877311599288
2024-01-06 12:33:16 INFO     use spaCy answer extraction model: positionrank
2024-01-06 12:33:17 INFO     Model `small_recreated_ckpt/model_efnljo/epoch_5`
2024-01-06 12:33:17 INFO     	 * Num of GPU in use: 1
2024-01-06 12:33:17 INFO     	 * Prefix: True
2024-01-06 12:33:17 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 12:33:18 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 12:40:56 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 12:47:44 INFO     	Bleu_1: 0.36446875695877273
2024-01-06 12:47:44 INFO     	Bleu_2: 0.21879936220654886
2024-01-06 12:47:44 INFO     	Bleu_3: 0.13593675394855362
2024-01-06 12:47:44 INFO     	Bleu_4: 0.09227499001032663
2024-01-06 12:47:45 INFO     	Bleu_1: 0.33086402048988284
2024-01-06 12:47:45 INFO     	Bleu_2: 0.19540675646038366
2024-01-06 12:47:45 INFO     	Bleu_3: 0.11967736352991866
2024-01-06 12:47:45 INFO     	Bleu_4: 0.08060369578834661
2024-01-06 12:48:10 INFO     use spaCy answer extraction model: positionrank
2024-01-06 12:48:11 INFO     Model `small_recreated_ckpt/model_efnljo/epoch_6`
2024-01-06 12:48:11 INFO     	 * Num of GPU in use: 1
2024-01-06 12:48:11 INFO     	 * Prefix: True
2024-01-06 12:48:11 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 12:48:13 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 12:55:34 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 13:01:38 INFO     	Bleu_1: 0.36363478341483557
2024-01-06 13:01:38 INFO     	Bleu_2: 0.2181630216394586
2024-01-06 13:01:38 INFO     	Bleu_3: 0.1357194284634444
2024-01-06 13:01:38 INFO     	Bleu_4: 0.09220628146346917
2024-01-06 13:01:39 INFO     	Bleu_1: 0.3281375434840765
2024-01-06 13:01:39 INFO     	Bleu_2: 0.19396671557046627
2024-01-06 13:01:39 INFO     	Bleu_3: 0.11905939067873476
2024-01-06 13:01:39 INFO     	Bleu_4: 0.08038762488331791
2024-01-06 13:01:56 INFO     use spaCy answer extraction model: positionrank
2024-01-06 13:01:56 INFO     Model `small_recreated_ckpt/model_efnljo/epoch_7`
2024-01-06 13:01:56 INFO     	 * Num of GPU in use: 1
2024-01-06 13:01:56 INFO     	 * Prefix: True
2024-01-06 13:01:56 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 13:01:57 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 13:08:51 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 13:15:46 INFO     	Bleu_1: 0.36848384674926893
2024-01-06 13:15:46 INFO     	Bleu_2: 0.22240767287233248
2024-01-06 13:15:46 INFO     	Bleu_3: 0.13877898148179765
2024-01-06 13:15:46 INFO     	Bleu_4: 0.09441059281382387
2024-01-06 13:15:47 INFO     	Bleu_1: 0.33175826908729
2024-01-06 13:15:47 INFO     	Bleu_2: 0.19662017609525415
2024-01-06 13:15:47 INFO     	Bleu_3: 0.12064378178092021
2024-01-06 13:15:47 INFO     	Bleu_4: 0.08108490437870346
2024-01-06 13:16:03 INFO     use spaCy answer extraction model: positionrank
2024-01-06 13:16:03 INFO     Model `small_recreated_ckpt/model_efnljo/epoch_8`
2024-01-06 13:16:03 INFO     	 * Num of GPU in use: 1
2024-01-06 13:16:03 INFO     	 * Prefix: True
2024-01-06 13:16:03 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 13:16:04 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 13:24:13 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 13:30:39 INFO     	Bleu_1: 0.36560567395452354
2024-01-06 13:30:39 INFO     	Bleu_2: 0.2204517796093806
2024-01-06 13:30:39 INFO     	Bleu_3: 0.13770317946307484
2024-01-06 13:30:39 INFO     	Bleu_4: 0.09379355856641781
2024-01-06 13:30:40 INFO     	Bleu_1: 0.3334393303722531
2024-01-06 13:30:40 INFO     	Bleu_2: 0.19857464973974218
2024-01-06 13:30:40 INFO     	Bleu_3: 0.12302768378694016
2024-01-06 13:30:40 INFO     	Bleu_4: 0.083560758917277
2024-01-06 13:30:54 INFO     use spaCy answer extraction model: positionrank
2024-01-06 13:30:55 INFO     Model `small_recreated_ckpt/model_efnljo/epoch_9`
2024-01-06 13:30:55 INFO     	 * Num of GPU in use: 1
2024-01-06 13:30:55 INFO     	 * Prefix: True
2024-01-06 13:30:55 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 13:30:56 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 13:37:49 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 13:43:53 INFO     	Bleu_1: 0.3694782798015311
2024-01-06 13:43:53 INFO     	Bleu_2: 0.22360295456352508
2024-01-06 13:43:53 INFO     	Bleu_3: 0.13996459723447294
2024-01-06 13:43:53 INFO     	Bleu_4: 0.09555755644929802
2024-01-06 13:43:54 INFO     	Bleu_1: 0.33304454184078924
2024-01-06 13:43:54 INFO     	Bleu_2: 0.19825193766077873
2024-01-06 13:43:54 INFO     	Bleu_3: 0.12258149862936549
2024-01-06 13:43:54 INFO     	Bleu_4: 0.08303858281630778
2024-01-06 13:43:54 INFO     ## 2nd RUN (EVAL): Configuration 1/5 ##
2024-01-06 13:44:08 INFO     use spaCy answer extraction model: positionrank
2024-01-06 13:44:10 INFO     Model `small_recreated_ckpt/model_dpyopu/epoch_1`
2024-01-06 13:44:10 INFO     	 * Num of GPU in use: 1
2024-01-06 13:44:10 INFO     	 * Prefix: True
2024-01-06 13:44:10 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 13:44:11 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 13:51:12 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 13:57:24 INFO     	Bleu_1: 0.3283614461851593
2024-01-06 13:57:24 INFO     	Bleu_2: 0.1909401109223588
2024-01-06 13:57:24 INFO     	Bleu_3: 0.1142975034196719
2024-01-06 13:57:24 INFO     	Bleu_4: 0.07616116620154732
2024-01-06 13:57:25 INFO     	Bleu_1: 0.2964283623734157
2024-01-06 13:57:25 INFO     	Bleu_2: 0.16953085464574244
2024-01-06 13:57:25 INFO     	Bleu_3: 0.10032514974533265
2024-01-06 13:57:25 INFO     	Bleu_4: 0.0664950537948636
2024-01-06 13:57:45 INFO     use spaCy answer extraction model: positionrank
2024-01-06 13:57:48 INFO     Model `small_recreated_ckpt/model_dpyopu/epoch_11`
2024-01-06 13:57:48 INFO     	 * Num of GPU in use: 1
2024-01-06 13:57:48 INFO     	 * Prefix: True
2024-01-06 13:57:48 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 13:57:49 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 14:04:55 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 14:11:10 INFO     	Bleu_1: 0.36459154041824976
2024-01-06 14:11:10 INFO     	Bleu_2: 0.22010638357956236
2024-01-06 14:11:10 INFO     	Bleu_3: 0.13771152392631406
2024-01-06 14:11:10 INFO     	Bleu_4: 0.09433399258898663
2024-01-06 14:11:11 INFO     	Bleu_1: 0.32900702015071176
2024-01-06 14:11:11 INFO     	Bleu_2: 0.1950395783550916
2024-01-06 14:11:11 INFO     	Bleu_3: 0.11975707086149624
2024-01-06 14:11:11 INFO     	Bleu_4: 0.08064431167014252
2024-01-06 14:11:25 INFO     use spaCy answer extraction model: positionrank
2024-01-06 14:11:27 INFO     Model `small_recreated_ckpt/model_dpyopu/epoch_12`
2024-01-06 14:11:27 INFO     	 * Num of GPU in use: 1
2024-01-06 14:11:27 INFO     	 * Prefix: True
2024-01-06 14:11:27 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 14:11:28 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 14:18:20 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 14:24:31 INFO     	Bleu_1: 0.36976407178082926
2024-01-06 14:24:31 INFO     	Bleu_2: 0.22459958952975526
2024-01-06 14:24:31 INFO     	Bleu_3: 0.14177450880362188
2024-01-06 14:24:31 INFO     	Bleu_4: 0.0973771528524182
2024-01-06 14:24:32 INFO     	Bleu_1: 0.33461177960734473
2024-01-06 14:24:32 INFO     	Bleu_2: 0.19991784869269114
2024-01-06 14:24:32 INFO     	Bleu_3: 0.12428297404529721
2024-01-06 14:24:32 INFO     	Bleu_4: 0.0843938592335815
2024-01-06 14:24:49 INFO     use spaCy answer extraction model: positionrank
2024-01-06 14:24:51 INFO     Model `small_recreated_ckpt/model_dpyopu/epoch_13`
2024-01-06 14:24:51 INFO     	 * Num of GPU in use: 1
2024-01-06 14:24:51 INFO     	 * Prefix: True
2024-01-06 14:24:51 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 14:24:52 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 14:32:46 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 14:39:56 INFO     	Bleu_1: 0.36987938225678973
2024-01-06 14:39:56 INFO     	Bleu_2: 0.22489038259122396
2024-01-06 14:39:56 INFO     	Bleu_3: 0.14149934833657776
2024-01-06 14:39:56 INFO     	Bleu_4: 0.09687616925568494
2024-01-06 14:39:57 INFO     	Bleu_1: 0.3352484654281222
2024-01-06 14:39:57 INFO     	Bleu_2: 0.19998788822250246
2024-01-06 14:39:57 INFO     	Bleu_3: 0.12358035140729623
2024-01-06 14:39:57 INFO     	Bleu_4: 0.08342572953274775
2024-01-06 14:40:12 INFO     use spaCy answer extraction model: positionrank
2024-01-06 14:40:13 INFO     Model `small_recreated_ckpt/model_dpyopu/epoch_14`
2024-01-06 14:40:13 INFO     	 * Num of GPU in use: 1
2024-01-06 14:40:13 INFO     	 * Prefix: True
2024-01-06 14:40:13 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 14:40:14 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 14:46:58 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 14:52:56 INFO     	Bleu_1: 0.3750580998341623
2024-01-06 14:52:56 INFO     	Bleu_2: 0.22761454728772731
2024-01-06 14:52:56 INFO     	Bleu_3: 0.14279676477841105
2024-01-06 14:52:56 INFO     	Bleu_4: 0.09761135462292148
2024-01-06 14:52:58 INFO     	Bleu_1: 0.33853420613528257
2024-01-06 14:52:58 INFO     	Bleu_2: 0.20146897777036551
2024-01-06 14:52:58 INFO     	Bleu_3: 0.12413431533829945
2024-01-06 14:52:58 INFO     	Bleu_4: 0.0836756064875648
2024-01-06 14:53:14 INFO     use spaCy answer extraction model: positionrank
2024-01-06 14:53:15 INFO     Model `small_recreated_ckpt/model_dpyopu/epoch_15`
2024-01-06 14:53:15 INFO     	 * Num of GPU in use: 1
2024-01-06 14:53:15 INFO     	 * Prefix: True
2024-01-06 14:53:15 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 14:53:16 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 15:01:13 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 15:08:12 INFO     	Bleu_1: 0.36970199148546895
2024-01-06 15:08:12 INFO     	Bleu_2: 0.22442998411617746
2024-01-06 15:08:12 INFO     	Bleu_3: 0.1411378612731519
2024-01-06 15:08:12 INFO     	Bleu_4: 0.09667847047811486
2024-01-06 15:08:14 INFO     	Bleu_1: 0.3340426077370711
2024-01-06 15:08:14 INFO     	Bleu_2: 0.19899309398965712
2024-01-06 15:08:14 INFO     	Bleu_3: 0.123296952348954
2024-01-06 15:08:14 INFO     	Bleu_4: 0.08370811410745826
2024-01-06 15:08:31 INFO     use spaCy answer extraction model: positionrank
2024-01-06 15:08:32 INFO     Model `small_recreated_ckpt/model_dpyopu/epoch_2`
2024-01-06 15:08:32 INFO     	 * Num of GPU in use: 1
2024-01-06 15:08:32 INFO     	 * Prefix: True
2024-01-06 15:08:32 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 15:08:33 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 15:15:39 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 15:21:32 INFO     	Bleu_1: 0.33864400373388426
2024-01-06 15:21:32 INFO     	Bleu_2: 0.19828378877593814
2024-01-06 15:21:32 INFO     	Bleu_3: 0.11961489315135988
2024-01-06 15:21:32 INFO     	Bleu_4: 0.08025792928090675
2024-01-06 15:21:33 INFO     	Bleu_1: 0.30475903160117507
2024-01-06 15:21:33 INFO     	Bleu_2: 0.17592516950648812
2024-01-06 15:21:33 INFO     	Bleu_3: 0.1046405189126726
2024-01-06 15:21:33 INFO     	Bleu_4: 0.06937277143941264
2024-01-06 15:21:48 INFO     use spaCy answer extraction model: positionrank
2024-01-06 15:21:49 INFO     Model `small_recreated_ckpt/model_dpyopu/epoch_3`
2024-01-06 15:21:49 INFO     	 * Num of GPU in use: 1
2024-01-06 15:21:49 INFO     	 * Prefix: True
2024-01-06 15:21:49 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 15:21:50 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 15:29:06 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 15:35:14 INFO     	Bleu_1: 0.3486864705054183
2024-01-06 15:35:14 INFO     	Bleu_2: 0.2083029433548779
2024-01-06 15:35:14 INFO     	Bleu_3: 0.12960428916878372
2024-01-06 15:35:14 INFO     	Bleu_4: 0.08874169940651182
2024-01-06 15:35:15 INFO     	Bleu_1: 0.31906642437286015
2024-01-06 15:35:15 INFO     	Bleu_2: 0.18737570370415765
2024-01-06 15:35:15 INFO     	Bleu_3: 0.11479496923278946
2024-01-06 15:35:15 INFO     	Bleu_4: 0.07768657216267257
2024-01-06 15:35:33 INFO     use spaCy answer extraction model: positionrank
2024-01-06 15:35:33 INFO     Model `small_recreated_ckpt/model_dpyopu/epoch_4`
2024-01-06 15:35:33 INFO     	 * Num of GPU in use: 1
2024-01-06 15:35:33 INFO     	 * Prefix: True
2024-01-06 15:35:33 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 15:35:35 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 15:42:15 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 15:48:09 INFO     	Bleu_1: 0.35678998028528147
2024-01-06 15:48:09 INFO     	Bleu_2: 0.2136528532089758
2024-01-06 15:48:09 INFO     	Bleu_3: 0.13236130255601938
2024-01-06 15:48:09 INFO     	Bleu_4: 0.08994229896408151
2024-01-06 15:48:10 INFO     	Bleu_1: 0.32142080013262847
2024-01-06 15:48:10 INFO     	Bleu_2: 0.1885709877436482
2024-01-06 15:48:10 INFO     	Bleu_3: 0.11491569048488044
2024-01-06 15:48:10 INFO     	Bleu_4: 0.07723877311599288
2024-01-06 15:48:27 INFO     use spaCy answer extraction model: positionrank
2024-01-06 15:48:30 INFO     Model `small_recreated_ckpt/model_dpyopu/epoch_5`
2024-01-06 15:48:30 INFO     	 * Num of GPU in use: 1
2024-01-06 15:48:30 INFO     	 * Prefix: True
2024-01-06 15:48:30 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 15:48:31 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 15:55:04 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 16:01:11 INFO     	Bleu_1: 0.36446875695877273
2024-01-06 16:01:11 INFO     	Bleu_2: 0.21879936220654886
2024-01-06 16:01:11 INFO     	Bleu_3: 0.13593675394855362
2024-01-06 16:01:11 INFO     	Bleu_4: 0.09227499001032663
2024-01-06 16:01:13 INFO     	Bleu_1: 0.33086402048988284
2024-01-06 16:01:13 INFO     	Bleu_2: 0.19540675646038366
2024-01-06 16:01:13 INFO     	Bleu_3: 0.11967736352991866
2024-01-06 16:01:13 INFO     	Bleu_4: 0.08060369578834661
2024-01-06 16:01:28 INFO     use spaCy answer extraction model: positionrank
2024-01-06 16:01:30 INFO     Model `small_recreated_ckpt/model_dpyopu/epoch_6`
2024-01-06 16:01:30 INFO     	 * Num of GPU in use: 1
2024-01-06 16:01:30 INFO     	 * Prefix: True
2024-01-06 16:01:30 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 16:01:31 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 16:08:59 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 16:15:17 INFO     	Bleu_1: 0.36363478341483557
2024-01-06 16:15:17 INFO     	Bleu_2: 0.2181630216394586
2024-01-06 16:15:17 INFO     	Bleu_3: 0.1357194284634444
2024-01-06 16:15:17 INFO     	Bleu_4: 0.09220628146346917
2024-01-06 16:15:18 INFO     	Bleu_1: 0.3281375434840765
2024-01-06 16:15:18 INFO     	Bleu_2: 0.19396671557046627
2024-01-06 16:15:18 INFO     	Bleu_3: 0.11905939067873476
2024-01-06 16:15:18 INFO     	Bleu_4: 0.08038762488331791
2024-01-06 16:15:34 INFO     use spaCy answer extraction model: positionrank
2024-01-06 16:15:35 INFO     Model `small_recreated_ckpt/model_dpyopu/epoch_7`
2024-01-06 16:15:35 INFO     	 * Num of GPU in use: 1
2024-01-06 16:15:35 INFO     	 * Prefix: True
2024-01-06 16:15:35 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 16:15:36 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 16:22:13 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 16:28:20 INFO     	Bleu_1: 0.36848384674926893
2024-01-06 16:28:20 INFO     	Bleu_2: 0.22240767287233248
2024-01-06 16:28:20 INFO     	Bleu_3: 0.13877898148179765
2024-01-06 16:28:20 INFO     	Bleu_4: 0.09441059281382387
2024-01-06 16:28:21 INFO     	Bleu_1: 0.33175826908729
2024-01-06 16:28:21 INFO     	Bleu_2: 0.19662017609525415
2024-01-06 16:28:21 INFO     	Bleu_3: 0.12064378178092021
2024-01-06 16:28:21 INFO     	Bleu_4: 0.08108490437870346
2024-01-06 16:28:36 INFO     use spaCy answer extraction model: positionrank
2024-01-06 16:28:37 INFO     Model `small_recreated_ckpt/model_dpyopu/epoch_8`
2024-01-06 16:28:37 INFO     	 * Num of GPU in use: 1
2024-01-06 16:28:37 INFO     	 * Prefix: True
2024-01-06 16:28:37 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 16:28:38 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 16:36:14 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 16:42:42 INFO     	Bleu_1: 0.36560567395452354
2024-01-06 16:42:42 INFO     	Bleu_2: 0.2204517796093806
2024-01-06 16:42:42 INFO     	Bleu_3: 0.13770317946307484
2024-01-06 16:42:42 INFO     	Bleu_4: 0.09379355856641781
2024-01-06 16:42:43 INFO     	Bleu_1: 0.3334393303722531
2024-01-06 16:42:43 INFO     	Bleu_2: 0.19857464973974218
2024-01-06 16:42:43 INFO     	Bleu_3: 0.12302768378694016
2024-01-06 16:42:43 INFO     	Bleu_4: 0.083560758917277
2024-01-06 16:42:58 INFO     use spaCy answer extraction model: positionrank
2024-01-06 16:42:59 INFO     Model `small_recreated_ckpt/model_dpyopu/epoch_9`
2024-01-06 16:42:59 INFO     	 * Num of GPU in use: 1
2024-01-06 16:42:59 INFO     	 * Prefix: True
2024-01-06 16:42:59 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 16:43:00 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 16:49:52 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 16:55:55 INFO     	Bleu_1: 0.3694782798015311
2024-01-06 16:55:55 INFO     	Bleu_2: 0.22360295456352508
2024-01-06 16:55:55 INFO     	Bleu_3: 0.13996459723447294
2024-01-06 16:55:55 INFO     	Bleu_4: 0.09555755644929802
2024-01-06 16:55:56 INFO     	Bleu_1: 0.33304454184078924
2024-01-06 16:55:56 INFO     	Bleu_2: 0.19825193766077873
2024-01-06 16:55:56 INFO     	Bleu_3: 0.12258149862936549
2024-01-06 16:55:56 INFO     	Bleu_4: 0.08303858281630778
2024-01-06 16:55:56 INFO     ## 2nd RUN (EVAL): Configuration 2/5 ##
2024-01-06 16:56:10 INFO     use spaCy answer extraction model: positionrank
2024-01-06 16:56:11 INFO     Model `small_recreated_ckpt/model_eszyci/epoch_1`
2024-01-06 16:56:11 INFO     	 * Num of GPU in use: 1
2024-01-06 16:56:11 INFO     	 * Prefix: True
2024-01-06 16:56:11 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 16:56:12 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 17:03:20 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 17:09:33 INFO     	Bleu_1: 0.3304770931373071
2024-01-06 17:09:33 INFO     	Bleu_2: 0.19413972790830783
2024-01-06 17:09:33 INFO     	Bleu_3: 0.1184066940448661
2024-01-06 17:09:33 INFO     	Bleu_4: 0.07995092116162765
2024-01-06 17:09:33 INFO     	Bleu_1: 0.299502632408956
2024-01-06 17:09:33 INFO     	Bleu_2: 0.17289735949737695
2024-01-06 17:09:33 INFO     	Bleu_3: 0.10376146698011363
2024-01-06 17:09:33 INFO     	Bleu_4: 0.06934100733921532
2024-01-06 17:09:48 INFO     use spaCy answer extraction model: positionrank
2024-01-06 17:09:49 INFO     Model `small_recreated_ckpt/model_eszyci/epoch_11`
2024-01-06 17:09:49 INFO     	 * Num of GPU in use: 1
2024-01-06 17:09:49 INFO     	 * Prefix: True
2024-01-06 17:09:49 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 17:09:50 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 17:17:03 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 17:23:30 INFO     	Bleu_1: 0.36388367520221454
2024-01-06 17:23:30 INFO     	Bleu_2: 0.22014528118649856
2024-01-06 17:23:30 INFO     	Bleu_3: 0.13733523162411065
2024-01-06 17:23:30 INFO     	Bleu_4: 0.09366932013629056
2024-01-06 17:23:31 INFO     	Bleu_1: 0.32865783815284283
2024-01-06 17:23:31 INFO     	Bleu_2: 0.19408987949761444
2024-01-06 17:23:31 INFO     	Bleu_3: 0.11786584060532912
2024-01-06 17:23:31 INFO     	Bleu_4: 0.07864007116336501
2024-01-06 17:23:47 INFO     use spaCy answer extraction model: positionrank
2024-01-06 17:23:49 INFO     Model `small_recreated_ckpt/model_eszyci/epoch_12`
2024-01-06 17:23:49 INFO     	 * Num of GPU in use: 1
2024-01-06 17:23:49 INFO     	 * Prefix: True
2024-01-06 17:23:49 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 17:23:50 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 17:30:41 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 17:36:44 INFO     	Bleu_1: 0.37280198285560695
2024-01-06 17:36:44 INFO     	Bleu_2: 0.22732292968658335
2024-01-06 17:36:44 INFO     	Bleu_3: 0.1430166513995183
2024-01-06 17:36:44 INFO     	Bleu_4: 0.09811174205362462
2024-01-06 17:36:45 INFO     	Bleu_1: 0.33612017955519113
2024-01-06 17:36:45 INFO     	Bleu_2: 0.20056477838909514
2024-01-06 17:36:45 INFO     	Bleu_3: 0.12387875465660725
2024-01-06 17:36:45 INFO     	Bleu_4: 0.08351864057015322
2024-01-06 17:37:03 INFO     use spaCy answer extraction model: positionrank
2024-01-06 17:37:04 INFO     Model `small_recreated_ckpt/model_eszyci/epoch_13`
2024-01-06 17:37:04 INFO     	 * Num of GPU in use: 1
2024-01-06 17:37:04 INFO     	 * Prefix: True
2024-01-06 17:37:04 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 17:37:05 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 17:44:17 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 17:50:44 INFO     	Bleu_1: 0.3758672664465367
2024-01-06 17:50:44 INFO     	Bleu_2: 0.2281274425940686
2024-01-06 17:50:44 INFO     	Bleu_3: 0.14265053257419946
2024-01-06 17:50:44 INFO     	Bleu_4: 0.09740899153012596
2024-01-06 17:50:45 INFO     	Bleu_1: 0.3416916206987111
2024-01-06 17:50:45 INFO     	Bleu_2: 0.20345264217676934
2024-01-06 17:50:45 INFO     	Bleu_3: 0.12534926812658423
2024-01-06 17:50:45 INFO     	Bleu_4: 0.08450120173517328
2024-01-06 17:51:05 INFO     use spaCy answer extraction model: positionrank
2024-01-06 17:51:06 INFO     Model `small_recreated_ckpt/model_eszyci/epoch_14`
2024-01-06 17:51:06 INFO     	 * Num of GPU in use: 1
2024-01-06 17:51:06 INFO     	 * Prefix: True
2024-01-06 17:51:06 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 17:51:08 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 17:58:46 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 18:06:24 INFO     	Bleu_1: 0.37370472208335015
2024-01-06 18:06:24 INFO     	Bleu_2: 0.22790954060864757
2024-01-06 18:06:24 INFO     	Bleu_3: 0.14330061580549217
2024-01-06 18:06:24 INFO     	Bleu_4: 0.09807814192752165
2024-01-06 18:06:25 INFO     	Bleu_1: 0.3394900302611484
2024-01-06 18:06:25 INFO     	Bleu_2: 0.2019251725032224
2024-01-06 18:06:25 INFO     	Bleu_3: 0.12405420820028279
2024-01-06 18:06:25 INFO     	Bleu_4: 0.08327344597737782
2024-01-06 18:06:45 INFO     use spaCy answer extraction model: positionrank
2024-01-06 18:06:56 INFO     Model `small_recreated_ckpt/model_eszyci/epoch_15`
2024-01-06 18:06:56 INFO     	 * Num of GPU in use: 1
2024-01-06 18:06:56 INFO     	 * Prefix: True
2024-01-06 18:06:56 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 18:06:57 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 18:17:01 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 18:26:55 INFO     	Bleu_1: 0.3766228518319659
2024-01-06 18:26:55 INFO     	Bleu_2: 0.2290883233556413
2024-01-06 18:26:55 INFO     	Bleu_3: 0.1437109187090603
2024-01-06 18:26:55 INFO     	Bleu_4: 0.0982179007514993
2024-01-06 18:26:56 INFO     	Bleu_1: 0.33748200436539094
2024-01-06 18:26:56 INFO     	Bleu_2: 0.20066084129944964
2024-01-06 18:26:56 INFO     	Bleu_3: 0.12309871643560681
2024-01-06 18:26:56 INFO     	Bleu_4: 0.08256724255503417
2024-01-06 18:27:15 INFO     use spaCy answer extraction model: positionrank
2024-01-06 18:27:19 INFO     Model `small_recreated_ckpt/model_eszyci/epoch_2`
2024-01-06 18:27:19 INFO     	 * Num of GPU in use: 1
2024-01-06 18:27:19 INFO     	 * Prefix: True
2024-01-06 18:27:19 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 18:27:20 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 18:36:14 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 18:43:03 INFO     	Bleu_1: 0.3434843950892584
2024-01-06 18:43:03 INFO     	Bleu_2: 0.2035898881564216
2024-01-06 18:43:03 INFO     	Bleu_3: 0.12436261733830965
2024-01-06 18:43:03 INFO     	Bleu_4: 0.08379595662987474
2024-01-06 18:43:04 INFO     	Bleu_1: 0.30989657605187554
2024-01-06 18:43:04 INFO     	Bleu_2: 0.18082959073489266
2024-01-06 18:43:04 INFO     	Bleu_3: 0.1091185891663345
2024-01-06 18:43:04 INFO     	Bleu_4: 0.07297376378669408
2024-01-06 18:43:30 INFO     use spaCy answer extraction model: positionrank
2024-01-06 18:43:42 INFO     Model `small_recreated_ckpt/model_eszyci/epoch_3`
2024-01-06 18:43:42 INFO     	 * Num of GPU in use: 1
2024-01-06 18:43:42 INFO     	 * Prefix: True
2024-01-06 18:43:42 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 18:43:44 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 18:52:16 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 19:00:24 INFO     	Bleu_1: 0.35450318366977773
2024-01-06 19:00:24 INFO     	Bleu_2: 0.2126934919491511
2024-01-06 19:00:24 INFO     	Bleu_3: 0.1325409205303177
2024-01-06 19:00:24 INFO     	Bleu_4: 0.09057510472902286
2024-01-06 19:00:25 INFO     	Bleu_1: 0.32485521950920143
2024-01-06 19:00:25 INFO     	Bleu_2: 0.19209473349168674
2024-01-06 19:00:25 INFO     	Bleu_3: 0.11836083552449975
2024-01-06 19:00:25 INFO     	Bleu_4: 0.08040204827439162
2024-01-06 19:00:49 INFO     use spaCy answer extraction model: positionrank
2024-01-06 19:01:01 INFO     Model `small_recreated_ckpt/model_eszyci/epoch_4`
2024-01-06 19:01:01 INFO     	 * Num of GPU in use: 1
2024-01-06 19:01:01 INFO     	 * Prefix: True
2024-01-06 19:01:01 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 19:01:03 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 19:09:39 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 19:17:01 INFO     	Bleu_1: 0.36381808802494686
2024-01-06 19:17:01 INFO     	Bleu_2: 0.21834051612696323
2024-01-06 19:17:01 INFO     	Bleu_3: 0.13587090141505132
2024-01-06 19:17:01 INFO     	Bleu_4: 0.09251955047143624
2024-01-06 19:17:02 INFO     	Bleu_1: 0.32866860416140004
2024-01-06 19:17:02 INFO     	Bleu_2: 0.1939027711167906
2024-01-06 19:17:02 INFO     	Bleu_3: 0.11855257721870943
2024-01-06 19:17:02 INFO     	Bleu_4: 0.07985717469265262
2024-01-06 19:17:20 INFO     use spaCy answer extraction model: positionrank
2024-01-06 19:17:28 INFO     Model `small_recreated_ckpt/model_eszyci/epoch_5`
2024-01-06 19:17:28 INFO     	 * Num of GPU in use: 1
2024-01-06 19:17:28 INFO     	 * Prefix: True
2024-01-06 19:17:28 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 19:17:30 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 19:25:58 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 19:33:20 INFO     	Bleu_1: 0.36691282849821705
2024-01-06 19:33:20 INFO     	Bleu_2: 0.220644010833488
2024-01-06 19:33:20 INFO     	Bleu_3: 0.13692269142344174
2024-01-06 19:33:20 INFO     	Bleu_4: 0.09288081286165471
2024-01-06 19:33:20 INFO     	Bleu_1: 0.33380842895933077
2024-01-06 19:33:20 INFO     	Bleu_2: 0.1980021612699369
2024-01-06 19:33:20 INFO     	Bleu_3: 0.12189659971774325
2024-01-06 19:33:20 INFO     	Bleu_4: 0.08237545577734218
2024-01-06 19:33:43 INFO     use spaCy answer extraction model: positionrank
2024-01-06 19:33:51 INFO     Model `small_recreated_ckpt/model_eszyci/epoch_6`
2024-01-06 19:33:51 INFO     	 * Num of GPU in use: 1
2024-01-06 19:33:51 INFO     	 * Prefix: True
2024-01-06 19:33:51 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 19:33:52 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 19:43:04 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 19:52:32 INFO     	Bleu_1: 0.36740045545510797
2024-01-06 19:52:32 INFO     	Bleu_2: 0.22116515945087484
2024-01-06 19:52:32 INFO     	Bleu_3: 0.13759116103921198
2024-01-06 19:52:32 INFO     	Bleu_4: 0.09331788918662001
2024-01-06 19:52:33 INFO     	Bleu_1: 0.3326369352970362
2024-01-06 19:52:33 INFO     	Bleu_2: 0.1976932552061682
2024-01-06 19:52:33 INFO     	Bleu_3: 0.12193087117956512
2024-01-06 19:52:33 INFO     	Bleu_4: 0.08242403024219006
2024-01-06 19:52:50 INFO     use spaCy answer extraction model: positionrank
2024-01-06 19:52:53 INFO     Model `small_recreated_ckpt/model_eszyci/epoch_7`
2024-01-06 19:52:53 INFO     	 * Num of GPU in use: 1
2024-01-06 19:52:53 INFO     	 * Prefix: True
2024-01-06 19:52:53 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 19:52:55 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 20:02:23 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 20:09:56 INFO     	Bleu_1: 0.37134511397158754
2024-01-06 20:09:56 INFO     	Bleu_2: 0.224744400193981
2024-01-06 20:09:56 INFO     	Bleu_3: 0.140589416531353
2024-01-06 20:09:56 INFO     	Bleu_4: 0.09589468279048785
2024-01-06 20:09:57 INFO     	Bleu_1: 0.33707300129090956
2024-01-06 20:09:57 INFO     	Bleu_2: 0.20103973365006172
2024-01-06 20:09:57 INFO     	Bleu_3: 0.12442237313168826
2024-01-06 20:09:57 INFO     	Bleu_4: 0.08417311302096825
2024-01-06 20:10:14 INFO     use spaCy answer extraction model: positionrank
2024-01-06 20:10:18 INFO     Model `small_recreated_ckpt/model_eszyci/epoch_8`
2024-01-06 20:10:18 INFO     	 * Num of GPU in use: 1
2024-01-06 20:10:18 INFO     	 * Prefix: True
2024-01-06 20:10:18 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 20:10:19 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 20:18:49 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 20:27:14 INFO     	Bleu_1: 0.3704585618874919
2024-01-06 20:27:15 INFO     	Bleu_2: 0.2243282927818667
2024-01-06 20:27:15 INFO     	Bleu_3: 0.13986979049482534
2024-01-06 20:27:15 INFO     	Bleu_4: 0.09511758448110293
2024-01-06 20:27:17 INFO     	Bleu_1: 0.33377173383165254
2024-01-06 20:27:17 INFO     	Bleu_2: 0.1984615407225027
2024-01-06 20:27:17 INFO     	Bleu_3: 0.12198643198559511
2024-01-06 20:27:17 INFO     	Bleu_4: 0.08200702187143831
2024-01-06 20:27:48 INFO     use spaCy answer extraction model: positionrank
2024-01-06 20:27:54 INFO     Model `small_recreated_ckpt/model_eszyci/epoch_9`
2024-01-06 20:27:54 INFO     	 * Num of GPU in use: 1
2024-01-06 20:27:54 INFO     	 * Prefix: True
2024-01-06 20:27:54 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 20:27:55 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 20:38:54 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 20:46:40 INFO     	Bleu_1: 0.3684149184149164
2024-01-06 20:46:40 INFO     	Bleu_2: 0.2242677686339821
2024-01-06 20:46:40 INFO     	Bleu_3: 0.14077534596041108
2024-01-06 20:46:40 INFO     	Bleu_4: 0.09620029180907264
2024-01-06 20:46:42 INFO     	Bleu_1: 0.33214935071386537
2024-01-06 20:46:42 INFO     	Bleu_2: 0.1982623548906425
2024-01-06 20:46:42 INFO     	Bleu_3: 0.12258146177406214
2024-01-06 20:46:42 INFO     	Bleu_4: 0.0830133481145704
2024-01-06 20:46:42 INFO     ## 2nd RUN (EVAL): Configuration 3/5 ##
2024-01-06 20:47:04 INFO     use spaCy answer extraction model: positionrank
2024-01-06 20:47:07 INFO     Model `small_recreated_ckpt/model_mzgdpa/epoch_1`
2024-01-06 20:47:07 INFO     	 * Num of GPU in use: 1
2024-01-06 20:47:07 INFO     	 * Prefix: True
2024-01-06 20:47:07 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 20:47:08 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 20:56:05 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 21:04:05 INFO     	Bleu_1: 0.3304770931373071
2024-01-06 21:04:05 INFO     	Bleu_2: 0.19413972790830783
2024-01-06 21:04:05 INFO     	Bleu_3: 0.1184066940448661
2024-01-06 21:04:05 INFO     	Bleu_4: 0.07995092116162765
2024-01-06 21:04:06 INFO     	Bleu_1: 0.299502632408956
2024-01-06 21:04:06 INFO     	Bleu_2: 0.17289735949737695
2024-01-06 21:04:06 INFO     	Bleu_3: 0.10376146698011363
2024-01-06 21:04:06 INFO     	Bleu_4: 0.06934100733921532
2024-01-06 21:04:40 INFO     use spaCy answer extraction model: positionrank
2024-01-06 21:04:45 INFO     Model `small_recreated_ckpt/model_mzgdpa/epoch_11`
2024-01-06 21:04:45 INFO     	 * Num of GPU in use: 1
2024-01-06 21:04:45 INFO     	 * Prefix: True
2024-01-06 21:04:45 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 21:04:46 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 21:14:11 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 21:22:57 INFO     	Bleu_1: 0.36537822387662655
2024-01-06 21:22:57 INFO     	Bleu_2: 0.2206276462970882
2024-01-06 21:22:57 INFO     	Bleu_3: 0.13724429870979613
2024-01-06 21:22:57 INFO     	Bleu_4: 0.09341458358175056
2024-01-06 21:23:05 INFO     	Bleu_1: 0.32622749188231664
2024-01-06 21:23:05 INFO     	Bleu_2: 0.19333596453978252
2024-01-06 21:23:05 INFO     	Bleu_3: 0.1182564787648003
2024-01-06 21:23:05 INFO     	Bleu_4: 0.07942186324196672
2024-01-06 21:23:28 INFO     use spaCy answer extraction model: positionrank
2024-01-06 21:23:34 INFO     Model `small_recreated_ckpt/model_mzgdpa/epoch_12`
2024-01-06 21:23:34 INFO     	 * Num of GPU in use: 1
2024-01-06 21:23:34 INFO     	 * Prefix: True
2024-01-06 21:23:34 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 21:23:35 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 21:32:37 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 21:40:24 INFO     	Bleu_1: 0.36952929853743544
2024-01-06 21:40:24 INFO     	Bleu_2: 0.22490415322781113
2024-01-06 21:40:24 INFO     	Bleu_3: 0.14139145649513185
2024-01-06 21:40:24 INFO     	Bleu_4: 0.09680492244377166
2024-01-06 21:40:25 INFO     	Bleu_1: 0.33417106811184094
2024-01-06 21:40:25 INFO     	Bleu_2: 0.19937425092521913
2024-01-06 21:40:25 INFO     	Bleu_3: 0.12350363225789997
2024-01-06 21:40:25 INFO     	Bleu_4: 0.08365754794707288
2024-01-06 21:40:45 INFO     use spaCy answer extraction model: positionrank
2024-01-06 21:40:56 INFO     Model `small_recreated_ckpt/model_mzgdpa/epoch_13`
2024-01-06 21:40:56 INFO     	 * Num of GPU in use: 1
2024-01-06 21:40:56 INFO     	 * Prefix: True
2024-01-06 21:40:56 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 21:40:57 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 21:49:31 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 21:56:54 INFO     	Bleu_1: 0.37558930177406114
2024-01-06 21:56:54 INFO     	Bleu_2: 0.2283593757467271
2024-01-06 21:56:54 INFO     	Bleu_3: 0.1431580290533367
2024-01-06 21:56:54 INFO     	Bleu_4: 0.09782715961340392
2024-01-06 21:56:55 INFO     	Bleu_1: 0.3373704192633964
2024-01-06 21:56:55 INFO     	Bleu_2: 0.20119844574938425
2024-01-06 21:56:55 INFO     	Bleu_3: 0.1240036252180278
2024-01-06 21:56:55 INFO     	Bleu_4: 0.08354446537630063
2024-01-06 21:57:14 INFO     use spaCy answer extraction model: positionrank
2024-01-06 21:57:25 INFO     Model `small_recreated_ckpt/model_mzgdpa/epoch_14`
2024-01-06 21:57:25 INFO     	 * Num of GPU in use: 1
2024-01-06 21:57:25 INFO     	 * Prefix: True
2024-01-06 21:57:25 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 21:57:26 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 22:06:38 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 22:15:54 INFO     	Bleu_1: 0.37158175728138626
2024-01-06 22:15:54 INFO     	Bleu_2: 0.22576905969469313
2024-01-06 22:15:54 INFO     	Bleu_3: 0.14122655148732277
2024-01-06 22:15:54 INFO     	Bleu_4: 0.09620798066573849
2024-01-06 22:15:56 INFO     	Bleu_1: 0.336432123789057
2024-01-06 22:15:56 INFO     	Bleu_2: 0.2003339201385338
2024-01-06 22:15:56 INFO     	Bleu_3: 0.12315140764858117
2024-01-06 22:15:56 INFO     	Bleu_4: 0.08285441673507955
2024-01-06 22:16:22 INFO     use spaCy answer extraction model: positionrank
2024-01-06 22:16:28 INFO     Model `small_recreated_ckpt/model_mzgdpa/epoch_15`
2024-01-06 22:16:28 INFO     	 * Num of GPU in use: 1
2024-01-06 22:16:28 INFO     	 * Prefix: True
2024-01-06 22:16:28 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 22:16:29 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 22:26:09 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 22:34:06 INFO     	Bleu_1: 0.3699454894776799
2024-01-06 22:34:06 INFO     	Bleu_2: 0.22444247894742203
2024-01-06 22:34:06 INFO     	Bleu_3: 0.14055648559784603
2024-01-06 22:34:06 INFO     	Bleu_4: 0.0960507353847
2024-01-06 22:34:07 INFO     	Bleu_1: 0.33773364426369035
2024-01-06 22:34:07 INFO     	Bleu_2: 0.20070356357487282
2024-01-06 22:34:07 INFO     	Bleu_3: 0.12333959246435923
2024-01-06 22:34:07 INFO     	Bleu_4: 0.0829807154400507
2024-01-06 22:34:29 INFO     use spaCy answer extraction model: positionrank
2024-01-06 22:34:37 INFO     Model `small_recreated_ckpt/model_mzgdpa/epoch_2`
2024-01-06 22:34:37 INFO     	 * Num of GPU in use: 1
2024-01-06 22:34:37 INFO     	 * Prefix: True
2024-01-06 22:34:37 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 22:34:38 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 22:43:00 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 22:50:45 INFO     	Bleu_1: 0.3434843950892584
2024-01-06 22:50:45 INFO     	Bleu_2: 0.2035898881564216
2024-01-06 22:50:45 INFO     	Bleu_3: 0.12436261733830965
2024-01-06 22:50:45 INFO     	Bleu_4: 0.08379595662987474
2024-01-06 22:50:46 INFO     	Bleu_1: 0.30989657605187554
2024-01-06 22:50:46 INFO     	Bleu_2: 0.18082959073489266
2024-01-06 22:50:46 INFO     	Bleu_3: 0.1091185891663345
2024-01-06 22:50:46 INFO     	Bleu_4: 0.07297376378669408
2024-01-06 22:51:12 INFO     use spaCy answer extraction model: positionrank
2024-01-06 22:51:17 INFO     Model `small_recreated_ckpt/model_mzgdpa/epoch_3`
2024-01-06 22:51:17 INFO     	 * Num of GPU in use: 1
2024-01-06 22:51:17 INFO     	 * Prefix: True
2024-01-06 22:51:17 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 22:51:18 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 22:59:44 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 23:07:16 INFO     	Bleu_1: 0.35450318366977773
2024-01-06 23:07:16 INFO     	Bleu_2: 0.2126934919491511
2024-01-06 23:07:16 INFO     	Bleu_3: 0.1325409205303177
2024-01-06 23:07:16 INFO     	Bleu_4: 0.09057510472902286
2024-01-06 23:07:23 INFO     	Bleu_1: 0.32485521950920143
2024-01-06 23:07:23 INFO     	Bleu_2: 0.19209473349168674
2024-01-06 23:07:23 INFO     	Bleu_3: 0.11836083552449975
2024-01-06 23:07:23 INFO     	Bleu_4: 0.08040204827439162
2024-01-06 23:07:45 INFO     use spaCy answer extraction model: positionrank
2024-01-06 23:07:51 INFO     Model `small_recreated_ckpt/model_mzgdpa/epoch_4`
2024-01-06 23:07:51 INFO     	 * Num of GPU in use: 1
2024-01-06 23:07:51 INFO     	 * Prefix: True
2024-01-06 23:07:51 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 23:07:53 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 23:15:12 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 23:22:06 INFO     	Bleu_1: 0.36381808802494686
2024-01-06 23:22:06 INFO     	Bleu_2: 0.21834051612696323
2024-01-06 23:22:06 INFO     	Bleu_3: 0.13587090141505132
2024-01-06 23:22:06 INFO     	Bleu_4: 0.09251955047143624
2024-01-06 23:22:08 INFO     	Bleu_1: 0.32866860416140004
2024-01-06 23:22:08 INFO     	Bleu_2: 0.1939027711167906
2024-01-06 23:22:08 INFO     	Bleu_3: 0.11855257721870943
2024-01-06 23:22:08 INFO     	Bleu_4: 0.07985717469265262
2024-01-06 23:22:27 INFO     use spaCy answer extraction model: positionrank
2024-01-06 23:22:27 INFO     Model `small_recreated_ckpt/model_mzgdpa/epoch_5`
2024-01-06 23:22:27 INFO     	 * Num of GPU in use: 1
2024-01-06 23:22:27 INFO     	 * Prefix: True
2024-01-06 23:22:27 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 23:22:28 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 23:29:54 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 23:36:36 INFO     	Bleu_1: 0.36691282849821705
2024-01-06 23:36:36 INFO     	Bleu_2: 0.220644010833488
2024-01-06 23:36:36 INFO     	Bleu_3: 0.13692269142344174
2024-01-06 23:36:36 INFO     	Bleu_4: 0.09288081286165471
2024-01-06 23:36:37 INFO     	Bleu_1: 0.33380842895933077
2024-01-06 23:36:37 INFO     	Bleu_2: 0.1980021612699369
2024-01-06 23:36:37 INFO     	Bleu_3: 0.12189659971774325
2024-01-06 23:36:37 INFO     	Bleu_4: 0.08237545577734218
2024-01-06 23:36:51 INFO     use spaCy answer extraction model: positionrank
2024-01-06 23:36:51 INFO     Model `small_recreated_ckpt/model_mzgdpa/epoch_6`
2024-01-06 23:36:51 INFO     	 * Num of GPU in use: 1
2024-01-06 23:36:51 INFO     	 * Prefix: True
2024-01-06 23:36:51 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 23:36:52 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 23:44:56 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-06 23:51:38 INFO     	Bleu_1: 0.36740045545510797
2024-01-06 23:51:38 INFO     	Bleu_2: 0.22116515945087484
2024-01-06 23:51:38 INFO     	Bleu_3: 0.13759116103921198
2024-01-06 23:51:38 INFO     	Bleu_4: 0.09331788918662001
2024-01-06 23:51:39 INFO     	Bleu_1: 0.3326369352970362
2024-01-06 23:51:39 INFO     	Bleu_2: 0.1976932552061682
2024-01-06 23:51:39 INFO     	Bleu_3: 0.12193087117956512
2024-01-06 23:51:39 INFO     	Bleu_4: 0.08242403024219006
2024-01-06 23:51:54 INFO     use spaCy answer extraction model: positionrank
2024-01-06 23:51:54 INFO     Model `small_recreated_ckpt/model_mzgdpa/epoch_7`
2024-01-06 23:51:54 INFO     	 * Num of GPU in use: 1
2024-01-06 23:51:54 INFO     	 * Prefix: True
2024-01-06 23:51:54 INFO     	 * Language: en (ignore at the training phase)
2024-01-06 23:51:55 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-06 23:59:23 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-07 00:06:24 INFO     	Bleu_1: 0.37134511397158754
2024-01-07 00:06:24 INFO     	Bleu_2: 0.224744400193981
2024-01-07 00:06:24 INFO     	Bleu_3: 0.140589416531353
2024-01-07 00:06:24 INFO     	Bleu_4: 0.09589468279048785
2024-01-07 00:06:25 INFO     	Bleu_1: 0.33707300129090956
2024-01-07 00:06:25 INFO     	Bleu_2: 0.20103973365006172
2024-01-07 00:06:25 INFO     	Bleu_3: 0.12442237313168826
2024-01-07 00:06:25 INFO     	Bleu_4: 0.08417311302096825
2024-01-07 00:06:39 INFO     use spaCy answer extraction model: positionrank
2024-01-07 00:06:40 INFO     Model `small_recreated_ckpt/model_mzgdpa/epoch_8`
2024-01-07 00:06:40 INFO     	 * Num of GPU in use: 1
2024-01-07 00:06:40 INFO     	 * Prefix: True
2024-01-07 00:06:40 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 00:06:41 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-07 00:14:23 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-07 00:20:58 INFO     	Bleu_1: 0.3704585618874919
2024-01-07 00:20:58 INFO     	Bleu_2: 0.2243282927818667
2024-01-07 00:20:58 INFO     	Bleu_3: 0.13986979049482534
2024-01-07 00:20:58 INFO     	Bleu_4: 0.09511758448110293
2024-01-07 00:20:59 INFO     	Bleu_1: 0.33377173383165254
2024-01-07 00:20:59 INFO     	Bleu_2: 0.1984615407225027
2024-01-07 00:20:59 INFO     	Bleu_3: 0.12198643198559511
2024-01-07 00:20:59 INFO     	Bleu_4: 0.08200702187143831
2024-01-07 00:21:14 INFO     use spaCy answer extraction model: positionrank
2024-01-07 00:21:14 INFO     Model `small_recreated_ckpt/model_mzgdpa/epoch_9`
2024-01-07 00:21:14 INFO     	 * Num of GPU in use: 1
2024-01-07 00:21:14 INFO     	 * Prefix: True
2024-01-07 00:21:14 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 00:21:15 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-07 00:29:22 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-07 00:36:44 INFO     	Bleu_1: 0.3684149184149164
2024-01-07 00:36:44 INFO     	Bleu_2: 0.2242677686339821
2024-01-07 00:36:44 INFO     	Bleu_3: 0.14077534596041108
2024-01-07 00:36:44 INFO     	Bleu_4: 0.09620029180907264
2024-01-07 00:36:45 INFO     	Bleu_1: 0.33214935071386537
2024-01-07 00:36:45 INFO     	Bleu_2: 0.1982623548906425
2024-01-07 00:36:45 INFO     	Bleu_3: 0.12258146177406214
2024-01-07 00:36:45 INFO     	Bleu_4: 0.0830133481145704
2024-01-07 00:36:45 INFO     ## 2nd RUN (EVAL): Configuration 4/5 ##
2024-01-07 00:36:59 INFO     use spaCy answer extraction model: positionrank
2024-01-07 00:36:59 INFO     Model `small_recreated_ckpt/model_woixzh/epoch_1`
2024-01-07 00:36:59 INFO     	 * Num of GPU in use: 1
2024-01-07 00:36:59 INFO     	 * Prefix: True
2024-01-07 00:36:59 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 00:37:01 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-07 00:44:51 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-07 00:51:47 INFO     	Bleu_1: 0.3289075975131562
2024-01-07 00:51:47 INFO     	Bleu_2: 0.19150255479390121
2024-01-07 00:51:47 INFO     	Bleu_3: 0.11539658810091581
2024-01-07 00:51:47 INFO     	Bleu_4: 0.07722170057467605
2024-01-07 00:51:48 INFO     	Bleu_1: 0.296044114354685
2024-01-07 00:51:48 INFO     	Bleu_2: 0.1687794471527558
2024-01-07 00:51:48 INFO     	Bleu_3: 0.09946122664860471
2024-01-07 00:51:48 INFO     	Bleu_4: 0.06574887918014105
2024-01-07 00:52:05 INFO     use spaCy answer extraction model: positionrank
2024-01-07 00:52:05 INFO     Model `small_recreated_ckpt/model_woixzh/epoch_11`
2024-01-07 00:52:05 INFO     	 * Num of GPU in use: 1
2024-01-07 00:52:05 INFO     	 * Prefix: True
2024-01-07 00:52:05 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 00:52:06 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-07 01:00:15 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-07 01:07:30 INFO     	Bleu_1: 0.35770525554349747
2024-01-07 01:07:30 INFO     	Bleu_2: 0.2145742478451125
2024-01-07 01:07:30 INFO     	Bleu_3: 0.13297157250217145
2024-01-07 01:07:30 INFO     	Bleu_4: 0.0904059892897553
2024-01-07 01:07:31 INFO     	Bleu_1: 0.3233831830257082
2024-01-07 01:07:31 INFO     	Bleu_2: 0.1914124242646957
2024-01-07 01:07:31 INFO     	Bleu_3: 0.11685526485894686
2024-01-07 01:07:31 INFO     	Bleu_4: 0.07852088136452019
2024-01-07 01:07:44 INFO     use spaCy answer extraction model: positionrank
2024-01-07 01:07:44 INFO     Model `small_recreated_ckpt/model_woixzh/epoch_12`
2024-01-07 01:07:44 INFO     	 * Num of GPU in use: 1
2024-01-07 01:07:44 INFO     	 * Prefix: True
2024-01-07 01:07:44 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 01:07:45 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-07 01:15:27 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-07 01:22:20 INFO     	Bleu_1: 0.36515159175556366
2024-01-07 01:22:20 INFO     	Bleu_2: 0.22118088154929408
2024-01-07 01:22:20 INFO     	Bleu_3: 0.13894979469611085
2024-01-07 01:22:20 INFO     	Bleu_4: 0.09525675118724611
2024-01-07 01:22:21 INFO     	Bleu_1: 0.33021176115953516
2024-01-07 01:22:21 INFO     	Bleu_2: 0.19677476562188737
2024-01-07 01:22:21 INFO     	Bleu_3: 0.12128265555202353
2024-01-07 01:22:21 INFO     	Bleu_4: 0.08181487474516687
2024-01-07 01:22:38 INFO     use spaCy answer extraction model: positionrank
2024-01-07 01:22:39 INFO     Model `small_recreated_ckpt/model_woixzh/epoch_13`
2024-01-07 01:22:39 INFO     	 * Num of GPU in use: 1
2024-01-07 01:22:39 INFO     	 * Prefix: True
2024-01-07 01:22:39 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 01:22:40 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-07 01:30:20 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-07 01:37:07 INFO     	Bleu_1: 0.3697935007880086
2024-01-07 01:37:08 INFO     	Bleu_2: 0.2237573145466537
2024-01-07 01:37:08 INFO     	Bleu_3: 0.13984868993564512
2024-01-07 01:37:08 INFO     	Bleu_4: 0.09533820580985451
2024-01-07 01:37:08 INFO     	Bleu_1: 0.337500130375375
2024-01-07 01:37:08 INFO     	Bleu_2: 0.20081164305614305
2024-01-07 01:37:08 INFO     	Bleu_3: 0.12337643536910971
2024-01-07 01:37:08 INFO     	Bleu_4: 0.08280526944179263
2024-01-07 01:37:21 INFO     use spaCy answer extraction model: positionrank
2024-01-07 01:37:22 INFO     Model `small_recreated_ckpt/model_woixzh/epoch_14`
2024-01-07 01:37:22 INFO     	 * Num of GPU in use: 1
2024-01-07 01:37:22 INFO     	 * Prefix: True
2024-01-07 01:37:22 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 01:37:23 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-07 01:44:57 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-07 01:51:55 INFO     	Bleu_1: 0.36975328787280004
2024-01-07 01:51:55 INFO     	Bleu_2: 0.22340294129093718
2024-01-07 01:51:55 INFO     	Bleu_3: 0.13927254227964092
2024-01-07 01:51:55 INFO     	Bleu_4: 0.09495754899412526
2024-01-07 01:51:56 INFO     	Bleu_1: 0.3349556726198107
2024-01-07 01:51:56 INFO     	Bleu_2: 0.19892571292362857
2024-01-07 01:51:56 INFO     	Bleu_3: 0.12179550461210627
2024-01-07 01:51:56 INFO     	Bleu_4: 0.08148450025123087
2024-01-07 01:52:10 INFO     use spaCy answer extraction model: positionrank
2024-01-07 01:52:11 INFO     Model `small_recreated_ckpt/model_woixzh/epoch_15`
2024-01-07 01:52:11 INFO     	 * Num of GPU in use: 1
2024-01-07 01:52:11 INFO     	 * Prefix: True
2024-01-07 01:52:11 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 01:52:12 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-07 02:00:14 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-07 02:06:54 INFO     	Bleu_1: 0.36961188439886977
2024-01-07 02:06:54 INFO     	Bleu_2: 0.22447559656788085
2024-01-07 02:06:54 INFO     	Bleu_3: 0.14106388614713097
2024-01-07 02:06:54 INFO     	Bleu_4: 0.09690655049529388
2024-01-07 02:06:55 INFO     	Bleu_1: 0.3339488221083551
2024-01-07 02:06:55 INFO     	Bleu_2: 0.19812240999710332
2024-01-07 02:06:55 INFO     	Bleu_3: 0.1218259926913584
2024-01-07 02:06:55 INFO     	Bleu_4: 0.08201296110666237
2024-01-07 02:07:13 INFO     use spaCy answer extraction model: positionrank
2024-01-07 02:07:15 INFO     Model `small_recreated_ckpt/model_woixzh/epoch_2`
2024-01-07 02:07:15 INFO     	 * Num of GPU in use: 1
2024-01-07 02:07:15 INFO     	 * Prefix: True
2024-01-07 02:07:15 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 02:07:16 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-07 02:14:44 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-07 02:21:37 INFO     	Bleu_1: 0.3351240493348886
2024-01-07 02:21:37 INFO     	Bleu_2: 0.19519495516050386
2024-01-07 02:21:37 INFO     	Bleu_3: 0.11675049477422157
2024-01-07 02:21:37 INFO     	Bleu_4: 0.07774306178314144
2024-01-07 02:21:38 INFO     	Bleu_1: 0.3025848784610016
2024-01-07 02:21:38 INFO     	Bleu_2: 0.17395360905662596
2024-01-07 02:21:38 INFO     	Bleu_3: 0.10282482391650402
2024-01-07 02:21:38 INFO     	Bleu_4: 0.06805731158669039
2024-01-07 02:21:55 INFO     use spaCy answer extraction model: positionrank
2024-01-07 02:21:55 INFO     Model `small_recreated_ckpt/model_woixzh/epoch_3`
2024-01-07 02:21:55 INFO     	 * Num of GPU in use: 1
2024-01-07 02:21:55 INFO     	 * Prefix: True
2024-01-07 02:21:55 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 02:21:56 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-07 02:29:28 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-07 02:36:08 INFO     	Bleu_1: 0.3484408867277524
2024-01-07 02:36:08 INFO     	Bleu_2: 0.20714814399644846
2024-01-07 02:36:08 INFO     	Bleu_3: 0.12777864996374155
2024-01-07 02:36:08 INFO     	Bleu_4: 0.08699247977841164
2024-01-07 02:36:09 INFO     	Bleu_1: 0.31768919911829374
2024-01-07 02:36:09 INFO     	Bleu_2: 0.18565144520342358
2024-01-07 02:36:09 INFO     	Bleu_3: 0.11255924884320423
2024-01-07 02:36:09 INFO     	Bleu_4: 0.07552012609352395
2024-01-07 02:36:30 INFO     use spaCy answer extraction model: positionrank
2024-01-07 02:36:31 INFO     Model `small_recreated_ckpt/model_woixzh/epoch_4`
2024-01-07 02:36:31 INFO     	 * Num of GPU in use: 1
2024-01-07 02:36:31 INFO     	 * Prefix: True
2024-01-07 02:36:31 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 02:36:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-07 02:44:06 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-07 02:50:13 INFO     	Bleu_1: 0.3587781250388588
2024-01-07 02:50:13 INFO     	Bleu_2: 0.2139904431072057
2024-01-07 02:50:13 INFO     	Bleu_3: 0.13208204034752952
2024-01-07 02:50:13 INFO     	Bleu_4: 0.08932439647031558
2024-01-07 02:50:14 INFO     	Bleu_1: 0.32505671925288704
2024-01-07 02:50:14 INFO     	Bleu_2: 0.19093959907488028
2024-01-07 02:50:14 INFO     	Bleu_3: 0.11638275319862663
2024-01-07 02:50:14 INFO     	Bleu_4: 0.07841496586818665
2024-01-07 02:50:30 INFO     use spaCy answer extraction model: positionrank
2024-01-07 02:50:31 INFO     Model `small_recreated_ckpt/model_woixzh/epoch_5`
2024-01-07 02:50:31 INFO     	 * Num of GPU in use: 1
2024-01-07 02:50:31 INFO     	 * Prefix: True
2024-01-07 02:50:31 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 02:50:32 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-07 02:58:09 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-07 03:04:57 INFO     	Bleu_1: 0.36324817392584885
2024-01-07 03:04:57 INFO     	Bleu_2: 0.2180253667486157
2024-01-07 03:04:57 INFO     	Bleu_3: 0.13571904270996943
2024-01-07 03:04:57 INFO     	Bleu_4: 0.09247003146659277
2024-01-07 03:04:58 INFO     	Bleu_1: 0.3293089119624014
2024-01-07 03:04:58 INFO     	Bleu_2: 0.1942734614434529
2024-01-07 03:04:58 INFO     	Bleu_3: 0.11901019665498988
2024-01-07 03:04:58 INFO     	Bleu_4: 0.08032365248316845
2024-01-07 03:05:15 INFO     use spaCy answer extraction model: positionrank
2024-01-07 03:05:15 INFO     Model `small_recreated_ckpt/model_woixzh/epoch_6`
2024-01-07 03:05:15 INFO     	 * Num of GPU in use: 1
2024-01-07 03:05:15 INFO     	 * Prefix: True
2024-01-07 03:05:15 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 03:05:16 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-07 03:12:30 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-07 03:19:17 INFO     	Bleu_1: 0.36450899353862065
2024-01-07 03:19:17 INFO     	Bleu_2: 0.21897723133654887
2024-01-07 03:19:17 INFO     	Bleu_3: 0.13620051146447826
2024-01-07 03:19:17 INFO     	Bleu_4: 0.09251435162557123
2024-01-07 03:19:18 INFO     	Bleu_1: 0.32932173118458047
2024-01-07 03:19:18 INFO     	Bleu_2: 0.19468475957859724
2024-01-07 03:19:18 INFO     	Bleu_3: 0.11954158097521993
2024-01-07 03:19:18 INFO     	Bleu_4: 0.08068201083803649
2024-01-07 03:19:34 INFO     use spaCy answer extraction model: positionrank
2024-01-07 03:19:34 INFO     Model `small_recreated_ckpt/model_woixzh/epoch_7`
2024-01-07 03:19:34 INFO     	 * Num of GPU in use: 1
2024-01-07 03:19:34 INFO     	 * Prefix: True
2024-01-07 03:19:34 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 03:19:35 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-07 03:27:12 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-07 03:33:33 INFO     	Bleu_1: 0.3638484369045261
2024-01-07 03:33:33 INFO     	Bleu_2: 0.21909865632944428
2024-01-07 03:33:33 INFO     	Bleu_3: 0.13634081069737297
2024-01-07 03:33:33 INFO     	Bleu_4: 0.09270816310986144
2024-01-07 03:33:34 INFO     	Bleu_1: 0.32860685510840204
2024-01-07 03:33:34 INFO     	Bleu_2: 0.1945290967061184
2024-01-07 03:33:34 INFO     	Bleu_3: 0.11928059782491794
2024-01-07 03:33:34 INFO     	Bleu_4: 0.08042979449210315
2024-01-07 03:33:50 INFO     use spaCy answer extraction model: positionrank
2024-01-07 03:33:51 INFO     Model `small_recreated_ckpt/model_woixzh/epoch_8`
2024-01-07 03:33:51 INFO     	 * Num of GPU in use: 1
2024-01-07 03:33:51 INFO     	 * Prefix: True
2024-01-07 03:33:51 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 03:33:52 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-07 03:41:47 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-07 03:48:50 INFO     	Bleu_1: 0.3658672519592726
2024-01-07 03:48:50 INFO     	Bleu_2: 0.22102474476473097
2024-01-07 03:48:50 INFO     	Bleu_3: 0.13848346868647962
2024-01-07 03:48:50 INFO     	Bleu_4: 0.09459327692025854
2024-01-07 03:48:51 INFO     	Bleu_1: 0.3335655818082333
2024-01-07 03:48:51 INFO     	Bleu_2: 0.19822915846052516
2024-01-07 03:48:51 INFO     	Bleu_3: 0.1221384903231381
2024-01-07 03:48:51 INFO     	Bleu_4: 0.08238668150574568
2024-01-07 03:49:08 INFO     use spaCy answer extraction model: positionrank
2024-01-07 03:49:09 INFO     Model `small_recreated_ckpt/model_woixzh/epoch_9`
2024-01-07 03:49:09 INFO     	 * Num of GPU in use: 1
2024-01-07 03:49:09 INFO     	 * Prefix: True
2024-01-07 03:49:09 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 03:49:10 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-07 03:57:19 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-07 04:03:56 INFO     	Bleu_1: 0.3636056661432724
2024-01-07 04:03:56 INFO     	Bleu_2: 0.21943463732650645
2024-01-07 04:03:56 INFO     	Bleu_3: 0.13698055700441542
2024-01-07 04:03:56 INFO     	Bleu_4: 0.09349011343028578
2024-01-07 04:03:57 INFO     	Bleu_1: 0.32960989514609884
2024-01-07 04:03:57 INFO     	Bleu_2: 0.19542347467133173
2024-01-07 04:03:57 INFO     	Bleu_3: 0.12027532218238257
2024-01-07 04:03:57 INFO     	Bleu_4: 0.08118226948038197
2024-01-07 04:03:57 INFO     2nd RUN RESULTS: 
[('small_recreated_ckpt/model_eszyci/epoch_15', 0.0982179007514993), ('small_recreated_ckpt/model_eszyci/epoch_12', 0.09811174205362462), ('small_recreated_ckpt/model_eszyci/epoch_14', 0.09807814192752165), ('small_recreated_ckpt/model_efnljo/epoch_13', 0.09802578050801573), ('small_recreated_ckpt/model_mzgdpa/epoch_13', 0.09782715961340392), ('small_recreated_ckpt/model_dpyopu/epoch_14', 0.09761135462292148), ('small_recreated_ckpt/model_efnljo/epoch_15', 0.09742177211005702), ('small_recreated_ckpt/model_eszyci/epoch_13', 0.09740899153012596), ('small_recreated_ckpt/model_dpyopu/epoch_12', 0.0973771528524182), ('small_recreated_ckpt/model_efnljo/epoch_14', 0.09719964539327132), ('small_recreated_ckpt/model_woixzh/epoch_15', 0.09690655049529388), ('small_recreated_ckpt/model_dpyopu/epoch_13', 0.09687616925568494), ('small_recreated_ckpt/model_mzgdpa/epoch_12', 0.09680492244377166), ('small_recreated_ckpt/model_dpyopu/epoch_15', 0.09667847047811486), ('small_recreated_ckpt/model_mzgdpa/epoch_14', 0.09620798066573849), ('small_recreated_ckpt/model_eszyci/epoch_9', 0.09620029180907264), ('small_recreated_ckpt/model_mzgdpa/epoch_9', 0.09620029180907264), ('small_recreated_ckpt/model_efnljo/epoch_12', 0.09605839390423555), ('small_recreated_ckpt/model_mzgdpa/epoch_15', 0.0960507353847), ('small_recreated_ckpt/model_eszyci/epoch_7', 0.09589468279048785), ('small_recreated_ckpt/model_mzgdpa/epoch_7', 0.09589468279048785), ('small_recreated_ckpt/model_efnljo/epoch_9', 0.09555755644929802), ('small_recreated_ckpt/model_dpyopu/epoch_9', 0.09555755644929802), ('small_recreated_ckpt/model_woixzh/epoch_13', 0.09533820580985451), ('small_recreated_ckpt/model_woixzh/epoch_12', 0.09525675118724611), ('small_recreated_ckpt/model_eszyci/epoch_8', 0.09511758448110293), ('small_recreated_ckpt/model_mzgdpa/epoch_8', 0.09511758448110293), ('small_recreated_ckpt/model_woixzh/epoch_14', 0.09495754899412526), ('small_recreated_ckpt/model_efnljo/epoch_10', 0.09485688097694292), ('small_recreated_ckpt/model_dpyopu/epoch_10', 0.09485688097694292), ('small_recreated_ckpt/model_eszyci/epoch_10', 0.09466239682555477), ('small_recreated_ckpt/model_mzgdpa/epoch_10', 0.09466239682555477), ('small_recreated_ckpt/model_woixzh/epoch_8', 0.09459327692025854), ('small_recreated_ckpt/model_woixzh/epoch_10', 0.09445276580206216), ('small_recreated_ckpt/model_efnljo/epoch_7', 0.09441059281382387), ('small_recreated_ckpt/model_dpyopu/epoch_7', 0.09441059281382387), ('small_recreated_ckpt/model_dpyopu/epoch_11', 0.09433399258898663), ('small_recreated_ckpt/model_efnljo/epoch_8', 0.09379355856641781), ('small_recreated_ckpt/model_dpyopu/epoch_8', 0.09379355856641781), ('small_recreated_ckpt/model_eszyci/epoch_11', 0.09366932013629056), ('small_recreated_ckpt/model_woixzh/epoch_9', 0.09349011343028578), ('small_recreated_ckpt/model_mzgdpa/epoch_11', 0.09341458358175056), ('small_recreated_ckpt/model_eszyci/epoch_6', 0.09331788918662001), ('small_recreated_ckpt/model_mzgdpa/epoch_6', 0.09331788918662001), ('small_recreated_ckpt/model_efnljo/epoch_11', 0.09299202692407522), ('small_recreated_ckpt/model_eszyci/epoch_5', 0.09288081286165471), ('small_recreated_ckpt/model_mzgdpa/epoch_5', 0.09288081286165471), ('small_recreated_ckpt/model_woixzh/epoch_7', 0.09270816310986144), ('small_recreated_ckpt/model_eszyci/epoch_4', 0.09251955047143624), ('small_recreated_ckpt/model_mzgdpa/epoch_4', 0.09251955047143624), ('small_recreated_ckpt/model_woixzh/epoch_6', 0.09251435162557123), ('small_recreated_ckpt/model_woixzh/epoch_5', 0.09247003146659277), ('small_recreated_ckpt/model_efnljo/epoch_5', 0.09227499001032663), ('small_recreated_ckpt/model_dpyopu/epoch_5', 0.09227499001032663), ('small_recreated_ckpt/model_efnljo/epoch_6', 0.09220628146346917), ('small_recreated_ckpt/model_dpyopu/epoch_6', 0.09220628146346917), ('small_recreated_ckpt/model_eszyci/epoch_3', 0.09057510472902286), ('small_recreated_ckpt/model_mzgdpa/epoch_3', 0.09057510472902286), ('small_recreated_ckpt/model_woixzh/epoch_11', 0.0904059892897553), ('small_recreated_ckpt/model_efnljo/epoch_4', 0.08994229896408151), ('small_recreated_ckpt/model_dpyopu/epoch_4', 0.08994229896408151), ('small_recreated_ckpt/model_woixzh/epoch_4', 0.08932439647031558), ('small_recreated_ckpt/model_efnljo/epoch_3', 0.08874169940651182), ('small_recreated_ckpt/model_dpyopu/epoch_3', 0.08874169940651182), ('small_recreated_ckpt/model_woixzh/epoch_3', 0.08699247977841164), ('small_recreated_ckpt/model_eszyci/epoch_2', 0.08379595662987474), ('small_recreated_ckpt/model_mzgdpa/epoch_2', 0.08379595662987474), ('small_recreated_ckpt/model_efnljo/epoch_2', 0.08025792928090675), ('small_recreated_ckpt/model_dpyopu/epoch_2', 0.08025792928090675), ('small_recreated_ckpt/model_eszyci/epoch_1', 0.07995092116162765), ('small_recreated_ckpt/model_mzgdpa/epoch_1', 0.07995092116162765), ('small_recreated_ckpt/model_woixzh/epoch_2', 0.07774306178314144), ('small_recreated_ckpt/model_woixzh/epoch_1', 0.07722170057467605), ('small_recreated_ckpt/model_efnljo/epoch_1', 0.07616116620154732), ('small_recreated_ckpt/model_dpyopu/epoch_1', 0.07616116620154732)]
2024-01-07 04:03:57 INFO     	 * rank: 0 | metric: 0.098 | model: small_recreated_ckpt/model_eszyci/epoch_15 |
2024-01-07 04:03:57 INFO     	 * rank: 1 | metric: 0.098 | model: small_recreated_ckpt/model_eszyci/epoch_12 |
2024-01-07 04:03:57 INFO     	 * rank: 2 | metric: 0.098 | model: small_recreated_ckpt/model_eszyci/epoch_14 |
2024-01-07 04:03:57 INFO     	 * rank: 3 | metric: 0.098 | model: small_recreated_ckpt/model_efnljo/epoch_13 |
2024-01-07 04:03:57 INFO     	 * rank: 4 | metric: 0.098 | model: small_recreated_ckpt/model_mzgdpa/epoch_13 |
2024-01-07 04:03:57 INFO     	 * rank: 5 | metric: 0.098 | model: small_recreated_ckpt/model_dpyopu/epoch_14 |
2024-01-07 04:03:57 INFO     	 * rank: 6 | metric: 0.097 | model: small_recreated_ckpt/model_efnljo/epoch_15 |
2024-01-07 04:03:57 INFO     	 * rank: 7 | metric: 0.097 | model: small_recreated_ckpt/model_eszyci/epoch_13 |
2024-01-07 04:03:57 INFO     	 * rank: 8 | metric: 0.097 | model: small_recreated_ckpt/model_dpyopu/epoch_12 |
2024-01-07 04:03:57 INFO     	 * rank: 9 | metric: 0.097 | model: small_recreated_ckpt/model_efnljo/epoch_14 |
2024-01-07 04:03:57 INFO     	 * rank: 10 | metric: 0.097 | model: small_recreated_ckpt/model_woixzh/epoch_15 |
2024-01-07 04:03:57 INFO     	 * rank: 11 | metric: 0.097 | model: small_recreated_ckpt/model_dpyopu/epoch_13 |
2024-01-07 04:03:57 INFO     	 * rank: 12 | metric: 0.097 | model: small_recreated_ckpt/model_mzgdpa/epoch_12 |
2024-01-07 04:03:57 INFO     	 * rank: 13 | metric: 0.097 | model: small_recreated_ckpt/model_dpyopu/epoch_15 |
2024-01-07 04:03:57 INFO     	 * rank: 14 | metric: 0.096 | model: small_recreated_ckpt/model_mzgdpa/epoch_14 |
2024-01-07 04:03:57 INFO     	 * rank: 15 | metric: 0.096 | model: small_recreated_ckpt/model_eszyci/epoch_9 |
2024-01-07 04:03:57 INFO     	 * rank: 16 | metric: 0.096 | model: small_recreated_ckpt/model_mzgdpa/epoch_9 |
2024-01-07 04:03:57 INFO     	 * rank: 17 | metric: 0.096 | model: small_recreated_ckpt/model_efnljo/epoch_12 |
2024-01-07 04:03:57 INFO     	 * rank: 18 | metric: 0.096 | model: small_recreated_ckpt/model_mzgdpa/epoch_15 |
2024-01-07 04:03:57 INFO     	 * rank: 19 | metric: 0.096 | model: small_recreated_ckpt/model_eszyci/epoch_7 |
2024-01-07 04:03:57 INFO     	 * rank: 20 | metric: 0.096 | model: small_recreated_ckpt/model_mzgdpa/epoch_7 |
2024-01-07 04:03:57 INFO     	 * rank: 21 | metric: 0.096 | model: small_recreated_ckpt/model_efnljo/epoch_9 |
2024-01-07 04:03:57 INFO     	 * rank: 22 | metric: 0.096 | model: small_recreated_ckpt/model_dpyopu/epoch_9 |
2024-01-07 04:03:57 INFO     	 * rank: 23 | metric: 0.095 | model: small_recreated_ckpt/model_woixzh/epoch_13 |
2024-01-07 04:03:57 INFO     	 * rank: 24 | metric: 0.095 | model: small_recreated_ckpt/model_woixzh/epoch_12 |
2024-01-07 04:03:57 INFO     	 * rank: 25 | metric: 0.095 | model: small_recreated_ckpt/model_eszyci/epoch_8 |
2024-01-07 04:03:57 INFO     	 * rank: 26 | metric: 0.095 | model: small_recreated_ckpt/model_mzgdpa/epoch_8 |
2024-01-07 04:03:57 INFO     	 * rank: 27 | metric: 0.095 | model: small_recreated_ckpt/model_woixzh/epoch_14 |
2024-01-07 04:03:57 INFO     	 * rank: 28 | metric: 0.095 | model: small_recreated_ckpt/model_efnljo/epoch_10 |
2024-01-07 04:03:57 INFO     	 * rank: 29 | metric: 0.095 | model: small_recreated_ckpt/model_dpyopu/epoch_10 |
2024-01-07 04:03:57 INFO     	 * rank: 30 | metric: 0.095 | model: small_recreated_ckpt/model_eszyci/epoch_10 |
2024-01-07 04:03:57 INFO     	 * rank: 31 | metric: 0.095 | model: small_recreated_ckpt/model_mzgdpa/epoch_10 |
2024-01-07 04:03:57 INFO     	 * rank: 32 | metric: 0.095 | model: small_recreated_ckpt/model_woixzh/epoch_8 |
2024-01-07 04:03:57 INFO     	 * rank: 33 | metric: 0.094 | model: small_recreated_ckpt/model_woixzh/epoch_10 |
2024-01-07 04:03:57 INFO     	 * rank: 34 | metric: 0.094 | model: small_recreated_ckpt/model_efnljo/epoch_7 |
2024-01-07 04:03:57 INFO     	 * rank: 35 | metric: 0.094 | model: small_recreated_ckpt/model_dpyopu/epoch_7 |
2024-01-07 04:03:57 INFO     	 * rank: 36 | metric: 0.094 | model: small_recreated_ckpt/model_dpyopu/epoch_11 |
2024-01-07 04:03:57 INFO     	 * rank: 37 | metric: 0.094 | model: small_recreated_ckpt/model_efnljo/epoch_8 |
2024-01-07 04:03:57 INFO     	 * rank: 38 | metric: 0.094 | model: small_recreated_ckpt/model_dpyopu/epoch_8 |
2024-01-07 04:03:57 INFO     	 * rank: 39 | metric: 0.094 | model: small_recreated_ckpt/model_eszyci/epoch_11 |
2024-01-07 04:03:57 INFO     	 * rank: 40 | metric: 0.093 | model: small_recreated_ckpt/model_woixzh/epoch_9 |
2024-01-07 04:03:57 INFO     	 * rank: 41 | metric: 0.093 | model: small_recreated_ckpt/model_mzgdpa/epoch_11 |
2024-01-07 04:03:57 INFO     	 * rank: 42 | metric: 0.093 | model: small_recreated_ckpt/model_eszyci/epoch_6 |
2024-01-07 04:03:57 INFO     	 * rank: 43 | metric: 0.093 | model: small_recreated_ckpt/model_mzgdpa/epoch_6 |
2024-01-07 04:03:57 INFO     	 * rank: 44 | metric: 0.093 | model: small_recreated_ckpt/model_efnljo/epoch_11 |
2024-01-07 04:03:57 INFO     	 * rank: 45 | metric: 0.093 | model: small_recreated_ckpt/model_eszyci/epoch_5 |
2024-01-07 04:03:57 INFO     	 * rank: 46 | metric: 0.093 | model: small_recreated_ckpt/model_mzgdpa/epoch_5 |
2024-01-07 04:03:57 INFO     	 * rank: 47 | metric: 0.093 | model: small_recreated_ckpt/model_woixzh/epoch_7 |
2024-01-07 04:03:57 INFO     	 * rank: 48 | metric: 0.093 | model: small_recreated_ckpt/model_eszyci/epoch_4 |
2024-01-07 04:03:57 INFO     	 * rank: 49 | metric: 0.093 | model: small_recreated_ckpt/model_mzgdpa/epoch_4 |
2024-01-07 04:03:57 INFO     	 * rank: 50 | metric: 0.093 | model: small_recreated_ckpt/model_woixzh/epoch_6 |
2024-01-07 04:03:57 INFO     	 * rank: 51 | metric: 0.092 | model: small_recreated_ckpt/model_woixzh/epoch_5 |
2024-01-07 04:03:57 INFO     	 * rank: 52 | metric: 0.092 | model: small_recreated_ckpt/model_efnljo/epoch_5 |
2024-01-07 04:03:57 INFO     	 * rank: 53 | metric: 0.092 | model: small_recreated_ckpt/model_dpyopu/epoch_5 |
2024-01-07 04:03:57 INFO     	 * rank: 54 | metric: 0.092 | model: small_recreated_ckpt/model_efnljo/epoch_6 |
2024-01-07 04:03:57 INFO     	 * rank: 55 | metric: 0.092 | model: small_recreated_ckpt/model_dpyopu/epoch_6 |
2024-01-07 04:03:57 INFO     	 * rank: 56 | metric: 0.091 | model: small_recreated_ckpt/model_eszyci/epoch_3 |
2024-01-07 04:03:57 INFO     	 * rank: 57 | metric: 0.091 | model: small_recreated_ckpt/model_mzgdpa/epoch_3 |
2024-01-07 04:03:57 INFO     	 * rank: 58 | metric: 0.09 | model: small_recreated_ckpt/model_woixzh/epoch_11 |
2024-01-07 04:03:57 INFO     	 * rank: 59 | metric: 0.09 | model: small_recreated_ckpt/model_efnljo/epoch_4 |
2024-01-07 04:03:57 INFO     	 * rank: 60 | metric: 0.09 | model: small_recreated_ckpt/model_dpyopu/epoch_4 |
2024-01-07 04:03:57 INFO     	 * rank: 61 | metric: 0.089 | model: small_recreated_ckpt/model_woixzh/epoch_4 |
2024-01-07 04:03:57 INFO     	 * rank: 62 | metric: 0.089 | model: small_recreated_ckpt/model_efnljo/epoch_3 |
2024-01-07 04:03:57 INFO     	 * rank: 63 | metric: 0.089 | model: small_recreated_ckpt/model_dpyopu/epoch_3 |
2024-01-07 04:03:57 INFO     	 * rank: 64 | metric: 0.087 | model: small_recreated_ckpt/model_woixzh/epoch_3 |
2024-01-07 04:03:57 INFO     	 * rank: 65 | metric: 0.084 | model: small_recreated_ckpt/model_eszyci/epoch_2 |
2024-01-07 04:03:57 INFO     	 * rank: 66 | metric: 0.084 | model: small_recreated_ckpt/model_mzgdpa/epoch_2 |
2024-01-07 04:03:57 INFO     	 * rank: 67 | metric: 0.08 | model: small_recreated_ckpt/model_efnljo/epoch_2 |
2024-01-07 04:03:57 INFO     	 * rank: 68 | metric: 0.08 | model: small_recreated_ckpt/model_dpyopu/epoch_2 |
2024-01-07 04:03:57 INFO     	 * rank: 69 | metric: 0.08 | model: small_recreated_ckpt/model_eszyci/epoch_1 |
2024-01-07 04:03:57 INFO     	 * rank: 70 | metric: 0.08 | model: small_recreated_ckpt/model_mzgdpa/epoch_1 |
2024-01-07 04:03:57 INFO     	 * rank: 71 | metric: 0.078 | model: small_recreated_ckpt/model_woixzh/epoch_2 |
2024-01-07 04:03:57 INFO     	 * rank: 72 | metric: 0.077 | model: small_recreated_ckpt/model_woixzh/epoch_1 |
2024-01-07 04:03:57 INFO     	 * rank: 73 | metric: 0.076 | model: small_recreated_ckpt/model_efnljo/epoch_1 |
2024-01-07 04:03:57 INFO     	 * rank: 74 | metric: 0.076 | model: small_recreated_ckpt/model_dpyopu/epoch_1 |
2024-01-07 04:03:57 INFO     ## 3rd RUN: target model: small_recreated_ckpt/model_eszyci (metric: 0.0982179007514993) ##
2024-01-07 04:03:57 INFO     ## 3rd RUN (TRAIN): epoch 16 ##
2024-01-07 04:03:57 INFO     initialize model trainer
2024-01-07 04:03:57 INFO     load config from existing checkpoint at small_recreated_ckpt/model_eszyci
2024-01-07 04:03:57 INFO     hyperparameters
2024-01-07 04:03:57 INFO     	 * dataset_path: lmqg/qag_squad
2024-01-07 04:03:57 INFO     	 * dataset_name: default
2024-01-07 04:03:57 INFO     	 * input_types: ['paragraph']
2024-01-07 04:03:57 INFO     	 * output_types: ['questions_answers']
2024-01-07 04:03:57 INFO     	 * prefix_types: ['qag']
2024-01-07 04:03:57 INFO     	 * model: t5-small
2024-01-07 04:03:57 INFO     	 * max_length: 512
2024-01-07 04:03:57 INFO     	 * max_length_output: 256
2024-01-07 04:03:57 INFO     	 * epoch: 16
2024-01-07 04:03:57 INFO     	 * batch: 2
2024-01-07 04:03:57 INFO     	 * lr: 0.0001
2024-01-07 04:03:57 INFO     	 * fp16: False
2024-01-07 04:03:57 INFO     	 * random_seed: 1
2024-01-07 04:03:57 INFO     	 * gradient_accumulation_steps: 2
2024-01-07 04:03:57 INFO     	 * label_smoothing: 0.15
2024-01-07 04:03:57 INFO     load checkpoint from small_recreated_ckpt/model_eszyci/epoch_15
2024-01-07 04:04:13 INFO     use spaCy answer extraction model: positionrank
2024-01-07 04:04:14 INFO     Model `small_recreated_ckpt/model_eszyci/epoch_15`
2024-01-07 04:04:14 INFO     	 * Num of GPU in use: 1
2024-01-07 04:04:14 INFO     	 * Prefix: True
2024-01-07 04:04:14 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 04:04:14 INFO     load optimizer from small_recreated_ckpt/model_eszyci/optimizers/optimizer.15.pt
2024-01-07 04:04:14 INFO     optimizer is loading on cuda
2024-01-07 04:04:49 INFO     dataset preprocessing
2024-01-07 04:04:55 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_featurelmqg/qag_squad/t5-small.512.256.paragraph.questions_answers.train.qag.pkl
2024-01-07 04:05:04 INFO     start model training
2024-01-07 04:05:18 INFO     	 * (global step 50: loss: 2.3608133792877197, lr: 0.0001
2024-01-07 04:05:26 INFO     	 * (global step 100: loss: 2.503671169281006, lr: 0.0001
2024-01-07 04:05:35 INFO     	 * (global step 150: loss: 2.448932647705078, lr: 0.0001
2024-01-07 04:05:44 INFO     	 * (global step 200: loss: 2.4725905656814575, lr: 0.0001
2024-01-07 04:05:57 INFO     	 * (global step 250: loss: 2.399389863014221, lr: 0.0001
2024-01-07 04:06:05 INFO     	 * (global step 300: loss: 2.57009756565094, lr: 0.0001
2024-01-07 04:06:12 INFO     	 * (global step 350: loss: 2.427782416343689, lr: 0.0001
2024-01-07 04:06:20 INFO     	 * (global step 400: loss: 2.4925129413604736, lr: 0.0001
2024-01-07 04:06:26 INFO     	 * (global step 450: loss: 2.3547359704971313, lr: 0.0001
2024-01-07 04:06:32 INFO     	 * (global step 500: loss: 2.307287573814392, lr: 0.0001
2024-01-07 04:06:38 INFO     	 * (global step 550: loss: 2.5229785442352295, lr: 0.0001
2024-01-07 04:06:44 INFO     	 * (global step 600: loss: 2.434333562850952, lr: 0.0001
2024-01-07 04:06:50 INFO     	 * (global step 650: loss: 2.408796787261963, lr: 0.0001
2024-01-07 04:06:57 INFO     	 * (global step 700: loss: 2.3874006271362305, lr: 0.0001
2024-01-07 04:07:03 INFO     	 * (global step 750: loss: 2.4243346452713013, lr: 0.0001
2024-01-07 04:07:09 INFO     	 * (global step 800: loss: 2.4395912885665894, lr: 0.0001
2024-01-07 04:07:15 INFO     	 * (global step 850: loss: 2.360618233680725, lr: 0.0001
2024-01-07 04:07:21 INFO     	 * (global step 900: loss: 2.3885527849197388, lr: 0.0001
2024-01-07 04:07:27 INFO     	 * (global step 950: loss: 2.4169509410858154, lr: 0.0001
2024-01-07 04:07:33 INFO     	 * (global step 1000: loss: 2.278367042541504, lr: 0.0001
2024-01-07 04:07:39 INFO     	 * (global step 1050: loss: 2.346583366394043, lr: 0.0001
2024-01-07 04:07:45 INFO     	 * (global step 1100: loss: 2.46504282951355, lr: 0.0001
2024-01-07 04:07:51 INFO     	 * (global step 1150: loss: 2.338167428970337, lr: 0.0001
2024-01-07 04:07:58 INFO     	 * (global step 1200: loss: 2.2677249908447266, lr: 0.0001
2024-01-07 04:08:04 INFO     	 * (global step 1250: loss: 2.2861239910125732, lr: 0.0001
2024-01-07 04:08:10 INFO     	 * (global step 1300: loss: 2.4573217630386353, lr: 0.0001
2024-01-07 04:08:16 INFO     	 * (global step 1350: loss: 2.37470281124115, lr: 0.0001
2024-01-07 04:08:22 INFO     	 * (global step 1400: loss: 2.4270975589752197, lr: 0.0001
2024-01-07 04:08:28 INFO     	 * (global step 1450: loss: 2.4701513051986694, lr: 0.0001
2024-01-07 04:08:34 INFO     	 * (global step 1500: loss: 2.4184176921844482, lr: 0.0001
2024-01-07 04:08:40 INFO     	 * (global step 1550: loss: 2.4351913928985596, lr: 0.0001
2024-01-07 04:08:46 INFO     	 * (global step 1600: loss: 2.461790084838867, lr: 0.0001
2024-01-07 04:08:53 INFO     	 * (global step 1650: loss: 2.5751391649246216, lr: 0.0001
2024-01-07 04:09:00 INFO     	 * (global step 1700: loss: 2.5423578023910522, lr: 0.0001
2024-01-07 04:09:06 INFO     	 * (global step 1750: loss: 2.2425384521484375, lr: 0.0001
2024-01-07 04:09:12 INFO     	 * (global step 1800: loss: 2.305253028869629, lr: 0.0001
2024-01-07 04:09:18 INFO     	 * (global step 1850: loss: 2.3578635454177856, lr: 0.0001
2024-01-07 04:09:25 INFO     	 * (global step 1900: loss: 2.320308208465576, lr: 0.0001
2024-01-07 04:09:31 INFO     	 * (global step 1950: loss: 2.38824725151062, lr: 0.0001
2024-01-07 04:09:37 INFO     	 * (global step 2000: loss: 2.4239760637283325, lr: 0.0001
2024-01-07 04:09:44 INFO     	 * (global step 2050: loss: 2.3481682538986206, lr: 0.0001
2024-01-07 04:09:51 INFO     	 * (global step 2100: loss: 2.490965485572815, lr: 0.0001
2024-01-07 04:10:00 INFO     	 * (global step 2150: loss: 2.445853114128113, lr: 0.0001
2024-01-07 04:10:09 INFO     	 * (global step 2200: loss: 2.4135113954544067, lr: 0.0001
2024-01-07 04:10:17 INFO     	 * (global step 2250: loss: 2.3610992431640625, lr: 0.0001
2024-01-07 04:10:23 INFO     	 * (global step 2300: loss: 2.367194175720215, lr: 0.0001
2024-01-07 04:10:30 INFO     	 * (global step 2350: loss: 2.3601258993148804, lr: 0.0001
2024-01-07 04:10:36 INFO     	 * (global step 2400: loss: 2.225447416305542, lr: 0.0001
2024-01-07 04:10:42 INFO     	 * (global step 2450: loss: 2.3645156621932983, lr: 0.0001
2024-01-07 04:10:48 INFO     	 * (global step 2500: loss: 2.4307068586349487, lr: 0.0001
2024-01-07 04:10:54 INFO     	 * (global step 2550: loss: 2.44688618183136, lr: 0.0001
2024-01-07 04:11:00 INFO     	 * (global step 2600: loss: 2.3253625631332397, lr: 0.0001
2024-01-07 04:11:06 INFO     	 * (global step 2650: loss: 2.4199864864349365, lr: 0.0001
2024-01-07 04:11:12 INFO     	 * (global step 2700: loss: 2.494969964027405, lr: 0.0001
2024-01-07 04:11:18 INFO     	 * (global step 2750: loss: 2.480357050895691, lr: 0.0001
2024-01-07 04:11:24 INFO     	 * (global step 2800: loss: 2.3362916707992554, lr: 0.0001
2024-01-07 04:11:31 INFO     	 * (global step 2850: loss: 2.486253499984741, lr: 0.0001
2024-01-07 04:11:37 INFO     	 * (global step 2900: loss: 2.501556634902954, lr: 0.0001
2024-01-07 04:11:43 INFO     	 * (global step 2950: loss: 2.303821086883545, lr: 0.0001
2024-01-07 04:11:49 INFO     	 * (global step 3000: loss: 2.44132924079895, lr: 0.0001
2024-01-07 04:11:55 INFO     	 * (global step 3050: loss: 2.361027956008911, lr: 0.0001
2024-01-07 04:12:01 INFO     	 * (global step 3100: loss: 2.3298580646514893, lr: 0.0001
2024-01-07 04:12:07 INFO     	 * (global step 3150: loss: 2.4411091804504395, lr: 0.0001
2024-01-07 04:12:13 INFO     	 * (global step 3200: loss: 2.337065100669861, lr: 0.0001
2024-01-07 04:12:20 INFO     	 * (global step 3250: loss: 2.2696129083633423, lr: 0.0001
2024-01-07 04:12:26 INFO     	 * (global step 3300: loss: 2.3481390476226807, lr: 0.0001
2024-01-07 04:12:32 INFO     	 * (global step 3350: loss: 2.4165858030319214, lr: 0.0001
2024-01-07 04:12:38 INFO     	 * (global step 3400: loss: 2.436815619468689, lr: 0.0001
2024-01-07 04:12:44 INFO     	 * (global step 3450: loss: 2.2694507837295532, lr: 0.0001
2024-01-07 04:12:50 INFO     	 * (global step 3500: loss: 2.3984800577163696, lr: 0.0001
2024-01-07 04:12:56 INFO     	 * (global step 3550: loss: 2.5236674547195435, lr: 0.0001
2024-01-07 04:13:02 INFO     	 * (global step 3600: loss: 2.2855944633483887, lr: 0.0001
2024-01-07 04:13:08 INFO     	 * (global step 3650: loss: 2.3954750299453735, lr: 0.0001
2024-01-07 04:13:16 INFO     	 * (global step 3700: loss: 2.3372409343719482, lr: 0.0001
2024-01-07 04:13:23 INFO     	 * (global step 3750: loss: 2.3958170413970947, lr: 0.0001
2024-01-07 04:13:29 INFO     	 * (global step 3800: loss: 2.4312819242477417, lr: 0.0001
2024-01-07 04:13:35 INFO     	 * (global step 3850: loss: 2.5474249124526978, lr: 0.0001
2024-01-07 04:13:41 INFO     	 * (global step 3900: loss: 2.4647927284240723, lr: 0.0001
2024-01-07 04:13:48 INFO     	 * (global step 3950: loss: 2.3871315717697144, lr: 0.0001
2024-01-07 04:13:54 INFO     	 * (global step 4000: loss: 2.513865828514099, lr: 0.0001
2024-01-07 04:14:00 INFO     [epoch 15/16] average loss: 2.413, lr: 0.0001
2024-01-07 04:14:00 INFO     saving model related files
2024-01-07 04:14:00 INFO     saving model
2024-01-07 04:14:01 INFO     saving tokenizer
2024-01-07 04:14:02 INFO     saving optimizer
2024-01-07 04:14:02 INFO     remove old optimizer files
2024-01-07 04:14:03 INFO     complete training: model ckpt was saved at small_recreated_ckpt/model_eszyci
2024-01-07 04:14:03 INFO     ## 3rd RUN (EVAL): epoch 16 ##
2024-01-07 04:14:07 INFO     use spaCy answer extraction model: positionrank
2024-01-07 04:14:07 INFO     Model `small_recreated_ckpt/model_eszyci/epoch_16`
2024-01-07 04:14:07 INFO     	 * Num of GPU in use: 1
2024-01-07 04:14:07 INFO     	 * Prefix: True
2024-01-07 04:14:07 INFO     	 * Language: en (ignore at the training phase)
2024-01-07 04:14:08 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.test.qag.pkl
2024-01-07 04:22:24 INFO     loading preprocessed feature from /home2/g.torresgamez/.cache/lmqg/encoded_feature/lmqg/qag_squadt5-small.512.256.paragraph.questions_answers.validation.qag.pkl
2024-01-07 04:29:48 INFO     	Bleu_1: 0.37330049480675587
2024-01-07 04:29:48 INFO     	Bleu_2: 0.22718432481660442
2024-01-07 04:29:48 INFO     	Bleu_3: 0.14217653551390586
2024-01-07 04:29:48 INFO     	Bleu_4: 0.0971127361358329
2024-01-07 04:29:49 INFO     	Bleu_1: 0.33485062444290714
2024-01-07 04:29:49 INFO     	Bleu_2: 0.1990001961360951
2024-01-07 04:29:49 INFO     	Bleu_3: 0.12160683925570676
2024-01-07 04:29:49 INFO     	Bleu_4: 0.08141145016291469
2024-01-07 04:29:50 INFO     	 tmp metric: 0.0971127361358329
2024-01-07 04:29:50 INFO     	 finish 3rd phase (no improvement)
2024-01-07 04:29:50 INFO     3rd RUN RESULTS: small_recreated_ckpt/model_eszyci
2024-01-07 04:29:50 INFO     	 epoch 15: 0.0982179007514993
2024-01-07 04:29:50 INFO     	 epoch 16: 0.0971127361358329
2024-01-07 04:29:50 INFO     creating small_recreated_ckpt/best_model
2024-01-07 04:29:50 INFO     copying small_recreated_ckpt/model_eszyci/epoch_15/config.json -> small_recreated_ckpt/best_model
2024-01-07 04:29:50 INFO     copying small_recreated_ckpt/model_eszyci/epoch_15/generation_config.json -> small_recreated_ckpt/best_model
2024-01-07 04:29:50 INFO     copying small_recreated_ckpt/model_eszyci/epoch_15/pytorch_model.bin -> small_recreated_ckpt/best_model
2024-01-07 04:29:50 INFO     copying small_recreated_ckpt/model_eszyci/epoch_15/tokenizer_config.json -> small_recreated_ckpt/best_model
2024-01-07 04:29:50 INFO     copying small_recreated_ckpt/model_eszyci/epoch_15/special_tokens_map.json -> small_recreated_ckpt/best_model
2024-01-07 04:29:50 INFO     copying small_recreated_ckpt/model_eszyci/epoch_15/added_tokens.json -> small_recreated_ckpt/best_model
2024-01-07 04:29:50 INFO     copying small_recreated_ckpt/model_eszyci/epoch_15/spiece.model -> small_recreated_ckpt/best_model
2024-01-07 04:29:50 INFO     copying small_recreated_ckpt/model_eszyci/epoch_15/tokenizer.json -> small_recreated_ckpt/best_model
2024-01-07 04:29:50 INFO     copying small_recreated_ckpt/model_eszyci/epoch_15/trainer_config.json -> small_recreated_ckpt/best_model
2024-01-07 04:29:50 INFO     creating small_recreated_ckpt/best_model/eval
2024-01-07 04:29:50 INFO     copying small_recreated_ckpt/model_eszyci/epoch_15/eval/samples.test.hyp.paragraph.questions_answers.lmqg_qag_squad.default.txt -> small_recreated_ckpt/best_model/eval
2024-01-07 04:29:50 INFO     copying small_recreated_ckpt/model_eszyci/epoch_15/eval/samples.validation.hyp.paragraph.questions_answers.lmqg_qag_squad.default.txt -> small_recreated_ckpt/best_model/eval
2024-01-07 04:29:51 INFO     copying small_recreated_ckpt/model_eszyci/epoch_15/eval/metric.first.answer.paragraph.questions_answers.lmqg_qag_squad.default.json -> small_recreated_ckpt/best_model/eval
